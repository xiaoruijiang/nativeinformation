{"citeStart": 258, "citeEnd": 270, "citeStartToken": 258, "citeEndToken": 270, "sectionName": "UNKNOWN SECTION NAME", "string": "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003) . The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. The shallow parsing technique is used to detect zero anaphors and identifies the noun phrases preceding the anaphors as antecedents.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, it heavily relies upon the availability of sufficiently large text corpora that are tagged, in particular, with referential information (Stuckardt, 2002) .", "mid_sen": "Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as partof-speech (POS) tagger and shallow parsers (Baldwin, 1997; Ferrández et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003) . ", "after_sen": "The resolution process works from the output of a POS tagger enriched with annotations of grammatical function of lexical items in the input text stream. "}
{"citeStart": 170, "citeEnd": 192, "citeStartToken": 170, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. Because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm~, troublesome in actual application environments. On the other hand, the algorithm in this paper has no such requirement, it requires only a minimum of linguistic knowledge, including parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules which lexicon based syntactic theories like HPSG CC etc. normally assume. The parser is not a deterministic parser, but a parser which produces all possible analyses. All of the results are used for calculation ant the system assumes that there is a correct answer among them. The algorithm builds correct structural descriptions of sentences and discovers semantic collocations at the same time. It works as a relaxation process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains.", "mid_sen": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. ", "after_sen": "It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. "}
{"citeStart": 38, "citeEnd": 52, "citeStartToken": 38, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "® Then put the argument structures into the predicate structures. This step performs a modified syntactic analysis. In principle any parsing strategy can be applied to execute the second step, since the number of structures produced js finite and since each of them corresponds to a token in the input string, the search space is finite and termination is guaranteed. In principle, one can proceed inside out, left to right or in any other way. Of course, standard parsing algorithm can be used too. In particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion. Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist. The use of restrictors as proposed by Shieber (1985) is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist. ", "mid_sen": "The use of restrictors as proposed by Shieber (1985) is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem.", "after_sen": "By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous."}
{"citeStart": 51, "citeEnd": 93, "citeStartToken": 51, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "The goal of a speech recognizer is to find the sequence of words l~ that is maximal given the acoustic signal A. However, for detecting and correcting speech repairs, and identifying boundary tones and discourse markers, we need to augment the model so that it incorporates shallow statistical analysis, in the form of POS tagging. The POS tagset, based on the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz, 1993) , includes special tags for denoting when a word is being used as a discourse marker. In this section, we give an overview of our basic language model that incorporates POS tagging. Full details can be found in (Heeman and Allen, 1997; Heeman~ 1997) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The goal of a speech recognizer is to find the sequence of words l~ that is maximal given the acoustic signal A. However, for detecting and correcting speech repairs, and identifying boundary tones and discourse markers, we need to augment the model so that it incorporates shallow statistical analysis, in the form of POS tagging. ", "mid_sen": "The POS tagset, based on the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz, 1993) , includes special tags for denoting when a word is being used as a discourse marker. ", "after_sen": "In this section, we give an overview of our basic language model that incorporates POS tagging. "}
{"citeStart": 0, "citeEnd": 27, "citeStartToken": 0, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "Over the last 20 years, statistical language models (SLMs) have been used successfully in many tasks in natural language processing, and the data available for modeling has steadily grown (Lapata and Keller, 2005) . Langkilde and Knight (1998) first applied SLMs to statistical natural language generation (SNLG) , showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Over the last 20 years, statistical language models (SLMs) have been used successfully in many tasks in natural language processing, and the data available for modeling has steadily grown (Lapata and Keller, 2005) . ", "mid_sen": "Langkilde and Knight (1998) first applied SLMs to statistical natural language generation (SNLG) , showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. ", "after_sen": "Since then, research in SNLG has explored a range of models for both dialogue and text generation."}
{"citeStart": 70, "citeEnd": 91, "citeStartToken": 70, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Here, ψ is a potential function representing the compatibility between the label y, the hidden variable m, and the observations x; this potential is parameterized by a vector of weights, w. The numerator expresses the compatibility of the label y and observations x, summed over all possible values of the hidden variable m. The denominator sums over both m and all possible labels y , yielding the conditional probability p(y|x; w). The use of hidden variables in a conditionally-trained model follows (Quattoni et al., 2004) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The denominator sums over both m and all possible labels y , yielding the conditional probability p(y|x; w). ", "mid_sen": "The use of hidden variables in a conditionally-trained model follows (Quattoni et al., 2004) .", "after_sen": "This model can be trained by a gradient-based optimization to maximize the conditional loglikelihood of the observations. "}
{"citeStart": 85, "citeEnd": 101, "citeStartToken": 85, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "• PDTB-Rel: For discourse relation extraction, we use \"PDTB-Styled End-to-End Discourse Parser\" (Lin et al., 2010) to extract discourse level relations as baseline. Since it is a general discourse relations identification algorithms, \"Cause\", \"Pragmatic Cause\", \"Instantiation\", and \"Restatement\" relation types are treated as explanatory relation in this work.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• PDTB-Rel: ", "mid_sen": "For discourse relation extraction, we use \"PDTB-Styled End-to-End Discourse Parser\" (Lin et al., 2010) to extract discourse level relations as baseline. ", "after_sen": "Since it is a general discourse relations identification algorithms, \"Cause\", \"Pragmatic Cause\", \"Instantiation\", and \"Restatement\" relation types are treated as explanatory relation in this work."}
{"citeStart": 212, "citeEnd": 223, "citeStartToken": 212, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus, in order to predict the most likely word in a given context, a global estimation of the sentence probability is derived which, in turn, is computed by estimating the probability of each word given its local context or history. Estimating terms of the form Pr(wlh ) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994) . It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996) , and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999) . Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (Chelba and Jelinek, 1998; Rosenfeld, 1996) . We believe that the main reason for that is that incorporating information sources in NLP needs to be coupled with a learning approach that is suitable for it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Estimating terms of the form Pr(wlh ) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). ", "mid_sen": "Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994) . ", "after_sen": "It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn better classifiers and language models. "}
{"citeStart": 99, "citeEnd": 112, "citeStartToken": 99, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "Hadar Shemtov and Ron Kaplan at Xerox PARC provided us with two LFG parsed corpora called the Verbmobil corpus and the Homecentre corpus. These contain parse forests for each sentence (packed according to scheme described in Maxwell and Kaplan (1995) ), together with a manual annotation as to which parse is correct. The Verbmobil corpus contains 540 sentences relating to appointment planning, while the Homecentre corpus contains 980 sentences from Xerox documentation on their \"homecentre\" multifunction devices. Xerox did not provide us with the base LFGs for intellectual property reasons, but from inspection of the parses it seems that slightly different grammars were used with each corpus, so we did not merge the corpora. We chose the features of our SLFG based solely on the basis of the Verbmobil corpus, so the Homecentre corpus can be regarded as a held-out evaluation corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Hadar Shemtov and Ron Kaplan at Xerox PARC provided us with two LFG parsed corpora called the Verbmobil corpus and the Homecentre corpus. ", "mid_sen": "These contain parse forests for each sentence (packed according to scheme described in Maxwell and Kaplan (1995) ), together with a manual annotation as to which parse is correct. ", "after_sen": "The Verbmobil corpus contains 540 sentences relating to appointment planning, while the Homecentre corpus contains 980 sentences from Xerox documentation on their \"homecentre\" multifunction devices. "}
{"citeStart": 154, "citeEnd": 188, "citeStartToken": 154, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by Garside, Leech, and Sampson (1987) , on tagging the LOB corpus, and Church (1988) , on assigning part-of-speech labels and parsing noun phrases. Success rates ranging between 95-99% are reported, depending on how 'success' is defined. These approaches are probabilistic and based on transitional probabilities calculated from extensive pretagged corpora.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It shows the descriptive power of low-level morphology-based constraints.", "mid_sen": "The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by Garside, Leech, and Sampson (1987) , on tagging the LOB corpus, and Church (1988) , on assigning part-of-speech labels and parsing noun phrases. ", "after_sen": "Success rates ranging between 95-99% are reported, depending on how 'success' is defined. "}
{"citeStart": 84, "citeEnd": 103, "citeStartToken": 84, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "absolute. As in the baseline system, a combination of structures performs best. As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. Here, the PET and GR kernel perform similar: this is different from the results of (Nguyen et al., 2009) where GR performed much worse than PET for ACE data. This exemplifies the difference in the nature of our event annotations from that of ACE relations. Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger. This means that implicit feature space is much sparser and thus not the best representation. Table 4 : Over-sampled System with transformation for relation detection. The proportion of positive examples in the training and test corpus is 51.7% and 20.6% respectively. Table 4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are \"close\" to the original examples. This method achieves a gain 16.78% over the baseline system. We expected this system to perform better than the over-sampled system but it does not. This suggests that our over-sampled system is not over-fitting; a concern with using oversampling techniques.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As in the undersampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall. ", "mid_sen": "Here, the PET and GR kernel perform similar: this is different from the results of (Nguyen et al., 2009) where GR performed much worse than PET for ACE data. ", "after_sen": "This exemplifies the difference in the nature of our event annotations from that of ACE relations. "}
{"citeStart": 177, "citeEnd": 186, "citeStartToken": 177, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "Sidner's algorithmic account, although not exhaustively specified, has lead to the implementation of focus-based approaches to anaphora resolution in several systems, e.g. PIE (Lin, 1995) . However, evaluation of the approach has mainly consisted of manual analyses of small sets of problematic cases mentioned in the literature. Precise evaluation over sizable corpora of real-world texts has only recently become possible, through the resources provided as part of the MUC evaluations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Sidner's algorithmic account, although not exhaustively specified, has lead to the implementation of focus-based approaches to anaphora resolution in several systems, e.g. PIE (Lin, 1995) . ", "after_sen": "However, evaluation of the approach has mainly consisted of manual analyses of small sets of problematic cases mentioned in the literature. "}
{"citeStart": 142, "citeEnd": 165, "citeStartToken": 142, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. Because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm~, troublesome in actual application environments. On the other hand, the algorithm in this paper has no such requirement, it requires only a minimum of linguistic knowledge, including parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules which lexicon based syntactic theories like HPSG CC etc. normally assume. The parser is not a deterministic parser, but a parser which produces all possible analyses. All of the results are used for calculation ant the system assumes that there is a correct answer among them. The algorithm builds correct structural descriptions of sentences and discovers semantic collocations at the same time. It works as a relaxation process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains.", "mid_sen": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. ", "after_sen": "It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. "}
{"citeStart": 159, "citeEnd": 170, "citeStartToken": 159, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a) , which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b) ). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We favor the PLTIG representation for two reasons. ", "mid_sen": "First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b) ). ", "after_sen": "Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities."}
{"citeStart": 309, "citeEnd": 327, "citeStartToken": 309, "citeEndToken": 327, "sectionName": "UNKNOWN SECTION NAME", "string": "Thougll a 50\"/,, precision and recall might be reasonable for human assisted tasks, like in lexicography, supervised translation, etc., it is not \"fair enough\" if collocational analysis must serve a fully automated system. In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. ", "mid_sen": "Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "after_sen": "In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a \"large enough\" number of words. "}
{"citeStart": 94, "citeEnd": 98, "citeStartToken": 94, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995; 1998] ), which makes use of POS and NP chunking, it tries to individuate pleonastic \"it\" occurrences, and assigns animacy. The weighting algorithm seems to contain the most original approach. It is organized with a filtering approach by a series of indicators that are used to boost or reduce the score for antecedenthood to a given NP. The indicators are the following ones: FNP (First NP); INDEF (Indefinite NP); IV (Indicating Verbs); REI (Lexical Reiteration); SH (Section Heading Preference); CM (Collocation Match); PNP (Prepositional Noun Phrases); IR (Immediate Reference); SI (Sequential Instructions); RD (Referential Distance); TP (Term Preference), As the author comments, antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the antecedent. The authors comment is that antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, \"non-prepositional\" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. However it is clear that most of the indicators have been suggested for lack of better information, in particular no syntactic constituency was available. In a more recent paper (Mitkov et al., 2003) MARS has been fully reimplemented and the indicators updated. The authors seem to acknowledge the fact that anaphora resolution is a much more difficult task than previous work had suggested, In unrestricted text analysis, the tasks involved in the anaphora resolution process contribute a lot of uncertainty and errors that may be the cause for low performance measures. The actual algorithm uses the output of Connexor's FDG Parser, filters instances of \"it\" and eliminates pleonastic cases, then produces a list of potential antecedents by extracting nominal and pronominal heads from NPs preceding the pronoun. Constraints are then applied to this list in order to produce the \"set of competing candidates\" to be considered further, i.e. those candidates that agree in number and gender with the pronoun, and also obey syntactic constraints. They also introduced the use of Genetic Algorithms in the evaluation phase.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995; 1998] ), which makes use of POS and NP chunking, it tries to individuate pleonastic \"it\" occurrences, and assigns animacy. ", "after_sen": "The weighting algorithm seems to contain the most original approach. "}
{"citeStart": 212, "citeEnd": 230, "citeStartToken": 212, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "mid_sen": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . ", "after_sen": "Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . "}
{"citeStart": 150, "citeEnd": 173, "citeStartToken": 150, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper presents a method for automatic induction of semantically annotated subcategorization frames from unannotated corpora. We use a statistical subcat-induction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame (Carroll and Rooth, 1998) . The statistical parser can also collect frequencies for the nominal fillers of slots in a subcat frame. The induction of labels for slots in a frame is based upon estimation of a probability distribution over tuples consisting of a class label, a selecting head, a grammatical relation, and a filler head. The class label is treated as hidden data in the EMframework for statistical estimation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper presents a method for automatic induction of semantically annotated subcategorization frames from unannotated corpora. ", "mid_sen": "We use a statistical subcat-induction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame (Carroll and Rooth, 1998) . ", "after_sen": "The statistical parser can also collect frequencies for the nominal fillers of slots in a subcat frame. "}
{"citeStart": 38, "citeEnd": 56, "citeStartToken": 38, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "Modeling the sentiment of a word has been a popular approach in sentiment analysis. There are many publicly available lexicon resources. The size, format, specificity, and reliability differ in all these lexicons. For example, lexicon sizes range from a few hundred to several hundred thousand. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) . There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) . ", "mid_sen": "There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006) .", "after_sen": "The goal of this paper is not to create or choose an appropriate sentiment lexicon, but rather it is to discover useful term features other than the sentiment properties. "}
{"citeStart": 213, "citeEnd": 235, "citeStartToken": 213, "citeEndToken": 235, "sectionName": "UNKNOWN SECTION NAME", "string": "Ideally, we would like to match structured representations derived from the question with those derived from MEDLINE citations (taking into consideration other EBMrelevant factors). However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. As an alternative, we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail-this is the standard pipeline architecture commonly employed in other question-answering systems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, we do not have access to the computational resources necessary to apply knowledge extractors to the 15 million plus citations in the MEDLINE database and directly index their results. ", "mid_sen": "As an alternative, we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail-this is the standard pipeline architecture commonly employed in other question-answering systems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001) .", "after_sen": "The architecture of our system is shown in Figure 1 . "}
{"citeStart": 478, "citeEnd": 491, "citeStartToken": 478, "citeEndToken": 491, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ( [Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "• Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996) , for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra struc-tural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967) . One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979) , which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992) . Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, it depends on the amount of supervision provided. ", "mid_sen": "Charniak (1996) , for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. ", "after_sen": "On the other hand, if the examples consist of raw sentences with no extra struc-tural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967) . "}
{"citeStart": 195, "citeEnd": 219, "citeStartToken": 195, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "The dialogue manager must keep track of the current state of the dialogue, determine the effects of observed conversation acts, generate utterances back, and send commands to the domain plan reasoner and domain plan executor when appropriate. Conversational action is represented using the theory of Conversation Acts [Traum and Hinkelman, 1992] which augments traditional Core Speech Acts with levels of acts for turn-taking, grounding [Clark and Schaefer, 1989] , and argumentation. Each utterance will generally contain acts (or partial acts) at each of these levels.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The dialogue manager must keep track of the current state of the dialogue, determine the effects of observed conversation acts, generate utterances back, and send commands to the domain plan reasoner and domain plan executor when appropriate. ", "mid_sen": "Conversational action is represented using the theory of Conversation Acts [Traum and Hinkelman, 1992] which augments traditional Core Speech Acts with levels of acts for turn-taking, grounding [Clark and Schaefer, 1989] , and argumentation. ", "after_sen": "Each utterance will generally contain acts (or partial acts) at each of these levels."}
{"citeStart": 57, "citeEnd": 82, "citeStartToken": 57, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990 Alshawi ( , 1992 , and implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990 Alshawi ( , 1992 , and implemented in SRI's Core Language Engine (CLE). ", "mid_sen": "In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. ", "after_sen": "Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context."}
{"citeStart": 122, "citeEnd": 136, "citeStartToken": 122, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003) , Charniak (2000) and Ratnaparkhi (1999) . Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has repeatedly been pointed out that N-grams model natural language only superficially: an Nthorder Markov chain is a very crude model of the complex dependencies between words in an utterance. ", "mid_sen": "More accurate statistical models of natural language have mainly been developed in the field of statistical parsing, e.g. Collins (2003) , Charniak (2000) and Ratnaparkhi (1999) . ", "after_sen": "Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition."}
{"citeStart": 116, "citeEnd": 135, "citeStartToken": 116, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "• In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model -HTMM which has different assumption from PLSA and LDA;", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora, which are far from full exploitation in the parallel data collection and mixture modeling framework.", "mid_sen": "• In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model -HTMM which has different assumption from PLSA and LDA;", "after_sen": "(2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling."}
{"citeStart": 138, "citeEnd": 149, "citeStartToken": 138, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "of feature fi is the number of times the ith production is used in the derivation w. This set of features induces a tree-structured dependency graph on the productions which is characteristic of Markov Branching Processes (Pearl, 1988; Frey, 1998) . This tree structure has the important consequence that simple \"relative-frequencies\" yield maximumlikelihood estimates for the Oi.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "of feature fi is the number of times the ith production is used in the derivation w. ", "mid_sen": "This set of features induces a tree-structured dependency graph on the productions which is characteristic of Markov Branching Processes (Pearl, 1988; Frey, 1998) . ", "after_sen": "This tree structure has the important consequence that simple \"relative-frequencies\" yield maximumlikelihood estimates for the Oi."}
{"citeStart": 66, "citeEnd": 78, "citeStartToken": 66, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "R98(m,s,) uses a variant of Kozima's semantic similarity measure (Kozima, 1993) to compute block similarity. Word similarity is a function of word cooccurrence statistics in the given document. Words that belong to the same sentence are considered to be related. Given the co-occurrence frequencies f(wi, wj), the transition probability matrix t is computed by equation 10. Equation 11 defines our spread activation scheme, s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix, x = 5 was used in the experiment. y(w ,wj) (10) t ,j = p(wj Iw ) = Ej s=norm(~t')i=l", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "6-8 9-11 H94(c,~t) 46% 44% 43% 48% H94(c,~) 46% 44% 44% 49% H94(~u. ) 54% 45% 52% 53% H94(~,a/ 0.67s 0.52s 0.66s 0.88s H94(c,~) 0.68s 0.52s 0.67s 0.92s H94(j,~) 3.77s 2.21s 3.69s 5.07s R98(m,dot) is the modularised version of R98 for experimenting with different similarity measures.", "mid_sen": "R98(m,s,) uses a variant of Kozima's semantic similarity measure (Kozima, 1993) to compute block similarity. ", "after_sen": "Word similarity is a function of word cooccurrence statistics in the given document. "}
{"citeStart": 95, "citeEnd": 128, "citeStartToken": 95, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "tences. It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996) . Unlike Erbach (1990) , however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure. Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "tences. ", "mid_sen": "It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996) . ", "after_sen": "Unlike Erbach (1990) , however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure. "}
{"citeStart": 13, "citeEnd": 32, "citeStartToken": 13, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "However, different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997) , on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal communication), this is fine for information retrieval. Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, different sets of GRs are useful for different purposes. ", "mid_sen": "For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. ", "after_sen": "The SPARKLE project (Carroll et al., 1997) , on the other hand, does not differentiate between these types of modifiers. "}
{"citeStart": 107, "citeEnd": 132, "citeStartToken": 107, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "This example represents a general phenomenon: many expressions allow intervening noun phrases and/or modifying terms. For example: \"stepped on <mods> toes\" Ex: stepped on the boss' toes \"dealt <np> <mods> blow\" Ex: dealt the company a decisive blow \"brought <np> to <mods> knees\" Ex: brought the man to his knees (Riloff and Wiebe, 2003) also showed that syntactic variations of the same verb phrase can be-have very differently. For example, they found that passive-voice constructions of the verb \"ask\" had a 100% correlation with opinion sentences, but active-voice constructions had only a 63% correlation with opinions. Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features. We used three types of features in our research: unigrams, bigrams, and IE patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003). 1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004) . AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. Au-toSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996) , as well as for opinion analysis (Riloff and Wiebe, 2003) . Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to infinitive VPs, and AuxVP refers to VPs where the main verb is a form of \"to be\" or \"to have\". Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be extracted by the patterns. 2", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003). ", "mid_sen": "1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004) . ", "after_sen": "AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. "}
{"citeStart": 92, "citeEnd": 103, "citeStartToken": 92, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and subjectivity clues listed in (Wiebe, 1990) . Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al., 2003) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Any data used to develop this vocabulary does not overlap with the test sets or the unannotated data used in this paper.", "mid_sen": "Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and subjectivity clues listed in (Wiebe, 1990) . ", "after_sen": "Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al., 2003) ."}
{"citeStart": 41, "citeEnd": 60, "citeStartToken": 41, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "Our baseline SMT system is the system of Quirk et al. (2005) . It translates by first deriving a dependency tree for the source sentence and then translating the source dependency tree to a target dependency tree, using a set of probabilistic models. The translation is based on treelet pairs. A treelet is a connected subgraph of the source or target dependency tree. A treelet translation pair is a pair of word-aligned source and target treelets. The baseline SMT model combines this treelet translation model with other feature functions -a target language model, a tree order model, lexical weighting features to smooth the translation probabilities, word count feature, and treelet-pairs count feature. These models are combined as feature functions in a (log)linear model for predicting a target sentence given a source sentence, in the framework proposed by (Och and Ney, 2002) . The weights of this model are trained to maximize BLEU (Och and Ney, 2004 ). The SMT system is trained using the same form of data as our order model: parallel source and target dependency trees as in Figure 2 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our baseline SMT system is the system of Quirk et al. (2005) . ", "after_sen": "It translates by first deriving a dependency tree for the source sentence and then translating the source dependency tree to a target dependency tree, using a set of probabilistic models. "}
{"citeStart": 82, "citeEnd": 95, "citeStartToken": 82, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002) 's NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999) , we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%. Teufel (1999) introduced a new rhetorical analysis for scientific texts called Argumentative Zoning. Each sentence of an article from the scientific literature is classified into one of seven basic rhetorical structures shown in Table 1 . The first three: Background, Other, and Own, are part of the basic schema and represent attribution of intellectual ownership. The four additional categories: aim, textual, contrast, and basis, are based upon Swales (1990) 's Creating A Research Space (CARS) model, and provide pointed information about the author's stance and the paper itself. Teufel assumes that each sentence only requires a single classification and that all sentences clearly fit into the above structure. The assumption is clearly not always correct, but is a useful approximation nevertheless.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first three: Background, Other, and Own, are part of the basic schema and represent attribution of intellectual ownership. ", "mid_sen": "The four additional categories: aim, textual, contrast, and basis, are based upon Swales (1990) 's Creating A Research Space (CARS) model, and provide pointed information about the author's stance and the paper itself. ", "after_sen": "Teufel assumes that each sentence only requires a single classification and that all sentences clearly fit into the above structure. "}
{"citeStart": 46, "citeEnd": 58, "citeStartToken": 46, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993) . They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992) . Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They require a relatively large tagged training text. ", "mid_sen": "Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. ", "after_sen": "No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992) . "}
{"citeStart": 0, "citeEnd": 35, "citeStartToken": 0, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "NP chunking is the task of marking the boundaries of simple noun-phrases in text. It is a well studied problem in English, and was the focus of CoNLL2000's Shared Task (Sang and Buchholz, 2000) . Early attempts at NP Chunking were rule learning systems, such as the Error Driven Pruning method of Pierce and Cardie (1998) . Following Ramshaw and Marcus (1995) , the current dominant approach is formulating chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O)outside of a chunk. Features for this classification usually involve local context features. Kudo and Matsumoto (2000) used SVM as a classification engine and achieved an F-Score of 93.79 on the shared task NPs. Since SVM is a binary classifier, to use it for the 3-class classification of the chunking task, 3 different classifiers {B/I, B/O, I/O} were trained and their majority vote was taken.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Early attempts at NP Chunking were rule learning systems, such as the Error Driven Pruning method of Pierce and Cardie (1998) . ", "mid_sen": "Following Ramshaw and Marcus (1995) , the current dominant approach is formulating chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O)outside of a chunk. ", "after_sen": "Features for this classification usually involve local context features. "}
{"citeStart": 61, "citeEnd": 87, "citeStartToken": 61, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983) , Granger (1983 ), Jensen et al. (1983 , Kwasny and Sondheimer (1981) , Riesbeek and Schank (1976) , Thompson (1980) , Weischedel and Black (1980) , and Weischedel and Sondheimer (1983) . A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. However, these methodologies have not used historical information at the dialogue level as described here. In most cases, the goal of these systems is to characterize the ill-formed input into classes of errors and to correct on that basis. The work described here makes no attempt to classify the errors, but treats them as random events that occur at any point in a sentence. Thus, an error in this work has no pattern but occurs probabilistically. A verb is just as likely to be mis-recognized or not recognized as is a noun, adjective, determiner, etc. Figure 16 . Average word and sentence error rate in percent, average speaking rate in words-spoken-per-minute.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983) .", "mid_sen": "The problem of handling ill-formed input has been studied by Carbonell and Hayes (1983) , Granger (1983 ), Jensen et al. (1983 , Kwasny and Sondheimer (1981) , Riesbeek and Schank (1976) , Thompson (1980) , Weischedel and Black (1980) , and Weischedel and Sondheimer (1983) . ", "after_sen": "A wide variety of techniques have been developed for addressing problems at the word, phrase, sentence, and in some cases, dialogue level. "}
{"citeStart": 26, "citeEnd": 49, "citeStartToken": 26, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "As for the other antecedents of Commit in Table 3 , it is not surprising that only 4 Open-Options occur given the circumstances in which this tag is used (see Figure 1 ). These Open-Options appear to function as tentative proposals like indeterminate AD+ Os, as the dialogue between the Open-Option and the Commit develops according to hypothesis 1.2. We were instead surprised that AD+Cs are a very common category among the antecedents of Commit (20%); the second commit appears to simply reconfirm the commitment expressed by the first (Walker, 1993; Walker, 1996) , and does not appear to count as a proposal. Finally, the Other column is a collection of miscellaneous antecedents, such as Info-Requests and cases where the antecedent is unclear, that need further analysis. For further details, see (Di Eugenio et al., 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally, the Other column is a collection of miscellaneous antecedents, such as Info-Requests and cases where the antecedent is unclear, that need further analysis. ", "mid_sen": "For further details, see (Di Eugenio et al., 1998) .", "after_sen": ""}
{"citeStart": 71, "citeEnd": 84, "citeStartToken": 71, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "The results we obtained from the above procedure are quite clean, in the sense that most of the pairs that are classified into the two types of relations with a probability greater than 50% are correct. Here are some sample pairs that we learned. have -effect … These are truly verb-object relations, but we may not want to keep them in our knowledge base for the following reasons. First of all, the verbs in such cases usually can take a wide range of objects and the strength of association between the verb and the object is weak. In other words, the objects are not \"typical\". Secondly, those verbs tend not to occur in the modifier-head relation with a following noun and we gain very little in terms of disambiguation by storing those pairs in the knowledge base. To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair. Pairs where there is high \"mutual information\" between the verb and noun would receive higher scores while pairs where the verb can co-occur with many different nouns would receive lower scores. Pairs with association scores below a certain threshold were then thrown out. This not only makes the remaining pairs more \"typical\" but helps to clean out more garbage. The resulting knowledge base therefore has higher quality.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Secondly, those verbs tend not to occur in the modifier-head relation with a following noun and we gain very little in terms of disambiguation by storing those pairs in the knowledge base. ", "mid_sen": "To prune away those pairs, we used the log-likelihood-ratio algorithm (Dunning, 1993) to compute the degree of association between the verb and the noun in each pair. ", "after_sen": "Pairs where there is high \"mutual information\" between the verb and noun would receive higher scores while pairs where the verb can co-occur with many different nouns would receive lower scores. "}
{"citeStart": 139, "citeEnd": 159, "citeStartToken": 139, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "We therefore rank sentences according to the classifier confidence score (probability) with which they were assigned a CoreSC category in (Liakata et al., 2012) . The intuition behind this is that sentences with high classifier confidence will be less noisy, high precision cases and more representative of a particular category. Indeed, (Liakata et al., 2012) report statistical significance for the correlation be-tween high classifier confidence and agreement between manual and automatic classification However, as mentioned in section 3.1, there is inter-dependence between sentences in the text, which is in turn inherited by the categories assigned to them. For example, the highest ranking MET sentence will be related to an Experiment (EXP) or Background (BAC) sentence, which may not be the ones with the highest confidence score in their category.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There isn't much point, for example, in identifying that we need to include a Method sentence (MET) and that this should be followed by an Experiment sentence (EXP), if we are not sure that those are indeed the categories of the sentences we are about to select.", "mid_sen": "We therefore rank sentences according to the classifier confidence score (probability) with which they were assigned a CoreSC category in (Liakata et al., 2012) . ", "after_sen": "The intuition behind this is that sentences with high classifier confidence will be less noisy, high precision cases and more representative of a particular category. "}
{"citeStart": 142, "citeEnd": 166, "citeStartToken": 142, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "As a baseline, we adopt the entity grid . This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu (2006) . The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. The internal syntax of the various referring expressions is ignored. Since it also uses the \"same head\" coreference heuristic, it also disregards pronouns.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a baseline, we adopt the entity grid . ", "mid_sen": "This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu (2006) . ", "after_sen": "The entity grid represents each entity by tracking the syntactic roles in which it appears throughout the document. "}
{"citeStart": 103, "citeEnd": 113, "citeStartToken": 103, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus the present system is unlike SMT (Och and Ney, 2003) , where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002) , which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus the present system is unlike SMT (Och and Ney, 2003) , where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. ", "mid_sen": "It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002) , which allows a one-to-one correspondence irrespective of the context. ", "after_sen": "Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent."}
{"citeStart": 56, "citeEnd": 76, "citeStartToken": 56, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "\"The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains\"", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also extrapolate the acronym for such phrases, e.g., in the example below, SCL would also be checked along with Structural Correspondence Learning.", "mid_sen": "\"The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains\"", "after_sen": "We also add n-grams of length 1 to 3 to this lexical feature set and compare the results obtained with an n-gram only baseline in Table 2 . N-grams have been shown to perform consistently well in various NLP tasks (Bergsma et al., 2010) ."}
{"citeStart": 57, "citeEnd": 95, "citeStartToken": 57, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992) . Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994 ) their relevance to computational grammars is apparent. On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5) , it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992) . ", "mid_sen": "Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994 ) their relevance to computational grammars is apparent. ", "after_sen": "On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5) , it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case."}
{"citeStart": 155, "citeEnd": 168, "citeStartToken": 155, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "The paper thus sheds light on two questions. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . J This result has been criticised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & BrSker, 1997) , and our use of a context-free backbone with further constraints imposed by dependency relations further supports the view that DG is not a notational ~riant of context-free grammar. The second question addressed is that of efficient processing of discontinuous DGs. By converting a native DG grammar into LFG rules, we are able to profit from the state of the art in context-free parsing technology. A context-free base (or skeleton) has often been cited as a prerequisite for practical applicability of a natural language grammar (Erbach & Uszkoreit, 1990 ), and we here show that a DG can meet this criterion with ease.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The paper thus sheds light on two questions. ", "mid_sen": "A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . J This result has been criticised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & BrSker, 1997) , and our use of a context-free backbone with further constraints imposed by dependency relations further supports the view that DG is not a notational ~riant of context-free grammar. ", "after_sen": "The second question addressed is that of efficient processing of discontinuous DGs. "}
{"citeStart": 55, "citeEnd": 67, "citeStartToken": 55, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "This type of learners constructs a representation for document vectors belonging to a certain class during the learning phase, e.g. decision trees, decision rules or probability weightings. During the categorization phase, the representation is used to assign the appropriate class to a new document vector. Several pruning or specialization heuristics can be used to control the amount of generalization. We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. Support Vector Machines (SVMs): SVMs are described in (Vapnik, 1995) . SVMs are binary learners in that they distinguish positive and negative examples for each class. Like eager learners, they construct a representation during the learning phase, namely a hyper plane supported by vectors of positive and negative examples. For each class, a categorizer is built by computing such a hyper plane. During the categorization phase, each categorizer is applied to the new document vector, yielding the probabilities of the document belonging to a class. The probability increases with the distance of thevector from the hyper plane. A document is said to belong to the class with the highest probability. We chose SVM_Light (Joachims, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. ", "mid_sen": "Support Vector Machines (SVMs): SVMs are described in (Vapnik, 1995) . SVMs are binary learners in that they distinguish positive and negative examples for each class. ", "after_sen": "Like eager learners, they construct a representation during the learning phase, namely a hyper plane supported by vectors of positive and negative examples. "}
{"citeStart": 125, "citeEnd": 137, "citeStartToken": 125, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005) . However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008 ; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) . It is in this area that our paper makes its contribution.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Developing models of the meanings of words and phrases is a key challenge for computational linguistics. ", "mid_sen": "Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005) . ", "after_sen": "However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008 ; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) . "}
{"citeStart": 91, "citeEnd": 111, "citeStartToken": 91, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "A probability of derivation tree D = { α i , η αj , r i } is generally defined as follows (Schabes et al., 1988; Chiang, 2000) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "where α i is an elementary tree, η αi represents a node in α j where substitution/adjunction has occurred, and r i is a label of the applied rule, i.e., adjunction or substitution.", "mid_sen": "A probability of derivation tree D = { α i , η αj , r i } is generally defined as follows (Schabes et al., 1988; Chiang, 2000) .", "after_sen": "p(D) = i p(α i |η αj , r i )"}
{"citeStart": 81, "citeEnd": 99, "citeStartToken": 81, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "Generally, the MCST-SVM is competitive against all the classifiers presented in (Pang and Lee, 2005) , and in some cases significantly outperforms these methods. Specifically, the hierar- chical classifier outperforms OVA+PSP by 7% in the three-class case for Author A (statistically significant), while in the four-class case the MCST-SVM outperforms the best competing methods by 7.72%, 3.89% and 4.98% for Authors A, B, and C respectively (statistically significant). The small improvement of 0.87% for Author D indicates that our approach has the most impact for reviews that contain a relatively large amount of subjective text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Generally, the MCST-SVM is competitive against all the classifiers presented in (Pang and Lee, 2005) , and in some cases significantly outperforms these methods. ", "after_sen": "Specifically, the hierar- chical classifier outperforms OVA+PSP by 7% in the three-class case for Author A (statistically significant), while in the four-class case the MCST-SVM outperforms the best competing methods by 7.72%, 3.89% and 4.98% for Authors A, B, and C respectively (statistically significant). "}
{"citeStart": 339, "citeEnd": 363, "citeStartToken": 339, "citeEndToken": 363, "sectionName": "UNKNOWN SECTION NAME", "string": "Some examples of English words and the top three ranking candidates among all of the potential target-language candidates were given in Tables 4,  5 To evaluate the proposed transliteration methods quantitatively, the Mean Reciprocal Rank (MRR), a measure commonly used in information retrieval when there is precisely one correct answer (Kandor and Vorhees, 2000) was measured, following Tao and Zhai (2005) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The candidate with the highest node activation score was selected as the transliteration of the given English name.", "mid_sen": "Some examples of English words and the top three ranking candidates among all of the potential target-language candidates were given in Tables 4,  5 To evaluate the proposed transliteration methods quantitatively, the Mean Reciprocal Rank (MRR), a measure commonly used in information retrieval when there is precisely one correct answer (Kandor and Vorhees, 2000) was measured, following Tao and Zhai (2005) .", "after_sen": "Since the evaluation data obtained from the comparable corpus was small, the systems were evaluated using both held-out data from the transliteration dictionary and comparable corpus."}
{"citeStart": 71, "citeEnd": 89, "citeStartToken": 71, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The previous section shows that deriving CCG rules from unary combinators allows us to derive the Drules while preserving Eisner NF. In this section, we present an alternate formulation of Eisner NF with Baldridge's (2002) CTL basis for CCG. This formulation allows us to derive the D-rules as before, and does so in a way that seamlessly integrates with Baldridge's system of modalized functors.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The previous section shows that deriving CCG rules from unary combinators allows us to derive the Drules while preserving Eisner NF. ", "mid_sen": "In this section, we present an alternate formulation of Eisner NF with Baldridge's (2002) CTL basis for CCG. ", "after_sen": "This formulation allows us to derive the D-rules as before, and does so in a way that seamlessly integrates with Baldridge's system of modalized functors."}
{"citeStart": 97, "citeEnd": 108, "citeStartToken": 97, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995) . The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical rules have not gone unchallenged as a mechanism for expressing generalizations over lexical information. ", "mid_sen": "In a number of proposals, lexical generalizations are captured using lexical underspecification (Kathol 1994; Krieger and Nerbonne 1992; Riehemann 1993; Oliva 1994; Frank 1994; Opalka 1995; Sanfilippo 1995) . ", "after_sen": "The lexical entries are only partially specified, and various specializations are encoded via the type hierarchy, definite clause attachments, or a macro hierarchy."}
{"citeStart": 178, "citeEnd": 196, "citeStartToken": 178, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "In translation model training we used the Chinese-English bilingual corpora relevant to GALE available through the LDC 1 . After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. Our preprocessing steps include tokenization on the English side and for Chinese: automatic word segmentation using the revised version of the Stanford Chinese Word Segmenter 2 (Tseng et al., 2005 ) from 2007, replacement of traditional by simplified Chinese characters and 2-byte to 1-byte ASCII character normalization. After data cleaning steps like e.g. removal of sentence pairs with very unbalanced sen-tence length etc., we used the remaining 10 million sentences with 260 million words (English) in translation model training (260M system).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. ", "mid_sen": "Our preprocessing steps include tokenization on the English side and for Chinese: automatic word segmentation using the revised version of the Stanford Chinese Word Segmenter 2 (Tseng et al., 2005 ) from 2007, replacement of traditional by simplified Chinese characters and 2-byte to 1-byte ASCII character normalization. ", "after_sen": "After data cleaning steps like e.g. removal of sentence pairs with very unbalanced sen-tence length etc., we used the remaining 10 million sentences with 260 million words (English) in translation model training (260M system)."}
{"citeStart": 108, "citeEnd": 121, "citeStartToken": 108, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Since semantic classes are often domain-specific, their automatic acquisition is not trivial. Such classes can be derived either by distributional means or from existing taxonomies, knowledge bases, dictionaries, thesauruses, and so on. A prime example of the latter is WordNet which has been used to 1The author is currently at Texas Instruments and all inquiries should be addressed to rajeev@csc.ti.com. provide such semantic classes (Resnik, 1993; Basili et al., 1994) to assist in text understanding. Our efforts to obtain such semantic clusters with limited human intervention have been described elsewhere (Agarwal, 1995) . This paper concentrates on the aspect of evahiating the obtained clusters against classes provided by human experts.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "provide such semantic classes (Resnik, 1993; Basili et al., 1994) to assist in text understanding. ", "mid_sen": "Our efforts to obtain such semantic clusters with limited human intervention have been described elsewhere (Agarwal, 1995) . ", "after_sen": "This paper concentrates on the aspect of evahiating the obtained clusters against classes provided by human experts."}
{"citeStart": 123, "citeEnd": 150, "citeStartToken": 123, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "The net that gave best results was a simple single layer net (Figure 3 ), derived from Wyard and Nightingale's Hodyne net (Wyard and Nightingale, 1990 ). This is conventionally a \"single layer\" net, since there is one layer of processing nodes. Multi-layer networks, which can process linearly inseparable data, were also investigated, but are not necessary for this particular processing task. The linear separability of data is related to its order, and this system uses higher order pairs and triples as input. The question of appropriate network architecture is examined in (Pao, 1989; Widrow and Lehr, 1992; Lyon, 1994 ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The net that gave best results was a simple single layer net (Figure 3 ), derived from Wyard and Nightingale's Hodyne net (Wyard and Nightingale, 1990 ). ", "after_sen": "This is conventionally a \"single layer\" net, since there is one layer of processing nodes. "}
{"citeStart": 86, "citeEnd": 99, "citeStartToken": 86, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "In another approach contextual dependencies are modelled statistically. Churcb (1988) and Kempe (1993) use second order Markov Models and train their systems on large handtagged corpora. Using this metbod, they are able to tag more than 96 % of their test words with the correct part-of-speech. The need for reliably tagged training data, however, is a problem for languages, where such data is not available in sufficient quantities. Jelinek (1985) and Cutting et al. (1992) circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm (also know as the forward-backward algorithm). They report rates of correctly tagged words which are comparable to that presented by Church (1988) and Kempe (1993) . A third and rather new approach is tagging with artificial neural networks. In the area of speech recognition neural networks have been used for a decade r, ow. They have shown performances comparable to that of IIidden Ivlarkov model systems or even better (Lippmann, 1989) . Part-of-speech prediction is another area, closer to POS tagging, where neural networks have been applied successfidly. Nakamura el; al. (1990) trained a d-layer feed-forward network with up to three preceding part-of-speech tags ,as input to predict the word category of the next word. The prediction accuracy was similar to that of a trigram-b,~sed predictor. Using tile predictor, Nakamura et al. were able to improve the recognition rate of their speech recognition system from 81.0 % to 86.9 %. Federici and Pirrelli (199a) developed a part-ofspeech tagger which is based on a special type of neural network. It disambiguates between alternative morphosyntactic tags which are generated by a roof phological analyzer. The tagger is trained with an analogy-driven learning procedure. Only preliminary results are presented, so that a comparison with other methods is difficult.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Jelinek (1985) and Cutting et al. (1992) circumvent this problem by training their taggers on untagged data using tile Itaum-Welch algorithm (also know as the forward-backward algorithm). ", "mid_sen": "They report rates of correctly tagged words which are comparable to that presented by Church (1988) and Kempe (1993) . ", "after_sen": "A third and rather new approach is tagging with artificial neural networks. "}
{"citeStart": 121, "citeEnd": 132, "citeStartToken": 121, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "\"The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains\"", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also extrapolate the acronym for such phrases, e.g., in the example below, SCL would also be checked along with Structural Correspondence Learning.", "mid_sen": "\"The paper compares Structural Correspondence Learning (Blitzer et al., 2006) with (various instances of) self-training (Abney, 2007; McClosky et al., 2006) for the adaptation of a parse selection model to Wikipedia domains\"", "after_sen": "We also add n-grams of length 1 to 3 to this lexical feature set and compare the results obtained with an n-gram only baseline in Table 2 . N-grams have been shown to perform consistently well in various NLP tasks (Bergsma et al., 2010) ."}
{"citeStart": 137, "citeEnd": 159, "citeStartToken": 137, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "All features are computed from hand and body pixel coordinates, which are obtained via computer vision; our vision system is similar to (Deutscher et al., 2000) . The feature set currently supports only single-hand gestures, using the hand that is farthest from the body center. As with the verbal feature set, supervised binning was applied to the continuousvalued features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is reported in the DTW-DISTANCE feature.", "mid_sen": "All features are computed from hand and body pixel coordinates, which are obtained via computer vision; our vision system is similar to (Deutscher et al., 2000) . ", "after_sen": "The feature set currently supports only single-hand gestures, using the hand that is farthest from the body center. "}
{"citeStart": 176, "citeEnd": 196, "citeStartToken": 176, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "Besides, an increasing concern in current projects is that of linguistic relevance of the analysis t)erformed by the grammar correction system. In this sense, the adequate integration of error detection and correction techniques within mainstream grammm\" formalisms has l)een addressed by a nunl|)er of these projects ([Iolioli eta/., 1992) , (Vosse, 1992) , ((]enthia.l ctal., t992), (O(~uthial et al., 1994) . l~bllowing this concern, this paper presents resuits fl'om the project GramCheck (A Grammar and Style Checker, MLAP93-11), flmded by the CEC. GramCheck has developed a grammar checker demonstrator for Spanish and Greek native writers using ALEP (ET6/1, 1991), (Simpkins, 1994) as the NLP development platform, a client-server architeeUlre as implenmnted in the X Windows system, Motif as the 'look ~md fe, el' interface and Xminfo as the kllowh!dge t)ase, storage format. Generalized use of extensions to the highly typed and unifi(:ation based formalism imi)Iemented in ALEP has been 1)erformed. These extensions (called Constraint Solvers, CSs) are nothing but pieces of PR()I,OG code l)erforlning different l)oolean and relational operations over feature wdues. Besides, GramCheck has used ongoing results Dora LS-GRAM (LRE61029), a project alining at the implementation of middle coverage ALEP grammars for a number of European languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Besides, an increasing concern in current projects is that of linguistic relevance of the analysis t)erformed by the grammar correction system. ", "mid_sen": "In this sense, the adequate integration of error detection and correction techniques within mainstream grammm\" formalisms has l)een addressed by a nunl|)er of these projects ([Iolioli eta/., 1992) , (Vosse, 1992) , ((]enthia.l ctal., t992), (O(~uthial et al., 1994) . l~bllowing this concern, this paper presents resuits fl'om the project GramCheck (A Grammar and Style Checker, MLAP93-11), flmded by the CEC. ", "after_sen": "GramCheck has developed a grammar checker demonstrator for Spanish and Greek native writers using ALEP (ET6/1, 1991), (Simpkins, 1994) as the NLP development platform, a client-server architeeUlre as implenmnted in the X Windows system, Motif as the 'look ~md fe, el' interface and Xminfo as the kllowh!"}
{"citeStart": 57, "citeEnd": 82, "citeStartToken": 57, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "Viewed another way, we must show how to esti-mate the average word length. Conversational English has short words (about 3 phones), because most grammatical morphemes are free-standing. Languages with many affixes have longer words, e.g. my Arabic data averages 5.6 phones per word. Pauses are vital for deciding what is an affix. Attempts to segment transcriptions without pauses, e.g. (Christiansen et al., 1998) , have worked poorly. Claims that humans can extract words without pauses seem to be based on psychological experiments such as (Saffran, 2001; Jusczyk and Aslin, 1995) which conflate words and morphemes. Even then, explicit boundaries seem to improve performance (Seidl and Johnson, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Pauses are vital for deciding what is an affix. ", "mid_sen": "Attempts to segment transcriptions without pauses, e.g. (Christiansen et al., 1998) , have worked poorly. ", "after_sen": "Claims that humans can extract words without pauses seem to be based on psychological experiments such as (Saffran, 2001; Jusczyk and Aslin, 1995) which conflate words and morphemes. "}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "Given a stem such as brother, Toutanova et. al's system might generate the \"stem and inflection\" corresponding to and his brother. Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required. The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008) , which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use more complex context features. ", "mid_sen": "Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. ", "after_sen": "Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). "}
{"citeStart": 439, "citeEnd": 464, "citeStartToken": 439, "citeEndToken": 464, "sectionName": "UNKNOWN SECTION NAME", "string": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "mid_sen": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "after_sen": "Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. "}
{"citeStart": 55, "citeEnd": 76, "citeStartToken": 55, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000) . Though CRFs (Lafferty et al., 2001 ) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection. Training a CRF tagger with features selected using an MEMM may result in yet another performance boost, but in this paper we concentrate on the MEMM as our n-best tagger, and consider CRFs as one of our future extensions. Table 1 shows the state transition table of our MEMM model. Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003) , our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000) . ", "after_sen": "Though CRFs (Lafferty et al., 2001 ) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection. "}
{"citeStart": 151, "citeEnd": 174, "citeStartToken": 151, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "The incremental parser learns while parsing, and it could, in principle, simply be evaluated for a single pass of the data. But, because the quality of the parses of the first sentences would be low, I first trained on the full corpus and then measured parsing accuracy on the corpus subset. By training on the full corpus, the procedure differs from that of Klein, Manning and Bod who only train on the subset of bounded length sentences. However, this excludes the induction of parts-of-speech for parsing from plain text. When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire (Klein and Manning, 2002) . The comparison between the algorithms remains, therefore, valid. Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (Klein and Manning, 2004) , U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a) . The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text. Results for the incremental parser are given for learning and parsing from left to right and from right to left.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The comparison between the algorithms remains, therefore, valid. ", "mid_sen": "Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (Klein and Manning, 2004) , U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a) . ", "after_sen": "The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text. "}
{"citeStart": 0, "citeEnd": 29, "citeStartToken": 0, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "is the negative of the distance between and the value predicted for H by the fitted hyperplane function. Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods. However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "is the negative of the distance between and the value predicted for H by the fitted hyperplane function. ", "mid_sen": "Wilson, Wiebe, and Hwa (2004) used SVM regression to classify clause-level strength of opinion, reporting that it provided lower accuracy than other methods. ", "after_sen": "However, independently of our work, Koppel and Schler (2005) found that applying linear regression to classify documents (in a different corpus than ours) with respect to a three-point rating scale provided greater accuracy than OVA SVMs and other algorithms."}
{"citeStart": 111, "citeEnd": 134, "citeStartToken": 111, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of these coreference-inspired models leads to significant improvements in the baseline. Of the two, the discourse-new detector is by far more effective. The pronoun model's main problem is that, although a pronoun may have been displaced from its original position, it can often find another seemingly acceptable referent nearby. Despite this issue it performs significantly better than chance and is capable of slightly improving the combined model. Both of these models are very different from the lexical and entity-based models currently used for this task (Soricut and Marcu, 2006) , and are probably capable of improving the state of the art.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Despite this issue it performs significantly better than chance and is capable of slightly improving the combined model. ", "mid_sen": "Both of these models are very different from the lexical and entity-based models currently used for this task (Soricut and Marcu, 2006) , and are probably capable of improving the state of the art.", "after_sen": "As mentioned, uses a coreference system to attempt to improve the entity grid, but with mixed results. "}
{"citeStart": 136, "citeEnd": 165, "citeStartToken": 136, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition, a χ 2 dependency analysis showed that the NM presence interacts significantly with both AsrMis (p<0.02) and SemMis (p<0.001), with fewer than expected AsrMis and SemMis in the 3 Due to random assignment to conditions, before the first problem the F and S populations are similar (e.g. no difference in pretest); thus any differences in metrics can be attributed to the NM presence/absence. However, in the second problem, the two populations are not similar anymore as they have received different forms of instruction; thus any difference has to be attributed to the NM presence/absence in this problem as well as to the NM absence/presence in the previous problem. 4 Due to logging issues, 2 S users are excluded from this analysis (13 F and 13 S users remaining). We run the subjective metric analysis from Section 5.1 on this subset and the results are similar. NM condition. The fact that in the second problem the differences are much smaller (e.g. 2% for AsrMis) and that the NM-AsrMis and NM-SemMis interactions are not significant anymore, suggests that our observations can not be attributed to a difference in population with respect to system's ability to recognize their speech. We hypothesize that these differences are due to the NM text influencing users' lexical choice. Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993) , essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001) , generation/interpretation of anaphoric expressions (Allen et al., 2001) , performance modeling (Rotaru and Litman, 2006) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We hypothesize that these differences are due to the NM text influencing users' lexical choice. ", "mid_sen": "Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993) , essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001) , generation/interpretation of anaphoric expressions (Allen et al., 2001) , performance modeling (Rotaru and Litman, 2006) ).", "after_sen": "In this paper, we study the utility of the discourse structure on the user side of a dialogue system. "}
{"citeStart": 62, "citeEnd": 79, "citeStartToken": 62, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "It is not clear whether unsupervised topic modelling such as (Chen et al., 2009) can be applied to scientific articles (over 100 sentences long), which by nature include repetition of topics. It would be interesting to make comparisons with summaries using content models learnt from our data automatically, following a similar approach to (Sauper et al., 2010) which learns a content model jointly with a particular supervised task in web-based documents.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, their evaluation involved newspaper articles and extracts which are a lot shorter (15 and 6 sentences, respectively).", "mid_sen": "It is not clear whether unsupervised topic modelling such as (Chen et al., 2009) can be applied to scientific articles (over 100 sentences long), which by nature include repetition of topics. ", "after_sen": "It would be interesting to make comparisons with summaries using content models learnt from our data automatically, following a similar approach to (Sauper et al., 2010) which learns a content model jointly with a particular supervised task in web-based documents."}
{"citeStart": 197, "citeEnd": 201, "citeStartToken": 197, "citeEndToken": 201, "sectionName": "UNKNOWN SECTION NAME", "string": "The rules for the allocation of control are based on the utterance type classification and allow a dialogue to be divided into segments that correspond to which speaker is the controller of the segment. The definition of controller can be seen to correspond to the intuitions behind the term INITI-ATING CONVERSATIONAL PARTICIPANT (ICP), who is defined as the initiator of a given discourse segment [GS86] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The rules for the allocation of control are based on the utterance type classification and allow a dialogue to be divided into segments that correspond to which speaker is the controller of the segment. ", "mid_sen": "The definition of controller can be seen to correspond to the intuitions behind the term INITI-ATING CONVERSATIONAL PARTICIPANT (ICP), who is defined as the initiator of a given discourse segment [GS86] .", "after_sen": "The OTHER CONVERSATIONAL PARTICIPANT(S), OCP, may speak some utterances in a segment, but the DISCOURSE SEGMENT PUR-POSE, must be the purpose of the ICP. "}
{"citeStart": 59, "citeEnd": 69, "citeStartToken": 59, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "but this led to unwanted effects elsewhere in the grammar (Bouma 1987) . Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \\ for normal args, ? for wh-args (Moortgat 1988) , or have introduced so called modal operators on the whargument (Morrill et al. 1990 ). Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted effects elsewhere in the grammar. However, there are problems with having just composition, the most basic of the non-applicative operations. In CGs which contain functions of functions (such as very, or slowly), the addition of composition adds both new analyses of sentences, and new strings to the language. This is due to the fact that composition can be used to form a function, which can then be used as an argument to a function of a function. For example, if the two types, n/n and n/n are composed to give the type n/n, then this can be modified by an adjectival modifier of type (n/n)/(n/n).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "5Lambek notation (Lambek 1958) .", "mid_sen": "but this led to unwanted effects elsewhere in the grammar (Bouma 1987) . ", "after_sen": "Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \\ for normal args, ? for wh-args (Moortgat 1988) , or have introduced so called modal operators on the whargument (Morrill et al. 1990 ). "}
{"citeStart": 89, "citeEnd": 103, "citeStartToken": 89, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004) . The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual training data L. Then, a set of source language sentences, U , is translated based on the current model. A subset of good translations and their sources, T i , is selected in each iteration and added to the training data. These selected sentence pairs are replaced in each iteration, and only the original bilingual training data, L, is kept fixed throughout the algorithm. The process of generating sentence pairs, selecting a subset of good sentence pairs, and updating the model is continued until a stopping condition is met. Note that we run this algorithm in a transductive setting which means that the set of sentences U is drawn either from a development set or the test set that will be used eventually to evaluate the SMT system or from additional data which is relevant to the development or test set. In Algorithm 1, changing the definition of Estimate, Score and Select will give us the different semi-supervised learning algorithms we will discuss in this paper. Given the probability model p(t | s), consider the distribution over all possible valid translations t for a particular input sentence s. We can initialize this probability distribution to the uniform distribution for each sentence s in the unlabeled data U . Thus, this distribution over translations of sentences from U will have the maximum entropy. Under certain precise conditions, as described in (Abney, 2004) , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U . However, this is true only when the functions Estimate, Score and Select have very prescribed definitions. In this paper, rather than analyze the convergence of Algorithm 1 we run it for a fixed number of iterations and instead focus on finding useful definitions for Estimate, Score and Select that can be experimentally shown to improve MT performance.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004) . ", "after_sen": "The algorithm works as follows: "}
{"citeStart": 50, "citeEnd": 60, "citeStartToken": 50, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "The example used to illustrate the power of ATNs (Woods 1986) , \"John was believed to have been shot,\" also parses correctly, because the [object] node following the verb \"believed\" acts as both an absorber and a (re)generator. Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. The wrong FLOAT-OBJECT is available at the position of the first trace, and the parse dies: *(Which books)/did you ask John (where)j Bill bought (ti) (tj)? ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Meanwhile, the [participial-phrase] passes along the original FLOAT-OBJECT (\"which hospital\") to its right sibling, the adverbial prepositional phrase, \"to [object] .\" The phrase \"which hospital\" is finally absorbed by the preposition's object.", "mid_sen": "The example used to illustrate the power of ATNs (Woods 1986) , \"John was believed to have been shot,\" also parses correctly, because the [object] node following the verb \"believed\" acts as both an absorber and a (re)generator. ", "after_sen": "Cases of crossed traces are automatically blocked because the second CURRENT-FOCUS gets moved into the FLOAT-OBJECT position at the time of the second activator, overriding the preexisting FLOAT-OBJECT set up by the earlier activator. "}
{"citeStart": 305, "citeEnd": 327, "citeStartToken": 305, "citeEndToken": 327, "sectionName": "UNKNOWN SECTION NAME", "string": "To train our DISE model, we first extracted the set of speech and dialogue features shown in Figure 2 from the user turns in our corpus. As shown, the acoustic-prosodic features represent duration, pausing, pitch, and energy, and were normalized by the first user turn, as well as totaled and averaged over each dialogue. The lexical and dialogue features consist of the current dialogue name (i.e., one of the six physics problems) and turn number, the current ITSPOKE question's name (e.g.,T 3 in Figure 1 has a unique identifier) and depth in the discourse structure (e.g., an ITSPOKE remediation question after an incorrect user answer would be at one greater depth than the prior question), a word occurrence vector for the automatically recognized text of the user turn, an automatic (in)correctness label, and lastly, the number of user turns since the last correct turn (\"incorrect runs\"). We also included two user-based features, gender and pretest score. Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges Schuller et al., 2009b) and made freely available in the openS-MILE Toolkit (Florian et al., 2010) . To date, however, these features have only decreased the crossvalidation performance of our models. 8 While some of our features are tutoring-specific, these have similar counterparts in other applications (i.e., answer (in)correctness corresponds to a more general notion of \"response appropriateness\" in other domains, while pretest score corresponds to the general notion of domain expertise). Moreover, all of our features are fully automatic and available in real-time, so that the model can be directly implemented and deployed. To that end, we now describe the results of our intrinsic and extrinsic evaluations of our DISE model, aimed at determining whether it is ready to be evaluated with real users. Table 2 shows the averaged results of the crossvalidation with the J48 decision tree algorithm. In addition to accuracy, we use Unweighted Average (UA) Precision 9 , Recall, and F-measure because they are the standard measures used to evaluate current affect recognition technology, particularly for unbalanced two-class problems (Schuller et al., 2009b) . In addition, we use the cross correlation (CC) measure and mean linear error (MLE) because these metrics were used in recent work for evaluating disengagement (level of interest) detectors for the Interspeech 2010 challenge Wang and Hirschberg, 2011; Jeon et al., 2010) ). Note however that the Interspeech 2010 task differs from ours not only in the corpus and features, but also in the learning task: they used regression to detect a continuous level of interest ranging from 0 to 1, while we detect a binary class. Thus comparison between our results and those are only suggestive rather than conclusive.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also included two user-based features, gender and pretest score. ", "mid_sen": "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments (Forbes-Riley and Litman, 2011a; Drummond and Litman, 2011), we have also experimented with other features, including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges Schuller et al., 2009b) and made freely available in the openS-MILE Toolkit (Florian et al., 2010) . ", "after_sen": "To date, however, these features have only decreased the crossvalidation performance of our models. "}
{"citeStart": 44, "citeEnd": 73, "citeStartToken": 44, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Putting these two quotes together is, however, misleading, since it suggests a more direct mapping between incremental sem~mtics and dyna.mh: semantics than is actually possible. In an incremental semantics, we would expect the informtttiou state, of an interpreter to be updated word by word. In contrast, in dynamic semantics, the ol:der in which states are updated is determined by semantic st;ructure, not by left-toright order (see e.g. I,ewiu, 1992 [br discussion). For example, in 1)ynanfic Predicate Logic ((~roenendijk ,~ Stokhof, 1991) , states are threaded from the antecedent of a conditional into I, he conseque~d~, and from a restrictor of' a quantitier into I;he body. Thus, in interpreting, 13) John will buy it right away, if a car impresses him the input state for evMuation of .John will bug it right away is the output state from the a.ntecedent a ear hnp,vsses hhn. in this ease the threading through semantic structure is in the opposite order to the order in which the two clauses appear in the sentence.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, in dynamic semantics, the ol:der in which states are updated is determined by semantic st;ructure, not by left-toright order (see e.g. I,ewiu, 1992 [br discussion). ", "mid_sen": "For example, in 1)ynanfic Predicate Logic ((~roenendijk ,~ Stokhof, 1991) , states are threaded from the antecedent of a conditional into I, he conseque~d~, and from a restrictor of' a quantitier into I;he body. ", "after_sen": "Thus, in interpreting, 13) John will buy it right away, if a car impresses him the input state for evMuation of ."}
{"citeStart": 196, "citeEnd": 217, "citeStartToken": 196, "citeEndToken": 217, "sectionName": "UNKNOWN SECTION NAME", "string": "Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima'an (2007) . In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go. Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (Levinger et al., 1995; ) will make the parser more robust and suitable for use in more realistic scenarios.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go. ", "mid_sen": "Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (Levinger et al., 1995; ) will make the parser more robust and suitable for use in more realistic scenarios.", "after_sen": "An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc."}
{"citeStart": 17, "citeEnd": 45, "citeStartToken": 17, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM's, which turned out to have the worst accuracy of the four competing methods. In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below). 2° TnT is a trigram tagger (Brants 2000) , which means that it considers the previous two tags as features for deciding on the current tag. Moreover, it considers the capitalization of the previous word as well in its state representation. The lexical probabilities depend on the identity of the current word for known words and on a suffix tree smoothed with successive abstraction (Samuelsson 1996) for guessing the tags of unknown words. As we will see below, it shows a surprisingly higher accuracy than our previous HMM implementation. When we compare it with the other taggers used in this paper, we see that a trigram HMM tagger uses a very limited set of features (Table 1) . on the other hand, it is able to access some information about the rest of the sentence indirectly, through its use of the Viterbi algorithm.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993) .", "mid_sen": "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM's, which turned out to have the worst accuracy of the four competing methods. ", "after_sen": "In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below). "}
{"citeStart": 126, "citeEnd": 136, "citeStartToken": 126, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "Our transliteration model takes a discriminative approach; the classifier is presented with a word pair (w s , w t ) , where w s is a named entity and it is asked to determine whether w t is a transliteration of the NE in the target language. We use a linear classifier trained with a regularized perceptron update rule (Grove and Roth, 2001 ) as implemented in SNoW, (Roth, 1998) . The classifier's confidence score is used for ranking of positively tagged transliteration candidates. Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006) , in which the feature space consists of n-gram pairs from the two languages. Given a sample, each word is decomposed into a set of substrings of up to a given length (including the empty string). Features are generated by pairing substrings from the two sets whose relative positions in the original words differ by one or less places; first each word is decomposed into a set of substrings then substrings from the two sets are coupled to complete the pair representation. Figure 2 depicts this process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our transliteration model takes a discriminative approach; the classifier is presented with a word pair (w s , w t ) , where w s is a named entity and it is asked to determine whether w t is a transliteration of the NE in the target language. ", "mid_sen": "We use a linear classifier trained with a regularized perceptron update rule (Grove and Roth, 2001 ) as implemented in SNoW, (Roth, 1998) . ", "after_sen": "The classifier's confidence score is used for ranking of positively tagged transliteration candidates. "}
{"citeStart": 124, "citeEnd": 148, "citeStartToken": 124, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007) . eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000) , retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster. Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred. However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous. Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering. Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.", "mid_sen": "There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007) . ", "after_sen": "eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000) , retrieves a list of request-response pairs and presents a ranked list of responses to the user. "}
{"citeStart": 53, "citeEnd": 67, "citeStartToken": 53, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. ", "mid_sen": "The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "after_sen": "Reichenbach's well -known account of the interpretation of the different tense forms uses the temporal relations between three temporal indices: the utterance time, event time and reference time. "}
{"citeStart": 92, "citeEnd": 105, "citeStartToken": 92, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "The literature on ellipsis and event reference is voluminous, and so we will not attempt a comprehensive comparison here. Instead, we briefly compare the current work to three previous studies that explicitly tie ellipsis 14Sag and Hankamer claim that all such cases of VPellipsis require syntactic antecedents, whereas we suggest that in Coherent Situation relations VP-eUipsis operates more like their Model-Interpretive Anaphora, of which do it is an example. resolution to an account of discourse structure and coherence, namely our previous account (Kehler, 1993b) and the accounts of Priist (1992) and Asher (1993) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Instead, we briefly compare the current work to three previous studies that explicitly tie ellipsis 14Sag and Hankamer claim that all such cases of VPellipsis require syntactic antecedents, whereas we suggest that in Coherent Situation relations VP-eUipsis operates more like their Model-Interpretive Anaphora, of which do it is an example. ", "mid_sen": "resolution to an account of discourse structure and coherence, namely our previous account (Kehler, 1993b) and the accounts of Priist (1992) and Asher (1993) .", "after_sen": "In Kehler (1993b) , we presented an analysis of VPellipsis that distinguished between two types of relationship between clauses, parallel and non-parallel. "}
{"citeStart": 95, "citeEnd": 114, "citeStartToken": 95, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007) . Xiong et al. (2005) experimented with first-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and first-level hypernyms produced a significant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance.", "mid_sen": "The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007) . ", "after_sen": "Xiong et al. (2005) experimented with first-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. "}
{"citeStart": 161, "citeEnd": 174, "citeStartToken": 161, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "To create a dataset of usages of the construction no NP is too AP to VP-referred to as the tar-get construction-we use two corpora: the British National Corpus (Burnard, 2000) , an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008) , approximately one billion words of non-newswire text from the New York Times from the years 1987-2006. We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words. We then manually filter all sentences that do not have no NP as the subject of is too, or that do not have to VP as an argument of is too. After removing duplicates, this results in 170 sentences. We randomly select 20 of these sentences for development data, leaving 150 sentences for testing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To create a dataset of usages of the construction no NP is too AP to VP-referred to as the tar-get construction-we use two corpora: the British National Corpus (Burnard, 2000) , an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008) , approximately one billion words of non-newswire text from the New York Times from the years 1987-2006. ", "after_sen": "We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words. "}
{"citeStart": 62, "citeEnd": 80, "citeStartToken": 62, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Although this methodology could be valid for certain NLP problems, such as English Part-of-Speech tagging, we think that there exists reasonable evidence to say that, in WSD, accuracy results cannot be simply extrapolated to other domains (contrary to the opinion of other authors (Ng, 1997b) ): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a) , in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover \"all\" potential types of examples.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. ", "mid_sen": "one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a) , in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. ", "after_sen": "Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover \"all\" potential types of examples."}
{"citeStart": 118, "citeEnd": 139, "citeStartToken": 118, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we present a supervised machinelearning approach that categorizes sentences in scientific abstracts into four sections, objective, methods, results, and conclusions. Figure 1 illustrates the task of this study. Given an unstructured abstract without section labels indicated by boldface type, the proposed method annotates section labels of each sentence. Assuming that this task is well formalized as a sequential labeling problem, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) to identify rhetorical roles in scientific abstracts.The proposed method outperforms previous approaches to this problem, achieving 95.5% per-OBJECTIVE: This study assessed the role of adrenergic signal transmission in the control of renal erythropoietin (EPO) production in humans. METHODS: Forty-six healthy male volunteers underwent a hemorrhage of 750 ml. After phlebotomy, they received (intravenously for 6 hours in a parallel, randomized, placebo-controlled and single-blind design) either placebo (0.9% sodium chloride), or the beta 2-adrenergic receptor agonist fenoterol (1.5 microgram/min), or the beta 1-adrenergic receptor agonist dobutamine (5 micrograms/kg/min), or the nonselective beta-adrenergic receptor antagonist propranolol (loading dose of 0.14 mg/kg over 20 minutes, followed by 0.63 micrograms/kg/min). RESULTS: The AUCEPO(0-48 hr)fenoterol was 37% higher (p ¡ 0.03) than AUCEPO(0-48 hr)placebo, whereas AUCEPO(0-48 hr)dobutamine and AUCEPO(0-48 hr)propranolol were comparable with placebo. Creatinine clearance was significantly increased during dobutamine treatment. Urinary cyclic adenosine monophosphate excretion was increased only by fenoterol treatment, whereas serum potassium levels were decreased. Plasma renin activity was significantly increased during dobutamine and fenoterol infusion. CONCLUSIONS: This study shows in a model of controlled, physiologic stimulation of renal erythropoietin production that the beta 2-adrenergic receptor agonist fenoterol but not the beta 1-adrenergic receptor agonist dobutamine is able to increase erythropoietin levels in humans. The result can be interpreted as a hint that signals for the control of erythropoietin production may be mediated by beta 2-adrenergic receptors rather than by beta 1-adrenergic receptors. It appears to be unlikely that an increase of renin concentrations or glomerular filtration rate is causally linked to the control of erythropoietin production in this experimental setting. (Gleiter et al., 1997) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given an unstructured abstract without section labels indicated by boldface type, the proposed method annotates section labels of each sentence. ", "mid_sen": "Assuming that this task is well formalized as a sequential labeling problem, we use Conditional Random Fields (CRFs) (Lafferty et al., 2001) to identify rhetorical roles in scientific abstracts.", "after_sen": "The proposed method outperforms previous approaches to this problem, achieving 95.5% per-OBJECTIVE: "}
{"citeStart": 9, "citeEnd": 31, "citeStartToken": 9, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "This work could be extended in a numberofways. For example, in this paper we assumed that one would always choose a left-corner production set that includes the minimal set L required to ensure that the transformed grammar is not left-recursive. However, Roark and Johnson 1999 report good performance from a stochastically-guided top-down parser, suggesting that left-recursion is not always fatal. It might be possible to judiciously choose a left-corner production set smaller than L 0 which eliminates pernicious left-recursion, so that the remaining left-recursive cycles have such low probability that they will e ectively never be used and a stochastically-guided top-down parser will never search them.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, in this paper we assumed that one would always choose a left-corner production set that includes the minimal set L required to ensure that the transformed grammar is not left-recursive. ", "mid_sen": "However, Roark and Johnson 1999 report good performance from a stochastically-guided top-down parser, suggesting that left-recursion is not always fatal. ", "after_sen": "It might be possible to judiciously choose a left-corner production set smaller than L 0 which eliminates pernicious left-recursion, so that the remaining left-recursive cycles have such low probability that they will e ectively never be used and a stochastically-guided top-down parser will never search them."}
{"citeStart": 147, "citeEnd": 158, "citeStartToken": 147, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge. The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing. Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing. ", "mid_sen": "Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993) ).", "after_sen": "Our corpus-based approach is designed to support fast semantic lexicon construction. "}
{"citeStart": 108, "citeEnd": 117, "citeStartToken": 108, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 138, "citeEnd": 150, "citeStartToken": 138, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, some systems extract synonyms directly without extracting and comparing contextual representations for each term. Instead, these systems recognise terms within certain linguistic patterns (e.g. X, Y and other Zs) which associate synonyms and hyponyms (Hearst, 1992; Caraballo, 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally, some systems extract synonyms directly without extracting and comparing contextual representations for each term. ", "mid_sen": "Instead, these systems recognise terms within certain linguistic patterns (e.g. X, Y and other Zs) which associate synonyms and hyponyms (Hearst, 1992; Caraballo, 1999) .", "after_sen": "Thesaurus extraction is a good task to use to experiment with scaling context spaces. "}
{"citeStart": 35, "citeEnd": 59, "citeStartToken": 35, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The same features were used for Hebrew, with the addition of m −2 . . . m 2 . These are the same settings as in (Kudo and Matsumoto, 2000; Goldberg et al., 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The same features were used for Hebrew, with the addition of m −2 . . . m 2 . ", "mid_sen": "These are the same settings as in (Kudo and Matsumoto, 2000; Goldberg et al., 2006) .", "after_sen": ""}
{"citeStart": 56, "citeEnd": 76, "citeStartToken": 56, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "The theme in this paper is to study the problem of sentence tokenization in the framework of formal languages, a direction that has recently attracted some attention. For instance, in Ma (1996) , words in a tokenization dictionary are represented as production rules and character strings are modeled as derivatives of these rules under a string concatenation operation. Although not stated explicitly in his thesis, this is obviously a finite-state model, as evidenced from his employment of (finite-) state diagrams for representing both the tokenization dictionary and character strings. The weighted finite-state transducer model developed by Sproat et al. (1996) is another excellent representative example.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although not stated explicitly in his thesis, this is obviously a finite-state model, as evidenced from his employment of (finite-) state diagrams for representing both the tokenization dictionary and character strings. ", "mid_sen": "The weighted finite-state transducer model developed by Sproat et al. (1996) is another excellent representative example.", "after_sen": "They both stop at merely representing possible tokenizations as a single large finite-state diagram (word graph). "}
{"citeStart": 240, "citeEnd": 252, "citeStartToken": 240, "citeEndToken": 252, "sectionName": "UNKNOWN SECTION NAME", "string": "What is the role of citation contexts in the overall structure of scientific context? We assume a hierarchical, rhetorical structure not unlike RST (Mann and Thompson, 1987) , but much flatter, where the atomic units are textual blocks which carry a certain functional role in the overall scientific argument for publication (Teufel, 2010; Hyland, 2000) . Under such a general model, citation blocks are certainly a functional unit, and their recognition is a rewarding task in their own right. If citation blocks can be recognised along with their sentiment, this is even more useful, as it restricts the possibilities for which rhetorical function the segment plays. For instance, in the motivation section of a paper, before the paper contribution is introduced, we often find negative sentiment assigned to citations, as any indication can serve as a justification for the current paper. In contrast, positive sentiment is more likely to be restricted to the description of an approach which the authors include in their solution, or further develop.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "What is the role of citation contexts in the overall structure of scientific context? ", "mid_sen": "We assume a hierarchical, rhetorical structure not unlike RST (Mann and Thompson, 1987) , but much flatter, where the atomic units are textual blocks which carry a certain functional role in the overall scientific argument for publication (Teufel, 2010; Hyland, 2000) . ", "after_sen": "Under such a general model, citation blocks are certainly a functional unit, and their recognition is a rewarding task in their own right. "}
{"citeStart": 71, "citeEnd": 92, "citeStartToken": 71, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "Events of type eat differ from those of type run in the way the result d~ is brought about. This can be illustrated by means of the notion of a nucleus-structure (Moens/Steedman (1988) ). A nucleus-structure consists of three parts: the inception-point (IP), the development-portion (DP) and the culmination-point (CP).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Events of type eat differ from those of type run in the way the result d~ is brought about. ", "mid_sen": "This can be illustrated by means of the notion of a nucleus-structure (Moens/Steedman (1988) ). ", "after_sen": "A nucleus-structure consists of three parts: the inception-point (IP), the development-portion (DP) and the culmination-point (CP)."}
{"citeStart": 203, "citeEnd": 220, "citeStartToken": 203, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural language word patterns. Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural language word patterns. ", "mid_sen": "Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "after_sen": "However, Sproat (1992) observes that, despite the existence of \"working systems that are capable of doing a great deal of morphological analysis\", \"there are still outstanding problems and areas which have not received much serious attention\" (ibid., 123) . "}
{"citeStart": 104, "citeEnd": 117, "citeStartToken": 104, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "As a seinant{c representation of words, distltllCe w~ctors are expected to depend very weakly on the particular source dictionary. We eolilpared two sets of distance vectors, one from I,I)OCE (Procter 1978) and the other from COBUILD (Sinclair 1987) , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves (Niwa and Nitta 1993 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a seinant{c representation of words, distltllCe w~ctors are expected to depend very weakly on the particular source dictionary. ", "mid_sen": "We eolilpared two sets of distance vectors, one from I,I)OCE (Procter 1978) and the other from COBUILD (Sinclair 1987) , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves (Niwa and Nitta 1993 ).", "after_sen": "We will now describe some technical details al)Ollt the derivation of distance vectors."}
{"citeStart": 66, "citeEnd": 85, "citeStartToken": 66, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "This algorithm was first implemented for the MUC-6 FASTUS system (Appelt et al., 1995) , and produced one of the top scores (a recall of 59% and precision of 72%) in the MUC-6 Coreference Task, which evaluated systems' ability to recog-nize coreference among noun phrases (Sundheim, 1995) . Note that only identity of reference was evaluated there. 2", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "This algorithm was first implemented for the MUC-6 FASTUS system (Appelt et al., 1995) , and produced one of the top scores (a recall of 59% and precision of 72%) in the MUC-6 Coreference Task, which evaluated systems' ability to recog-nize coreference among noun phrases (Sundheim, 1995) . ", "after_sen": "Note that only identity of reference was evaluated there. "}
{"citeStart": 10, "citeEnd": 38, "citeStartToken": 10, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "In the unigram syllable adaptor grammar shown in Figure 7 , Consonant expands to any consonant and Vowel expands to any vowel. This grammar defines a Word to consist of up to three Syllables, where each Syllable consists of an Onset and a Rhyme and a Rhyme consists of a Nucleus and a Coda. Following Goldwater and Johnson (2005) , the grammar differentiates between OnsetI, which expands to word-initial onsets, and Onset, which expands to non-word-initial onsets, and between CodaF, which expands to word-final codas, and Coda, which expands to non-word-final codas. Note that we do not need to distinguish specific positions within the Onset and Coda clusters as Goldwater and Johnson (2005) did, since the adaptor grammar learns these clusters directly. Just like the unigram morphology grammar, the unigram syllable grammar also defines a HDP because the base distribution for Word is defined in terms of the Onset and Rhyme distributions.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This grammar defines a Word to consist of up to three Syllables, where each Syllable consists of an Onset and a Rhyme and a Rhyme consists of a Nucleus and a Coda. ", "mid_sen": "Following Goldwater and Johnson (2005) , the grammar differentiates between OnsetI, which expands to word-initial onsets, and Onset, which expands to non-word-initial onsets, and between CodaF, which expands to word-final codas, and Coda, which expands to non-word-final codas. ", "after_sen": "Note that we do not need to distinguish specific positions within the Onset and Coda clusters as Goldwater and Johnson (2005) did, since the adaptor grammar learns these clusters directly. "}
{"citeStart": 316, "citeEnd": 335, "citeStartToken": 316, "citeEndToken": 335, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the Wall Street Journal as contained in the Penn Treebank for our experiments. The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993) . This evaluation only uses the part-ofspeech annotation. The Wall Street Journal part of the Penn Treebank consists of approx. 50,000 sentences (1.2 million tokens).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use the Wall Street Journal as contained in the Penn Treebank for our experiments. ", "mid_sen": "The annotation consists of four parts: 1) a context-free structure augmented with traces to mark movement and discontinuous constituents, 2) phrasal categories that are annotated as node labels, 3) a small set of grammatical functions that are annotated as extensions to the node labels, and 4) part-of-speech tags (Marcus et al., 1993) . ", "after_sen": "This evaluation only uses the part-ofspeech annotation. "}
{"citeStart": 3, "citeEnd": 16, "citeStartToken": 3, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "In Kehler (1993b) , we presented an analysis of VPellipsis that distinguished between two types of relationship between clauses, parallel and non-parallel. An architecture was presented whereby utterances were parsed into propositional representations which were subsequently integrated into a discourse model. It was posited that VP-ellipsis could access either propositional or discourse model representations: in the case of parallel constructions, the source resided in the propositional representation; in the case of non-parallel constructions, the source had been integrated into the discourse model. In Kehler (1994) , we showed how this architecture also accounted for the facts that Levin and Prince noted about gapping.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was posited that VP-ellipsis could access either propositional or discourse model representations: in the case of parallel constructions, the source resided in the propositional representation; in the case of non-parallel constructions, the source had been integrated into the discourse model. ", "mid_sen": "In Kehler (1994) , we showed how this architecture also accounted for the facts that Levin and Prince noted about gapping.", "after_sen": "The current work improves upon that analysis in several respects. "}
{"citeStart": 12, "citeEnd": 39, "citeStartToken": 12, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "The work of Efron and Tibshirani (1993) enabled Breiman's refinement and application of their techniques for machine learning (Breiman, 1996) . His technique is called bagging, short for \"bootstrap aggregating\". In brief, bootstrap techniques and bag-ging in particular reduce the systematic biases many estimation techniques introduce by aggregating estimates made from randomly drawn representative resamplings of those datasets.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The work of Efron and Tibshirani (1993) enabled Breiman's refinement and application of their techniques for machine learning (Breiman, 1996) . ", "after_sen": "His technique is called bagging, short for \"bootstrap aggregating\". "}
{"citeStart": 84, "citeEnd": 106, "citeStartToken": 84, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Results are better using the AA (ρ max = 0.70 for AA-cos) than using the TAA (ρ max = 0.56). Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (ρ max,wlm = 0.60 vs ρ max,cos = 0.70). These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008 ) (ρ = 0.69) and ESA (Gabrilovich and Markovitch, 2007) ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (ρ max,wlm = 0.60 vs ρ max,cos = 0.70). ", "mid_sen": "These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008 ) (ρ = 0.69) and ESA (Gabrilovich and Markovitch, 2007) ", "after_sen": "(ρ = 0.75)."}
{"citeStart": 107, "citeEnd": 122, "citeStartToken": 107, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach to the problem is more compatible with the empirical evidence we presented in our prior work (Li et al., 2014) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality. Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences. This prior work was carried over a dataset containing a single reference translation for each Chinese sentence. In the work presented in this paper, we strengthen our findings by examining multiple reference translations for each Chinese sentence. We define heavy sentences based on agreement of translator choices and reader preferences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, the model we propose here can be used to predict when segmentation is at all necessary.", "mid_sen": "Our approach to the problem is more compatible with the empirical evidence we presented in our prior work (Li et al., 2014) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality. ", "after_sen": "Rather we showed that the quality of translation was markedly inferior, compared to overall translation quality, for sentences that were translated into multiple English sentences. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. Gerdemann's generator follows a head-driven strategy in order to avoid inefficient evaluation orders. More specifically, the head of the right-hand side of each grammar rule is distinguished, and distinguished categories are scanned or predicted upon first. The resulting evaluation strategy is similar to that of the head-corner approach (Shieber et al., 1990 ; Gerdemann and IIinrichs, in press): prediction follows the main flow of semantic information until a lexical pivot is reached, and only then are the headdependent subparts of the construction built up in a bottom-up fashion. This mixture of top-down and bottom-up information flow is crucial since the topdown semantic information from the goal category must be integrated with the bottom-up subcategorization information from the lexicon. A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing. Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing. ", "mid_sen": "Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. ", "after_sen": "By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) ."}
{"citeStart": 41, "citeEnd": 55, "citeStartToken": 41, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "To handle unseen or infrequent words, all words whose frequency falls below a threshold Ω are grouped together in an 'unknown word' token, which is then treated like an additional word. For our experiments, we use Ω ¤ 10. We consider several variations of this simple model by changing both P r and P w . In addition to the standard formulation in Equation (1), we consider two alternative variants of P r . The first is a Markov context-free rule (Magerman, 1995; Charniak, 2000) . A rule may be turned into a Markov rule by first binarizing it, then making independence assumptions on the new binarized rules. Binarizing the rule A B 1©©¨Bn results in a number of smaller rules A", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition to the standard formulation in Equation (1), we consider two alternative variants of P r . ", "mid_sen": "The first is a Markov context-free rule (Magerman, 1995; Charniak, 2000) . ", "after_sen": "A rule may be turned into a Markov rule by first binarizing it, then making independence assumptions on the new binarized rules. "}
{"citeStart": 95, "citeEnd": 115, "citeStartToken": 95, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service. We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service. ", "mid_sen": "We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text.", "after_sen": "http://translate.google.com/translate_t 3 http://babelfish.yahoo.com/translate_txt 4 http://www.windowslivetranslator.com/ Unlabeled Chinese Reviews Labeled English Reviews"}
{"citeStart": 175, "citeEnd": 184, "citeStartToken": 175, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the original KL distance is asymmetric and is not defined when zero frequency occurs. Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991) , which introducing a probabilistic variable m, or α -Skew Divergence (Lee, 1999) , by adopting adjustable variable α. Research shows that Skew Divergence achieves better performance than other measures. (Lee, 2001) To convert distance to similarity value, we adopt the formula inspired by Mochihashi, and Matsumoto 2002.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Due to the original KL distance is asymmetric and is not defined when zero frequency occurs. ", "mid_sen": "Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991) , which introducing a probabilistic variable m, or α -Skew Divergence (Lee, 1999) , by adopting adjustable variable α. Research shows that Skew Divergence achieves better performance than other measures. ", "after_sen": "(Lee, 2001) To convert distance to similarity value, we adopt the formula inspired by Mochihashi, and Matsumoto 2002."}
{"citeStart": 84, "citeEnd": 110, "citeStartToken": 84, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.", "mid_sen": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "after_sen": "We have a different perspective than these lines of inquiry. "}
{"citeStart": 33, "citeEnd": 50, "citeStartToken": 33, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. Such resources exist for Hebrew (Itai et al., 2006) , but unfortunately use a tagging scheme which is incom-patible with the one of the Hebrew Treebank. 8 For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007) . We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses. ", "mid_sen": "Such resources exist for Hebrew (Itai et al., 2006) , but unfortunately use a tagging scheme which is incom-patible with the one of the Hebrew Treebank. ", "after_sen": "8 For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007) . "}
{"citeStart": 133, "citeEnd": 152, "citeStartToken": 133, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994 , Sproat et al., 1996 . Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994 , Sproat et al., 1996 . ", "after_sen": "Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys."}
{"citeStart": 99, "citeEnd": 118, "citeStartToken": 99, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "In the last decade, SVM has become one of the most studied techniques for text classification, due to the positive results it has shown. This technique uses the vector space model for the documents' representation, and assumes that documents in the same class should fall into separable spaces of the representation. Upon this, it looks for a hyperplane that separates the classes; therefore, this hyperplane should maximize the distance between it and the nearest documents, what is called the margin. The following function is used to define the hyperplane (see Figure  1 ): In order to resolve this function, all the possible values should be considered and, after that, the values of w and b that maximize the margin should be selected. This would be computationally expensive, so the following equivalent function is used to relax it (Boser et al. , 1992) (Cortes and Vapnik, 1995) :", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to resolve this function, all the possible values should be considered and, after that, the values of w and b that maximize the margin should be selected. ", "mid_sen": "This would be computationally expensive, so the following equivalent function is used to relax it (Boser et al. , 1992) (Cortes and Vapnik, 1995) :", "after_sen": "f (x) = w • x + b"}
{"citeStart": 183, "citeEnd": 202, "citeStartToken": 183, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. ", "mid_sen": "Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . ", "after_sen": "All systems were trained on the same data and the outputs used the same tokenization. "}
{"citeStart": 10, "citeEnd": 34, "citeStartToken": 10, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexical stress is the \"accentuation of syllables within words\" (Cutler, 2005) and has long been argued to play an important role in adult word recognition. Following Cutler and Carter (1987) 's observation that stressed syllables tend to occur at the beginnings of words in English, Jusczyk et al. (1993) investigated whether infants acquiring English take advantage of this fact. Their study demonstrated that this is indeed the case for 9 month olds, although they found no indication of using stressed syllables as cues for word boundaries in 6 month olds. Their findings have been replicated and extended in subsequent work (Jusczyk et al., 1999b; Thiessen and Saffran, 2003; Curtin et al., 2005; Thiessen and Saffran, 2007) . A brief summary of the key findings is as follows: English infants treat stressed syllables as cues for the beginnings of words from roughly 7 months of age, suggesting that the role played by stress needs to be acquired, and that this requires antecedent segmentation by nonstress-based means (Thiessen and Saffran, 2007) . They also exhibit a preference for low-pass filtered stress-initial words from this age, suggesting that it is indeed stress and not other phonetic or phonotactic properties that are treated as a cue for wordbeginnings (Jusczyk et al., 1993) . In fact, phontactic cues seem to be used later than stress cues (Jusczyk et al., 1999a ) and seem to be outweighed by stress cues (Mattys and Jusczyk, 2000) . The earliest computational model for word segmentation incorporating stress cues we are aware of is the recurrent network model of Christiansen et al. (1998) and Christiansen and Curtin (1999) . They only reported a word-token f-score of 44% (roughly, segmentation accuracy: see Section 4), which is considerably below the performance of subsequent models, making a direct comparison complicated. Yang (2004) introduced a simple incremental algorithm that relies on stress by embodying a Unique Stress Constraint (USC) that allows at most a single stressed syllable per word. On pre-syllabified child directed speech, he reported a word token fscore of 85.6% for a non-statistical algorithm that exploits the USC. While the USC has been argued to be near-to-universal and follows from the \"culminative function of stress\" (Fromkin, 2001; Cutler, 2005) , the high score Yang reported crucially depends on every word token carrying stress, including function words. More recently, Lignos (2010 Lignos ( , 2011 Lignos ( , 2012 further explored Yang's original algorithm, taking into account that function words should not be assumed to possess lexical stress cues. While his scores are in line with those reported by Yang, the importance of stress for this learner were more modest, providing a gain of around 2.5% (Lignos, 2011) . Also, the Yang/Lignos learner is unable to acquire knowledge about the role stress plays in the language, e.g. that stress tends to fall on particular positions within words. Doyle and Levy (2013) extend the Bigram model of by adding stresstemplates to the lexical generator. A stress-template indicates how many syllables the word has, and which of these syllables (if any) are stressed. This allows the model to acquire knowledge about the stress patterns of its input by assigning different probabilities to the different stress-templates. However, Doyle and Levy (2013) do not directly examine the probabilities assigned to the stress-templates; they only report that their model does slightly prefer stress-initial words over the baseline model by calculating the fraction of stress-initial word types in the output segmentations of their models. They also demonstrate that stress cues do indeed aid segmentation, although their reported gain of 1% in token f-score is even smaller than that reported by Lignos (2011). Our own approach differs from theirs in assuming phonemic rather than pre-syllabified input (although our model could, trivially, be run on syllabified input as well) and makes use of Adaptor Grammars instead of the Bigram model, providing us with a flexible framework for exploring the usefulness of stress in differ-ent models.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical stress is the \"accentuation of syllables within words\" (Cutler, 2005) and has long been argued to play an important role in adult word recognition. ", "mid_sen": "Following Cutler and Carter (1987) 's observation that stressed syllables tend to occur at the beginnings of words in English, Jusczyk et al. (1993) investigated whether infants acquiring English take advantage of this fact. ", "after_sen": "Their study demonstrated that this is indeed the case for 9 month olds, although they found no indication of using stressed syllables as cues for word boundaries in 6 month olds. "}
{"citeStart": 83, "citeEnd": 97, "citeStartToken": 83, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "We present a comparative study for four languages (English, Swedish, Spanish, and Basque) by performing 5-fold cross-validation on the SENSEVAL-2 lexical-sample training data, using the fine-grained sense inventory. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001 ) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995) , Golding and Roth (1996) and Mangu and Brill (1997) . Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 and Senseval-2 test data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. ", "mid_sen": "For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995) , Golding and Roth (1996) and Mangu and Brill (1997) . ", "after_sen": "Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 and Senseval-2 test data."}
{"citeStart": 10, "citeEnd": 34, "citeStartToken": 10, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "(21) Bill supported, and Hillary opposed, two trade bills. Partee and Rooth (1983) observe, and we agree, that the quantifier in such cases only scopes once, resulting in the reading where Bill supported and Hillary opposed the same two bills. 5 Our analysis predicts this fact in the same way as Partee and Rooth's analysis does. The meanings contributed by the lexieal items and f-structure dependencies are the same as in the previous example, except for that of the object NP. Following Dalrymple et al. (1994a) , the meaning derived using the contributions from an f-structure h for two trade bills is: two-trade-bills:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The meanings contributed by the lexieal items and f-structure dependencies are the same as in the previous example, except for that of the object NP. ", "mid_sen": "Following Dalrymple et al. (1994a) , the meaning derived using the contributions from an f-structure h for two trade bills is: two-trade-bills:", "after_sen": "VH, S. (Vz. h~-.~x --o H~S(~)) -o g..~two(z, tradebill(z), S(z))"}
{"citeStart": 242, "citeEnd": 255, "citeStartToken": 242, "citeEndToken": 255, "sectionName": "UNKNOWN SECTION NAME", "string": "Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002) . The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen data we would like to bound. We assume that the algorithm is fed with a sequence of examples x 1 , . . . , x T , and at each epoch k = 1, . . . , K it makes a prediction y t,k . The correct label is y * t . For k = 1, . . . , K and t = 1, . . . , T , let t,k = U (x t , y * t ) − U (x t , y t,k ), and denote by ∆ t,k and ξ t,k the distance at epoch k for example t, and the slack at epoch k for example t, respectively. Finally, we denote by D T,K = T t=1 ∆ 2 t,K , and by w T,K the final weight vector returned after K epochs. We state a condition of convergence 2 :", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Generalization for Online-to-Batch Conversion. ", "mid_sen": "In practice, perceptron-type algorithms are often applied in a batch learning scenario, i.e., the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire, 1999; Collins, 2002) . ", "after_sen": "The difference to the online learning scenario is that we treat the multi-epoch algorithm as an empirical risk minimizer that selects a final weight vector w T,K whose expected loss on unseen data we would like to bound. "}
{"citeStart": 181, "citeEnd": 194, "citeStartToken": 181, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "In this dialogue, speaker B is not confident that he will be able to identify the intersection at Lowell Street, and so suggests that the intersection might be marked. Speaker A replies with an elaboration of the initial expression, and B finds that he is now confident, and so accepts the reference. This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent (Appelt, 1985a; Appelt and Kronfeld, 1987; Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1992; Searle, 1969) , are copreseut with the referent (Heeman and Hirst, 1992; Cohen, 1981) , or have the referent in their focus of attention (Reiter and Dale, 1992) . In these theories, the speaker has the intention that the hearer either know the referent or identijy it immediately.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Speaker A replies with an elaboration of the initial expression, and B finds that he is now confident, and so accepts the reference. ", "mid_sen": "This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent (Appelt, 1985a; Appelt and Kronfeld, 1987; Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1992; Searle, 1969) , are copreseut with the referent (Heeman and Hirst, 1992; Cohen, 1981) , or have the referent in their focus of attention (Reiter and Dale, 1992) . ", "after_sen": "In these theories, the speaker has the intention that the hearer either know the referent or identijy it immediately."}
{"citeStart": 273, "citeEnd": 287, "citeStartToken": 273, "citeEndToken": 287, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural language word patterns. Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural language word patterns. ", "mid_sen": "Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "after_sen": "However, Sproat (1992) observes that, despite the existence of \"working systems that are capable of doing a great deal of morphological analysis\", \"there are still outstanding problems and areas which have not received much serious attention\" (ibid., 123) . "}
{"citeStart": 294, "citeEnd": 307, "citeStartToken": 294, "citeEndToken": 307, "sectionName": "UNKNOWN SECTION NAME", "string": "If one of the features of the ANLT grammar formalism, the kleene operator (allowing indefinite repetition of rule daughters), is disallowed, then the complexity of the BU-LC parser with respect to the length of the input string is O(np+l), where p is the maximum number of daughters in a rule (Carroll, 1993) . The inclusion of the operator increases the complexity to exponential. To retain the polynomial time bound, new rules can be introduced to produce recursive tree structures instead of an iterated fiat tree structure. However, when this technique is applied to the ANLT grammar the increased overheads in rule invocation and structure building actually slow the parser down.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although the complexity of returning all parses for a string is always related exponentially to its length (since the number of parses is exponential, and they must all at least be enumerated), the complexity of a parser is usually measured for the computation of a parse forest (unless extracting a single analysis from the forest is worse than linear) 5.", "mid_sen": "If one of the features of the ANLT grammar formalism, the kleene operator (allowing indefinite repetition of rule daughters), is disallowed, then the complexity of the BU-LC parser with respect to the length of the input string is O(np+l), where p is the maximum number of daughters in a rule (Carroll, 1993) . ", "after_sen": "The inclusion of the operator increases the complexity to exponential. "}
{"citeStart": 95, "citeEnd": 115, "citeStartToken": 95, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et al., 1992; Taylor, Grover &= Briscoe, 1989) 2. However, although the practical throughput of parsers with such realistic grammars is important, for example when process-1This research was supported by SERC/DTI project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by EC ESPRIT BRA-7315 'ACQUILEX-II'. I am grateful to Ted Briscoe for comments on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. 2For example, Taylor et al. demonstrate that the ANLT grammar is in principle able to analyse 96.8% of a corpus of 10,000 noun phrases taken from a variety of corpora.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Alshawi et al., 1992; .", "mid_sen": "Evaluations of the grammars in these particular systems have shown them to have wide coverage (Alshawi et al., 1992; Taylor, Grover &= Briscoe, 1989) 2. However, although the practical throughput of parsers with such realistic grammars is important, for example when process-1This research was supported by SERC/DTI project 4/1/1261 'Extensions to the Alvey Natural Language Tools' and by EC ESPRIT BRA-7315 'ACQUILEX-II'. I am grateful to Ted Briscoe for comments on an earlier version of this paper, to David Weir for valuable discussions, and to Hiyan Alshawi for assistance with the CLARE system. ", "after_sen": "2For example, Taylor et al. "}
{"citeStart": 228, "citeEnd": 244, "citeStartToken": 228, "citeEndToken": 244, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Different concepts have been used in the literature as primitives. ", "mid_sen": "These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. ", "after_sen": "An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. "}
{"citeStart": 172, "citeEnd": 183, "citeStartToken": 172, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "The ability to identify the most characteristic features of words can have additional benefits, beyond their impact on traditional word similarity measures (as evaluated in this article). A demonstration of such potential appears in Geffet and Dagan (2005) , which presents a novel feature inclusion scheme for vector comparison. That scheme utilizes our bootstrapping method to identify the most characteristic features of a word and then tests whether these particular features co-occur also with a hypothesized entailed word. The empirical success reported in that paper provides additional evidence for the utility of the bootstrapping method. More generally, our motivation and methodology can be extended in several directions by future work on acquiring lexical entailment or other lexical-semantic relations. One direction is to explore better vector comparison methods that will utilize the improved feature weighting, as shown in Geffet and Dagan (2005) . Another direction is to integrate distributional similarity and pattern-based acquisition approaches, which were shown to provide largely complementary information (Mirkin, Dagan, and Geffet 2006 ). An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005) , and typically overlaps to a rather limited extent with the output of automatic acquisition methods. As a parallel direction, future research should explore in detail the impact of different lexical-semantic acquisition methods on text understanding applications.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another direction is to integrate distributional similarity and pattern-based acquisition approaches, which were shown to provide largely complementary information (Mirkin, Dagan, and Geffet 2006 ). ", "mid_sen": "An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005) , and typically overlaps to a rather limited extent with the output of automatic acquisition methods. ", "after_sen": "As a parallel direction, future research should explore in detail the impact of different lexical-semantic acquisition methods on text understanding applications."}
{"citeStart": 58, "citeEnd": 70, "citeStartToken": 58, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "Step OPINE uses a version of Turney's PMI-based approach (Turney, 2003) in order to derive the initial probability estimates (P (l(w) = L) (0) ) for a subset S of the words. OPINE computes a SO score so(w) for each w in S as the difference between the PMI of w with positive keywords (e.g., \"excellent\") and the PMI of w with negative keywords (e.g., \"awful\"). When so(w) is small, or w rarely co-occurs with the keywords, w is classified as neutral. If so(w) > 0, then w is positive, otherwise w is negative. OPINE then uses the labeled S set in order to compute prior probabilities P (l(w) = L), L ∈ {pos, neg, neutral} by computing the ratio between the number of words in S labeled L and |S|. Such probabilities are used as initial probability estimates associated with the labels of the remaining words.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Step OPINE uses a version of Turney's PMI-based approach (Turney, 2003) in order to derive the initial probability estimates (P (l(w) = L) (0) ) for a subset S of the words. ", "after_sen": "OPINE computes a SO score so(w) for each w in S as the difference between the PMI of w with positive keywords (e.g., \"excellent\") and the PMI of w with negative keywords (e.g., \"awful\"). "}
{"citeStart": 160, "citeEnd": 176, "citeStartToken": 160, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiments we use sets of reviews for 7 product classes (1621 total reviews) which include the publicly available data sets for 5 product classes from (Hu and Liu, 2004 ). Hu's system is the review mining system most relevant to our work. It uses association rule mining to extract frequent review noun phrases as features. Frequent features are used to find potential opinion words (only adjectives) and the system uses Word-Net synonyms/antonyms in conjunction with a set of seed words in order to find actual opinion words. Finally, opinion words are used to extract associated infrequent features. The system only extracts explicit features.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In our experiments we use sets of reviews for 7 product classes (1621 total reviews) which include the publicly available data sets for 5 product classes from (Hu and Liu, 2004 ). ", "after_sen": "Hu's system is the review mining system most relevant to our work. "}
{"citeStart": 1, "citeEnd": 34, "citeStartToken": 1, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Ino earlyi tradingr ino Hongl Kongz MondayB ,o goldz waso quotedo ato $r 366.50z anB ounce/ -o This set of three tags is sufficient for encoding baseNP structures since these structures are nonrecursive and nonoverlapping. (Tjong Kim Sang and Veenstra, 1999) have presented three variants of this tagging representation. First, the B tag can be used for the first word of every noun phrase (IOB2 representation). Second, instead of the B tag an E tag can be used to mark the last word of a baseNP immediately before another baseNP (IOE1). And third, the E tag can be used for every noun phrase final word (IOE2). They have used the (Ramshaw and Marcus, 1995) representation as well (IOB1). We will use these four tagging representations as well as the O+C representation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Ino earlyi tradingr ino Hongl Kongz MondayB ,o goldz waso quotedo ato $r 366.50z anB ounce/ -o This set of three tags is sufficient for encoding baseNP structures since these structures are nonrecursive and nonoverlapping. ", "mid_sen": "(Tjong Kim Sang and Veenstra, 1999) have presented three variants of this tagging representation. ", "after_sen": "First, the B tag can be used for the first word of every noun phrase (IOB2 representation). "}
{"citeStart": 112, "citeEnd": 128, "citeStartToken": 112, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes methods (Teh et al., 2008) for DPs to adaptor grammars and other linguistic applications of HDPs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. ", "mid_sen": "It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes methods (Teh et al., 2008) for DPs to adaptor grammars and other linguistic applications of HDPs.", "after_sen": ""}
{"citeStart": 148, "citeEnd": 159, "citeStartToken": 148, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 176, "citeEnd": 195, "citeStartToken": 176, "citeEndToken": 195, "sectionName": "UNKNOWN SECTION NAME", "string": "For the first scenario a corpus containing 500 sentences for each of the verbs was constructed. The text was randomly selected from corpora of different domains and genres, including literary fiction, Bible, computer science dissertation abstracts, operational system user manuals, newspapers and European Parliament proceedings. This corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus, statistical information and translation dictionaries (Specia et al., 2005) , followed by a manual revision. For each verb, the sense repository was defined as the set of all the possible translations of that verb in the corpus. 80% of the corpus was randomly selected and used for training, with the remainder retained for testing. The 10 verbs, number of possible translations and the percentage of sentences for each verb which use the most frequent translation are shown in Table 1 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The text was randomly selected from corpora of different domains and genres, including literary fiction, Bible, computer science dissertation abstracts, operational system user manuals, newspapers and European Parliament proceedings. ", "mid_sen": "This corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus, statistical information and translation dictionaries (Specia et al., 2005) , followed by a manual revision. ", "after_sen": "For each verb, the sense repository was defined as the set of all the possible translations of that verb in the corpus. "}
{"citeStart": 174, "citeEnd": 196, "citeStartToken": 174, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "the dependency from the event head to an event argument dep i,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "| | | | | | − Arguments − −", "mid_sen": "the dependency from the event head to an event argument dep i,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004) .", "after_sen": ""}
{"citeStart": 57, "citeEnd": 76, "citeStartToken": 57, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "For the local polarity classifier, we employ opinion lexicons, dialog information, and unigram fea- tures. We use lexicons that have been successfully used in previous work (the polarity lexicon from (Wilson et al., 2005) and the arguing lexicon (Somasundaran et al., 2007) ). Previous work used features based on parse trees, e.g., (Wilson et al., 2005; Kanayama and Nasukawa, 2006) , but our data has very different characteristics from monologic texts -the utterances and sentences are much shorter, and there are frequent disfluencies, restarts, hedging and repetitions. Because of this, we cannot rely on parsing features. On the other hand, in this data, we have dialog act information 1 (Dialog Acts), which we can exploit. Note that the IPC uses only the Dialog Act tags (instance level tags like Inform, Suggest) and not the dialog structure information.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use lexicons that have been successfully used in previous work (the polarity lexicon from (Wilson et al., 2005) and the arguing lexicon (Somasundaran et al., 2007) ). ", "mid_sen": "Previous work used features based on parse trees, e.g., (Wilson et al., 2005; Kanayama and Nasukawa, 2006) , but our data has very different characteristics from monologic texts -the utterances and sentences are much shorter, and there are frequent disfluencies, restarts, hedging and repetitions. ", "after_sen": "Because of this, we cannot rely on parsing features. "}
{"citeStart": 95, "citeEnd": 106, "citeStartToken": 95, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. Preliminary experiments acquiring a few verbal subcategorization classes have been reported by Brent (1991 Brent ( , 1993 , Manning (1993), and Ushioda et al. (1993) . In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. attempt to derive relative subcategorization frequency for individual predicates.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. ", "mid_sen": "Preliminary experiments acquiring a few verbal subcategorization classes have been reported by Brent (1991 Brent ( , 1993 , Manning (1993), and Ushioda et al. (1993) . ", "after_sen": "In these experiments the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al. attempt to derive relative subcategorization frequency for individual predicates."}
{"citeStart": 0, "citeEnd": 19, "citeStartToken": 0, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. (Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. However, their subject and object finders are independent of their chunker (i.e. not cascaded).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "He suggests to first find the chunks and then the dependecies between these chunks. ", "mid_sen": "Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. ", "after_sen": "Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. "}
{"citeStart": 22, "citeEnd": 43, "citeStartToken": 22, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the data from (Mohammad et al., 2009) ... Naturally, our current work on question answering for the reading comprehension task is most related to those of (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000 ; Wang et al. , 2000) . In fact, all of this body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the same development set of stories. The work of (Hirschman et al. , 1999) initiated this series of work, and it reported an accuracy of 36.3% on answering the questions in the test stories. Subsequently, the work of (Riloffand Thelen , 2000) and (Chaxniak et al. , 2000) improved the accuracy further to 39.7% and 41%, respectively. However, all of these three systems used handcrafted, deterministic rules and algorithms... ...The cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM > S-SVM > Retrieval-M. Compared with retrieval-based [Yang et al. 2003 ], pattern-based [Ravichandran et al. 2002 and Soubbotin et al. 2002 ], and deep NLP-based [Moldovan et al. 2002 , Hovy et al. 2001 and Pasca et al. 2001] answer selection, machine learning techniques are more effective in constructing QA components from scratch. These techniques suffer, however, from the problem of requiring an adequate number of handtagged question-answer training pairs. It is too expensive and labor intensive to collect such training pairs for supervised machine learning techniques ... ... As expected, the definition and person-bio answer types are covered well by these resources. The web has been employed for pattern acquisition (Ravichandran et al. , 2003) , document retrieval (Dumais et al. , 2002) , query expansion (Yang et al. , 2003) , structured information extraction, and answer validation (Magnini et al. , 2002) . Some of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a less complex approach to find correct answers ... Table 8 : A portion of the QA survey generated by LexRank using the context information. that contains two sets of cited papers and corresponding citing sentences, one on Question Answering (QA) with 10 papers and the other on Dependency Parsing (DP) with 16 papers. The QA set contains two different sets of nuggets extracted by experts respectively from paper abstracts and citation sentences. The DP set includes nuggets extracted only from citation sentences. We use these nugget sets, which are provided in form of regular expressions, to evaluate automatically generated summaries. To perform this experiment we needed to build a new corpus that includes context sentences. For each citation sentence, BP 4 is used on the citing paper to extract the proper context. Here, we limit the context size to be 4 on each side. That is, we attach to a citing sentence any of its 4 preceding and following sentences if citation survey context survey QA CT nuggets 0.416 0.634 AB nuggets 0.397 0.594 DP CT nuggets 0.324 0.379 Table 9 : Pyramid F β=3 scores of automatic surveys of QA and DP data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here, we show how the surveys generated using citations and their context sentences are better than those generated using citation sentences alone.", "mid_sen": "We use the data from (Mohammad et al., 2009) ... Naturally, our current work on question answering for the reading comprehension task is most related to those of (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000 ; Wang et al. , 2000) . ", "after_sen": "In fact, all of this body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the same development set of stories. "}
{"citeStart": 66, "citeEnd": 83, "citeStartToken": 66, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "We have thus studied new means of capturing the semantic context, other than the frame, which can be easily annotated on FrameNet and are available on a larger scale (i.e. have a better coverage). A very good candidate seems to be the Intersective Levin classes (Dang et al., 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000) . Thus, we have designed a semiautomatic algorithm for assigning an Intersective Levin class to each FrameNet verb predicate.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have thus studied new means of capturing the semantic context, other than the frame, which can be easily annotated on FrameNet and are available on a larger scale (i.e. have a better coverage). ", "mid_sen": "A very good candidate seems to be the Intersective Levin classes (Dang et al., 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000) . ", "after_sen": "Thus, we have designed a semiautomatic algorithm for assigning an Intersective Levin class to each FrameNet verb predicate."}
{"citeStart": 163, "citeEnd": 189, "citeStartToken": 163, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "Once a set of beliefs forming justification chains is identified, the system must then select from this set those belief chains which, when presented to the user, are predicted to convince the user of .bel. Our system will first construct a singleton set for each such justification chain and select the sets containing justification which, when presented, is predicted to convince the user of _bel. If no single justification chain is predicted to be sufficient to change the nser's beliefs, new sets will be constructed by combining the single justification chains, and the selection ~ is repeated. This will produce a set of possible candidate justification chains, and three heuristics will then be applied to select from among them. The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form (Luchok and McCroskey, 1978) . Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. The second heuristic prefers evidence that is novel to the user, since studies have shown that evidence is most persuasive ff it is previously unknown to the hearer (Wyer, 1970; Morley, 1987) . The third heuristic is based on C.nice's maxim of quantity and prefers justification chains that contain the fewest beliefs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This will produce a set of possible candidate justification chains, and three heuristics will then be applied to select from among them. ", "mid_sen": "The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form (Luchok and McCroskey, 1978) . ", "after_sen": "Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. "}
{"citeStart": 68, "citeEnd": 96, "citeStartToken": 68, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "However, most strategies are based on internal or external methods (Grabar and Zweigenbaum, 2002) , i.e. methods that rely on the form of terms or on the information gathered from contexts. (In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identication process.) The work reported here infers specic semantic relationships based on sets of examples and counterexamples.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Dierent strategies have been developed in order to identify pairs of terms that share a specic semantic relationship (such as hyperonymy or meronymy) or to build classes of terms.", "mid_sen": "However, most strategies are based on internal or external methods (Grabar and Zweigenbaum, 2002) , i.e. methods that rely on the form of terms or on the information gathered from contexts. ", "after_sen": "(In some cases, an additional resource, such as a dictionary or a thesaurus, is used during the identication process.) The work reported here infers specic semantic relationships based on sets of examples and counterexamples."}
{"citeStart": 191, "citeEnd": 213, "citeStartToken": 191, "citeEndToken": 213, "sectionName": "UNKNOWN SECTION NAME", "string": "Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006) , respectively. We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our system showed clear improvement over many of the machine-learning-based systems reported to date, and also proved comparable to the existing state-ofthe-art systems that use rule-based post-processing.", "mid_sen": "Our future plans include further sophistication of features, such as the use of external gazetteers which is reported to improve the F-score by 1.0 and 2.7 points in (Zhou and Su, 2004) and (Friedrich et al., 2006) , respectively. ", "after_sen": "We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase."}
{"citeStart": 117, "citeEnd": 149, "citeStartToken": 117, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "Our results agree with previous work on Arabic and Hebrew in that marking the definite article is helpful for parsing. We go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; . Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We go beyond previous work, however, and explore additional lexical and inflectional features. ", "mid_sen": "Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; . ", "after_sen": "Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. "}
{"citeStart": 234, "citeEnd": 258, "citeStartToken": 234, "citeEndToken": 258, "sectionName": "UNKNOWN SECTION NAME", "string": "We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008) . We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005) , or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error,", "mid_sen": "We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008) . ", "after_sen": "We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005) , or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006) ."}
{"citeStart": 287, "citeEnd": 320, "citeStartToken": 287, "citeEndToken": 320, "sectionName": "UNKNOWN SECTION NAME", "string": "These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994) . However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983) . Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Speech repairs, for example, are often signaled by syntactic anomalies. ", "mid_sen": "Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994) . ", "after_sen": "However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983) . "}
{"citeStart": 76, "citeEnd": 100, "citeStartToken": 76, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis 3 . In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999) , and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set. We also used a thresholding technique which discards features with low frequency. This is also optimized using the development set, and the best threshold was 4 for the MEMM tagger, and 50 for the reranker 4 . For both of the MEMM tagger and reranker, combinations of feature classes are manually selected to improve the accuracies on the development set. Our final models include 49 and 148 feature class combinations for the MEMM tagger and reranker, respectively. Table 3 shows the performance of the MEMM tagger on the development set. As reported in many of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006) , features of shallow parsers had a large contribution to the performance. The information of the previous labels was also quite effective, which indicates that label unigram models (i.e. 0th order Markov models, so to speak) would have been insufficient for good performance.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis 3 . ", "mid_sen": "In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999) , and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set. ", "after_sen": "We also used a thresholding technique which discards features with low frequency. "}
{"citeStart": 213, "citeEnd": 226, "citeStartToken": 213, "citeEndToken": 226, "sectionName": "UNKNOWN SECTION NAME", "string": "The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog, but instead needs to be implemented by additional programming. While theoretically possible, it becomes quite problematic to actually implement. The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. ", "mid_sen": "This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "after_sen": ""}
{"citeStart": 152, "citeEnd": 172, "citeStartToken": 152, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "Collocations have been studied by computational linguists in different contexts. For instance, there is a substantial body of papers on the extraction of \"frequently co-occurring words\" from corpora using statistical methods (e.g., (Choueka et al., 1983) , (Church and Hanks, 1989) , (Smadja, 1993) to list only a few). These authors focus on techniques for providing material that can be used in other processing tasks such as", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Collocations have been studied by computational linguists in different contexts. ", "mid_sen": "For instance, there is a substantial body of papers on the extraction of \"frequently co-occurring words\" from corpora using statistical methods (e.g., (Choueka et al., 1983) , (Church and Hanks, 1989) , (Smadja, 1993) to list only a few). ", "after_sen": "These authors focus on techniques for providing material that can be used in other processing tasks such as"}
{"citeStart": 8, "citeEnd": 12, "citeStartToken": 8, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "As noted in (Partee, 1984) , this analysis does not extend in a straightforward manner to cases in which the operator when is replaced by (an unrestricted) before or after, in such quantified contexts. Constructing a similar DRS for such sentences gives the wrong truth conditions. For example, Figure la shows a DRS for sentence 1, according to the principles above, rl -the reference time, used for the interpretation of the main clause is placed in the universe of the antecedent box. Because the temporal connective is 'before', rl is restricted to lie before el. The embedding conditions determine, that this reference time be universally quantified over, causing an erroneous reading in which for each event, el, of John's calling, for each earlier time rl, he lights up a cigarette. Paraphrasing this, we could say that John lights up cigarettes at all times preceding each phone call, not just once preceding each phone call. We did not encounter this problem in the DRS in Figure 4 , since although the reference time rl, is universally quantified over in that DRS as well, it is also restricted, to immediately follow el. It is similarly restricted if 'before' is replaced with 'just before' or 'ten minutes before'. But, (unrestricted) 'before' is analyzed as 'some time before', and thus the problem arises. We will henceforth informally refer to this problem as Partee's quantification problem. Partee (1984) suggests that in these cases we somehow have to insure that the reference time, rz, appears in the universe of the consequent DRS, causing it to be existentially quantified over, giving the desired interpretation. De Swart (1991) notes that simply moving rl to the right-hand box does not agree with Hinrichs' assumption, that temporal clauses are processed before the main clause, since they update the reference time, with respect to which the main clause will be inter-preted. In our proposed solution, the 'reference time' is indeed moved to the right box, but it is a different notion of reference time, and (as will be shown) exempt from this criticism.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We will henceforth informally refer to this problem as Partee's quantification problem. ", "mid_sen": "Partee (1984) suggests that in these cases we somehow have to insure that the reference time, rz, appears in the universe of the consequent DRS, causing it to be existentially quantified over, giving the desired interpretation. ", "after_sen": "De Swart (1991) notes that simply moving rl to the right-hand box does not agree with Hinrichs' assumption, that temporal clauses are processed before the main clause, since they update the reference time, with respect to which the main clause will be inter-preted. "}
{"citeStart": 12, "citeEnd": 29, "citeStartToken": 12, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "We further optimized string recognition and plurality detection for handling citation-strings. See Table 3 for the full list of our features. While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999) . SVMs are known for being reliable and having good performance.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "See Table 3 for the full list of our features. ", "mid_sen": "While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999) . SVMs are known for being reliable and having good performance.", "after_sen": ""}
{"citeStart": 169, "citeEnd": 182, "citeStartToken": 169, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. The version proposed here combines a basic insight from Lewin (1990) with higher-order unification to give an analysis that has a strong resemblance to that proposed in Pereira (1990 Pereira ( , 1991 , with some differences that are commented on below. Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. We analyze quantified NPs at the QLF level as illustrated in the QLF for:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We can implement a deductive theory of quantifier scope using the conditional equivalence mechanism. ", "mid_sen": "The version proposed here combines a basic insight from Lewin (1990) with higher-order unification to give an analysis that has a strong resemblance to that proposed in Pereira (1990 Pereira ( , 1991 , with some differences that are commented on below. ", "after_sen": "Like Pereira's approach, it avoids the need for a free variable constraint, nor does it need the explicit recursion on the quantifier restriction imposed by Lewin. "}
{"citeStart": 10, "citeEnd": 22, "citeStartToken": 10, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "Since we want to study the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant-absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since we want to study the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. ", "mid_sen": "Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant-absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. ", "after_sen": "What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference."}
{"citeStart": 32, "citeEnd": 45, "citeStartToken": 32, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language. Techniques such as restriction (Shieber, 1985) can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Attention is restricted here to approximations of context-free grammars because context-free languages are the smallest class of formal language that can realistically be applied to the analysis of natural language. ", "mid_sen": "Techniques such as restriction (Shieber, 1985) can be used to construct context-free approximations of many unification-based formalisms, so techniques for constructing finite-state approximations of context-free grammars can then be applied to these formalisms too.", "after_sen": ""}
{"citeStart": 139, "citeEnd": 159, "citeStartToken": 139, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997 ) with the Alembic system (Aberdeen et al. 1995) : a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989) , who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000) . We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. ", "mid_sen": "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997 ) with the Alembic system (Aberdeen et al. 1995) : a 0.5% error rate. ", "after_sen": "The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989) , who trained a decision tree classifier on a 25-million-word corpus. "}
{"citeStart": 60, "citeEnd": 79, "citeStartToken": 60, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes methods (Teh et al., 2008) for DPs to adaptor grammars and other linguistic applications of HDPs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. ", "mid_sen": "It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes methods (Teh et al., 2008) for DPs to adaptor grammars and other linguistic applications of HDPs.", "after_sen": ""}
{"citeStart": 97, "citeEnd": 98, "citeStartToken": 97, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "My claim is that mformationally redundant utterances (IRU's) have two main discourse functions: (1) to provide EVIUENCI~ to support the assumptions underlying the inference of mutual beliefs, (2) to CENTER a proposition, is. make or keep a proposition salient [6] . This paper will focus on (1) leaving (2) for future work.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So why does Ray (r) in (8) RF~PEAT Harry's (h) assertion of it does, and why does lfarry PARAPHRASE himself and Kay in (9) ?", "mid_sen": "My claim is that mformationally redundant utterances (IRU's) have two main discourse functions: (1) to provide EVIUENCI~ to support the assumptions underlying the inference of mutual beliefs, (2) to CENTER a proposition, is. make or keep a proposition salient [6] . ", "after_sen": "This paper will focus on (1) leaving (2) for future work."}
{"citeStart": 136, "citeEnd": 160, "citeStartToken": 136, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset. This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments. However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions. While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a) . The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998 ). And Collins (2000) argues for \"keeping track of counts of arbitrary fragments within parse trees\", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The importance of including nonheadwords has become uncontroversial (e.g. Collins 1999; Charniak 2000; Goodman 1998 ). ", "mid_sen": "And Collins (2000) argues for \"keeping track of counts of arbitrary fragments within parse trees\", which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992) .", "after_sen": "Thus the major innovations of DOP are:"}
{"citeStart": 37, "citeEnd": 47, "citeStartToken": 37, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper gives an overview of how natural language is converted to a representation that the neural nets can handle, and how the problem is reduced to a manageable size. It then outlines the neural net selection process. A comprehensive account is given in (Lyon, 1994) ; descriptions of the neural net process are also in (Lyon, 1993; Lyon and Frank, 1992) . This is a hybrid system. The core process is data driven, as the parameters of the neural networks are derived from training text. The neural net is trained in supervised mode on examples that have been manually marked \"correct\" and \"incorrect\". It will then be able to classify unseen examples. However, the initial processing stages, in which the problem size is constrained, operate within a skeletal grammatic framework. Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints. The pruning process is remarkably effective.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It then outlines the neural net selection process. ", "mid_sen": "A comprehensive account is given in (Lyon, 1994) ; descriptions of the neural net process are also in (Lyon, 1993; Lyon and Frank, 1992) . ", "after_sen": "This is a hybrid system. "}
{"citeStart": 174, "citeEnd": 200, "citeStartToken": 174, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we experiment with hierarchical Levin-style clustering. We adopt as our baseline method a well-known hierarchical method -agglomerative clustering (AGG) -which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008) . The method has also been popular in the related task of noun clus-tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we experiment with hierarchical Levin-style clustering. ", "mid_sen": "We adopt as our baseline method a well-known hierarchical method -agglomerative clustering (AGG) -which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Ferrer, 2004; Schulte im Walde, 2008) . ", "after_sen": "The method has also been popular in the related task of noun clus-tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011) ."}
{"citeStart": 98, "citeEnd": 120, "citeStartToken": 98, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances (Almuallim and Dietterich, 1991, Langley and Sage, in press ). Unfortunately, the task of designing an appropriate instance representation --also known as feature set selection --can be extraordinarily difficult, time-consuming, and knowledge-intensive (Quinlan, 1983) . This poses a problem for current statistical and machine learning approaches to natural language understanding where a new instance representation is typically required for each linguistic task tackled.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). ", "mid_sen": "Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . ", "after_sen": "In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. "}
{"citeStart": 52, "citeEnd": 69, "citeStartToken": 52, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "bels. We performed the experiments with the weka toolkit (Hall et al., 2009) , using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm. Figure 4 With cross-lingual settings, we loose from about 4% to 8% accuracy, and with the higher quality SMT system (LM filter), CL-b setting is slightly better than CL-a. The same three experiments were conducted for the ES test set (last three rows of Table 3 ). We achieved an accuracy of 81.1% in the monolingual case. Here the CL-b setting achieved a clearly better accuracy than the CL-a setting (at least 5% more), and only from 2.3% to 3.5% below the monolingual one. Thus with the higher quality SMT system, it is always better to translate the test data (CL-b setting) than the training corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "bels. ", "mid_sen": "We performed the experiments with the weka toolkit (Hall et al., 2009) , using a filter to convert strings into word vectors, and two learning algorithms: SVMs and bagging with Fast Decision Tree Learner as base algorithm. ", "after_sen": "Figure 4 With cross-lingual settings, we loose from about 4% to 8% accuracy, and with the higher quality SMT system (LM filter), CL-b setting is slightly better than CL-a. "}
{"citeStart": 113, "citeEnd": 134, "citeStartToken": 113, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de ned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999) . Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sentences y f r o m a s e t Y, observed with empirical Input Reference model p 0 , property-functions vector with constant # , parses X(y) for each y in incomplete-data sample from Y. Output MLE model p on X. Procedure", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . ", "mid_sen": "For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. ", "after_sen": "However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. "}
{"citeStart": 251, "citeEnd": 268, "citeStartToken": 251, "citeEndToken": 268, "sectionName": "UNKNOWN SECTION NAME", "string": "Furthermore, we describe a software toolchain for easy extraction of RDF data from existing information structures, such as classes or database records, and delivery of this data via web applications and services based on the popular framework Rails (Ruby et al., 2011) . This tool chain is designed to be easy to integrate with existing libraries in a plugin-like fashion, in order to reduce the effort of integrating existing systems into Linked Data networks and infrastructures.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "source more widely available and to enable a long and successful lifecycle for the resource.", "mid_sen": "Furthermore, we describe a software toolchain for easy extraction of RDF data from existing information structures, such as classes or database records, and delivery of this data via web applications and services based on the popular framework Rails (Ruby et al., 2011) . ", "after_sen": "This tool chain is designed to be easy to integrate with existing libraries in a plugin-like fashion, in order to reduce the effort of integrating existing systems into Linked Data networks and infrastructures."}
{"citeStart": 43, "citeEnd": 60, "citeStartToken": 43, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999) , which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. Section 2 overviews our CLIR system, and Section 3 elaborates on the translation module focusing on compound word translation and transliteration. Section 4 then evaluates the effectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999) , which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. ", "mid_sen": "For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . ", "after_sen": "Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. "}
{"citeStart": 34, "citeEnd": 57, "citeStartToken": 34, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus the present system is unlike SMT (Och and Ney, 2003) , where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002) , which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The challenge is to generate a list of usable solutions and to rank them such that the best are at the top.", "mid_sen": "Thus the present system is unlike SMT (Och and Ney, 2003) , where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. ", "after_sen": "It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002) , which allows a one-to-one correspondence irrespective of the context. "}
{"citeStart": 97, "citeEnd": 114, "citeStartToken": 97, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003) . We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007) , Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by. Cao et al. (2007) , like us, used a 300GB collection of web documents as input. They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learning and does not make a distinction between translations and transliterations. Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The extracted translations may serve as training data for statistical machine translation systems. ", "mid_sen": "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003) . ", "after_sen": "We then added the extracted translation pairs as additional parallel training corpus. "}
{"citeStart": 64, "citeEnd": 82, "citeStartToken": 64, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997) . Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998) . We assume a good algorithm is one that finds the most prominent topic boundaries.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997) . ", "after_sen": "Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. "}
{"citeStart": 161, "citeEnd": 186, "citeStartToken": 161, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de ned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999) . Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sentences y f r o m a s e t Y, observed with empirical Input Reference model p 0 , property-functions vector with constant # , parses X(y) for each y in incomplete-data sample from Y. Output MLE model p on X. Procedure", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . ", "after_sen": "For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. "}
{"citeStart": 64, "citeEnd": 83, "citeStartToken": 64, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "A similar problem concerns examples such as the following, from Gibson et al (1993) : in the above, Gibson et al have manipulated number agreement to force low (12), middle (13) and high (14) attachment of the bracketed relative clause. The results of their on-and off-line experiments show clearly that the low attachment (corresponding to 12) is easiest, but the middle attachment (corresponding to (13)) is most difficult. This behaviour cannot be captured whether we adopt a bottom-up or a top-down search for tree-lowering. However, even if we can incorporate the required preferences into the parser, the constraint of incrementality will force us to make the decision on encountering that. This means that, assuming we decide initially to attach low, but number agreement on was subsequently forces high attachment, as in 14, then a conscious garden path effect will be predicted, as lowering cannot derive the reanalysis. This is true on the abstract level as well, since there will be nodes in the description which precede the original low position of the relative clause, but are dominated by the subsequent high position of the relative clause.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "7", "mid_sen": "A similar problem concerns examples such as the following, from Gibson et al (1993) : in the above, Gibson et al have manipulated number agreement to force low (12), middle (13) and high (14) attachment of the bracketed relative clause. ", "after_sen": "The results of their on-and off-line experiments show clearly that the low attachment (corresponding to 12) is easiest, but the middle attachment (corresponding to (13)) is most difficult. "}
{"citeStart": 94, "citeEnd": 106, "citeStartToken": 94, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988) , who identifies and quantifies the linguistic features associated with different spoken and written text types. Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000) . Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation. A practical discussion of a central technical concern is Vossen (2001) , which tailors a general-language resource for a domain. Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures. His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The word representative has tended to fall out of discussions, to be replaced by the meeker balanced.", "mid_sen": "The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988) , who identifies and quantifies the linguistic features associated with different spoken and written text types. ", "after_sen": "Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. "}
{"citeStart": 167, "citeEnd": 190, "citeStartToken": 167, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "We should note that PROFILE is part of a large system for information retrieval and summarization of news through information extraction and symbolic text generation (McKeown and Radev, 1995) . We intend to use PROFILE to improve lexical choice in the summary generation component, especially when producing user-centered summaries or summary updates (Radev and McKeown, 1998 to appear). There are two particularly appealing cases -(1) when the extraction component has failed to extract a description and (2) when the user model (user's interests, knowledge of the entity and personal preferences for sources of information and for either conciseness or verbosity) dictates that a description should be used even when one doesn't appear in the texts being summarized.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We should note that PROFILE is part of a large system for information retrieval and summarization of news through information extraction and symbolic text generation (McKeown and Radev, 1995) . ", "after_sen": "We intend to use PROFILE to improve lexical choice in the summary generation component, especially when producing user-centered summaries or summary updates (Radev and McKeown, 1998 to appear). "}
{"citeStart": 181, "citeEnd": 202, "citeStartToken": 181, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "(a) A \"semantic distance\" rating between the new DCU and each previous thread is determined. (If there are no existing threads a new thread is started.) (b) Other preferences, such as a preference for relating the new DCU to a thread with parallel tense, are employed (See (Kameyama et al., 1993; Poesio, 1994) for details), and the resulting ratings are factored into the rating for each thread. (c) If the thread currently being followed is among the highest rated threads, this thread is continued. (This corresponds to temporal centering's preference to continue the current thread.) (d) If not, the DCU may continue any of the highest rated threads, and each of these solutions is generated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(a) A \"semantic distance\" rating between the new DCU and each previous thread is determined. ", "mid_sen": "(If there are no existing threads a new thread is started.) (b) Other preferences, such as a preference for relating the new DCU to a thread with parallel tense, are employed (See (Kameyama et al., 1993; Poesio, 1994) for details), and the resulting ratings are factored into the rating for each thread. ", "after_sen": "(c) If the thread currently being followed is among the highest rated threads, this thread is continued. "}
{"citeStart": 114, "citeEnd": 129, "citeStartToken": 114, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006) . We used the revised experimental setup (Gurevych, 2005) , based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. We annotated semantic relatedness instead of similarity and included also non noun-noun pairs. Additionally, our corpusbased approach includes domain-specific technical terms and enables evaluation of the robustness of a measure. Figure 1 gives an overview of our automatic corpus-based system for creating test datasets for evaluating SR measures.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To our knowledge, this study is the first to include concept pairs and to automatically generate the test dataset.", "mid_sen": "In our experiment, we annotated a high number of pairs similar in size to the test sets by Finkelstein (2002) and Gurevych (2006) . ", "after_sen": "We used the revised experimental setup (Gurevych, 2005) , based on discrete relatedness scores and presentation of word pairs in isolation, that is scalable to the higher number of pairs. "}
{"citeStart": 68, "citeEnd": 81, "citeStartToken": 68, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "where type1 to typen are exhaustive and disjoint subtypes of type entry, entry need not necessarily be a single type; it can be a logical expression over types formed with the connectors AND and oR. A systemic grammar therefore resembles more a type lattice than a type hierarchy in the HPSG tradition. In systemic grammar, these basic type axioms, the systems, are named; we will use entry(s) to denote the left-hand side of some named system s, and out(s) to denote the set of subtypes {type1, type2, ..., type,}- the output of the system. The following type axioms taken from the large systemic English grammar NXGI~L (Matthiessen, 1983) The meaning of these type axioms is fairly obvious: Nominal groups can be subcategorized in classnames and individual-names on the one hand, they can be subcategorized with respect to their WHcontainment into WH-containing nominal-groups and nominal-groups without WH-element on the other hand. Universal principles and rules are in systemic grammar not factored out. The lexicon contains stem forms and has a detailed word class type hierarchy at its top. Morphology is also organized as a monotonic type hierarchy. Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Morphology is also organized as a monotonic type hierarchy. ", "mid_sen": "Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "after_sen": "Our subgrammar extraction has been applied and tested in the context of the KPML environment. "}
{"citeStart": 144, "citeEnd": 164, "citeStartToken": 144, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "We have presented novel algorithms for efficient subsumption checking and pro-and retroactive local ambiguity packing with large feature structures, and have provided strong empirical evidence that our approach can be applied beneficially to chart parsing with a large, broad-coverage HPSG of English. By comparison to previous work in unification-based parsing we have demonstrated that pro-and retroactive packing are well-suited to achieve optimal packing; furthermore, experimental results obtained with a publicly-available HPSG processing platform confirm that ambiguity packing can greatly reduce average parse complexity for this type of grammars. In related work, Miyao (1999) describes an approach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments. Although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. (1999) , who report large speed-ups from the elimination of disjunction processing during unification. Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In related work, Miyao (1999) describes an approach to packing in which alternative feature structures are represented as packed, distributed disjunctions of feature structure fragments. ", "mid_sen": "Although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. (1999) , who report large speed-ups from the elimination of disjunction processing during unification. ", "after_sen": "Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insufficient to allow a precise comparison."}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (\"certain\" and \"right\") with their immediate neighbors.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Brill et al. (1990) try to infer grammatical category from bigram statistics. ", "mid_sen": "Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. ", "after_sen": "Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. "}
{"citeStart": 298, "citeEnd": 310, "citeStartToken": 298, "citeEndToken": 310, "sectionName": "UNKNOWN SECTION NAME", "string": "Northwest Airlines settled the remaining lawsuits filed on behalf of 156 people killed in a 1987 crash, but claims against the jetliner's maker are being pursued, a federal judge said. (\"Northwest Airlines Settles Rest of Suits,\" Wall Street Journal, November 1, 1989) A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues. It is most fully presented in Wiebe and Rapaport (1986 , 1988 , 1991 and Wiebe (1990 Wiebe ( , 1994 . It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory. The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973 Kuroda ( , 1976 ) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory. ", "mid_sen": "The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973 Kuroda ( , 1976 ) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) ", "after_sen": ""}
{"citeStart": 132, "citeEnd": 145, "citeStartToken": 132, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "To summarize: we define ranked alphabets in a standard way, adding the requirements that every sort includes a countable infinity of variables, a finite number of function letters, and at least one ground term. We then define the set of terms in a standard way. All unification in this paper is unification of terms, as in Robinson 1965--not graphs or other structures, as in much recent work (Shieber 1985b) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We then define the set of terms in a standard way. ", "mid_sen": "All unification in this paper is unification of terms, as in Robinson 1965--not graphs or other structures, as in much recent work (Shieber 1985b) .", "after_sen": "A unification grammar is a five-tuple G = (S, (~,r) T, P, Z) where S is a set of sorts, (~,r) an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in (~,r). "}
{"citeStart": 141, "citeEnd": 163, "citeStartToken": 141, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "If word A is used in the definition of word B, t.he,m words are expected to be strongly related. This is the basis of our hypothesis that the distances in the refi~rence network reflect the associative distances between words (Nitta 1933 Vdronis and Ide (1990) for word sense disainl)iguation) and as fields for artificial association, such its spreading activation (by Kojiina and l:urugori (1993) for context-coherence measurement). The distance vector of a word can be considered to be a list, of the activation strengths at the origin nodes when the word node is activated. Therefore, distance w~ctors can be expected to convey almost the santo information as the entire network, and clearly they are Ili~icli easier to handle.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If word A is used in the definition of word B, t.he,m words are expected to be strongly related. ", "mid_sen": "This is the basis of our hypothesis that the distances in the refi~rence network reflect the associative distances between words (Nitta 1933 Vdronis and Ide (1990) for word sense disainl)iguation) and as fields for artificial association, such its spreading activation (by Kojiina and l:urugori (1993) for context-coherence measurement). ", "after_sen": "The distance vector of a word can be considered to be a list, of the activation strengths at the origin nodes when the word node is activated. "}
{"citeStart": 60, "citeEnd": 86, "citeStartToken": 60, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "Among the statistical approaches, the Maximum Entropy framework has a very strong position. Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words. This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Among the statistical approaches, the Maximum Entropy framework has a very strong position. ", "mid_sen": "Nevertheless, a recent independent comparison of 7 taggets (Zavrel and Daelemans, 1999) has shown that another approach even works better: Markov models combined with a good smoothing technique and with handling of unknown words. ", "after_sen": "This tagger, TnT, not only yielded the highest accuracy, it also was the fastest both in training and tagging."}
{"citeStart": 306, "citeEnd": 330, "citeStartToken": 306, "citeEndToken": 330, "sectionName": "UNKNOWN SECTION NAME", "string": "Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) ; the COMLEX Syntax dictionary, Grishman et al. (1994) ). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989 ; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) ; the COMLEX Syntax dictionary, Grishman et al. (1994) ). ", "mid_sen": "Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989 ; see also section 3.1 below for an example). ", "after_sen": "Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. "}
{"citeStart": 80, "citeEnd": 104, "citeStartToken": 80, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we investigate whether the same techniques can be applied in case the grammar is a constraint-based grammar rather than a CFG. For specificity we will take the grammar to be a Definite Clause Grammar (DCG) (Pereira and Warren, 1980) . A DCG is a simple example of a family of constraintbased grammar formalisms that are widely used in natural language analysis (and generation). The main findings of this paper can be extended to other members of that family of constraint-based grammar formalisms.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we investigate whether the same techniques can be applied in case the grammar is a constraint-based grammar rather than a CFG. ", "mid_sen": "For specificity we will take the grammar to be a Definite Clause Grammar (DCG) (Pereira and Warren, 1980) . ", "after_sen": "A DCG is a simple example of a family of constraintbased grammar formalisms that are widely used in natural language analysis (and generation). "}
{"citeStart": 156, "citeEnd": 169, "citeStartToken": 156, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we will address the problem of sense discrimination as defined above. That is, we will not be concerned with the sense-labeling component of word sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. ", "mid_sen": "Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "after_sen": "What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses."}
{"citeStart": 16, "citeEnd": 20, "citeStartToken": 16, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "We have followed the same approach with French spatial prepositions, but using a structuration of the space induced by the location introduced in the PP by the preposition, and not induced by the lref as for verbs. Following Laur (1993) , we consider simple prepositions (like in) as well as prepositional phrases (like in front o]). We have classified 199 such French prepositions into 16 groups using in addition of our zones two other criteria:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have followed the same approach with French spatial prepositions, but using a structuration of the space induced by the location introduced in the PP by the preposition, and not induced by the lref as for verbs. ", "mid_sen": "Following Laur (1993) , we consider simple prepositions (like in) as well as prepositional phrases (like in front o]). ", "after_sen": "We have classified 199 such French prepositions into 16 groups using in addition of our zones two other criteria:"}
{"citeStart": 182, "citeEnd": 209, "citeStartToken": 182, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "The weights are automatically determined by the multiple regression analysis. We divide 14 texts into 7 groups each consisting of 2 texts and use 6 groups for training and the remaining group for test. Changing the group for the test, we evaluate the performance by the cross validation (Weiss and Kulikowski, 1991) . 5. Use only selected cues by applying the stepwise method. As mentioned in section 4, we use the stepwise method for selecting useful cues for training sets. The condition is the same as for the case 4 except for the cue selection. 6. Answer from five human subjects. By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task (Passonneau and Litman, 1993; Gale et al., 1992) . Table 1 shows the 5 subjects' mean performance of 14 texts (experiment 6). We think table 1 shows the upper bound of the performance of the text segmentation task. We also calculate the lower bound of the performance of the task(\"lowerbound\" in figure 2). It can be calculated by considering the case where the system selects boundary candidates at random. In the case, the precision equals to the mean probability that each candidate will be a correct boundary. The recall is equal to the ratio of outputs. In figure 1, comparing the performance among the case without lexical chains(\"ex.l\"), the one only with lexical chains(\"ex.2\"), and the one with multiple linguistic cues(\"ex.3\"), the results show that better performance can be yielded by using the whole set of the cues. In figure 2, comparing the performance of the case where the hand-tuned weights are used for multiple linguistic cues(\"ex.3\") and the one where the automatic weights are determined with the training texts(\"ex.4.test'), the results show that better performance can be yielded by automatically training the weights in general. Furthermore, since it can avoid the labor-intensive work and yield objective weights, automatic weighting is better than handtuning.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "6. Answer from five human subjects. ", "mid_sen": "By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task (Passonneau and Litman, 1993; Gale et al., 1992) . ", "after_sen": "Table 1 shows the 5 subjects' mean performance of 14 texts (experiment 6). "}
{"citeStart": 177, "citeEnd": 195, "citeStartToken": 177, "citeEndToken": 195, "sectionName": "UNKNOWN SECTION NAME", "string": "The TAG formalism (for a recent introduction, see [Joshi 1987a ]) is well suited for linguistic description because (1) it provides a larger domain of locality than a CFG or other augmented CFG-based formalisms such as tlPSG or LFG, and (2) it allows factoring of recursion from the domain of dependencies. This extended domain of locality, provided by the elementary trees of TAG, allows us to \"lexicalize\" a TAG grammar: we can associate each tree in a grammar with a lexical item [Schabes et al 1988 , Schabes 1990 ] 4. The tree will contain the lexical item, and all of its syntac-3Some verbs allow scrambling out of their Complements more freely than others. It appears that all subject-control verbs and most object-control verbs governing the dative allow scrambling fairly f~ely, while scrambling with objectcontrol verbs governing the accusative is more restricted (cir. [Bayer and Kornfilt 1989] 1985] . Ilere, \"naturally\" means that dependencies are stated within the larger domain of locality (the elementary tree), i.e., each clausal tree still contains a verb and all of its arguments. Thus, in a lexicalized TAG, the effects of long-distance movement are achieved by adjunction. The word order freedom possible in the context of unconstrained scrambling, however, eludes the scope of TAGs. In this section, we will informally argue this formal result.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The TAG formalism (for a recent introduction, see [Joshi 1987a ]) is well suited for linguistic description because (1) it provides a larger domain of locality than a CFG or other augmented CFG-based formalisms such as tlPSG or LFG, and (2) it allows factoring of recursion from the domain of dependencies. ", "mid_sen": "This extended domain of locality, provided by the elementary trees of TAG, allows us to \"lexicalize\" a TAG grammar: we can associate each tree in a grammar with a lexical item [Schabes et al 1988 , Schabes 1990 ] 4. ", "after_sen": "The tree will contain the lexical item, and all of its syntac-3Some verbs allow scrambling out of their Complements more freely than others. "}
{"citeStart": 175, "citeEnd": 188, "citeStartToken": 175, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "The optimisations improve throughput by a factor of more than three. describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF 'backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the 'residue' of features in the unification grammar that are not encoded in the backbone. In this parser, the LALR(1) technique (Aho, Sethi Ullman, 1986 ) is used, in conjunction with a graph-structured stack (Tomita, 1987) , adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF 'backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the 'residue' of features in the unification grammar that are not encoded in the backbone. ", "mid_sen": "In this parser, the LALR(1) technique (Aho, Sethi Ullman, 1986 ) is used, in conjunction with a graph-structured stack (Tomita, 1987) , adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.", "after_sen": "On each reduction the parser performs the unifications specified by the unification grammar version of the CF backbone rule being applied. "}
{"citeStart": 13, "citeEnd": 46, "citeStartToken": 13, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994 ) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE ) may be used to convert a plain text document into the acceptable input format.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our segmentation algorithm takes a list of tokenized sentences as input. ", "mid_sen": "A tokenizer (Grefenstette and Tapanainen, 1994 ) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE ) may be used to convert a plain text document into the acceptable input format.", "after_sen": ""}
{"citeStart": 62, "citeEnd": 87, "citeStartToken": 62, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1 . This data has since been used in studies by (Mooney, 1996) , (Towell and Voorhees, 1998) , and (Leacock et al., 1998) . In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These senses and their frequency distribution are shown in Table 1 . ", "mid_sen": "This data has since been used in studies by (Mooney, 1996) , (Towell and Voorhees, 1998) , and (Leacock et al., 1998) . ", "after_sen": "In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. "}
{"citeStart": 139, "citeEnd": 150, "citeStartToken": 139, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993) , robust parsing (Abney, 1991) , and general parsing (deMarcken, 1990; Charniak et al., 1994) . The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. ", "mid_sen": "Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993) , robust parsing (Abney, 1991) , and general parsing (deMarcken, 1990; Charniak et al., 1994) . ", "after_sen": "The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. "}
{"citeStart": 9, "citeEnd": 23, "citeStartToken": 9, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008) . These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. 2010combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)'s one-class HMM.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. ", "mid_sen": "Recently Lee et al. 2010combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. ", "after_sen": "However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)'s one-class HMM."}
{"citeStart": 99, "citeEnd": 113, "citeStartToken": 99, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to the nature of different punctuation marks, there are several phenomena described by Nunberg [1990] which it is useful to consider before implementing any treatment of punctuation:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "( 3)Earlier, work was halted.", "mid_sen": "In addition to the nature of different punctuation marks, there are several phenomena described by Nunberg [1990] which it is useful to consider before implementing any treatment of punctuation:", "after_sen": "Point absorption: strong point symbols (comma, dash, semicolon, etc.) absorb weaker adjacent ones (4)."}
{"citeStart": 162, "citeEnd": 185, "citeStartToken": 162, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog, but instead needs to be implemented by additional programming. While theoretically possible, it becomes quite problematic to actually implement. The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. ", "mid_sen": "This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "after_sen": ""}
{"citeStart": 169, "citeEnd": 185, "citeStartToken": 169, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea behind this is to create a specialized grammar that retains a high coverage but allows very much faster parsing. This has turned out to be possible -speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a] .1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The idea behind this is to create a specialized grammar that retains a high coverage but allows very much faster parsing. ", "mid_sen": "This has turned out to be possible -speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a] .1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. ", "after_sen": "In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation."}
{"citeStart": 115, "citeEnd": 144, "citeStartToken": 115, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "Just like top-down evaluation of the original grammar bottom-up evaluation of its magic compiled version falls prey to non-termination in the face of head recursion. It is however possible to eliminate the subsumption check through fine-tuning the magic predicates derived for a particular grammar in an off-line fashion. In order to illustrate how the magic predicates can be adapted such that the subsumption check can be eliminated it is necessary to take a closer look at the relation between the magic predicates and the facts they derive. In figure 4 the relation between the magic predicates for the example grammar is represented by an unfolding tree (Pettorossi and Proietti, 1994) . This, however, is not an ordinary unfolding tree as it is constructed on the basis of an abstract seed, i.e., a seed adorned with a specification of which arguments are to be considered bound. Note that an abstract seed can be derived from the user-specified abstract query. Only the magic part of the abstract unfolding tree is represented. The abstract unfolding tree in figure 4 clearly shows why there exists the need for subsumption checking: Rule 13 in figure 2 produces infinitely many magic_vp facts. This 'cyclic' magic rule is derived from the head-recursive vp rule in the example grammar. There is however no reason to keep this rule in the magic-compiled grammar. It influences neither the efficiency of processing with the grammar nor the completeness of the evaluation process.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to illustrate how the magic predicates can be adapted such that the subsumption check can be eliminated it is necessary to take a closer look at the relation between the magic predicates and the facts they derive. ", "mid_sen": "In figure 4 the relation between the magic predicates for the example grammar is represented by an unfolding tree (Pettorossi and Proietti, 1994) . ", "after_sen": "This, however, is not an ordinary unfolding tree as it is constructed on the basis of an abstract seed, i.e., a seed adorned with a specification of which arguments are to be considered bound. "}
{"citeStart": 162, "citeEnd": 184, "citeStartToken": 162, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . ", "mid_sen": "This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997) .", "after_sen": "Our position will be that dependency relations are motivated semantically (Tesni~re, 1959) , and need not be projective (i.e., may cross if projected onto the surface ordering). "}
{"citeStart": 72, "citeEnd": 89, "citeStartToken": 72, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "Purely statistical methods of word segmentation (e.g. de Marcken 1996 , Sproat et al 1996 , Tung and Lee 1994 , Lin et al (1993 , Chiang et al (1992) , Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In many cases the machine will not even realize that there is an unfound word in the sentence since most single Chinese characters can be words by themselves.", "mid_sen": "Purely statistical methods of word segmentation (e.g. de Marcken 1996 , Sproat et al 1996 , Tung and Lee 1994 , Lin et al (1993 , Chiang et al (1992) , Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.", "after_sen": "There are also hybrid approaches such as (Nie dt al 1995) where statistical approaches and heuristic rules are combined to identify new words. "}
{"citeStart": 94, "citeEnd": 116, "citeStartToken": 94, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994) , word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004) , collocation extraction (Schone and Jurafsky, 2001 ), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975) . In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997 ) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998) . Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Examples include automatic thesaurus extraction (Grefenstette, 1994) , word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004) , collocation extraction (Schone and Jurafsky, 2001 ), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975) . ", "mid_sen": "In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997 ) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998) . ", "after_sen": "Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004) ."}
{"citeStart": 304, "citeEnd": 323, "citeStartToken": 304, "citeEndToken": 323, "sectionName": "UNKNOWN SECTION NAME", "string": "Baselines We compared our model against four baselines, two with full supervision: Support Vector Machines (SVM) and Maximum Entropy Models (MaxEnt), and two with light supervision: Trans- (Vapnik, 1998; Jiao et al., 2006) . SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011) ) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This enables us to properly evaluate the impact of our modeling decision which augments a feature-based model with constraints.", "mid_sen": "Baselines We compared our model against four baselines, two with full supervision: Support Vector Machines (SVM) and Maximum Entropy Models (MaxEnt), and two with light supervision: Trans- (Vapnik, 1998; Jiao et al., 2006) . SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011) ) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles.", "after_sen": "Parameter tuning The boundaries of the reference probabilities (a k and b k in Equation 8)were defined and optimized on the development data which consists of one third of the corpus. "}
{"citeStart": 104, "citeEnd": 116, "citeStartToken": 104, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993) . More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992) . We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like \"say\".", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993) . ", "mid_sen": "More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992) . ", "after_sen": "We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like \"say\"."}
{"citeStart": 115, "citeEnd": 127, "citeStartToken": 115, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "Because of the skewed class distribution, we use both the F macro and F micro scores with 10-fold cross-validation. The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by Athar (2011) . However, we can observe that the F scores decrease as more context is introduced. This may be attributed to the increase in the vocabulary size of the n-grams and a consequent reduction in the discriminating power of the decision boundaries. These results show that the task of jointly detecting sentiment and context is a hard problem.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because of the skewed class distribution, we use both the F macro and F micro scores with 10-fold cross-validation. ", "mid_sen": "The baseline score, shown in bold, is obtained with no context window and is comparable to the results reported by Athar (2011) . ", "after_sen": "However, we can observe that the F scores decrease as more context is introduced. "}
{"citeStart": 151, "citeEnd": 171, "citeStartToken": 151, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "Our baseline coreference system uses the C4.5 decision tree learner (Quinlan, 1993) to acquire a classifier on the training texts for determining whether two NPs are coreferent. Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . . ., NP j−1 . Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003) , as described below. Lexical features. Nine features allow different types of string matching operations to be performed on the given pair of NPs, NP x and NP y 10 , including (1) exact string match for pronouns, proper nouns, and non-pronominal NPs (both before and after determiners are removed); (2) substring match for proper nouns and non-pronominal NPs; and (3) head noun match. In addition, one feature tests whether all the words that appear in one NP also appear in the other NP. Finally, a nationality matching feature is used to match, for instance, British with Britain. Grammatical features. 22 features test the grammatical properties of one or both of the NPs. These include ten features that test whether each of the two NPs is a pronoun, a definite NP, an indefinite NP, a nested NP, and a clausal subject. A similar set of five features is used to test whether both NPs are pronouns, definite NPs, nested NPs, proper nouns, and clausal subjects. In addition, five features determine whether the two NPs are compatible with respect to gender, number, animacy, and grammatical role. Furthermore, two features test whether the two NPs are in apposition or participate in a predicate nominal construction (i.e., the IS-A relation). Soon et al. (2001) , we have a semantic feature that tests whether one NP is a name alias or acronym of the other.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following previous work (e.g., Soon et al. (2001) and Ponzetto and Strube (2006) ), we generate training instances as follows: a positive instance is created for each anaphoric NP, NP j , and its closest antecedent, NP i ; and a negative instance is created for NP j paired with each of the intervening NPs, NP i+1 , NP i+2 , . . ., NP j−1 . ", "mid_sen": "Each instance is represented by 33 lexical, grammatical, semantic, and positional features that have been employed by highperforming resolvers such as Ng and Cardie (2002) and Yang et al. (2003) , as described below. ", "after_sen": "Lexical features. "}
{"citeStart": 60, "citeEnd": 71, "citeStartToken": 60, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, an alignment selection approach was proposed in (Huang, 2009) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009) . Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. ", "mid_sen": "There is no need for a pre-determined threshold as used in (Huang, 2009) . ", "after_sen": "Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. "}
{"citeStart": 97, "citeEnd": 115, "citeStartToken": 97, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "Although we report promising results, parse selection that is sufficiently accurate for many practical applications will require a more lexicalised system. Magerman's (1995) parser is an extension of the history-based parsing approach developed at IBM (Black et al., 1993) in which rules are conditioned on lexical and other (essentially arbitrary) information available in the parse history. In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicateargument structures derived from the grammar are ranked probabilistically. However, the massively increased coverage obtained here by relaxing subcategorisation constraints underlines the need to acquire accurate and complete subcategorisation frames in a corpus-driven fashion, before such constraints can be exploited robustly and effectively with free text.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although we report promising results, parse selection that is sufficiently accurate for many practical applications will require a more lexicalised system. ", "mid_sen": "Magerman's (1995) parser is an extension of the history-based parsing approach developed at IBM (Black et al., 1993) in which rules are conditioned on lexical and other (essentially arbitrary) information available in the parse history. ", "after_sen": "In future work, we intend to explore a more restricted and semantically-driven version of this approach in which, firstly, probabilities are associated with different subcategorisation possibilities, and secondly, alternative predicateargument structures derived from the grammar are ranked probabilistically. "}
{"citeStart": 80, "citeEnd": 114, "citeStartToken": 80, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "We call the set of all descriptions related to the same entity in a corpus, a profile of that entity. Profiles for a large number of entities were compiled using our earlier system, PRO-FILE (Radev and McKeown, 1997) . It turns out that there is a large variety in the size of the profile (number of distinct descriptions) for different entities. Table 1 shows a subset of the profile for Ung Huot, the former foreign minister of Cambodia, who was elected prime minister at some point of time during the run of our experiment. A few sample semantic features of the descriptions in Table 1 are shown as separate columns.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We call the set of all descriptions related to the same entity in a corpus, a profile of that entity. ", "mid_sen": "Profiles for a large number of entities were compiled using our earlier system, PRO-FILE (Radev and McKeown, 1997) . ", "after_sen": "It turns out that there is a large variety in the size of the profile (number of distinct descriptions) for different entities. "}
{"citeStart": 149, "citeEnd": 170, "citeStartToken": 149, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "We further improved a typed extension of Gerdemann's Earley generator with a number of techniques that reduce the number of edges created during generation. Three optimizations were especially helpful. The first supplies each edge in the chart with two indices, a backward index pointing to the state in the chart that the edge is predicted from, and a forward index poinfing to the states that are predicted from the edge. By matching forward and backward indices, the edges that must be combined for completion can be located faster. This indexing technique, as illustrated below, improves upon the more complex indices in Gerdemann (1991) and is closely related to OLDT-resolution (Tamaki and Sato, 1986) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By matching forward and backward indices, the edges that must be combined for completion can be located faster. ", "mid_sen": "This indexing technique, as illustrated below, improves upon the more complex indices in Gerdemann (1991) and is closely related to OLDT-resolution (Tamaki and Sato, 1986) .", "after_sen": "1) active(Xo---~Xl*X2,1~2~ 2) active(X:--~.Y1Y2,~3)) 3) active(X2---*Y1."}
{"citeStart": 60, "citeEnd": 78, "citeStartToken": 60, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, the field of dialectometry, as introduced by S~guy (1971, 1973) , has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. There is however no firm agreement on just how to compute the distance matrices. Sfiguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make. The need to figure out such systems as the comparative phonology of various linguistic sites can be very time-consuming and fraught with arbitrary choices.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such conundrums led Paris and others to conclude that the dialect boundary, and therefore the very notion of dialect, is an ill-defined concept.", "mid_sen": "More recently, the field of dialectometry, as introduced by S~guy (1971, 1973) , has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. ", "after_sen": "They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. "}
{"citeStart": 60, "citeEnd": 78, "citeStartToken": 60, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008) . The citation summary data from the ACL Anthology Network 1 (Radev et al., 2009) was used. This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008) . ", "mid_sen": "The citation summary data from the ACL Anthology Network 1 (Radev et al., 2009) was used. ", "after_sen": "This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset."}
{"citeStart": 29, "citeEnd": 46, "citeStartToken": 29, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993) . In recent years, many researchers have employed statistical models (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003) or association measures (Smadja et al., 1996; Ahrenberg et al., 1998; Tufis and Barbu, 2002) to build alignment links. In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997) . However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When the large-scale bilingual corpus is not available, some researchers use existing dictionaries to improve word alignment (Ker and Chang, 1997) . ", "mid_sen": "However, only a few studies (Wu and Wang, 2004) directly address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available.", "after_sen": "In this paper, we address the problem of word alignment in a specific domain, in which only a small-scale corpus is available. "}
{"citeStart": 20, "citeEnd": 39, "citeStartToken": 20, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "• SVM-Rel: We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. Table 3 shows the comparisons of the proposed method with the state-of-the-art systems on subjectivity classification and explanatory relation extraction. From the results, we can observe that recursive autoencoders based subjectivity classification method achieves slightly better performance than our method and conditional random fields based method. The performances of the proposed method are similar as CRFs'. We think that the main reason is that only lexical features are used in MLN models for subjective classification. However, conditional random fields consider not only lexical information but also inference of the contexts of sentences. RAE method learns vector space representations for multi-word phrases and uses compositional semantics to understand sentiment. For evaluating the performance of relation extraction, we combine the results of RAE with PDTB-Rel and SVM-Rel. For all the subjective clauses identified by RAE, PDTB-Rel and SVM-Rel are used to extract corresponding explanatory clauses. The results are shown in the last three rows in the Table 3 . From the results, we can observe that the proposed joint model achieves best F1 score and precision among all methods. Although the proposed method achieve slightly worse result in processing subjectivity classification. We think that the error propagation is the main reason for worse results of cascaded methods. The relative improvement of MLN over SVM-Rel is more than 33.4%.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• SVM-Rel: ", "mid_sen": "We also use LibSVM (Chang and Lin, 2011) to classify the relations between clauses. ", "after_sen": "Following the configurations reported by Feng and Hirst (2012), we use linear kernel and probability estimation to model it. "}
{"citeStart": 90, "citeEnd": 109, "citeStartToken": 90, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "Questions Q1-6 were inspired by previous work on spoken dialogue system evaluation (e.g. (Walker et al., 2000) ) and measure user's overall perception of the system. We find that the NM presence significantly improves user's perception of the system in terms of their ability to concentrate on the instruction (Q3), in terms of their inclination to reuse the system (Q6) and in terms of the system's matching of their expectations (Q4). There is a trend that it was easier for them to learn from the NM enabled version of the system (Q2).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Questions Q1-6 were inspired by previous work on spoken dialogue system evaluation (e.g. (Walker et al., 2000) ) and measure user's overall perception of the system. ", "after_sen": "We find that the NM presence significantly improves user's perception of the system in terms of their ability to concentrate on the instruction (Q3), in terms of their inclination to reuse the system (Q6) and in terms of the system's matching of their expectations (Q4). "}
{"citeStart": 100, "citeEnd": 116, "citeStartToken": 100, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Two speech samples from each of three subjects were used in the simulations in one sample a mother was speaking to her daughter and in the other, the same mother was speaking to the researcher. The samples were taken from the CHILDES database (MacWhinney ~ Snow, 1990) from studies reported in Bernstein (1982) . Each sample was checked for consistent word spellings (e.g., 'ts wits changed to its), then was transcribed into an ASCll-I)ased I)honemic rel)res(mtation :l. 'Fhe transcription sysl, em was based on IPA an(I used one character for each consonant or vowel; diphthongs, r-colored vowels and syllabic consonants were each represented as one character. For example, \"boy\" was written as bT, \"bird\" as bRd and \"label\" as lebL. For purposes of phonotactic constraints, syllabic consonants were treate(! as vowels. Sample lengths were selected to make the nmnber of available segmentation points nearly equal (about 1,350) when no ph0notactic constraints were applied; child-directed samples had 498-536 tokens and 153-166 types, adult-directed sa.ml)les had 443-484 tokens and 196--205 types. I\"inMly, Iwl'ore tim saml)les were fi~(I to the sinmlations, divisions bel,wcell words (but not l)(%w(~en s(HIt(qIcos) wcr(~ reiuovc'.(L The sl)ace of l)Ossil)le hyl)oi, hcses is vlmt 4, so sonl(~ nmthod of finding a minimum-length hypothesis without considering all hypotheses is necessary. We used the following method: first, evaluate the input sample with no segmentation points added; then evaluate all hypotheses obtained by adding one or two segmentation points; take the shortest hypothesis found in the previous step and evaluate all hypotheses obtained by adding one or two more segmentation points; continue this way until the sample has been segmented into the smallest possible units and report the shortest hypothesis ever found. Two variants of this simulation wcre used: (1) DIST-FREE was free of any phonotactic restrictions on the hypotheses it could form (DIST refers to the measurement of distri-butionaJ information), whereas (2) DIST-PtloNO ,Ise~l I.Iw i)hon~,t;wtic r (,sl.ricl,i(ms (lescril.,I ;,.I,,w,'. 3The I,ranisi:riptioli nil%hod CUlSiix'(!d the identh:al trallscripl~ioii of all occurrences of a word.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Two speech samples from each of three subjects were used in the simulations in one sample a mother was speaking to her daughter and in the other, the same mother was speaking to the researcher. ", "mid_sen": "The samples were taken from the CHILDES database (MacWhinney ~ Snow, 1990) from studies reported in Bernstein (1982) . ", "after_sen": "Each sample was checked for consistent word spellings (e.g., 'ts wits changed to its), then was transcribed into an ASCll-I)ased I)honemic rel)res(mtation :l. 'Fhe transcription sysl, em was based on IPA an(I used one character for each consonant or vowel; diphthongs, r-colored vowels and syllabic consonants were each represented as one character. "}
{"citeStart": 36, "citeEnd": 54, "citeStartToken": 36, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, inspired by KNN-SVM (Zhang et al., 2006) , we propose a local training method, which trains sentence-wise weights instead of a single weight, to address the above two problems. Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. This online fashion has an advantage in that it can adapt the weights for each of the test sentences, by dynamically tuning the weights on translation examples which are similar to these test sentences. Similar to the method of development set automatical selection, the local training method may also suffer the problem of efficiency. To put it into practice, we propose incremental training methods which avoid retraining and iterative decoding on a development set.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, we can achieve this with two weights: 1, 1 for f 1 and −1, 1 for f 2 .", "mid_sen": "In this paper, inspired by KNN-SVM (Zhang et al., 2006) , we propose a local training method, which trains sentence-wise weights instead of a single weight, to address the above two problems. ", "after_sen": "Compared with global training methods, such as MERT, in which training and testing are separated, our method works in an online fashion, in which training is performed during testing. "}
{"citeStart": 183, "citeEnd": 194, "citeStartToken": 183, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997) . Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998) . In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997) . ", "after_sen": "Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998) . "}
{"citeStart": 1, "citeEnd": 20, "citeStartToken": 1, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "When parses are postulated for a sentence negative as well as positive examples are likely to occur. Now, in natural language negative correlations are an important source of information: the occurrence of some words or groups of words inhibit others from following. We wish to exploit these constraints. (Brill et al. , 1990) recognised this, and introduced the idea of distituents. These are elements of a sentence that should be separated, as opposed to elements of constituents that cling together. Brill addresses the problem of finding a valid metric for distituency by using a generalized mutual information statistic. Distituency is marked by a mutual information minima. His method is supported by a small 4 rule grammar.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We wish to exploit these constraints. ", "mid_sen": "(Brill et al. , 1990) recognised this, and introduced the idea of distituents. ", "after_sen": "These are elements of a sentence that should be separated, as opposed to elements of constituents that cling together. "}
{"citeStart": 40, "citeEnd": 69, "citeStartToken": 40, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "We also tested the method proposed in (Li and Abe, 1995) of learning case frames patterns using all existing thesaurus. In particular, we used this method with WordNet (Miller et al., 1993) and using the same training data., and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns. We show the result of this experiment as 'WordNet' in Table 1 . We can see that in terms of 'coverage', ~WordNet' outperforms 'MDL-Thesaurus', but in terms of \"accuracy', 'MDL-Thesaurus' outperforms 'Word-Net.'. These results can be interpreted as follows. An automa.tically constructed thesaurus is more domaiu dependent and captures the domain dependent features better, and thus using it achieves high accuracy. On the other hand, since training data. we had available is insufficient, its coverage is smaller than that of a hand made thesaurus. In practice, it makes sense to combine both types of thesauri. More specifically, an atttomatically constructed thesaurus can be used within its coverage, and outside its coverage, a hand made thesaurus can be used. Given the current state of the word clustering technique (namely, it requires data size that is usually not available, and it tends to be computationally demanding), this strategy is practical. We show the result of this combined 2. [lsing a tlwsaltrus consl.rt~cl,cd l>y ottr met hod can inq>rov(; pp-;fl.t, ach)nent, disaml)igtmtion results.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also tested the method proposed in (Li and Abe, 1995) of learning case frames patterns using all existing thesaurus. ", "mid_sen": "In particular, we used this method with WordNet (Miller et al., 1993) and using the same training data., and then conducted pp-attachment disambiguation experiment using the obtained case frame patterns. ", "after_sen": "We show the result of this experiment as 'WordNet' in Table 1 . "}
{"citeStart": 99, "citeEnd": 109, "citeStartToken": 99, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "We have presented a polynomial complexity generation algorithm which can form part of any Shakeand-Bake style MT system with suitable grammars and information transfer. The transfer module is free to attempt structural transfer in order to produce the best possible first guess. We tested a TNCB-based generator in the SLEMaT MT system with the pathological cases described in (Brew, 1992) against Whitelock's original generation algorithm, and have obtained speed improvements of several orders of magnitude. Somewhat more surprisingly, even for short sentences which were not problematic for Whitelock's system, the generation component has performed consistently better.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The transfer module is free to attempt structural transfer in order to produce the best possible first guess. ", "mid_sen": "We tested a TNCB-based generator in the SLEMaT MT system with the pathological cases described in (Brew, 1992) against Whitelock's original generation algorithm, and have obtained speed improvements of several orders of magnitude. ", "after_sen": "Somewhat more surprisingly, even for short sentences which were not problematic for Whitelock's system, the generation component has performed consistently better."}
{"citeStart": 142, "citeEnd": 160, "citeStartToken": 142, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. ", "after_sen": "Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . "}
{"citeStart": 145, "citeEnd": 157, "citeStartToken": 145, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "The literature on ellipsis and event reference is voluminous, and so we will not attempt a comprehensive comparison here. Instead, we briefly compare the current work to three previous studies that explicitly tie ellipsis 14Sag and Hankamer claim that all such cases of VPellipsis require syntactic antecedents, whereas we suggest that in Coherent Situation relations VP-eUipsis operates more like their Model-Interpretive Anaphora, of which do it is an example. resolution to an account of discourse structure and coherence, namely our previous account (Kehler, 1993b) and the accounts of Priist (1992) and Asher (1993) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Instead, we briefly compare the current work to three previous studies that explicitly tie ellipsis 14Sag and Hankamer claim that all such cases of VPellipsis require syntactic antecedents, whereas we suggest that in Coherent Situation relations VP-eUipsis operates more like their Model-Interpretive Anaphora, of which do it is an example. ", "mid_sen": "resolution to an account of discourse structure and coherence, namely our previous account (Kehler, 1993b) and the accounts of Priist (1992) and Asher (1993) .", "after_sen": "In Kehler (1993b) , we presented an analysis of VPellipsis that distinguished between two types of relationship between clauses, parallel and non-parallel. "}
{"citeStart": 188, "citeEnd": 189, "citeStartToken": 188, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "A claim of this paper i8 that one role of IRU's is to ensure that these assumptions are supported by evidence, thus decreasing the defensibility of the mutual beliefs that depend on them [4] . ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Augmenting the strength of an assumption thus decreases its relative defensibility.", "mid_sen": "A claim of this paper i8 that one role of IRU's is to ensure that these assumptions are supported by evidence, thus decreasing the defensibility of the mutual beliefs that depend on them [4] . ", "after_sen": ""}
{"citeStart": 42, "citeEnd": 61, "citeStartToken": 42, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996) ), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is motivated by the observation that enhancing the feature set or learning algorithm used in a corpus-based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm.", "mid_sen": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. ", "after_sen": "Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). "}
{"citeStart": 14, "citeEnd": 30, "citeStartToken": 14, "citeEndToken": 30, "sectionName": "UNKNOWN SECTION NAME", "string": "Shallow lexical features such as co-occurrences and collocations are recognized as potent sources of disambiguation information. While many other contextual features are often employed, it isn't clear that they offer substantial advantages. For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%. Preliminary experiments for this paper used feature sets that included collocates, co-occurrences, part-ofspeech and grammatical information for surrounding words. However, it was clear that no combination of features resulted in disambiguation accuracy significantly higher than that achieved with co-occurrence features.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While many other contextual features are often employed, it isn't clear that they offer substantial advantages. ", "mid_sen": "For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%. ", "after_sen": "Preliminary experiments for this paper used feature sets that included collocates, co-occurrences, part-ofspeech and grammatical information for surrounding words. "}
{"citeStart": 90, "citeEnd": 109, "citeStartToken": 90, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "AProlog is a logic programming language based on higher-order hereditary Harrop formulae (Miller et al., 1991) . It differs from Prolog in that first-order terms and unification are replaced with simply-typed A-terms and higher-order unification 7, respectively. It also permits universal quantification and implication in the goals of clauses. The crucial aspect for this paper is that together these features permits the usage of abstract syntax to express the logical forms terms computed by CCG. The built-in A-term manipulation is used as a \"meta-language\" in which the \"object-language\" of CCG logical forms is expressed, and variables in the object-language are mapped to variables in the meta-language. The AProlog code fragment shown in Figure 3 declares how the CCG logical forms are represented. Each CCG LF is represented as an untyped A-term, namely type t=. abe represents object-level abstraction Az.M by the meta-level expression (abe I), sit is not established if this schema should actually produce an unbounded family of rules. See (Weir, 1988) and (Weir and Joshi, 1988) for a discussion of the implications for automata-theoretic power of generalized coordination and composition, and (Gazda~, 1988) for linguistic axguments that languages like Dutch may require this power, and (Steedman, 1990) for some further discussion of the issue. In this paper we use the generalized rule to illustrate the elegance of the representation, but it is an easy change to implement a bounded coordination rule.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "AProlog is a logic programming language based on higher-order hereditary Harrop formulae (Miller et al., 1991) . ", "after_sen": "It differs from Prolog in that first-order terms and unification are replaced with simply-typed A-terms and higher-order unification 7, respectively. "}
{"citeStart": 82, "citeEnd": 104, "citeStartToken": 82, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "A reference of this sort is often achieved through a collaboration between the conversants. In such cases, the speaker has the goal of having the hearer know how to identify an object. The speaker attempts to achieve this goal by building a description of the object that she believes will give the hearer the ability to identify it when it is possible to do so. The hearer needs to be confident that the description will be adequate as a means of identifying the referent, but because of the inevitable differences in beliefs about the world, he might not be. When the hearer is not confident, the speaker and hearer collaborate to make a new referring expression that the bearer believes is adequate. This can be seen in the following portion of a telephone conversation recorded by Psathas (1991, p. 196) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When the hearer is not confident, the speaker and hearer collaborate to make a new referring expression that the bearer believes is adequate. ", "mid_sen": "This can be seen in the following portion of a telephone conversation recorded by Psathas (1991, p. 196) .", "after_sen": ""}
{"citeStart": 78, "citeEnd": 97, "citeStartToken": 78, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the di erent trigger situations we performed two tests: One where we use only acoustic triggers and another where the existence of a perfect word fragment detector is assumed. The input were unsegmented transliterated utterance to exclude in uences a word recognizer. We restrict the processing time on a SUN ULTRA 300MHZ to 10 seconds. The parser was simulated by a w ord trigram. Training and testing were done on two separated parts of the German part of the Verbmobil corpus 12558 turns training 1737 turns test. Test 1 49  70  70  Test 2 71  85 62  83 A direct comparison to other groups is rather di cult due to very di erent corpora, evaluation conditions and goals. Nakatani and Hirschberg, 1993 suggest a acoustic prosodic detector to identify IPs but don't discuss the problem of nding the correct segmentation in depth. Also their results are obtained on a corpus where every utterance contains at least one repair. Shriberg, 1994 also addresses the acoustic aspects of repairs. Parsing approaches like in Bear et al., 1992; Hindle, 1983; Core and Schubert, 1999 must be proved to work with lattices rather than transliterated text. An algorithm which is inherently capable of lattice processing is proposed by Heeman Heeman, 1997 . He rede nes the word recognition problem to identify the best sequence of words, corresponding POS tags and special repair tags. He reports a recall rate of 81 and a precision of 83 for detection and 78 80 for correction. The test settings are nearly the same as test 2. Unfortunately, nothing is said about the processing time of his module.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Parsing approaches like in Bear et al., 1992; Hindle, 1983; Core and Schubert, 1999 must be proved to work with lattices rather than transliterated text. ", "mid_sen": "An algorithm which is inherently capable of lattice processing is proposed by Heeman Heeman, 1997 . ", "after_sen": "He rede nes the word recognition problem to identify the best sequence of words, corresponding POS tags and special repair tags. "}
{"citeStart": 23, "citeEnd": 42, "citeStartToken": 23, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "Lower type arguments such as NP2 pose a different kind of ambiguity problem. Although they are required in unbounded constructions, they may yield alternative derivations of local scrambling cases in a labelled CCG. For instance, when NP2 is peripheral in a ditransitive construction and the verb can form a constituent with all the other arguments (S\\NP2 or S/NP2), the parser allows NP2. This is unavoidable unless the parser is made aware of the local and nonlocal context. In other words, this method solves the spurious ambiguity problem between higher types, but not among higher and lower types. One can try to remedy this problem by making the availability of types dependent on some measures of prominence, e.g., allowing subjects only in higher types to account for subject-complement asymmetries. But, as pointed out by Eisner (1996, p.85) , this is not spurious ambiguity in the technical sense, just multiple derivations due to alternative lexical category assignments. Eliminating ambiguity in such cases remains to be solved.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One can try to remedy this problem by making the availability of types dependent on some measures of prominence, e.g., allowing subjects only in higher types to account for subject-complement asymmetries. ", "mid_sen": "But, as pointed out by Eisner (1996, p.85) , this is not spurious ambiguity in the technical sense, just multiple derivations due to alternative lexical category assignments. ", "after_sen": "Eliminating ambiguity in such cases remains to be solved."}
{"citeStart": 122, "citeEnd": 124, "citeStartToken": 122, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "A subcase of ensuring that certain inferences get made involves the juxtaposition of two propositions. These cases challenge the assumption that: (4) The context of a discourse is an undifferentiated set of propositions with no specific relations between them. While this assumption is certainly not made in most discourse models, it is often made in semantic models of the context [14] . In the following segment, Jane (j) describes her financial situation to Iiarry (h) and a choice between a setthment and an annuity. Evidence of acceptance may be given explicitly, but acceptance can be inferred in sonm dialogue situations via the operation of a simple principle of cooperative dialogueS:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These cases challenge the assumption that: (4) The context of a discourse is an undifferentiated set of propositions with no specific relations between them. ", "mid_sen": "While this assumption is certainly not made in most discourse models, it is often made in semantic models of the context [14] . ", "after_sen": "In the following segment, Jane (j) describes her financial situation to Iiarry (h) and a choice between a setthment and an annuity. "}
{"citeStart": 13, "citeEnd": 30, "citeStartToken": 13, "citeEndToken": 30, "sectionName": "UNKNOWN SECTION NAME", "string": "It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990) . To understand con-versationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994) . Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. ", "mid_sen": "For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. ", "after_sen": "To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990) . "}
{"citeStart": 108, "citeEnd": 113, "citeStartToken": 108, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Why should we, as computational linguists, be interested in factors that contribute to the interactivity of a discourse? There are both theoretical and practical motivations. First, we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant, multi-utterance discourses [Po186, CP86] . Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . Since conversation is a collaborative process [CWG86, SSJ74] , models of conversation can provide the basis for extending planning theories [GS90, CLNO90] . When the situation requires the negotiation of a collaborative plan, these theories must account for the interacting beliefs and intentions of multiple participants. ~,From a practical perspective, there is ample evidence that limited mixed-initiative has contributed to lack of system usability. Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems: They don't allow users to participate in the reasoning process, or to ask the questions they want answered [PHW82, Kid85, FL89] . In addition, question answering systems often fail to take account of the system's role as a conversational partner.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems: ", "mid_sen": "They don't allow users to participate in the reasoning process, or to ask the questions they want answered [PHW82, Kid85, FL89] . ", "after_sen": "In addition, question answering systems often fail to take account of the system's role as a conversational partner."}
{"citeStart": 89, "citeEnd": 112, "citeStartToken": 89, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "Current semantic role labeling systems rely primarily on syntactic features in order to identify and classify roles. Features derived from a syntactic parse of the sentence have proven particularly useful (Gildea & Jurafsky, 2002) . For example, the syntactic subject of \"give\" is nearly always the Giver. Path features allow systems to capture both general patterns, e.g., that the ARG0 of a sentence tends to be the subject of the sentence, and specific usage, e.g., that the ARG2 of \"give\" is often a post-verbal prepositional phrase headed by \"to\". An example sentence with extracted path features is shown in Figure 1 . A major problem with this approach is that the path from an argument to the verb can be quite complicated. In the sentence \"He expected to receive a prize for winning,\" the path from \"win\" to its ARG0, \"he\", involves the verbs \"expect\" and \"receive\" and the preposition \"for.\" The corresponding path through the parse tree likely occurs a relatively small number of times (or not at all) in the training corpus. If the test set contained exactly the same sentence but with \"expected\" replaced by \"did not expect\" we would extract a different parse path feature; therefore, as far as the classifier is concerned, the syntax of the two sentences is totally unrelated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Current semantic role labeling systems rely primarily on syntactic features in order to identify and classify roles. ", "mid_sen": "Features derived from a syntactic parse of the sentence have proven particularly useful (Gildea & Jurafsky, 2002) . ", "after_sen": "For example, the syntactic subject of \"give\" is nearly always the Giver. "}
{"citeStart": 115, "citeEnd": 128, "citeStartToken": 115, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Such structures can straightforwardly be thought of as models, in the usual sense of first order model theory (see Hodges (1993) ). Viewing the LFG ontology in such terms does no violence to intuition: indeed, as we shall see, a more direct mathematical embodiment of the LFG universe can hardly be imagined.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As has already been noted, the basic entities underlying the LFG analyses are composite structures consisting of a finite tree, a finite feature structure, and a function that links the two.", "mid_sen": "Such structures can straightforwardly be thought of as models, in the usual sense of first order model theory (see Hodges (1993) ). ", "after_sen": "Viewing the LFG ontology in such terms does no violence to intuition: indeed, as we shall see, a more direct mathematical embodiment of the LFG universe can hardly be imagined."}
{"citeStart": 86, "citeEnd": 108, "citeStartToken": 86, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997) . However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997) . ", "mid_sen": "However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b) .", "after_sen": "One possible use for this technique is for parser adaptation -initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993) ) and then self-training on a second type of data in order to adapt the parser to the second domain. "}
{"citeStart": 164, "citeEnd": 183, "citeStartToken": 164, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "We obtained a further improvement (our F score increased from 73.94 to 76.99) by training on handannotated POS and chunk data from the Treebank. Table  3 compares precision, recall, and F scores for our system with CoNLL-2001 results training on sections 15-18 of the Penn Treebank and testing on section 21 (Marcus et al., 1993) . The F score is more than 10 points above the average scores, failing to surpass only the best performing CoNLL system.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We obtained a further improvement (our F score increased from 73.94 to 76.99) by training on handannotated POS and chunk data from the Treebank. ", "mid_sen": "Table  3 compares precision, recall, and F scores for our system with CoNLL-2001 results training on sections 15-18 of the Penn Treebank and testing on section 21 (Marcus et al., 1993) . ", "after_sen": "The F score is more than 10 points above the average scores, failing to surpass only the best performing CoNLL system."}
{"citeStart": 82, "citeEnd": 102, "citeStartToken": 82, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. Gerdemann's generator follows a head-driven strategy in order to avoid inefficient evaluation orders. More specifically, the head of the right-hand side of each grammar rule is distinguished, and distinguished categories are scanned or predicted upon first. The resulting evaluation strategy is similar to that of the head-corner approach (Shieber et al., 1990 ; Gerdemann and IIinrichs, in press): prediction follows the main flow of semantic information until a lexical pivot is reached, and only then are the headdependent subparts of the construction built up in a bottom-up fashion. This mixture of top-down and bottom-up information flow is crucial since the topdown semantic information from the goal category must be integrated with the bottom-up subcategorization information from the lexicon. A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing. Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More specifically, the head of the right-hand side of each grammar rule is distinguished, and distinguished categories are scanned or predicted upon first. ", "mid_sen": "The resulting evaluation strategy is similar to that of the head-corner approach (Shieber et al., 1990 ; Gerdemann and IIinrichs, in press): prediction follows the main flow of semantic information until a lexical pivot is reached, and only then are the headdependent subparts of the construction built up in a bottom-up fashion. ", "after_sen": "This mixture of top-down and bottom-up information flow is crucial since the topdown semantic information from the goal category must be integrated with the bottom-up subcategorization information from the lexicon. "}
{"citeStart": 179, "citeEnd": 200, "citeStartToken": 179, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "• ability to integrate rich contextual features into the model More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001 ) which slightly outperforms MEMMs on a standard partof-speech tagging task. In a similar vein, the work of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000) . We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004) . Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• no concept of \"out-of-vocabulary\" word: subword features are very useful in dealing with words not seen in the training data", "mid_sen": "• ability to integrate rich contextual features into the model More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001 ) which slightly outperforms MEMMs on a standard partof-speech tagging task. ", "after_sen": "In a similar vein, the work of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. "}
{"citeStart": 58, "citeEnd": 71, "citeStartToken": 58, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993) . They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992) . Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. ", "mid_sen": "No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992) . ", "after_sen": "Still, a lexicon is needed that specifies the possible parts of speech for every word. "}
{"citeStart": 156, "citeEnd": 175, "citeStartToken": 156, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "where U is a square matrix with U(i, j) = 1/N for all i and j, N is the number of nodes, and d is the escape probability chosen in the interval [0.1, 0.2] (Brin and Page, 1998) . Equation 4is known as PageRank (Page et al., 1999) and is used for determining prestige on the web in the Google search engine.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "EQUATION", "mid_sen": "where U is a square matrix with U(i, j) = 1/N for all i and j, N is the number of nodes, and d is the escape probability chosen in the interval [0.1, 0.2] (Brin and Page, 1998) . Equation 4is known as PageRank (Page et al., 1999) and is used for determining prestige on the web in the Google search engine.", "after_sen": ""}
{"citeStart": 89, "citeEnd": 114, "citeStartToken": 89, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "Since earlier versions of the SNoW based CSCL were used only to identify single phrases (Punyakanok and Roth, 2001; Munoz et al., 1999) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) to compare it to other shallow parsers. Table 1 shows that it ranks among the top shallow parsers evaluated there 1 . ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The outcomes of these classifiers are then combined in a way that satisfies some constraints -non-overlapping constraints in this case -using an efficient constraint satisfaction mechanism that makes use of the confidence in the classifier's outcomes.", "mid_sen": "Since earlier versions of the SNoW based CSCL were used only to identify single phrases (Punyakanok and Roth, 2001; Munoz et al., 1999) and never to identify a collection of several phrases at the same time, as we do here, we also trained and tested it under the exact conditions of CoNLL-2000 (Tjong Kim Sang and Buchholz, 2000) to compare it to other shallow parsers. ", "after_sen": "Table 1 shows that it ranks among the top shallow parsers evaluated there 1 . "}
{"citeStart": 1, "citeEnd": 20, "citeStartToken": 1, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting befiefs; therefore we employ a re, cursive model for collaboration that captures extended negotiation and represents the structure of the discourse. Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. In addition, we provide a process for selecting among multiple possible pieces of evidence.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Caswey et al. ", "mid_sen": "(Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain. ", "after_sen": "They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . "}
{"citeStart": 11, "citeEnd": 31, "citeStartToken": 11, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Since our experiments, other related work in NLP has been performed. Some of this work addresses related but different classification tasks. Three studies classify reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, Pennock 2003) . The input is assumed to be a review, so this task does not include finding subjective documents in the first place. The first study listed above (Turney 2002) uses a variation of the semantic similarity procedure presented in Wiebe (2000) (Section 3.4). The third (Dave, Lawrence, and Pennock 2003) uses ngram features identified with a variation of the procedure presented in (Section 3.3) . Tong (2001) addresses finding sentiment timelines, that is, tracking sentiments over time in multiple documents. For clues of subjectivity, he uses manually developed lexical rules, rather than automatically learning them from corpora. Similarly, Gordon et al. (2003) use manually developed grammars to detect some types of subjective language. Agrawal et al. (2003) partition newsgroup authors into camps based on quotation links. They do not attempt to recognize subjective language.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For clues of subjectivity, he uses manually developed lexical rules, rather than automatically learning them from corpora. ", "mid_sen": "Similarly, Gordon et al. (2003) use manually developed grammars to detect some types of subjective language. ", "after_sen": "Agrawal et al. (2003) partition newsgroup authors into camps based on quotation links. "}
{"citeStart": 122, "citeEnd": 133, "citeStartToken": 122, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that the unfolding of the magic_s literal leads to the instantiation of the argument VFORM to finite. As a result of the fact that there are no other magic_s literals in the remainder of the magic-compiled grammar the magic_s rule can be discarded. This filter optimization is reminiscent of computing the deterministic closure over the magic part of a compiled grammar (DSrre, 1993) at compile time. Performing this optimization throughout the magic part of the grammar in figure 2 not only leads to a more succinct grammar, but brings about a different processing behavior. Generation with the resulting grammar can be compared best with head corner generation (Shieber et al., 1990 ) (see next section).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a result of the fact that there are no other magic_s literals in the remainder of the magic-compiled grammar the magic_s rule can be discarded. ", "mid_sen": "This filter optimization is reminiscent of computing the deterministic closure over the magic part of a compiled grammar (DSrre, 1993) at compile time. ", "after_sen": "Performing this optimization throughout the magic part of the grammar in figure 2 not only leads to a more succinct grammar, but brings about a different processing behavior. "}
{"citeStart": 171, "citeEnd": 175, "citeStartToken": 171, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, fragmentary utterances may be interpreted with respect to the previous user input, but what users say is often in reaction to the system's previous response [CP82, Sid83] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, question answering systems often fail to take account of the system's role as a conversational partner.", "mid_sen": "For example, fragmentary utterances may be interpreted with respect to the previous user input, but what users say is often in reaction to the system's previous response [CP82, Sid83] .", "after_sen": "In this paper we focus on interactive discourse. "}
{"citeStart": 268, "citeEnd": 287, "citeStartToken": 268, "citeEndToken": 287, "sectionName": "UNKNOWN SECTION NAME", "string": "Durational features: duration of A if A and B do not overlap: time separating A and B if they do overlap: duration of overlap seconds of overlap with any other speaker speech rate in A Lexical features: number of words in A number of content words in A ratio of words of A (respectively B) that are also in B (respectively A) ratio of content words of A (respectively B) that are also in B (respectively A) number of ¡ -grams present both in A and B (we built 3 features for ¡ ranging from 2 to 4) first and last word of A number of instances at any position of A of each cue word listed in (Hirschberg and Litman, 1994) does A contain the first/last name of speaker B? Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package. 6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2. We also wanted to determine if information about dialog acts (DA) helps the ranking task. If we hypothesize that only a limited set of paired DAs (e.g. offer-accept, question-answer, and apologydownplay) can be realized as adjacency pairs, then knowing the DA category of the B part and of all potential A parts should help in finding the most meaningful dialog act tag among all potential A parts; for example, the question-accept pair is admittedly more likely to correspond to an AP than e.g. backchannel-accept. We used the DA annotation that we also had available, and used the DA tag sequence of part A and B as a feature. 7 When we add the DA feature set, the accuracy reaches 91.34%, which is only slightly better than our 90.20% accuracy, which indicates that lexical, durational, and structural features capture most of the informativeness provided by DAs. This improved accuracy with DA information should of course not be considered as the actual accuracy of our system, since DA information is difficult to acquire automatically (Stolcke et al., 2000) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Durational features: duration of A if A and B do not overlap: time separating A and B if they do overlap: duration of overlap seconds of overlap with any other speaker speech rate in A Lexical features: number of words in A number of content words in A ratio of words of A (respectively B) that are also in B (respectively A) ratio of content words of A (respectively B) that are also in B (respectively A) number of ¡ -grams present both in A and B (we built 3 features for ¡ ranging from 2 to 4) first and last word of A number of instances at any position of A of each cue word listed in (Hirschberg and Litman, 1994) does A contain the first/last name of speaker B? ", "mid_sen": "Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package. ", "after_sen": "6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2. "}
{"citeStart": 306, "citeEnd": 325, "citeStartToken": 306, "citeEndToken": 325, "sectionName": "UNKNOWN SECTION NAME", "string": "To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . ", "mid_sen": "Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . ", "after_sen": "We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training."}
{"citeStart": 148, "citeEnd": 172, "citeStartToken": 148, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "Our Earley generator and the described compiler for off-line grammar optimization have been extensively tested with a large HPSG grammar. This testgrammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994 ) in the Troll system (Gerdemann and King, 1994) . Testing the developed techniques uncovered important constraints on the form of the phrase structure rules in a grammar imposed by the compiler.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our Earley generator and the described compiler for off-line grammar optimization have been extensively tested with a large HPSG grammar. ", "mid_sen": "This testgrammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994 ) in the Troll system (Gerdemann and King, 1994) . ", "after_sen": "Testing the developed techniques uncovered important constraints on the form of the phrase structure rules in a grammar imposed by the compiler."}
{"citeStart": 70, "citeEnd": 92, "citeStartToken": 70, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "In the training stage, a feature vector is constructed for each sense-annotated word covered by a semantic model. The features are model-specific, and feature vectors are added to the training set pertaining to the corresponding model. The label of each such feature vector consists of the target word and the corresponding sense, represented as word#sense. Table 1 shows the number of feature vectors constructed in this learning stage for each semantic model. To annotate new text, similar vectors are created for all content-words in the raw text. Similar to the training stage, feature vectors are created and stored separately for each semantic model. Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model. For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001) , which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002) , (Mihalcea, 2002) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model. ", "mid_sen": "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001) , which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002) , (Mihalcea, 2002) .", "after_sen": "Following the learning stage, each vector in the test data set is labeled with a predicted word and sense. "}
{"citeStart": 118, "citeEnd": 131, "citeStartToken": 118, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Top-down propagation can be precomputed to form a teachability table. Each entry in the table is a compiled dag which represents the relation between a non-terminal category and a rule used to rewrite the constituents in the teachability relation (i.e., reflexive, transitive closure of the left-corner path). For example, consider the following fragment of a grammar used in the syntax/semantics integrated system called LINK (Lytinen, 1992) : (This rule is used to parse phrases such as \"Kris's desk\" .)", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each entry in the table is a compiled dag which represents the relation between a non-terminal category and a rule used to rewrite the constituents in the teachability relation (i.e., reflexive, transitive closure of the left-corner path). ", "mid_sen": "For example, consider the following fragment of a grammar used in the syntax/semantics integrated system called LINK (Lytinen, 1992) : ", "after_sen": "(This rule is used to parse phrases such as \"Kris's desk\" .)"}
{"citeStart": 111, "citeEnd": 143, "citeStartToken": 111, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "To handle this problem, we point out some statistical tests, like the matched-pair t, sign and Wilcoxon tests (Harnett, 1982, Sec. 8.7 and 15.5) , which do not make this assumption. One can use these tests on the recall metric, but the precision and balanced F-score metric have too complex a form for these tests. For such complex metrics, we use a compute-intensive randomization test (Cohen, 1995, Sec. 5.3) , which also avoids this independence assumption.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This underestimate comes from these methods assuming that the techniques being compared produce independent results when in our experiments, the techniques being compared tend to produce positively correlated results.", "mid_sen": "To handle this problem, we point out some statistical tests, like the matched-pair t, sign and Wilcoxon tests (Harnett, 1982, Sec. 8.7 and 15.5) , which do not make this assumption. ", "after_sen": "One can use these tests on the recall metric, but the precision and balanced F-score metric have too complex a form for these tests. "}
{"citeStart": 106, "citeEnd": 128, "citeStartToken": 106, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to labeling as described in the preceding paragraph, we run MADA on the tuning and test sets. This gives us a set of model predictions for every feature of every word in the tuning and test sets. We use an implementation of a Downhill Simplex Method in many dimensions based on the method developed by Nelder and Mead (1965) to tune the weights applied to each feature. In a given iteration, the Simplex algorithm proposes a set of feature weights. These weights are given to a weight evaluation function; this function determines how effective a particular set of weights is at a given disambiguation task by calculating an overall score for the weight set: the number of words in the tuning set that were correctly disambiguated. In order to compute this score, the weight evaluation function examines each proposed analysis for each word in the tuning set. If the analysis and the model prediction for a feature of a given word agree, the analysis score for that analysis is incremented by the weight corresponding to that feature. The analysis with the highest analysis score is selected as the proper analysis for that word. If the selected analysis has a positive task label (i.e., it is a good answer for the disambiguation task in question), the overall score for the proposed weight set is incremented. The Simplex algorithm seeks to maximize this overall score (and thus choose the weight set that performs best for a given task).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This gives us a set of model predictions for every feature of every word in the tuning and test sets. ", "mid_sen": "We use an implementation of a Downhill Simplex Method in many dimensions based on the method developed by Nelder and Mead (1965) to tune the weights applied to each feature. ", "after_sen": "In a given iteration, the Simplex algorithm proposes a set of feature weights. "}
{"citeStart": 14, "citeEnd": 32, "citeStartToken": 14, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . A number of other systems have addressed part of the task. Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A number of other systems have addressed part of the task. ", "mid_sen": "Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. ", "after_sen": "Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. "}
{"citeStart": 86, "citeEnd": 107, "citeStartToken": 86, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "For the adjacency model, when the given compound is WlW2W3, we can estimate this ratio as: In both cases, we sum over all possible categories for the words in the compound. Because the dependency model equations have two factors, they are affected more severely by data sparseness. If the probability estimate for Pr(t2 ~ t3) is zero for all possible categories t2 and t3 then both the numerator and the denominator will be zero. This will conceal any preference given by the parameters involving Q. In such cases, we observe that the test instance itself provides the information that the event t2 --~ t3 can occur and we recalculate the ratio using Pr(t2 ---* t3) = k for all possible categories t2,t a where k is any non-zero constant. However, no correction is made to the probability estimates for Pr(tl --~ t2) and Pr(Q --* t3) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above. The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994) in one way. There, an additional weighting factor (of 2.0) is used to favour a left-branching analysis. This arises because their construction is based on the dependency model which predicts that left-branching analyses should occur twice as often. Also, the work reported in Lauer and Dras (1994) uses simplistic estimates of the probability of a word given its thesaurus category. The equations above assume these probabilities are uniformly constant. Section 3.2 below shows the result of making these two additions to the method. sit either probability estimate is zero, the other analysis is chosen. If both are zero the analysis is made as if the ratio were exactly unity. ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, no correction is made to the probability estimates for Pr(tl --~ t2) and Pr(Q --* t3) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above. ", "mid_sen": "The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994) in one way. ", "after_sen": "There, an additional weighting factor (of 2.0) is used to favour a left-branching analysis. "}
{"citeStart": 74, "citeEnd": 88, "citeStartToken": 74, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "This problem is analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach. We were led to seek a solution for this problem within DRT, because of DRT's advantages as a general theory of discourse, and its choice as the underlying formalism in another research project of ours, which deals with sentences such as 1-4, in the context of natural language specifications of computerized systems. In this paper, we propose such a solution, based on a careful distinction between different roles of Reichenbach's reference time (Reichenbach, 1947) , adapted from (Kamp and Reyle, 1993) . Figure 1 shows a 'minimal pair' of DRS's for sentence 1, one according to Partee's(1984) analysis and one according to ours.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose such a solution, based on a careful distinction between different roles of Reichenbach's reference time (Reichenbach, 1947) , adapted from (Kamp and Reyle, 1993) . ", "mid_sen": "Figure 1 shows a 'minimal pair' of DRS's for sentence 1, one according to Partee's(1984) analysis and one according to ours.", "after_sen": ""}
{"citeStart": 56, "citeEnd": 67, "citeStartToken": 56, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "Finite-state devices have important applications to many areas of computer science, including pattern matching, databases, and compiler technology. Although their linguistic adequacy to natural language processing has been questioned in the past (Chomsky, 1964) , there has recently been a dramatic renewal of interest in the application of finitestate devices to several aspects of natural language processing. This renewal of interest is due to the speed and compactness of finite-state representations. This efficiency is explained by two properties: finite-state devices can be made deterministic, and they can be turned into a minimal form. Such representations have been successfully applied to different aspects of natural language processing, such as morphological analysis and generation (Karttunen, Kaplan, and Zaenen 1992; Clemenceau 1993) , parsing (Roche 1993; Tapanainen and Voutilainen 1993) , phonology (Laporte 1993; Kaplan and Kay 1994) and speech recognition (Pereira, Riley, and Sproat 1994) . Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993) , none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993 ), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although finite-state machines have been used for part-of-speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993) , none of these approaches has the same flexibility as stochastic techniques. ", "mid_sen": "Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993 ), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired.", "after_sen": "Recently, Brill (1992) described a rule-based tagger that performs as well as taggers based upon probabilistic models and overcomes the limitations common in rule-based approaches to language processing: it is robust and the rules are automatically ac-quired. "}
{"citeStart": 64, "citeEnd": 75, "citeStartToken": 64, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, an alignment selection approach was proposed in (Huang, 2009) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009) . Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A greedy search method was utilized and it achieved higher translation performance than the baseline.", "mid_sen": "More recently, an alignment selection approach was proposed in (Huang, 2009) , which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. ", "after_sen": "The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). "}
{"citeStart": 20, "citeEnd": 40, "citeStartToken": 20, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "The fact that citing sentences cover different aspects of the cited paper and highlight its most important contributions motivates the idea of using citing sentences to summarize research. The comparison that Elkiss et al. (2008) performed between abstracts and citing sentences suggests that a summary generated from citing sentences will be different and probably more concise and informative than the paper abstract or a summary generated from the full text of the paper. For example, Table 5 shows the abstract of Resnik (1999) and 5 selected sentences that cite it in AAN. We notice that citing sentences con-tain additional facts that are not in the abstract, not only ones that summarize the paper contributions, but also those that criticize it (e.g., the last citing sentence in the Table) . Previous work has explored this research direction. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the sentences that cite it, and then applying network analysis techniques to find a set of sentences that covers as much of the paper facts as possible. proposed another summarization method that first extracts a number of important key phrases from the set of citing sentences, and then finds the best subset of sentences that covers as many key phrases as possible.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The fact that citing sentences cover different aspects of the cited paper and highlight its most important contributions motivates the idea of using citing sentences to summarize research. ", "mid_sen": "The comparison that Elkiss et al. (2008) performed between abstracts and citing sentences suggests that a summary generated from citing sentences will be different and probably more concise and informative than the paper abstract or a summary generated from the full text of the paper. ", "after_sen": "For example, Table 5 shows the abstract of Resnik (1999) and 5 selected sentences that cite it in AAN. "}
{"citeStart": 64, "citeEnd": 66, "citeStartToken": 64, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, there has been a lot of interest in Earley deduction [10] with applications to parsing and generation [13, 6, 7, 3] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Recently, there has been a lot of interest in Earley deduction [10] with applications to parsing and generation [13, 6, 7, 3] .", "after_sen": "Earley deduction is a very attractive framwork for natural language processing because it has the following properties and applications."}
{"citeStart": 176, "citeEnd": 188, "citeStartToken": 176, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "This dimension characterizes the potential effect that an utterance Ui has on the subsequent dialogue, and roughly corresponds to the classical notion of an illocutionary act (Austin, 1962; Searle, 1975) . As each Ui may simultaneously achieve multiple effects, it can be coded for three different aspects: Statement, Influence-on-Hearer, Influence-on-Speaker. ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "This dimension characterizes the potential effect that an utterance Ui has on the subsequent dialogue, and roughly corresponds to the classical notion of an illocutionary act (Austin, 1962; Searle, 1975) . ", "after_sen": "As each Ui may simultaneously achieve multiple effects, it can be coded for three different aspects: Statement, Influence-on-Hearer, Influence-on-Speaker. "}
{"citeStart": 75, "citeEnd": 89, "citeStartToken": 75, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. For instance, Grimshaw (1990) defines the thematic hierarchy as:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). ", "mid_sen": "PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . ", "after_sen": "PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . "}
{"citeStart": 69, "citeEnd": 87, "citeStartToken": 69, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "The over all number of features we have generated for all 278 target verbs was around 400,000. In all tables below the NB columns represent results of the naive Bayes algorithm as implemented within SNoW and the SNoW column represents the results of the sparse Winnow algorithm within SNOW. Table 1 summarizes the results of the experiments with the features sets (1), (2) above. The baseline experiment uses MLE, the majority predictor. In addition, we conducted the same experiment using trigram with backoff and the WER is 29.3%. From Table 2 : Comparison of the improvement achieved using similarity methods (Dagan et al., 1999) and using the methods presented in this paper. Results are shown in percentage of improvement in accuracy over the baseline. Table 2 compares our method to methods that use similarity measures (Dagan et al., 1999; Lee, 1999 ). Since we could not use the same corpus as in those experiments, we compare the ratio of improvement and not the WER. The baseline in this studies is different, but other than that the experiments are identical. We show an improvement over the best similarity method. Furthermore, we train using only 73,184 examples while (Dagan et al., 1999) train using 587, 833 examples. Given our experience with our approach on other data sets we conjecture that we could have improved the results further had we used that many training examples.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Results are shown in percentage of improvement in accuracy over the baseline. ", "mid_sen": "Table 2 compares our method to methods that use similarity measures (Dagan et al., 1999; Lee, 1999 ). ", "after_sen": "Since we could not use the same corpus as in those experiments, we compare the ratio of improvement and not the WER. "}
{"citeStart": 29, "citeEnd": 54, "citeStartToken": 29, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "Takes a value of 1 if the citing sentence contains multiple references, and 0 otherwise. If the citing sentence contains multiple references, it becomes less likely that the surrounding sentences are related. Table 1 : Features used for citation context identification nent in the sentence, we remove it to reduce parsing errors. Following our previous work (Abu Jbara and Radev, 2012) , we use a rule-based algorithm to determine whether a reference should be removed from the sentence or kept. The algorithm uses stylistic and linguistic features such as the style of the reference, the position of the reference, and the surrounding words to make the decision. When a reference is removed, the head of the closest noun phrase (NP) immediately before the position of the removed reference is used as a representative of the reference. This is needed for feature extraction as shown later in the paper.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 1 : Features used for citation context identification nent in the sentence, we remove it to reduce parsing errors. ", "mid_sen": "Following our previous work (Abu Jbara and Radev, 2012) , we use a rule-based algorithm to determine whether a reference should be removed from the sentence or kept. ", "after_sen": "The algorithm uses stylistic and linguistic features such as the style of the reference, the position of the reference, and the surrounding words to make the decision. "}
{"citeStart": 76, "citeEnd": 105, "citeStartToken": 76, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995) , these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). ", "mid_sen": "Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995) , these methods are not of practical interest, due to large hidden constants. ", "after_sen": "More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants."}
{"citeStart": 61, "citeEnd": 82, "citeStartToken": 61, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "We use a state-of-the-art English IE system as our baseline (Grishman et al., 2005) . This system extracts events independently for each sentence. Its training and test procedures are as follows.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use a state-of-the-art English IE system as our baseline (Grishman et al., 2005) . ", "after_sen": "This system extracts events independently for each sentence. "}
{"citeStart": 126, "citeEnd": 147, "citeStartToken": 126, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "The term dependent on tile grammar in the time complexity of the BU-LC unification-based parser described above is O (IC[2[RI3) , where ICI is the number of categories implicit in the grammar, and ]RI, the number of rules. The space complexity is dominated by the size of the parse forest, O(]C[) (these results are proved by Carroll, 1993) . For the ANLT grammar, in which features are nested to a maximum depth of two, ICI is finite but nevertheless extremely large (Briscoe et al., 1987b) 4.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The space complexity is dominated by the size of the parse forest, O(]C[) (these results are proved by Carroll, 1993) . ", "mid_sen": "For the ANLT grammar, in which features are nested to a maximum depth of two, ICI is finite but nevertheless extremely large (Briscoe et al., 1987b) 4.", "after_sen": "The grammar-dependent complexity of the LR parser makes it also appear intractable: Johnson (1989) shows that the number of LR(0) states for certain (pathological) grammars is exponentially related to the size of the grammar, and that there are some inputs which force an LR parser to visit all of these states in the course of a parse."}
{"citeStart": 74, "citeEnd": 96, "citeStartToken": 74, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we present a set of experiments aimed at improving on previous work on the task of supervised word-level detection of linguistic metaphor in running text. The use of supervised machine learning techniques for metaphor identification has increased manyfold in the recent years (see section 10, Related Work, for a review and references), partially due to the availability of largescale annotated resources for training and evaluating the algorithms, such as the VU Amsterdam corpus (Steen et al., 2010) , datasets built as part of a U.S. government-funded initiative to advance the state-of-art in metaphor identification and interpretation (Mohler et al., 2013; Strzalkowski et al., 2013) , and recent annotation efforts with other kinds of data (Beigman Klebanov and Flor, 2013; Jang et al., 2014) . Some of these data are publicly available (Steen et al., 2010) , allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014) , and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations. The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al. (2014) , which we use as a baseline.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some of these data are publicly available (Steen et al., 2010) , allowing for benchmarking and for measuring incremental improvements, which is the approach taken in this paper. ", "mid_sen": "We start with a baseline set of features and training regime from Beigman Klebanov et al. (2014) , and investigate the impact of re-weighting of training examples and of a suite of features related to concreteness of the target concept, as well as to the difference in concreteness within certain types of dependency relations. ", "after_sen": "The usage of concreteness features was previously discussed in the literature; to our knowledge, these features have not yet been evaluated for their impact in a comprehensive system for word-level metaphor detection, apart from the concreteness features as used in Beigman Klebanov et al. (2014) , which we use as a baseline."}
{"citeStart": 145, "citeEnd": 157, "citeStartToken": 145, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "Natural Language Generation, i.e., the process of building an adequate utterance for some given content, is by nature a decision making problem (Appelt, 1985) . Interna.1 decisions are made on the basis of the specified input. Unfortunately, input information can be insufficient in two respects:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Natural Language Generation, i.e., the process of building an adequate utterance for some given content, is by nature a decision making problem (Appelt, 1985) . Interna.", "after_sen": "1 decisions are made on the basis of the specified input. "}
{"citeStart": 114, "citeEnd": 127, "citeStartToken": 114, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a new space and a new metric for computing this distance. Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of questionanswer pairs (Q, S A ), we can train a probabilistic model for estimating the conditional probability P(Q | S A ). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR engine, one can find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a new space and a new metric for computing this distance. ", "mid_sen": "Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. ", "after_sen": "This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. "}
{"citeStart": 141, "citeEnd": 154, "citeStartToken": 141, "citeEndToken": 154, "sectionName": "UNKNOWN SECTION NAME", "string": "The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in (Collins, 1996) and the SPATTER parser described in (Jelinek et al., 1994; Magerman, 1995) . The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions. The bigram parser is a statistical CKY-style chart parser, which uses cooccurrence statistics of headmodifier pairs to find the best parse. The maximum entropy parser is a statistical shift-reduce style parser that cannot always access head-modifier pairs. For example, the checkcons(m,n) predicate of the maximum entropy parser may use two words such that neither is the intended head of the proposed consituent that the CHECK procedure must judge. And unlike the bigram parser, the maximum entropy parser cannot use head word information besides \"flat\" chunks in the right context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in (Collins, 1996) and the SPATTER parser described in (Jelinek et al., 1994; Magerman, 1995) . ", "after_sen": "The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions. "}
{"citeStart": 196, "citeEnd": 214, "citeStartToken": 196, "citeEndToken": 214, "sectionName": "UNKNOWN SECTION NAME", "string": "In terms of ACE entity classes, we only consider referential mentions for annotation, i.e. only \"Specific Referential (SPC)\", \"Generic Referential (GEN)\", and \"Under-specified Referential (USP)\" (LDC, 2004, p. 17f.) entities, but we do not explicitly differentiate between these classes in our corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because of this we do not even distinguish entity types in our annotation scheme.", "mid_sen": "In terms of ACE entity classes, we only consider referential mentions for annotation, i.e. only \"Specific Referential (SPC)\", \"Generic Referential (GEN)\", and \"Under-specified Referential (USP)\" (LDC, 2004, p. 17f.) entities, but we do not explicitly differentiate between these classes in our corpus.", "after_sen": "Only noun phrases (NPs, including coordinations of NPs), possessive determiners (\"my\", \"your\", . . . ) and proper names (which may be part of NPs as in \"Sheffield's GATE system\") were considered as possible entity mentions. "}
{"citeStart": 94, "citeEnd": 104, "citeStartToken": 94, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 , all of them share a certain limitation in their formal techniques for analyzing paragraph structure. Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. Our interest, however, lies precisely in that area.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Although there are other discussions of the paragraph as a central element of discourse (e.g. Chafe 1979 , Halliday and Hasan 1976 , Longacre 1979 , Haberlandt et al. 1980 , all of them share a certain limitation in their formal techniques for analyzing paragraph structure. ", "after_sen": "Discourse linguists show little interest in making the structural descriptions precise enough so that a computational grammar of text could adapt them and use them. "}
{"citeStart": 122, "citeEnd": 134, "citeStartToken": 122, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Before explaining our CLIR system, we classify existing CLIR into three approaches in terms of the implementation of the translation phase. The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . We prefer the first approach, the \"query translation\", to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. At the same time, we concede that other CLIR approaches are worth further exploration. Figure 1 depicts the overall design of our CLIR system, where most components are the same as those for monolingual IR, excluding \"translator\".", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . ", "mid_sen": "The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . ", "after_sen": "We prefer the first approach, the \"query translation\", to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. "}
{"citeStart": 154, "citeEnd": 180, "citeStartToken": 154, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 188, "citeEnd": 200, "citeStartToken": 188, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. In principle, this might be done by providing the generator with vague input-in which case no special algorithms are needed-but suitably contextualized vague input is often not available (Mellish 2000) . The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. It is this avenue that we have explored in this article, in combination with various (incremental and other) approaches to GRE. Far from being a peculiarity of a few adjectives, vagueness is widespread. We believe that our approach can be applied to a variety of situations in which vagueness affects referring expressions including, for example, r color terms (Section 9.3); r nouns that allow different degrees of strictness (Section 9.5); r degrees of salience (Section 9.4); and r imprecise pointing (Section 9.5).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the usefulness of NLG resides in its ability to present data in human-accessible form, then vagueness must surely be one of its central instruments, because it allows the suppression of irrelevant detail. ", "mid_sen": "In principle, this might be done by providing the generator with vague input-in which case no special algorithms are needed-but suitably contextualized vague input is often not available (Mellish 2000) . ", "after_sen": "The only practical alternative is to provide the generator with \"crisp\" (i.e., quantitative) input, allowing the generator to be hooked on to a general-purpose database. "}
{"citeStart": 54, "citeEnd": 64, "citeStartToken": 54, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "N: number of triples in the corpus. We use it instead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. Weighted mutual information meliorates this effect by adding", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "N: number of triples in the corpus. ", "mid_sen": "We use it instead of point-wise mutual information in Lin (1998) because the latter tends to overestimate the association between two parts with low frequencies. ", "after_sen": "Weighted mutual information meliorates this effect by adding"}
{"citeStart": 10, "citeEnd": 27, "citeStartToken": 10, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000) . Here, we argue that even with its flexibility, CCG as standardly defined is not permissive enough for certain linguistic constructions and greater incrementality. Following Wittenburg (1987) , we remedy this by adding a set of rules based on the D combinator of combinatory logic (Curry and Feys, 1958) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here, we argue that even with its flexibility, CCG as standardly defined is not permissive enough for certain linguistic constructions and greater incrementality. ", "mid_sen": "Following Wittenburg (1987) , we remedy this by adding a set of rules based on the D combinator of combinatory logic (Curry and Feys, 1958) .", "after_sen": "(1) x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx)"}
{"citeStart": 150, "citeEnd": 171, "citeStartToken": 150, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al., 2006) . Since the test data of the shared-task consists of articles that represent the different publication years, the effects of the publication years of the texts used for self-training would be interesting to study.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We expect that reranking architecture can readily accommodate dictionarybased features, because we can apply elaborated string-matching algorithms to the qualified candidate strings available at reranking phase.", "mid_sen": "We also plan to apply self-training of n-best tagger which successfully boosted the performance of one of the best existing English syntactic parser (McClosky et al., 2006) . ", "after_sen": "Since the test data of the shared-task consists of articles that represent the different publication years, the effects of the publication years of the texts used for self-training would be interesting to study."}
{"citeStart": 1, "citeEnd": 19, "citeStartToken": 1, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "Another view of intent is more generic, mining or learning search intents without any kind of pre-defined intent category and clustering method is often used. Methods including (Sadikov et al., 2010; Yamamoto et al., 2012; Cheung and Li, 2012) cast intent as represented by a pattern or template consisting of a sequence of semantic concepts or lexical items. (Tan et al., 2012) encode intent in language models, aware of long-lasting interests. (Ren et al., 2014) uses an unsupervised heterogeneous clustering. (Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Ren et al., 2014) uses an unsupervised heterogeneous clustering. ", "mid_sen": "(Yin and Shah, 2010) capture generic intents around a certain named entities and model their relationships in a tree taxonomy and mine broad latent modifiers of intent aspect , which are similar to our motivation, while we model more than intent phrases, but intent topics. ", "after_sen": "We do not split queries into clusters or subtopics relevant to the original query to indicate a intent, but link them in an graph with intent feature similarity, weakly or strongly, in a holistical view."}
{"citeStart": 215, "citeEnd": 235, "citeStartToken": 215, "citeEndToken": 235, "sectionName": "UNKNOWN SECTION NAME", "string": "unlike MaxEnt, cannot be used as a probabilistic component in a larger model. MaxEnt can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the POS tags, such as noun phrases, or entire parse trees, as in (Jelinek et al., 1994 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "unlike MaxEnt, cannot be used as a probabilistic component in a larger model. ", "mid_sen": "MaxEnt can provide a probability for each tagging decision, which can be used in the probability calculation of any structure that is predicted over the POS tags, such as noun phrases, or entire parse trees, as in (Jelinek et al., 1994 .", "after_sen": "Thus MaxEnt has at least one advantage over each of the reviewed POS tagging techniques. "}
{"citeStart": 145, "citeEnd": 154, "citeStartToken": 145, "citeEndToken": 154, "sectionName": "UNKNOWN SECTION NAME", "string": "There are several possibilities for the definition of \"clear deviation\" above. One could look at differences in the ranking over all words, using a mea- sure such as pairwise agreement of rankings or a ranking correlation coefficient, such as Spearman's. One could also use the rankings to estimate probability distributions and compare the distributions with measures such as alpha-skew divergence (Lee, 1999) . A simple definition would be where the rankings assign different predominant senses to a word. Taking this simple definition of deviation, we demonstrate how this might be done for our corpora.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One could look at differences in the ranking over all words, using a mea- sure such as pairwise agreement of rankings or a ranking correlation coefficient, such as Spearman's. ", "mid_sen": "One could also use the rankings to estimate probability distributions and compare the distributions with measures such as alpha-skew divergence (Lee, 1999) . ", "after_sen": "A simple definition would be where the rankings assign different predominant senses to a word. "}
{"citeStart": 72, "citeEnd": 87, "citeStartToken": 72, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research. An interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rulebased probability models. Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997) , Charniak (1997) , or Ratnaparkhi (1997) . However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice. Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which o b viates the standard e ort required for treebank training handannotating large corpora of speci c domains of speci c languages with speci c parse types. Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. Experimental results con rming this wisdom have beenpresented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. ", "mid_sen": "Experimental results con rming this wisdom have beenpresented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs.", "after_sen": "In this paper, we present a new lexicalized stochastic model for constraint-based grammars that employs a combination of headword frequencies and EM-based clustering for grammar lexicalization. "}
{"citeStart": 139, "citeEnd": 160, "citeStartToken": 139, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "For the problem with multi-sentence discourses, and the \"threads\" that sentences continue, we use an implementation of tempo-rM centering (Kameyama et al., 1993; Poesio, 1994 ). This is a technique similar to the type of centering used for nominal anaphora (Sidner, 1983; Grosz et al., 1983) . Centering assumes that discourse understanding requires some notion of \"aboutness.\" While nominal centering assumes there is one object that the current discourse is \"about,\" temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread. Kameyama et al. (1993) confirmed these preferences when testing their ideas on the Brown corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2", "mid_sen": "For the problem with multi-sentence discourses, and the \"threads\" that sentences continue, we use an implementation of tempo-rM centering (Kameyama et al., 1993; Poesio, 1994 ). ", "after_sen": "This is a technique similar to the type of centering used for nominal anaphora (Sidner, 1983; Grosz et al., 1983) . "}
{"citeStart": 86, "citeEnd": 110, "citeStartToken": 86, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The results of each of the predictors used in the Inside/0utside method are presented in Table 3 . The results are comparable to other results reported using the Inside/Outside method (Ramshaw and Marcus, 1995 ) (see Table 7 . We have observed that most of the mistaken predictions of base NPs involve predictions with respect to conjunctions, gerunds, adverbial NPs and some punctuation marks. As reported in (Argamon et al., 1998) , most base NPs present in ~he data are less or equal than 4 words long. This implies that our predictors tend to break up long base NPs into smaller ones.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results of each of the predictors used in the Inside/0utside method are presented in Table 3 . ", "mid_sen": "The results are comparable to other results reported using the Inside/Outside method (Ramshaw and Marcus, 1995 ) (see Table 7 . ", "after_sen": "We have observed that most of the mistaken predictions of base NPs involve predictions with respect to conjunctions, gerunds, adverbial NPs and some punctuation marks. "}
{"citeStart": 293, "citeEnd": 315, "citeStartToken": 293, "citeEndToken": 315, "sectionName": "UNKNOWN SECTION NAME", "string": "Strong empirical evidence has been presented over the past 15 years indicating that the human sentence processing mechanism makes online use of contextual information in the preceding discourse (Crain and Steedman, 1985; Altmann and Steedman, 1988; Britt, 1994) and in the visual environment (Tanenhaus et al., 1995) . These results lend support to Mark Steedman's (1989) \"intuition\" that sentence interpretation takes place incrementally, and that partial interpretations are being built while the sentence is being perceived. This is a very commonly held view among psycholinguists today.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Strong empirical evidence has been presented over the past 15 years indicating that the human sentence processing mechanism makes online use of contextual information in the preceding discourse (Crain and Steedman, 1985; Altmann and Steedman, 1988; Britt, 1994) and in the visual environment (Tanenhaus et al., 1995) . ", "after_sen": "These results lend support to Mark Steedman's (1989) \"intuition\" that sentence interpretation takes place incrementally, and that partial interpretations are being built while the sentence is being perceived. "}
{"citeStart": 98, "citeEnd": 109, "citeStartToken": 98, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. (Quinlan 1986; Bahl et al. 1989 ) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994) . We tested the Satz system using the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. These results are discussed in Section 4.10.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. ", "mid_sen": "(Quinlan 1986; Bahl et al. 1989 ) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994) . ", "after_sen": "We tested the Satz system using the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. "}
{"citeStart": 11, "citeEnd": 38, "citeStartToken": 11, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "We employ the Gibbs sampling algorithm (Gilks et al., 1996) . Unlike in (Mareček andŽabokrtský, 2012) , where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming. The algorithm works as follows:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We employ the Gibbs sampling algorithm (Gilks et al., 1996) . ", "mid_sen": "Unlike in (Mareček andŽabokrtský, 2012) , where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming. ", "after_sen": "The algorithm works as follows:"}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999) , which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. Section 2 overviews our CLIR system, and Section 3 elaborates on the translation module focusing on compound word translation and transliteration. Section 4 then evaluates the effectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. ", "mid_sen": "Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. ", "after_sen": "However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. "}
{"citeStart": 161, "citeEnd": 172, "citeStartToken": 161, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "Recall Precision which, while demonstrating that the focusbased algorithm is applicable to real-world text, does question whether the more complex algorithm has any real advantage over LaSIE's original simple approach. The lower performance of the focus-based algorithm is mainly due to an increased reliance on the accuracy and completeness of the grammatical structure identified by the parser. For example, the resolution of a pronoun will be skipped altogether if its role as a verb argument is missed by the parser. Partial parses will also affect the identification of EE boundaries, on which the focus update rules depend. For example, if the parser fails to attach a prepositional phrase containing an antecedent, it will then be missed from the focus registers and so the IRs (see (Azzam, 1995) ). The simple LaSIE approach, however, will be unaffected in this case.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Partial parses will also affect the identification of EE boundaries, on which the focus update rules depend. ", "mid_sen": "For example, if the parser fails to attach a prepositional phrase containing an antecedent, it will then be missed from the focus registers and so the IRs (see (Azzam, 1995) ). ", "after_sen": "The simple LaSIE approach, however, will be unaffected in this case."}
{"citeStart": 19, "citeEnd": 43, "citeStartToken": 19, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "We have built a system that explicitly uses discourse obligations and communicative intentions to partake in natural dialogue. This system plays the role of the dialogue manager in the TRAINS dialogue system which acts as an intelligent planning assistant in a transportation domain. While this is a domain where the assumption of co-operation is generally valid, the obligation model still provides for a much simpler analysis of the discourse behavior than a strongly plan-based account. An example of a dialogue that the TRAINS system can engage in is shown in Figure 1 . Below we describe parts of the discourse model in more detail and then show how it is used to account for aspects of this dialogue. The TRAINS System [Allen and Schubert, 1991] is alarge integrated natural language conversation and plan reasoning 2This is a slightly simplified version of a spoken dialogue between two people. The original is dialogue 91-6.1 from [Gross et al., 1993] . The utterance numbering system used here reflects the relation to the turn and utterance numbering used there. '3-7' represents utterance 7 within turn 3. '=' is used to indicate merged utterances. Thus '3-3=6' spans four utterances in turn 3 of the original, and 9=13 replaces turns 9 through 13 in the original. system. We concentrate here, however, on just one part of that system, the discourse actor which drives the actions of the dialogue manager module. The dialogue manager is responsible for maintaining the flow of conversation and making sure that the conversational goals are met. For this system, the main goals are that an executable plan which meets the user's goals is constructed and agreed upon by both the system and the user and then that the plan is executed.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Below we describe parts of the discourse model in more detail and then show how it is used to account for aspects of this dialogue. ", "mid_sen": "The TRAINS System [Allen and Schubert, 1991] is alarge integrated natural language conversation and plan reasoning 2This is a slightly simplified version of a spoken dialogue between two people. ", "after_sen": "The original is dialogue 91-6.1 from [Gross et al., 1993] . "}
{"citeStart": 142, "citeEnd": 154, "citeStartToken": 142, "citeEndToken": 154, "sectionName": "UNKNOWN SECTION NAME", "string": "Figure 2: Example of \"relaxed\" maximal common sui)graph and maximal join algorithms Semantic distance between concepts. In the maximal common subgraph algorithm proposed by (Sow% :1984), two concepts (C1,CY) could be matched if one snbsumed the other in the concept hierarchy. We can relax that criteria to match two concepts when a third concept C which subsumes C1 and C2 has a high enough degree of informativeness (Resnik, 1995) . The concept hierarchy can be useful in many cases, but it is generated from the dictionary and might not be complete enough to find all similar concepts. In the example of Figure 2 , when using tile concept hierarchy to establish the similarity between pen and crayon, we find that; one is a subclass of lool and the other of wax, both then are substoned by the general concept something. We have reached the root of the noun tree in the concept hierarchy and this would give a similarity of 0 based on the informativeness notion.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the maximal common subgraph algorithm proposed by (Sow% :1984), two concepts (C1,CY) could be matched if one snbsumed the other in the concept hierarchy. ", "mid_sen": "We can relax that criteria to match two concepts when a third concept C which subsumes C1 and C2 has a high enough degree of informativeness (Resnik, 1995) . ", "after_sen": "The concept hierarchy can be useful in many cases, but it is generated from the dictionary and might not be complete enough to find all similar concepts. "}
{"citeStart": 282, "citeEnd": 301, "citeStartToken": 282, "citeEndToken": 301, "sectionName": "UNKNOWN SECTION NAME", "string": "Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily. Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. Luhn, 1958; Edmundson, 1969) or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g. Kupiec et al., 1995; Mani and Bloedorn, 1998) . Neither approach is very satisfactory. Relying only on your own intuitions inevitably creates a biased resource; indeed, Rath et al. (1961) report low agreement between human judges carrying out this kind of task. On the other hand, using abstracts as targets is not necessarily a good gold standard for comparison of the systems' results, although abstracts are the only kind of gold standard that comes for free with the papers. Even if the abstracts are written by professional abstractors, there are considerable differences in length, structure, and information content. This is due to differences in the common abstract presentation style in different disciplines and to the projected use of the abstracts (cf. Liddy, 1991) . In the case of our corpus, an additional problem was the fact that the abstracts are written by the authors themselves and thus susceptible to differences in individual writing style.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily. ", "mid_sen": "Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. Luhn, 1958; Edmundson, 1969) or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g. Kupiec et al., 1995; Mani and Bloedorn, 1998) . ", "after_sen": "Neither approach is very satisfactory. "}
{"citeStart": 95, "citeEnd": 113, "citeStartToken": 95, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. We refer to these as linguistic level representations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. ", "mid_sen": "The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. ", "after_sen": "Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. "}
{"citeStart": 148, "citeEnd": 161, "citeStartToken": 148, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Schutze, 1998) . In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human judges has been made by McCarthy, Keller and Caroll (McCarthy et al., 2003) for verb-particle constructions. (See Section for more details). Some preliminary work on recognition of V-N collocations was presented in (Venkatapathy and Joshi, 2004) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Various statistical measures have been suggested for ranking expressions based on their compositionality. ", "mid_sen": "Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Schutze, 1998) . ", "after_sen": "In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). "}
{"citeStart": 346, "citeEnd": 366, "citeStartToken": 346, "citeEndToken": 366, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach also makes use of the scientific discourse for summarisation purposes. We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative. We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme (Liakata et al., 2010; Liakata et al., 2012) . The CoreSC scheme is \"uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence\" (White et al., 2011) , which are not readily identifiable by other annotation schemes. Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZ-II (Teufel et al., 2009) , it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZ-II focusses on the attribution of knowledge claims and the relation with previous work (Liakata et al., 2010) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The CoreSC scheme is \"uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence\" (White et al., 2011) , which are not readily identifiable by other annotation schemes. ", "mid_sen": "Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZ-II (Teufel et al., 2009) , it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZ-II focusses on the attribution of knowledge claims and the relation with previous work (Liakata et al., 2010) .", "after_sen": "We then use the distribution of CoreSC categories observed in abstracts to create a content model which provides a skeleton for extractive summaries. "}
{"citeStart": 168, "citeEnd": 181, "citeStartToken": 168, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we have categorized several forms of ellipsis and event reference according to two features: (1) whether the form leaves behind an empty constituent in the syntax, and (2) whether the form is anaphoric in the semantics. We have also described two forms of discourse inference, namely Common Topic inference and Coherent Situation inference. The interaction between the two features and the two types of discourse inference predicts facts concerning gapping, VP-ellipsis, event reference, and interclausal coherence for which it is otherwise difficult to account. In future work we will address other forms of ellipsis and event reference, as well as integrate a previous account of strict and sloppy ambiguity into this framework (Kehler, 1993a) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The interaction between the two features and the two types of discourse inference predicts facts concerning gapping, VP-ellipsis, event reference, and interclausal coherence for which it is otherwise difficult to account. ", "mid_sen": "In future work we will address other forms of ellipsis and event reference, as well as integrate a previous account of strict and sloppy ambiguity into this framework (Kehler, 1993a) .", "after_sen": ""}
{"citeStart": 103, "citeEnd": 116, "citeStartToken": 103, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007) ). The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unsupervised methods are in principle 6 languageindependent, and can therefore easily be applied to other languages.", "mid_sen": "We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007) ). ", "after_sen": "The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. "}
{"citeStart": 121, "citeEnd": 140, "citeStartToken": 121, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "We h a ve presented a new approach t o s t o c hastic modeling of constraint-based grammars. Our experimental results show that EM training can in fact be very helpful for accurate stochastic modeling in natural language processing. We conjecture that this result is due partly to the fact that the space of parses produced by a constraint-based grammar is only mildly incomplete , i.e. the ambiguity rate can bekept relatively low. Another reason may be that EM is especially useful for log-linear models, where the search space in maximization can be kept under control. Furthermore, we have introduced a new class-based grammar lexicalization, which again uses EM training and incorporates a predisambiguation routine into log-linear models. An impressive gain in performance could also be demonstrated for this method. Clearly, a central task of future work is a further exploration of the relation between complete-data and incomplete-data estimation for larger, manually disambiguated treebanks. An interesting question is whether a systematic variation of training data size along the lines of the EM-experiments of Nigam et al. (2000) for text classi cation will show similar results, namely a systematic dependence of the relative gain due to EM training from the relative sizes of unannotated and annotated data. Furthermore, it is important to show that EMbased methods can be applied successfully also to other statistical parsing frameworks.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clearly, a central task of future work is a further exploration of the relation between complete-data and incomplete-data estimation for larger, manually disambiguated treebanks. ", "mid_sen": "An interesting question is whether a systematic variation of training data size along the lines of the EM-experiments of Nigam et al. (2000) for text classi cation will show similar results, namely a systematic dependence of the relative gain due to EM training from the relative sizes of unannotated and annotated data. ", "after_sen": "Furthermore, it is important to show that EMbased methods can be applied successfully also to other statistical parsing frameworks."}
{"citeStart": 4, "citeEnd": 26, "citeStartToken": 4, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "The best correlation result for the 1987 Roget's Thesaurus is 0.8725 when the mean is 4, the POS. The maximum correlation for the 1911 Thesaurus is 0.8367, where the mean is 5, the Head. The maximum for WordNet is 0.8506, where the mean is 3, or the first hypernym synset. This suggests that the POS and Head are most important for representing text in Roget's Thesaurus, while the first hypernym is most important for representing text using Word-Net. For the Simple method, we found a more modest correlation of 0.6969. Several other methods have given very good scores on this data set. For the system in (Li et al., 2006) , where this data set was first introduced, a correlation of 0.816 with the human annotators was achieved. The mean of all human annotators had a score of 0.825, with a standard deviation of 0.072. In (Islam and Inkpen, 2007) , an even better system was proposed, with a correlation of 0.853.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The mean of all human annotators had a score of 0.825, with a standard deviation of 0.072. ", "mid_sen": "In (Islam and Inkpen, 2007) , an even better system was proposed, with a correlation of 0.853.", "after_sen": "Selecting the mean that gives the best correlation could be considered as training on test data. "}
{"citeStart": 51, "citeEnd": 76, "citeStartToken": 51, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work by Manny Rayner and the author, see [Samuelsson &~ Rayner 1991] attempts to tailor an existing natural-language system to a specific application domain by extracting a specialized grammar from the original one using a large set of training examples. The training set is a treebank consisting of implicit parse trees that each specify a verified analysis of an input sentence. The parse trees are implicit in the sense that each node in the tree is the (mnemonic) name of the grammar rule resolved on at that point, rather than the syntactic category of the LHS of the grammar rule as is the case in an ordinary parse tree. Figure 1 shows five examples of implicit parse trees. The analyses are verified in the sense that each analysis has been judged to be the preferred one for that input sentence by a human evaluator using a semi-automatic evaluation method.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Previous work by Manny Rayner and the author, see [Samuelsson &~ Rayner 1991] attempts to tailor an existing natural-language system to a specific application domain by extracting a specialized grammar from the original one using a large set of training examples. ", "after_sen": "The training set is a treebank consisting of implicit parse trees that each specify a verified analysis of an input sentence. "}
{"citeStart": 141, "citeEnd": 165, "citeStartToken": 141, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "We believe our current grammar of German could be extended to a robust free-text chunk/phrase grammar in the style of the English grammar of Carroll and Rooth (1998) with about a month's work, and to a free-text grammar treating verb-second clauses and additional complementation structures (notably extraposed clausal complements) with about one year of additional grammar development and experiment. These increments in the grammar could easily double the number of rules. However this would probably not pose a problem for the parsing and estimation software.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much work remains to be done in this area, and we feel that we are just beginning to develop understanding of the time course of parameter estimation, and of the general efficacy of EM estimation of lexicalized PCFGs as evaluated by linguistic criteria.", "mid_sen": "We believe our current grammar of German could be extended to a robust free-text chunk/phrase grammar in the style of the English grammar of Carroll and Rooth (1998) with about a month's work, and to a free-text grammar treating verb-second clauses and additional complementation structures (notably extraposed clausal complements) with about one year of additional grammar development and experiment. ", "after_sen": "These increments in the grammar could easily double the number of rules. "}
{"citeStart": 115, "citeEnd": 126, "citeStartToken": 115, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The linguistic statements made by developers of current grammar checkers based on NLP ted> niques are often contradictory regarding the types of errors that grammar checkers must correct automatically. (Veronis, 1988) claims that native writers are unlikely to produce errors involving morphological features, while (Vosse, 1992) acce.t)ts such morpho-syntactic errors, in spite of tile fact that an examination of texts by the author revealed that their appearance in native writer's texts is not frequent. Both authors agree in characterizing morpho-syntactic errors as a sainple of lack of competence. On the other hand, an examination of real texts produced by Spanish writers revealed that they do produce morpho-syntactic errors I . Spanish is an inflectiolml language, which increases the possibilities of such exrors. Nevertheless, other errors related to structural configuration of the language ark: produced as well.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The linguistic statements made by developers of current grammar checkers based on NLP ted> niques are often contradictory regarding the types of errors that grammar checkers must correct automatically. ", "mid_sen": "(Veronis, 1988) claims that native writers are unlikely to produce errors involving morphological features, while (Vosse, 1992) acce.t)ts such morpho-syntactic errors, in spite of tile fact that an examination of texts by the author revealed that their appearance in native writer's texts is not frequent. ", "after_sen": "Both authors agree in characterizing morpho-syntactic errors as a sainple of lack of competence. "}
{"citeStart": 113, "citeEnd": 134, "citeStartToken": 113, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative. The model learned simple aspects of narrative structure ('narrative chains') by extracting events that share a single participant, the protagonist. In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004) , and FrameNet frames (Baker et al., 1998) . This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The model learned simple aspects of narrative structure ('narrative chains') by extracting events that share a single participant, the protagonist. ", "mid_sen": "In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004) , and FrameNet frames (Baker et al., 1998) . ", "after_sen": "This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles."}
{"citeStart": 26, "citeEnd": 36, "citeStartToken": 26, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, our procedure induces a \"hard\" part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994 ) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, \"fun\" SBecause of phrases like \"I had sweet potatoes\", forms of \"have\" cannot serve as a reliable discriminator either.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is by no means generally accepted that such a classification is linguistically adequate. ", "mid_sen": "There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994 ) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. ", "after_sen": "For example, \"fun\" SBecause of phrases like \"I had sweet potatoes\", forms of \"have\" cannot serve as a reliable discriminator either."}
{"citeStart": 14, "citeEnd": 37, "citeStartToken": 14, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "In 3athere is a causal relationship between Mary's pushing John and his falling, and the second event is understood to precede the first. In (3b), the second sentence is an elaboration of the first, and they therefore refer to aspects of the same event rather than to two sequential events. It has been suggested that only world knowledge allows one to detect that the default is being overridden here. For example , Lascarides Asher (1991) suggest that general knowledge postulates (in the case of (3a): that a pushing can cause a falling) can be invoked to generate the backward movement reading.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has been suggested that only world knowledge allows one to detect that the default is being overridden here. ", "mid_sen": "For example , Lascarides Asher (1991) suggest that general knowledge postulates (in the case of (3a): that a pushing can cause a falling) can be invoked to generate the backward movement reading.", "after_sen": "The problem for practical systems is twofold: we could assume that in the case of narrative the Kamp/Hinrichs/Partee algorithm is the default, but each time the default is applied we would need to check all our available world knowledge to see whether there isn't a world knowledge postulate which might be overriding this assumption. "}
{"citeStart": 212, "citeEnd": 236, "citeStartToken": 212, "citeEndToken": 236, "sectionName": "UNKNOWN SECTION NAME", "string": "Empirical results presented in the literature show that bidirectional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998) . We leave this for future work.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). ", "mid_sen": "We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998) . ", "after_sen": "We leave this for future work."}
{"citeStart": 226, "citeEnd": 243, "citeStartToken": 226, "citeEndToken": 243, "sectionName": "UNKNOWN SECTION NAME", "string": "For acquisition of definitional patterns and pattern refinement we used the ACL Anthology, a digital archive of scientific papers from conferences, workshops, and journals on Computational Linguistics and Language Technology (Bird et al., 2008) . 2 The corpus consisted of 18,653 papers published between 1965 and 2011, with a total of 66,789,624 tokens and 3,288,073 sentences. This corpus was also used to extract sentences for the evaluation using both extraction methods.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At the initial stage two development corpora were used: a digitalized early draft of the Jurafsky-Martin textbook (Jurafsky and Martin, 2000) and the WeScience Corpus, a set of Wikipedia articles in the domain of Natural Language Processing (Ytrestøl et al., 2009) . 1 The former served as a source of seed domain terms with definitions, while the latter was used for seed pattern creation.", "mid_sen": "For acquisition of definitional patterns and pattern refinement we used the ACL Anthology, a digital archive of scientific papers from conferences, workshops, and journals on Computational Linguistics and Language Technology (Bird et al., 2008) . 2 The corpus consisted of 18,653 papers published between 1965 and 2011, with a total of 66,789,624 tokens and 3,288,073 sentences. ", "after_sen": "This corpus was also used to extract sentences for the evaluation using both extraction methods."}
{"citeStart": 88, "citeEnd": 110, "citeStartToken": 88, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . EventMine-MK is available as a component of the U-Compare interoperable text mining system 4 (Kano et al., 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . ", "mid_sen": "Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . ", "after_sen": "In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. "}
{"citeStart": 202, "citeEnd": 229, "citeStartToken": 202, "citeEndToken": 229, "sectionName": "UNKNOWN SECTION NAME", "string": "phology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology. An additional case of super-segmental morphology is the case of Pronominal Clitics. Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements. The additional morphological material in such cases appears after the stem and realizes the extended meaning. The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The additional morphological material in such cases appears after the stem and realizes the extended meaning. ", "mid_sen": "The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008) .", "after_sen": ""}
{"citeStart": 87, "citeEnd": 113, "citeStartToken": 87, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "In this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal (1999) in their combination of content and language model probabilities, by backtracking at every state in order to discourage repeated words and avoid loops.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We then use a Viterbi-style algorithm to find the most likely word sequence.", "mid_sen": "In this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal (1999) in their combination of content and language model probabilities, by backtracking at every state in order to discourage repeated words and avoid loops.", "after_sen": "Supertag This is a variant of the approach above, but using supertags (Bangalore and Joshi, 1999) instead of PoS tags. "}
{"citeStart": 92, "citeEnd": 93, "citeStartToken": 92, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "(2) equally distribute frequency of word collcations to all possible category collocations [4] (3) calculate the probability of each category collocation and distribute frequency based on these probabilities; the probability of collocations are calculated by using method (2) [4] (4) determine the correct category collocation by using statistical methods other than word collocations [2, 10, 9, 6] Fortunately, there are few words that are itssigned multiple categories in BGH. Therefore, we use method (1) . Word collocations containing words with multiple categories represent about 1/3 of the corpus. If we used other thesauruses, which assign multiple categories to more words, we would need to use method (2), (3), or (4).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(1) use word collocations with all words is assigned a single category.", "mid_sen": "(2) equally distribute frequency of word collcations to all possible category collocations [4] (3) calculate the probability of each category collocation and distribute frequency based on these probabilities; the probability of collocations are calculated by using method (2) [4] (4) determine the correct category collocation by using statistical methods other than word collocations [2, 10, 9, 6] Fortunately, there are few words that are itssigned multiple categories in BGH. ", "after_sen": "Therefore, we use method (1) . "}
{"citeStart": 59, "citeEnd": 77, "citeStartToken": 59, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "An artificial ambiguous word can be coined with the monosemous words in table 1. This process is similar to the use of general pseudowords (Gale et al., 1992b; Gaustad, 2001; Nakov and Hearst, 2003) , but has some essential differences. This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does. Thus, we call it an equivalent pseudoword (EP) for its equivalence with the real ambiguous word. It's apparent that the equivalent pseudoword has provided a new way to unsupervised WSD. Table 1 . Synonymous Monosemous Words for the Ambiguous Word \"把握\"", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An artificial ambiguous word can be coined with the monosemous words in table 1. ", "mid_sen": "This process is similar to the use of general pseudowords (Gale et al., 1992b; Gaustad, 2001; Nakov and Hearst, 2003) , but has some essential differences. ", "after_sen": "This artificial ambiguous word need to simulate the function of the real ambiguous word, and to acquire semantic knowledge as the real ambiguous word does. "}
{"citeStart": 301, "citeEnd": 314, "citeStartToken": 301, "citeEndToken": 314, "sectionName": "UNKNOWN SECTION NAME", "string": "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev's parserfree version of Lappin and Leass' RAP (Kennedy and Boguraev, 1996) , Baldwin's pronoun resolution method (Baldwin, 1997 ) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b) . All three of these algorithms share a similar pre-processing methodology: they do not rely on a parser to process the input and instead use POS taggers and NP extractors; nor do any of the methods make use of semantic or real-world knowledge. We re-implemented all three algorithms based on their original description and personal consultation with the authors to avoid misinterpretations. Since the original version of CogNiac is non-robust and resolves only anaphors that obey certain rules, for fairer and comparable results we implemented the \"resolve-all\" version as described in (Baldwin, 1997) . Although for the current experiments we have only included three knowledge-poor anaphora resolvers, it has to be emphasised that the current implementation of the workbench does not restrict in any way the number or the type of the anaphora resolution methods included. Its modularity allows any such method to be added in the system, as long as the preprocessing tools necessary for that method are available.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first phase of our project included comparison of knowledge-poorer approaches which share a common pre-processing philosophy.", "mid_sen": "We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraev's parserfree version of Lappin and Leass' RAP (Kennedy and Boguraev, 1996) , Baldwin's pronoun resolution method (Baldwin, 1997 ) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b) . ", "after_sen": "All three of these algorithms share a similar pre-processing methodology: they do not rely on a parser to process the input and instead use POS taggers and NP extractors; nor do any of the methods make use of semantic or real-world knowledge. "}
{"citeStart": 114, "citeEnd": 140, "citeStartToken": 114, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hampie, 1985) . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988) . On the other hand, Cn'ice's maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is required, s Thus, it is important that a collaborative agent selects suffmient and effective, but not excessive, evidence to justify an intended mutual belief.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hampie, 1985) . ", "after_sen": "Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988) . "}
{"citeStart": 113, "citeEnd": 127, "citeStartToken": 113, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas (Doan et al. 2002) . Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999) , (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003) . Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993) , or require human-annotated training data with relation information for each domain (Craven et al. 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas (Doan et al. 2002) . ", "mid_sen": "Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999) , (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003) . ", "after_sen": "Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993) , or require human-annotated training data with relation information for each domain (Craven et al. 1998) ."}
{"citeStart": 50, "citeEnd": 70, "citeStartToken": 50, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "IMPORTANT! Many words have several distinct meanings. For example, the word \"horse\" can refer to an animal, a piece of gymnastics equipment, or it can mean to fool around (e.g., \"Don't horse around!\"). If a word has ANY meaning associated with the given category, then only consider that meaning when assigning numbers. For example, the word \"horse\" would be a 5 because one of its meanings refers to an ANIMAL. a cartridge or trigger is mentioned in the context of an event, then one can infer that a gun was used. And for some applications, any word that is strongly associated with a category might be useful to include in the semantic lexicon. For example, words like ammunition or bullets are highly suggestive of a weapon. In the UMass/MUC-4 information extraction system (Lehnert et al., 1992) , the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions. The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category. Since the instructions allowed the users to assign a zero to a word if they did not know what it meant, we manually removed the zeros and assigned ratings that we thought were appropriate. We considered ignoring the zeros, but some of the categories would have been severely impacted. For example, many of the legitimate weapons (e.g., M-16 and AR-15) were not known to the judges. Fortunately, most of the unknown words were proper nouns with relatively unambiguous semantics, so we do not believe that this process compromised the integrity of the experiment.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, words like ammunition or bullets are highly suggestive of a weapon. ", "mid_sen": "In the UMass/MUC-4 information extraction system (Lehnert et al., 1992) , the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions. ", "after_sen": "The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category. "}
{"citeStart": 104, "citeEnd": 123, "citeStartToken": 104, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "Tables 4,5 present the results of using artificially simulated speech recognizer using a method of general phonetic classes. That is, instead of transcribing a word by the phoneme, the word is transcribed by the phoneme classes (Jurafsky and Martin, 200) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Tables 4,5 present the results of using artificially simulated speech recognizer using a method of general phonetic classes. ", "mid_sen": "That is, instead of transcribing a word by the phoneme, the word is transcribed by the phoneme classes (Jurafsky and Martin, 200) .", "after_sen": "Specifically, these experiments deviate from the task definition given above. "}
{"citeStart": 148, "citeEnd": 176, "citeStartToken": 148, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "The resource-sensitive system of linear logic is used to compute meanings in accordance with relationships manifest in LFG f-structures. The properties of the system ensure that meanings are used exactly once, allowing coherence and completeness conditions on f-structures (Kaplan and Bresnan, 1982, pages 211-212) to be maintained.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The resource-sensitive system of linear logic is used to compute meanings in accordance with relationships manifest in LFG f-structures. ", "mid_sen": "The properties of the system ensure that meanings are used exactly once, allowing coherence and completeness conditions on f-structures (Kaplan and Bresnan, 1982, pages 211-212) to be maintained.", "after_sen": "However. "}
{"citeStart": 78, "citeEnd": 90, "citeStartToken": 78, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The evaluator builds entries by taking the patterns for a given predicate built from successful parses and records the number of observations of each subcategorization class. Patterns provide several types of information which can be used to rank or select between patterns in the patternset for a given sentence exemplifying an instance of a predicate, such as the ranking of the parse from which it was extracted or the proportion of subanalyses supporting a specific pattern. Currently, we simply select the pattern supported by the highest ranked parse. However, we are experimenting with alternative approaches. The resulting set of putative classes for a predicate are filtered, following Brent (1993) , by hypothesis testing on binomial frequency data.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, we are experimenting with alternative approaches. ", "mid_sen": "The resulting set of putative classes for a predicate are filtered, following Brent (1993) , by hypothesis testing on binomial frequency data.", "after_sen": "Evaluating putative entries on binomial frequency data requires that we record the total number of patternsets n for a given predicate, and the number of these patternsets containing a pattern supporting an entry for given class m. These figures are straightforwardly computed from the output of the classifier; however, we also require an estimate of the probability that a pattern for class i will occur with a verb which is not a member of subcategorization class i. Brent proposes estimating these probabilities experimentally on the basis of the behaviour of the extractor. "}
{"citeStart": 52, "citeEnd": 77, "citeStartToken": 52, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "We selected a four-class scheme for annotation. Every sentence that is in a window of 4 sentences of the citation and does not contain any direct or indirect mention of the citation was labelled as being excluded (x). The window length was motivated by recent research (Qazvinian and Radev, 2010) which shows the best score for a four-sentence boundary when detecting non-explicit citation. The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Every sentence that is in a window of 4 sentences of the citation and does not contain any direct or indirect mention of the citation was labelled as being excluded (x). ", "mid_sen": "The window length was motivated by recent research (Qazvinian and Radev, 2010) which shows the best score for a four-sentence boundary when detecting non-explicit citation. ", "after_sen": "The rest of the sentences were marked either positive (p), negative (n) or objective/neutral (o)."}
{"citeStart": 211, "citeEnd": 240, "citeStartToken": 211, "citeEndToken": 240, "sectionName": "UNKNOWN SECTION NAME", "string": "Eadl proper branch is a binary branching structure, and so all grammatical constraints will need to be encoded locally. Crocker [Crocker19921 develops \"a 'representational' reformulation of the transformational model which decomposes syntactic analysis into sew,.ral representation types --including phrase structure, chains, and coindexation --allowing one to maintain the strictly local characterisation of prindples with respect to their relevant representation types,\" [Crocker and Lewin1992, p. 511] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Crocker [Crocker19921 develops \"a 'representational' reformulation of the transformational model which decomposes syntactic analysis into sew,.", "mid_sen": "ral representation types --including phrase structure, chains, and coindexation --allowing one to maintain the strictly local characterisation of prindples with respect to their relevant representation types,\" [Crocker and Lewin1992, p. 511] .", "after_sen": "By using the proper branch method of axiomatising the grammar, the structure building section of the parser is only constrained in that it must produce proper branches; it is therefore possible to experiment with different interpreters (i.e. structure proposing engines) while keeping the grammar constant."}
{"citeStart": 71, "citeEnd": 86, "citeStartToken": 71, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992) . Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994 ) their relevance to computational grammars is apparent. On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5) , it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. ", "mid_sen": "Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992) . ", "after_sen": "Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994 ) their relevance to computational grammars is apparent. "}
{"citeStart": 146, "citeEnd": 160, "citeStartToken": 146, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "To learn an accurate dependency parser from data, the first approach I investigated is based on a strictly lexical parsing model where all the parameters are based on words (Wang et al., 2005) . The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997) , which focus on maximizing the joint probability of the parse tree and the sentence.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The advantage of this approach is that it does not rely on part-ofspeech tags nor grammatical categories. ", "mid_sen": "Furthermore, I based training on maximizing the conditional probability of a parse tree given a sentence, unlike most previous generative models (Magerman, 1995; Collins, 1997; Charniak, 1997) , which focus on maximizing the joint probability of the parse tree and the sentence.", "after_sen": "An efficient training algorithm can be achieved by maximizing the conditional probability of each parsing decision, hence minimizing a loss based on each local link decision independently. "}
{"citeStart": 16, "citeEnd": 40, "citeStartToken": 16, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "(2) The work of Ringger and Allen (1996) is similar in spirit to this method, but uses a factored sourcechannel model. Note that the decision rule (1) is over whole documents. Therefore we processes complete documents at a time without prior segmentation into sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "p(S, T ) = M i=1 p(s i , τ i |s i−m+1 , τ i−m+1 , . . . s i−1 , τ i−1 )", "mid_sen": "(2) The work of Ringger and Allen (1996) is similar in spirit to this method, but uses a factored sourcechannel model. ", "after_sen": "Note that the decision rule (1) is over whole documents. "}
{"citeStart": 177, "citeEnd": 198, "citeStartToken": 177, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the character of RTE problems, we do not expect NatLog to be a good general-purpose solution to solving RTE problems. First, most RTE problems depend on forms of inference, such as paraphrase, temporal reasoning, or relation extraction, which NatLog is not designed to address. Second, in most RTE problems, the edit distance between premise and hypothesis is relatively large. More atomic edits means a greater chance that prediction errors made by the atomic entailment model will propagate, via entailment composition, to the system's final output. Rather, in applying NatLog to RTE, we hope to make reliable predictions on a subset of RTE problems, trading recall for precision. If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Rather, in applying NatLog to RTE, we hope to make reliable predictions on a subset of RTE problems, trading recall for precision. ", "mid_sen": "If we succeed, then we may be able to hybridize with a broad-coverage RTE system to obtain better results than either system individually-the same strategy that was adopted by (Bos and Markert, 2006) for their FOL-based system.", "after_sen": "For this purpose, we have chosen to use the Stanford RTE system described in . "}
{"citeStart": 30, "citeEnd": 31, "citeStartToken": 30, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "The second problem is that all non-unit clauses of tile program are added to the chart. The addition of non-unit clauses should be made dependent on the goal and the base cases in order to go from a purely bottora-up algorithm to a directed algorithm that combines the advantages of top-down and bottom-up processing. It has been repeatedly noted [8, 17, 1] that directed methods are more efficient than pure top-down or bottomup methods. However, it is not clear how well the directed methods are applicable to grammars which do not depend on concatenation and have no unique 'left cornet\" which should be connected to the start symbol.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The addition of non-unit clauses should be made dependent on the goal and the base cases in order to go from a purely bottora-up algorithm to a directed algorithm that combines the advantages of top-down and bottom-up processing. ", "mid_sen": "It has been repeatedly noted [8, 17, 1] that directed methods are more efficient than pure top-down or bottomup methods. ", "after_sen": "However, it is not clear how well the directed methods are applicable to grammars which do not depend on concatenation and have no unique 'left cornet\" which should be connected to the start symbol."}
{"citeStart": 730, "citeEnd": 742, "citeStartToken": 730, "citeEndToken": 742, "sectionName": "UNKNOWN SECTION NAME", "string": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992) , (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003) , (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. ", "mid_sen": "They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992) , (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003) , (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995) .", "after_sen": "As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. "}
{"citeStart": 81, "citeEnd": 102, "citeStartToken": 81, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "ASO has been demonstrated to be an effective semi-supervised learning algorithm (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) . However, we have been unable to use unlabeled data to improve the accuracy. One possible reason is the cumulative noise from the many cascading steps involved in automatic SRL of unlabeled data: syntactic parse, predicate identification (where we identify nouns with at least one argument), argument identification, and finally argument classification, which reduces the effectiveness of adding unlabeled data using ASO. (Caruana, 1997) discusses configurations where both used inputs and unused inputs (due to excessive noise) are utilized as additional outputs. In contrast, our work concerns linear predictors using empirical risk minimization.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For instance, \"predicting unused features\" type of auxiliary problems might hold some hope for further improvement in argument identification, if a larger number of auxiliary problems can be used.", "mid_sen": "ASO has been demonstrated to be an effective semi-supervised learning algorithm (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) . ", "after_sen": "However, we have been unable to use unlabeled data to improve the accuracy. "}
{"citeStart": 71, "citeEnd": 82, "citeStartToken": 71, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz et al., 2010) . Some texts were removed in order to balance the corpus. The balanced corpus contains 2848 documents and has been split into a development and a training and test set. 570 documents were used for the manual creation of features. The remaining 2278 documents were used to train and evaluate classifiers using 10-fold cross-validation with the WEKA machine learning toolkit (Hall et al., 2009) and various classifiers (cf. Table 1 ).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A useful distinction for political scientists dealing with newspaper articles is the distinction between articles that report objectively on events or backgrounds and editorials or press commentaries.", "mid_sen": "We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz et al., 2010) . ", "after_sen": "Some texts were removed in order to balance the corpus. "}
{"citeStart": 152, "citeEnd": 184, "citeStartToken": 152, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999 ): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "CASE is relevant, not redundant, and can be predicted with sufficient accuracy. ", "mid_sen": "It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; .", "after_sen": "In this article we investigate morphological features for dependency parsing of Modern Standard Arabic (MSA). "}
{"citeStart": 85, "citeEnd": 110, "citeStartToken": 85, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "mid_sen": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "after_sen": "Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. "}
{"citeStart": 87, "citeEnd": 108, "citeStartToken": 87, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "According to the inv metric, our results are considerably worse than those reported by Ringger et al. (2004) . As mentioned in Section 3, the fact that they generate the order for every non-terminal node seriously inflates their numbers. Apart from that, they do not report accuracy, and it is unknown, how many sentences they actually reproduced correctly.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Insignificant for clauses with two or three constituents, for clauses with 10 constituents, the number of comparisons is reduced drastically from 163,296,000 to 45.", "mid_sen": "According to the inv metric, our results are considerably worse than those reported by Ringger et al. (2004) . ", "after_sen": "As mentioned in Section 3, the fact that they generate the order for every non-terminal node seriously inflates their numbers. "}
{"citeStart": 123, "citeEnd": 140, "citeStartToken": 123, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al., 2003) , as described in Clark and Curran (2007a) . Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before this was unlikely.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. ", "mid_sen": "To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al., 2003) , as described in Clark and Curran (2007a) . ", "after_sen": "Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. "}
{"citeStart": 10, "citeEnd": 36, "citeStartToken": 10, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "dataset are almost the same as previous work parsing the Brown corpus with similar models (Gildea, 2001) , which suggests that our dataset is representative of this corpus. The improvement in PP attachment was larger (20.5% ERR), and also statistically significant. The results for PP attachment are especially important, as we demonstrate that the sense information has high utility when embedded within a parser, where the parser needs to first identify the ambiguity and heads correctly. Note that Atterer and Schütze (2007) have shown that the Bikel parser performs as well as the state-of-the-art in PP attachment, which suggests our method improves over the current stateof-the-art. The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. We also observed that while better PP-attachment usually improves parsing, there is some small variation. This means that the best configuration for PP-attachment does not always produce the best results for parsing One surprising finding was the strong performance of the automatic WSD systems, actually outperforming the gold-standard annotation overall. Our interpretation of this result is that the approach of annotating all occurrences of the same word with the same sense allows the model to avoid the data sparseness associated with the gold-standard distinctions, as well as supporting the merging of different words into single semantic classes. While the results for gold-standard senses were intended as an upper bound for WordNet-based sense information, in practice there was very little difference between gold-standard senses and automatic WSD in all cases barring the Bikel parser and PP attachment.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results for PP attachment are especially important, as we demonstrate that the sense information has high utility when embedded within a parser, where the parser needs to first identify the ambiguity and heads correctly. ", "mid_sen": "Note that Atterer and Schütze (2007) have shown that the Bikel parser performs as well as the state-of-the-art in PP attachment, which suggests our method improves over the current stateof-the-art. ", "after_sen": "The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. "}
{"citeStart": 171, "citeEnd": 184, "citeStartToken": 171, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2006) ). In the discrimination task , a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent 4 .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata (2006) ). ", "after_sen": "In the discrimination task , a document is compared with a random permutation of its sentences, and we score the system correct if it indicates the original as more coherent 4 ."}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand, there has been relatively little work on sentiment-based clustering and the related task of unsupervised polarity classification, where the goal is to cluster (or classify) a set of documents (e.g., reviews) according to the polarity (e.g., \"thumbs up\" or \"thumbs down\") expressed by the author in an unsupervised manner. Despite the large amount of recent work on sentiment analysis and opinion mining, much of it has focused on supervised methods (e.g., Pang et al. (2002) , Kim and Hovy (2004) , Mullen and Collier (2004) ). One weakness of these existing supervised polarity classification systems is that they are typically domain-and language-specific. Hence, when given a new domain or language, one needs to go through the expensive process of collecting a large amount of annotated data in order to train a high-performance polarity classifier. Some recent attempts have been made to leverage existing sentiment corpora or lexica to automatically create annotated resources for new domains or languages. However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008) , or a domain that is \"similar\" enough to the target domain (Blitzer et al., 2007) . When the target domain or language fails to meet this requirement, sentiment-based clustering or unsupervised polarity classification become appealing alternatives. Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community. Turney's (2002) work is perhaps one of the most notable examples of unsupervised polarity classification. However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict the polarity of a review heuristically.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, to our knowledge, these tasks are largely under-investigated in the NLP community. ", "mid_sen": "Turney's (2002) work is perhaps one of the most notable examples of unsupervised polarity classification. ", "after_sen": "However, while his system learns the semantic orientation of the phrases in a review in an unsupervised manner, this information is used to predict the polarity of a review heuristically."}
{"citeStart": 176, "citeEnd": 191, "citeStartToken": 176, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to enable collaboration between researchers, the VRE makes use of a number of repositories and services to store research resources, and offers a number of tools to manage and visualise such resources (see Figure 2) . One of the most important components of the VRE is a Text Generator service which is able to generate short textual descriptions from the RDF metadata associated with resources stored in the Metadata Repository (e.g. title, author, date of publication). In order to generate the text, we have implemented a RESTful service that invokes a Text Generator service based on the RDF ID of the resource being described, passed as a parameter by the Web interface. This service generates text containing a description of the resource using a deep model of the syntactic structure of sentences and their combinations, inspired by the work of Hielkema (2010) The Text Generator builds an internal RDF model of the resource being described by querying the Metadata Repository. The text is then produced by converting axioms inside the model to plain text using the appropriate language specifications. A language specification is composed of a set of lexicons encoded in XML which describe how to render the text corresponding to a RDF property (e.g. syntactic category, source node, target node, verb tense). For example, if the property transcribedBy of a resource of type Transcript has a value of \"Thomas Bouttaz\", the XML file corresponding to that property will specify that this information must be rendered as: \"It was transcribed by Thomas Bouttaz\" (see Figure  5 left). By following the hyperlinks available in the resource description, the user is then able to expand the text to access more information about related resources. For instance, in this example the user can click on the hyperlink Thomas Bouttaz to get more information about that person. This is done by invoking the Text Generator service with the ID of the RDF representation of that person. The description returned by the service is then appended to the original text by the Text Interface.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to generate the text, we have implemented a RESTful service that invokes a Text Generator service based on the RDF ID of the resource being described, passed as a parameter by the Web interface. ", "mid_sen": "This service generates text containing a description of the resource using a deep model of the syntactic structure of sentences and their combinations, inspired by the work of Hielkema (2010) The Text Generator builds an internal RDF model of the resource being described by querying the Metadata Repository. ", "after_sen": "The text is then produced by converting axioms inside the model to plain text using the appropriate language specifications. "}
{"citeStart": 208, "citeEnd": 220, "citeStartToken": 208, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense in VP-ellipsis illustrates how categories can be put to work. In (24) I enjoyed it. And so will you the ellipsis is contained within a form expression whose category is vp_ellipsis It ense=inf ,modalffivill ,perf ectffi_, progressive=_,pol=pos .... ] This states the syntactic tense, aspect and polarity marked on the ellipsis (underscores indicate lack of specification). The category constrains resolution to look for verb phrase/sentence sources, which come wrapped in forms with categories like [t ease=past, modalffino, pexf ectffino, Heuristics simi]ar to those described by Hardt (1992) may be used for this. The category also says that, for this kind of VP match 9, the term in the antecedent whose category identifies it as being the subject should be treated as parallel to the explicit term in the ellipsis. As this example illustrates, tense and aspect on ellipsis and antecedent do not have to agree. When Sforns axe described in . 9Not all VP ellipses have VP antecedents. this is so, the antecedent and ellipsis categories are both used to determine what fozm should be substituted for the antecedent form. This comprises the restriction of the antecedent form and a new category constructed by taking the features of the antecedent category, unless overridden by those on the ellipsis--a kind of (monotonic) priority union (Grover et ai., 1994) except using skeptical as opposed to credulous default unification (Carpenter, 1993) . When a new category is constructed for the antecedent, any tense resolutions also need to be undone, since the original ones may no longer be appropriate for the revised category. One thus merges the category information from source and antecedent to determine what verb phrase form should be substituted for the original. In this case, it will have a category", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "And so will you the ellipsis is contained within a form expression whose category is vp_ellipsis It ense=inf ,modalffivill ,perf ectffi_, progressive=_,pol=pos .... ] This states the syntactic tense, aspect and polarity marked on the ellipsis (underscores indicate lack of specification). ", "mid_sen": "The category constrains resolution to look for verb phrase/sentence sources, which come wrapped in forms with categories like [t ease=past, modalffino, pexf ectffino, Heuristics simi]ar to those described by Hardt (1992) may be used for this. ", "after_sen": "The category also says that, for this kind of VP match 9, the term in the antecedent whose category identifies it as being the subject should be treated as parallel to the explicit term in the ellipsis. "}
{"citeStart": 70, "citeEnd": 88, "citeStartToken": 70, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Similar results for German intransitive scalar motion verbs are shown in Fig. 9 . The data for these experiments were extracted from the maximal-probability parses of a 4.1 million word :1.1.1111\"11 : The lexicalized probabilistic grammar for German used is described in Beil et al. (1999) . We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of \"einfache Anderungsverben\" (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there at all. Fig. i0 shows the most probable pair of classes for increase as a transitive verb, together with estimated frequencies for the head filler pair. Note that the object label 17 is the class found with intransitive scalar motion verbs; this correspondence is exploited in the next section. one-place predicate in the structure on the left in Fig. 11 . Linguistically, such representations are motivated by argument alternations (diathesis), case linking and deep word order, language acquistion, scope ambiguity, by the desire to represent aspects of lexical meaning, and by the fact that in some languages, the postulated decomposed representations are overt, with each primitive predicate corresponding to a morpheme. For references and recent discussion of this kind of theory see Hale and Keyser (1993) and Kural (1996) . We will sketch an understanding of the lexical representations induced by latent-class labeling in terms of the linguistic theories mentioned above, aiming at an interpretation which combines computational leaxnability, linguistic motivation, and denotational-semantic adequacy. The basic idea is that latent classes are computational models of the atomic relation symbols occurring in lexical-semantic representations. As a first implementation, consider replacing the relation symbols in the first tree in Fig. 11 with relation symbols derived from the latent class labeling. In the second tree in Fig 11, R17 and R8 are relation symbols with indices derived from the labeling procedure of Sect. 4. Such representations can be semantically interpreted in standard ways, for instance by interpreting relation symbols as denoting relations between events and individuals.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similar results for German intransitive scalar motion verbs are shown in Fig. 9 . The data for these experiments were extracted from the maximal-probability parses of a 4.1 million word :1.1.1111\"11 : ", "mid_sen": "The lexicalized probabilistic grammar for German used is described in Beil et al. (1999) . ", "after_sen": "We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of \"einfache Anderungsverben\" (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there at all. "}
{"citeStart": 21, "citeEnd": 43, "citeStartToken": 21, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach follows Langkilde-Geary (2002) and Callaway (2003) in aiming to leverage the Penn Treebank to develop a broad-coverage surface realizer for English. However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. In this regard, our approach is more similar to the ones pursued more recently by Carroll, Oepen and Velldal (2005; 2005; 2006) , Nakanishi et al. (2005) and Cahill and van Genabith (2006) with HPSG and LFG grammars.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our approach follows Langkilde-Geary (2002) and Callaway (2003) in aiming to leverage the Penn Treebank to develop a broad-coverage surface realizer for English. ", "after_sen": "However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. "}
{"citeStart": 175, "citeEnd": 189, "citeStartToken": 175, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "By combining Pierrehumbert and Hirschberg's (1990) analysis of intonational meaning with Grosz, Joshi and Weinstein's (1989) theory of centering in discourse, the attentional affect of pitch accents becomes evident, and the paradox of pitch accented pronominals unravels. My goal here is to develop an analysis and a line of inquiry and to suggest that my derivative claims are plausible, and even extensible to an attentional analysis of pitch accents on nonpronominals. The proof, of course, will come from investigation by multiple means --constructed examples (e.g., Cahn, 1990) , computer simulation, empirical analysis of speech data (e.g., Nakatani, 1993) , and psycholinguistic experiments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "My goal here is to develop an analysis and a line of inquiry and to suggest that my derivative claims are plausible, and even extensible to an attentional analysis of pitch accents on nonpronominals. ", "mid_sen": "The proof, of course, will come from investigation by multiple means --constructed examples (e.g., Cahn, 1990) , computer simulation, empirical analysis of speech data (e.g., Nakatani, 1993) , and psycholinguistic experiments.", "after_sen": ""}
{"citeStart": 106, "citeEnd": 170, "citeStartToken": 106, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "A feature of principle-based grammars is their potential to assign some meaningful representation to a string which is strictly ungrammatical. It is an inherent feature of phrase structure grammars that they classify the strings of words from a language into two (infinite) sets, one containing the grammatical strings and the other the ungrammatical strings. Although attempts have been made to modify PS grammars/parsers to cope with extragrammatical input, e.g. [Carbonell and Hayes1983, Douglas and Dale1992, Jensen et al.1983 , this is a feature which has to be 'added on' and tends to affect the statement of the grammar.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is an inherent feature of phrase structure grammars that they classify the strings of words from a language into two (infinite) sets, one containing the grammatical strings and the other the ungrammatical strings. ", "mid_sen": "Although attempts have been made to modify PS grammars/parsers to cope with extragrammatical input, e.g. [Carbonell and Hayes1983, Douglas and Dale1992, Jensen et al.1983 , this is a feature which has to be 'added on' and tends to affect the statement of the grammar.", "after_sen": "Due to the lack of an accepted formalism for the specification of prindple-based grammars, Crocker and Lewi, [Crocker and Lewin1992] define the declarative 'Proper Branch' formalism, which can be used with a number of different parsing methods. "}
{"citeStart": 81, "citeEnd": 100, "citeStartToken": 81, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "We use presence in a dictionary as a features for all available dictionaries in previous experiments. 3. Length of words (L): Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; and create multiple features for length using a decision tree (J48). We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. 4. Capitalization (C): We use 3 boolean features to encode capitalization information: whether any letter in the word is capitalized, whether all letters in the word are capitalized and whether the first letter is capitalized.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "3. Length of words (L): ", "mid_sen": "Instead of using the raw length value as a feature, we follow our previous work (Rubino et al., 2013; and create multiple features for length using a decision tree (J48). ", "after_sen": "We use length as the only feature to train a decision tree for each fold and use the nodes obtained from the tree to create boolean features. "}
{"citeStart": 123, "citeEnd": 145, "citeStartToken": 123, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "ME approach has the merit of easily combining different features to predict the probability of each class. We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009) : 1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We incorporate into the ME based model the following informative context-based features to train CBSM and CBTM. ", "mid_sen": "These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work Gimpel and Smith, 2008; Marton and Resnik, 2008; Chiang et al., 2009; Setiawan et al., 2009; Shen et al., 2009; Xiong et al., 2009) : ", "after_sen": "1. Function word features, which indicate whether the hierarchical source-side/targetside rule strings and sub-phrases covered by non-terminals contain function words that are often important clues of predicting syntactic structures."}
{"citeStart": 144, "citeEnd": 160, "citeStartToken": 144, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Computational approaches fail to account for the cancellation of pragmatic inferences: once presuppositions (Weischedel, 1979) or implicatures (Hirschberg, 1985; Green, 1992) are generated, they can never be cancelled. We are not aware of any formalism or computational approach that offers a unified explanation for the cancellability of pragmatic inferences in general, and of no approach that handles cancellations that occur in sequences of utterances.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mercer (1987) formalizes pre-suppositions in a logical framework that handles defaults (Reiter, 1980) , but this approach is not tractable and it treats natural disjunction as an exclusiveor and implication as logical equivalence.", "mid_sen": "Computational approaches fail to account for the cancellation of pragmatic inferences: once presuppositions (Weischedel, 1979) or implicatures (Hirschberg, 1985; Green, 1992) are generated, they can never be cancelled. ", "after_sen": "We are not aware of any formalism or computational approach that offers a unified explanation for the cancellability of pragmatic inferences in general, and of no approach that handles cancellations that occur in sequences of utterances."}
{"citeStart": 136, "citeEnd": 155, "citeStartToken": 136, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "The problem of boundary friction is clearly visible here: We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. However, rather than output this wrong translation directly, we use a post hoc validation and (if required) correction process based on Grefenstette (1999) . Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. Rather than search for competing candidates, we select the \"best\" translation and have its morphological variants searched for on-line. In the example above, namely, the personal computers, we search for les ordinateurs personnels versus the wrong alternatives le/la/l'ordinateurs personnels. Interestingly, using Lycos, and setting the search language to French, the correct form les ordinateurs personnels is uniquely preferred over the other alternatives, as it is found 2,454 times, whereas the others are not found at all. In this case, this translation overrides the highest-ranked translation (50) and is output as the final translation. In fact, in checking the translations obtained for NPs using system combination ABC, we noted that 251 NPs out of the test set of 500 could be improved. Of these 251, 207 (82.5%) were improved post hoc via the Web, with no improvement for the remaining 43 cases. We consider this to be quite a significant result.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have inserted a feminine singular determiner into a chunk that was generalized from a masculine plural NP. ", "mid_sen": "However, rather than output this wrong translation directly, we use a post hoc validation and (if required) correction process based on Grefenstette (1999) . ", "after_sen": "Grefenstette shows that the Web can be used as a filter on translation quality simply by searching for competing translation candidates and selecting the one that is found most often. "}
{"citeStart": 148, "citeEnd": 170, "citeStartToken": 148, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "A statistical language model is a way to assign probabilities to all possible documents in a given language. Most such models can be classified in one of two categories: they can directly assign probabilities to sequences of word types, such as is done in n-gram models, or they can operate in a lower-dimensional latent space, to which word types are mapped. While most state-ofthe-art language models are n-gram models, the representations used in models of the latter category, henceforth referred to as \"embeddings,\" have been found to be useful in many NLP applications which don't actually need a language model. The underlying intuition is that when language models compress the information about the word types in a latent space they capture much of the commonalities and differences between word types. Hence features extracted from these models then can generalize better than features derived from the word types themselves. One simple language model that discovers useful embeddings is known as Brown clustering (Brown et al., 1992) . A Brown clustering is a class-based bigram model in which (1) the probability of a document is the product of the probabilities of its bigrams, (2) the probability of each bigram is the product of the probability of a bigram model over latent classes and the probability of each class generating the actual word types in the bigram, and (3) each word type has non-zero probability only on a single class. Given a one-toone assignment of word types to classes, then, and a corpus of text, it is easy to estimate these probabilities with maximum likelihood by counting the frequencies of the different class bigrams and the frequencies of word tokens of each type in the corpus. The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. This process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications (Ratinov and Roth, 2009; Koo et al., 2008; Miller et al., 2004) . There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Brown clustering algorithm works by starting with an initial assignment of word types to classes (which is usually either one unique class per type or a small number of seed classes corresponding to the most frequent types in the corpus), and then iteratively selecting the pair of classes to merge that would lead to the highest post-merge log-likelihood, doing so until all classes have been merged. ", "mid_sen": "This process produces a hierarchical clustering of the word types in the corpus, and these clusterings have been found useful in many applications (Ratinov and Roth, 2009; Koo et al., 2008; Miller et al., 2004) . ", "after_sen": "There are other similar models of distributional clustering of English words which can be similarly effective (Pereira et al., 1993) ."}
{"citeStart": 114, "citeEnd": 143, "citeStartToken": 114, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Another question is why using transformation-based (rule) learning seems to be slightly better than memory-based learning for these type 1 GRs. Memory-based learning keeps all of the training instances and does not try to find generalizations such as rules (Daelemans et al., 1999, Ch. 4 ). However, with type 1 GRs, a few simple generalizations can account for many of the instances. In the manner of Stevenson (1998) , we wrote a set of six simple rules that when run on the test set type 1 GRs produces an F-score of 77%. This is better than what our reconstructed MB system originally achieved and is close to the TR system's original results (close enough not to be statistically significantly different). An example of these six rules: IF (1) the center chunk is a verb chunk and (2) is not considered as possibly passive and (3) its headword is not some form of to be and (4) the right neighbor is a noun or verb chunk, THEN consider that chunk to the right as being an object of the center chunk.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another question is why using transformation-based (rule) learning seems to be slightly better than memory-based learning for these type 1 GRs. ", "mid_sen": "Memory-based learning keeps all of the training instances and does not try to find generalizations such as rules (Daelemans et al., 1999, Ch. 4 ). ", "after_sen": "However, with type 1 GRs, a few simple generalizations can account for many of the instances. "}
{"citeStart": 106, "citeEnd": 128, "citeStartToken": 106, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to allow for direct comparison with prior work, we used the same subset of these data as Beigman Klebanov et al. (2014) , in the same crossvalidation setting. The total of 90 fragments are used in cross-validation: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and 12 on Academic. All instances from the same text were always placed in the same fold. Table 1 shows the sizes of the datasets for each genre, as well as the proportion of metaphors therein.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The data is annotated according to the MIPVU procedure (Steen et al., 2010) with the interannotator reliability of κ > 0.8.", "mid_sen": "In order to allow for direct comparison with prior work, we used the same subset of these data as Beigman Klebanov et al. (2014) , in the same crossvalidation setting. ", "after_sen": "The total of 90 fragments are used in cross-validation: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and 12 on Academic. "}
{"citeStart": 95, "citeEnd": 118, "citeStartToken": 95, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Although much faster than full-integration, cube pruning still computes a fixed amount of +LM items at each node, many of which will not be useful for arriving at the 1-best hypothesis at the root. It would be more efficient to compute as few +LM items at each node as are needed to obtain the 1-best hypothesis at the root. This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005) , is a lazy version of Algorithm 2 (see Table 1 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It would be more efficient to compute as few +LM items at each node as are needed to obtain the 1-best hypothesis at the root. ", "mid_sen": "This new method, called cube growing, is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang (2005) , is a lazy version of Algorithm 2 (see Table 1 ).", "after_sen": "Instead of traversing the forest bottom-up, cube growing visits nodes recursively in depth-first order from the root node (Figure 4 ). "}
{"citeStart": 119, "citeEnd": 129, "citeStartToken": 119, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "The theoretical ground for incorporating negative examples in a language learning process originates with Gold's work (Gold, 1967; Angluin, 1980) . He examined the process of learning the grammar of a formal language from examples. He showed that, for languages at least as high in the Chomsky hierarchy as CFGs, inference from positive data alone is strictly less powerful than inference from both positive and negative data together. To illustrate this informally consider a case of inference from a number of examples: as they are presented to the inference machine, pos- Table 3 : Performance on text from Perkins manuals, using improved representation and larger training set, after 2% sentences have been excluded sible grammars are postulated. However, with positive data alone a problem of over generalization arises: the postulated grammar may be a superset of the real grammar, and sentences that are outside the real grammar could be accepted. If both positive and negative data is used, counter examples will reduce the postulated grammar so that it is nearer the real grammar. Gold developed his theory for formal languages: it is argued that similar considerations apply here. A grammar may be inferred from positive examples alone for certain subsets of regular languages (Garcia and Vidal, 1990) , or an inference process may degenerate into a look up procedure if every possible positive example is stored. In these cases negative information is not required, but they are not plausible models for unbounded natural language. In our method the required parse is found by inferring the grammar from both positive and negative information, which is effectively modelled by the neural net. ~-hture work will investigate the effect of training the networks on the positive examples alone. With our current size corpus there is not enough data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order that \"improbabilities\" can be modelled by inhibitory connections (Niles and Silverman, 1990) show how a Hidden Markov Model can be implemented by a neural network.", "mid_sen": "The theoretical ground for incorporating negative examples in a language learning process originates with Gold's work (Gold, 1967; Angluin, 1980) . ", "after_sen": "He examined the process of learning the grammar of a formal language from examples. "}
{"citeStart": 55, "citeEnd": 75, "citeStartToken": 55, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. A significant difference is that we apply maximum entropy estimation for feature forests to avoid the inherent problem with estimation: the exponential explosion of parsing results given by the grammar. They assumed that parsing results would be suppressed to a reasonable number through using heuristic rules, or by carefully implementing a fully restrictive and wide-coverage grammar, which requires a considerable amount of effort to develop. Our contention is that this problem can be solved in a more sophisticated way as is discussed in this paper. Another difference is that our model is separated into syntax and semantics probabilities, which will benefit computational/linguistic investigations into the relation between syntax and semantics, and allow separate improvements to both models.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If a complete structure is represented with a feature forest of a tractable size, the parameters can be efficiently estimated by dynamic programming.", "mid_sen": "A series of studies on parsing with wide-coverage LFG (Johnson et al., 1999; Riezler et al., 2000; Riezler et al., 2002) have had a similar motivation to ours. ", "after_sen": "Their models have also been based on a discriminative model to select a parsing result from all candidates given by the grammar. "}
{"citeStart": 99, "citeEnd": 109, "citeStartToken": 99, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "By combining Pierrehumbert and Hirschberg's (1990) analysis of intonational meaning with Grosz, Joshi and Weinstein's (1989) theory of centering in discourse, the attentional affect of pitch accents becomes evident, and the paradox of pitch accented pronominals unravels. My goal here is to develop an analysis and a line of inquiry and to suggest that my derivative claims are plausible, and even extensible to an attentional analysis of pitch accents on nonpronominals. The proof, of course, will come from investigation by multiple means --constructed examples (e.g., Cahn, 1990) , computer simulation, empirical analysis of speech data (e.g., Nakatani, 1993) , and psycholinguistic experiments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "My goal here is to develop an analysis and a line of inquiry and to suggest that my derivative claims are plausible, and even extensible to an attentional analysis of pitch accents on nonpronominals. ", "mid_sen": "The proof, of course, will come from investigation by multiple means --constructed examples (e.g., Cahn, 1990) , computer simulation, empirical analysis of speech data (e.g., Nakatani, 1993) , and psycholinguistic experiments.", "after_sen": ""}
{"citeStart": 38, "citeEnd": 59, "citeStartToken": 38, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "For this particular data, the misses of correct alignments is more than 3 times the number of false positives, representing a loss rate of 2.2% of all possible correct alignments. This implies that if the size of a parallel corpus for training a statistical machine translator model is very large, the loss would be irrelevant since the amount of training data would still be very large. For such a purpose and under such conditions, an excellent precision rate is much more relevant than a perfect recall. Note that the highest possible precision rate is essential because otherwise wrong sentence alignments necessarily produce wrong word misalignments and consequently wrong translations. However when the number of wrong sentence alignments present in the parallel corpora is minimal (i.e. less than 1%), lesser will be the errors introduced to the posterior training of word alignments. In fact, good translation models depend not only on the size of a parallel corpus, but also on the high quality of the sentence alignments. In Table 1, we present the results of the evaluation by using three methods: 1) sentence length (SL) information (baseline), 2) sentence length + bilingual dictionary (SL+Dic) information and 3) our method, which is based on the high quality of existing alignments with the pivot language. Note also that the method proposed by Gale and Church, 1991 is indicated as a baseline when there is no other source of information available than the sentences themselves. However, when a good bilingual dictionary is available, an improvement is observed and the precision rate rises in 15% = (100-(1,146*100/1,354))/100 for the tested data. But an even better result is obtained when a high quality alignment has been previously performed with a pivot language. The improvement we could observe from applying our method was 92% = (100-(98*100/1,354))/100 for the tested data. This excellent result suggests that our method is efficient to transfer the original alignment information from a pair of parallel corpora sharing a common language to aligning the new pair of languages in question. A number of natural language processing applications heavily depend upon the availability of a parallel corpus. Statistical machine translation for instance requires a parallel corpus containing a huge amount of aligned sentence pairs in both languages. However, the lack of availability of almost perfectly aligned non-English parallel corpus makes unfeasible the development of such applications and researches.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Table 1, we present the results of the evaluation by using three methods: 1) sentence length (SL) information (baseline), 2) sentence length + bilingual dictionary (SL+Dic) information and 3) our method, which is based on the high quality of existing alignments with the pivot language. ", "mid_sen": "Note also that the method proposed by Gale and Church, 1991 is indicated as a baseline when there is no other source of information available than the sentences themselves. ", "after_sen": "However, when a good bilingual dictionary is available, an improvement is observed and the precision rate rises in 15% = (100-(1,146*100/1,354))/100 for the tested data. "}
{"citeStart": 210, "citeEnd": 220, "citeStartToken": 210, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Different concepts have been used in the literature as primitives. ", "mid_sen": "These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. ", "after_sen": "An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. "}
{"citeStart": 1, "citeEnd": 23, "citeStartToken": 1, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "[+ IO IO +] (Ramshaw and Marcus, 1995) (Veenstra, 1998) (Argamon et al., 1998) 90.9 Table 6 : The F~=I scores for the (Ramshaw and Marcus, 1995) test set after training with their training data set. The data was processed with the optimal input feature combinations found in the fourth experiment series. The accuracy rate contains the fraction of chunk tags that was correct. The other three rates regard baseNP recognition. The bottom part of the table shows some other reported results with this data set. With all but two formats IBI-IG achieves better FZ=l rates than the best published result in (Ramshaw and Marcus, 1995) . (Veenstra, 1998) uses cascaded decision tree learning (IGTree) for baseNP recognition. This algorithm stores context information of words, POS tags and chunking tags in a decision tree and classifies new items by comparing them to the training items. The algorithm is very fast and it reaches the same performance as (Argamon et al., 1998) (F,~=1=91.6). (Daelemans et al., 1999) uses cascaded MBL (IBI-IG) in a similar way for several tasks among which baseNP recognition. They do not report F~=~ rates but their tag accuracy rates are a lot better than accuracy rates reported by others. However, they use the (Ramshaw and Marcus, 1995) data set in a different training-test division (10-fold cross validation) which makes it (tifficult to compare their results with others.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The algorithm is very fast and it reaches the same performance as (Argamon et al., 1998) (F,~=1=91.6). ", "mid_sen": "(Daelemans et al., 1999) uses cascaded MBL (IBI-IG) in a similar way for several tasks among which baseNP recognition. ", "after_sen": "They do not report F~=~ rates but their tag accuracy rates are a lot better than accuracy rates reported by others. "}
{"citeStart": 51, "citeEnd": 72, "citeStartToken": 51, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Having built the sentence quotation graph with different measures of cohesion, in this section, we develop two summarization approaches. One is the generalization of the CWS algorithm in (Carenini et al., 2007) and one is the well-known Page-Rank algorithm. Both algorithms compute a score, SentScore(s), for each sentence (node) s, which is used to select the top-k% sentences as the summary.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Having built the sentence quotation graph with different measures of cohesion, in this section, we develop two summarization approaches. ", "mid_sen": "One is the generalization of the CWS algorithm in (Carenini et al., 2007) and one is the well-known Page-Rank algorithm. ", "after_sen": "Both algorithms compute a score, SentScore(s), for each sentence (node) s, which is used to select the top-k% sentences as the summary."}
{"citeStart": 203, "citeEnd": 223, "citeStartToken": 203, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "Then, we used the following combination of datasets: RTE3-Sp, RTE4-Sp, RTE3-Sp+RTE4-Sp, SPARTE-Bal (balanced SPARTE Corpus with the same number of true and false cases), and SPARTE-Bal+ RTE3-Sp+RTE4-Sp. The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing pairs, with 676 true and 676 false pairs. We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten & Frank, 2005) . In all the tables results we show only the accuracy of the best classifier.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The training set SPARTE-Balanced was created by taking all true cases and randomly taking false cases, and then we build a balanced training set containing pairs, with 676 true and 676 false pairs. ", "mid_sen": "We used four classifiers to learn every development set: (1) Support Vector Machine, (2) Ada Boost, (3) Multilayer Perceptron (MLP) and (4) Decision Tree using the open source WEKA Data Mining Software (Witten & Frank, 2005) . ", "after_sen": "In all the tables results we show only the accuracy of the best classifier."}
{"citeStart": 78, "citeEnd": 96, "citeStartToken": 78, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "We focused on the automatically induced clusters of multi-word nouns (MNs) as the source of gazetteers. We call the constructed gazetteers cluster gazetteers. In the context of tagging, there are several studies that utilized word clusters to prevent the data sparseness problem (Kazama et al., 2001; Miller et al., 2004) . However, these methods cannot produce the MN clusters required for constructing gazetteers. In addition, the clustering methods used, such as HMMs and Brown's algorithm (Brown et al., 1992) , seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. We utilized richer", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, these methods cannot produce the MN clusters required for constructing gazetteers. ", "mid_sen": "In addition, the clustering methods used, such as HMMs and Brown's algorithm (Brown et al., 1992) , seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words. ", "after_sen": "We utilized richer"}
{"citeStart": 101, "citeEnd": 122, "citeStartToken": 101, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "To better understand which of the features listed in Table 1 are more important for the task, we use Guyon et al.'s (2002) method for feature selection using SVM to rank the features based on their importance. The results of the experiments and the feature analysis are presented and discussed in the following subsection.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally, 10) we compare our methods to the baseline method proposed by Abu-Jbara and Radev (2011) which was described in Section 4 (AR-2011).", "mid_sen": "To better understand which of the features listed in Table 1 are more important for the task, we use Guyon et al.'s (2002) method for feature selection using SVM to rank the features based on their importance. ", "after_sen": "The results of the experiments and the feature analysis are presented and discussed in the following subsection."}
{"citeStart": 87, "citeEnd": 96, "citeStartToken": 87, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "Attribute grammar is an elegant formalization of the augmented context-free grammars characteristic of most current NLP systems. It is more general than members of the family of unification-based grammar formalisms (Kay, 1985; Shieber, 1986) , mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values, and hence can lead to computationally efficient grammatical definitions, while maintaining the advantages of a well-understood declarative formalism. Attribute grammar has been used in the past by the author to define computational models of Chomsky's Government-binding theory, from which practical parsing programs were developed (Correa, 1987a) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Attribute grammar is an elegant formalization of the augmented context-free grammars characteristic of most current NLP systems. ", "mid_sen": "It is more general than members of the family of unification-based grammar formalisms (Kay, 1985; Shieber, 1986) , mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values, and hence can lead to computationally efficient grammatical definitions, while maintaining the advantages of a well-understood declarative formalism. ", "after_sen": "Attribute grammar has been used in the past by the author to define computational models of Chomsky's Government-binding theory, from which practical parsing programs were developed (Correa, 1987a) ."}
{"citeStart": 164, "citeEnd": 171, "citeStartToken": 164, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "The third key issue of EBMT, that is exploiting the retrieved translation example, is usually dealt with by integrating into the system conventional MT techniques [Kaji 92 ], [Sumita 91] . Simple modifications of the translation proposal, such as word substitution, would also be possible, provided that alignment of the translation archive at word level was awdlable.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "[Brown 93 ], [Sadler 901 and [Kaji 92 ] have tackled the problem of establishing correspondences between words and phrases in bilingual texts.", "mid_sen": "The third key issue of EBMT, that is exploiting the retrieved translation example, is usually dealt with by integrating into the system conventional MT techniques [Kaji 92 ], [Sumita 91] . ", "after_sen": "Simple modifications of the translation proposal, such as word substitution, would also be possible, provided that alignment of the translation archive at word level was awdlable."}
{"citeStart": 118, "citeEnd": 123, "citeStartToken": 118, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "The possible sources of Sits are: introspection by lexicographers, machine-readable dictionaries, ~nd on-line corpora. 'l'he main advantage of the latter is that they provide experimental evidence of words uses. tl.e-cently, several approaches on acquiring different kinds of lexical information from corpora have been developed [BPV92, CG[III91, CH90, Res92] . This paper is interested in exploring the amenability of using a method f(~r extracting SI{~ from textual data, in the line of these works. The aim of the proposed technique is to learn the Sl~,s that a word is imposing, from the analysis of the examples of use of that word contaim'd in the corpus. An illustration of such a learning is shown in Figure l, where the system, departing from the three examples of use, and knowing that prosec'utor, buyer attd lawmaker are nouns belonging to the semantic class <pera-m~, individual >, and that i~zdictme~d, assura~zce and legislation are members of < legal_in.strttmeuZ >, should induce that the verb see/.' imposes SILs that constraint the subject to be a nmm-. bet of the semantic type <peraon, individval>, and the object to be a kind of < legaLiustrurnent >. Coneluding, the system should extract for each word (with contplemeut,..~) having enough number occurrences of use in the corpus and for each of its syntactic eomple-melttS, a li:;t of the alternative Sl~s that this word is imposing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "'l'he main advantage of the latter is that they provide experimental evidence of words uses. ", "mid_sen": "tl.e-cently, several approaches on acquiring different kinds of lexical information from corpora have been developed [BPV92, CG[III91, CH90, Res92] . ", "after_sen": "This paper is interested in exploring the amenability of using a method f(~r extracting SI{~ from textual data, in the line of these works. "}
{"citeStart": 85, "citeEnd": 106, "citeStartToken": 85, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "To answer these questions, we have performed experiments that compare the parsing qualities of grammars induced under different training conditions using both adaptation and direct induction. We vary the number of labeled brackets and the linguistic classes of the labeled brackets. The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We vary the number of labeled brackets and the linguistic classes of the labeled brackets. ", "mid_sen": "The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993) .", "after_sen": "Our results show that the training examples do not need to be fully parsed for either strategy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data. "}
{"citeStart": 140, "citeEnd": 163, "citeStartToken": 140, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "Moreover, PDFBox cannot reliably recover reading order from text typeset in multiple columns (again, depending on the PDF generator used). OCR introduces sporadic character and layout recognition errors, but overall works robustly, cf. discussion and a recent and even more accurate approach in Schäfer et al. (2012) . The main part of the corpus creation endeavor consisted in manual annotation assisted by a customized version of the MMAX2 annotation tool (Müller and Strube, 2006) , operating on the extracted raw texts (the annotators had the possibility to open and view the original PDF files). In a second step, the corpus was then augmented with automatically created annotations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "OCR introduces sporadic character and layout recognition errors, but overall works robustly, cf. discussion and a recent and even more accurate approach in Schäfer et al. (2012) . ", "mid_sen": "The main part of the corpus creation endeavor consisted in manual annotation assisted by a customized version of the MMAX2 annotation tool (Müller and Strube, 2006) , operating on the extracted raw texts (the annotators had the possibility to open and view the original PDF files). ", "after_sen": "In a second step, the corpus was then augmented with automatically created annotations."}
{"citeStart": 243, "citeEnd": 271, "citeStartToken": 243, "citeEndToken": 271, "sectionName": "UNKNOWN SECTION NAME", "string": "The automata resulting from word class specialization group the lexical entries into natural classes. In case the automata corresponding to two lexical entries are identical, the entries belong to the same natural class. However, each lexical rule application, i.e., each transition in an automaton, calls a frame predicate that can have a large number of defining clauses. Intuitively understood, each defining clause of a frame predicate corresponds to a subclass of the class of lexical entries to which a lexical rule can be applied. During word class specialization, though, when the finite-state automaton representing global lexical rule application is pruned with respect to a particular base lexical entry, we know which subclass we are dealing with. For each interaction definition we can therefore check which of the flame clauses are applicable and discard the non-applicable ones. We thereby eliminate the redundant nondeterminism resulting from multiply defined frame predicates. The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques (Tamaki and Sato 1984) . 29 The unfolding transformation is also referred to as partial execution, for example, by Pereira and Shieber (1987) . Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body (Pettorossi and Proietti 1994) . Given a lexical entry as in Figure 15 , we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. To eliminate the frame predicates completely, we can successively unfold the frame predicates and the lexical rule predicates with respect to the interaction predicates. 3° The successive unfolding steps are schematically represented in Figure 20 . Such a transformation, however, would result in the loss of a representation of the lexical rule predicates that is independent of a particular word class, but an independent representation of lexical rules constitutes an advantage in space in case lexical rules can be applied across word classes. Our compiler therefore performs what can be viewed as \"partial\" unfolding: it unfolds the frame predicates directly with respect to the interaction predicates, as shown in Figure 21 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Intuitively understood, unfolding comprises the evaluation of a particular literal in the body of a clause at compile-time. ", "mid_sen": "As a result, the literal can be removed from the body of 29 This improvement of the covariation encoding can also be viewed as an instance of the program transformation technique referred to as deletion of clauses with a finitely failed body (Pettorossi and Proietti 1994) . ", "after_sen": "Given a lexical entry as in Figure 15 , we can discard all frame clauses that presuppose tl as the value of c, as discussed in the previous section. "}
{"citeStart": 55, "citeEnd": 70, "citeStartToken": 55, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition. Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model. Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names. They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998) . Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999) . Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of Yarowsky (1993) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). ", "mid_sen": "This is similar to \"one sense per collocation\" idea of Yarowsky (1993) .", "after_sen": "The description of the EAGLE workbench for linguistic engineering (Baldwin et al. 1997 ) mentions a case normalization module that uses a heuristic in which a capitalized word in an ambiguous position should be rewritten without capitalization if it is found lower-cased in the same document. "}
{"citeStart": 16, "citeEnd": 42, "citeStartToken": 16, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "The proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound. Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest. Again, there is no evaluation of the method other than a demonstration that four examples work correctly.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Nor is there any evaluation of the algorithm.", "mid_sen": "The proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound. ", "after_sen": "Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest. "}
{"citeStart": 133, "citeEnd": 150, "citeStartToken": 133, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "paradigmatic relation: how the words are associated with each other. Similarity between words can be defined by either a syntagmatic or a paradigmatic relation. Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. This paper concentrates on paradigmatic similarity, because a paradigmatic relation can be established both inside a sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside a sentence --like syntax deals with sentence structure. The rest of this section focuses on two related works on measuring paradigmatic similarity --a psycholinguistic approach and a thesaurus-based approach.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similarity between words can be defined by either a syntagmatic or a paradigmatic relation. ", "mid_sen": "Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. ", "after_sen": "Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. "}
{"citeStart": 19, "citeEnd": 38, "citeStartToken": 19, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP). It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP). ", "after_sen": "It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing."}
{"citeStart": 17, "citeEnd": 34, "citeStartToken": 17, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. The Temporal Centering framework (Kameyama et al., 1993) integrates lWe will limit the scope of this paper by restricting the discussion to accomplishments and achievements. aspects of both approaches, but patterns with the first in treating tense as anaphoric.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. ", "mid_sen": "For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. ", "after_sen": "This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. "}
{"citeStart": 1, "citeEnd": 13, "citeStartToken": 1, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same way that tags are allocated to words, or to punctuation marks, they can represent the boundaries of syntactic constituents, such as noun phrases and verb phrases. Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do. (Atwell, 1987) and (Church, 1989) have used this approach. If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994) . Our approach uses a similar concept, but differs in that embedded syntactic constituents are detected one at a time in separate steps. There are only 2 hypertags -the opening and closing brackets marking the possible location(s) of the syntactic constituent in question. Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do. ", "mid_sen": "(Atwell, 1987) and (Church, 1989) have used this approach. ", "after_sen": "If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994) . "}
{"citeStart": 97, "citeEnd": 119, "citeStartToken": 97, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "For each language, we ran two experiments: standard parsing and MWE identification. The evaluation included the Stanford, Stanford+factored lexicon, and DP-TSG models. All experiments used gold tokenization/segmentation. Unlike the ATB, the FTB does not contain the raw source documents, so we could not start from raw text for both languages. We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unlike the ATB, the FTB does not contain the raw source documents, so we could not start from raw text for both languages. ", "mid_sen": "We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010) .", "after_sen": "Morphological analysis accuracy was another experimental resource asymmetry between the two languages. "}
{"citeStart": 111, "citeEnd": 121, "citeStartToken": 111, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that we allow the input to be a full FSA (possibly including cycles, etc.) since some of the above-mentioned techniques indeed result in cycles. Whereas an ordinary word-graph always defines a finite language, a FSA of course can easily define an infinite number of sentences. Cycles might emerge to treat unknown sequences of words, i.e. sentences with unknown parts of unknown lengths (Lang, 1988) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Whereas an ordinary word-graph always defines a finite language, a FSA of course can easily define an infinite number of sentences. ", "mid_sen": "Cycles might emerge to treat unknown sequences of words, i.e. sentences with unknown parts of unknown lengths (Lang, 1988) .", "after_sen": "As suggested by an ACL reviewer, one could also try to model haplology phenomena (such as the's in English sentences like 'The chef at Joe's hat', where 'Joe's\" is the name of a restaurant) using a finite state transducer. "}
{"citeStart": 156, "citeEnd": 179, "citeStartToken": 156, "citeEndToken": 179, "sectionName": "UNKNOWN SECTION NAME", "string": "There are several stategies that might be pursued. One is to adopt Pinkal's \"radical underspecification\" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs, either individually or in a \"packed\" structure (Alshawi and Carter 1994) , with the resolution process as described here. Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000) , which would mesh nicely with the theory presented here.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One is to adopt Pinkal's \"radical underspecification\" approach (Pinkal 1995) and use underspecified representations for all types of ambiguity, even syntactic ambiguity. ", "mid_sen": "The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs, either individually or in a \"packed\" structure (Alshawi and Carter 1994) , with the resolution process as described here. ", "after_sen": "Alternatively, I believe it is worth exploring the approach to disambiguation described in Pulman (2000) , which would mesh nicely with the theory presented here."}
{"citeStart": 320, "citeEnd": 334, "citeStartToken": 320, "citeEndToken": 334, "sectionName": "UNKNOWN SECTION NAME", "string": "Syntax The allowed sequences of morphemes, and the syntactic and semantic properties of morphemes and of the words derived by combining them, are specified by morphosyntactic production rules (dimension (b)) and lexical entries both for affixes (dimension (b)) and for roots (dimension (c)), essentially as described by Alshawi (1992) (where the production rules are referred to as \"morphology rules\"). Affixes may appear explicitly in production rules or, like roots, they may be assigned complex feature-valued categories. Information, including the creation of logical forms, is passed between constituents in a rule by the sharing of variables. These feature-augmented production rules are just the same device as those used in the CLE's syntactico-semantic descriptions, and are a much more natural way to express morphotactic information than finite-state devices such as continuation classes (see Trost and Matiasek, 1994 , for a related approach).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Syntax The allowed sequences of morphemes, and the syntactic and semantic properties of morphemes and of the words derived by combining them, are specified by morphosyntactic production rules (dimension (b)) and lexical entries both for affixes (dimension (b)) and for roots (dimension (c)), essentially as described by Alshawi (1992) (where the production rules are referred to as \"morphology rules\"). ", "after_sen": "Affixes may appear explicitly in production rules or, like roots, they may be assigned complex feature-valued categories. "}
{"citeStart": 206, "citeEnd": 218, "citeStartToken": 206, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "The essential contribution of our study is that we treat preverbal and postverbal parts of the sentence differently. The sentence-initial position, which in German is the VF, has been shown to be cognitively more prominent than other positions (Gernsbacher & Hargreaves, 1988) . Motivated by the theoretical work by Chafe (1976) and Jacobs (2001) , we view the VF as the place for elements which modify the situation described in the sentence, i.e. for so called frame-setting topics (Jacobs, 2001) . For example, temporal or locational constituents, or anaphoric adverbs are good candidates for the VF. We hypothesize that the reasons which bring a constituent to the VF are different from those which place it, say, to the beginning of the MF, for the order in the MF has been shown to be relatively rigid (Keller, 2000; Kempen & Harbusch, 2004) . Speakers have the freedom of selecting the outgoing point for a sentence. Once they have selected it, the remaining constituents are arranged in the MF, mainly according to their grammatical properties.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The sentence-initial position, which in German is the VF, has been shown to be cognitively more prominent than other positions (Gernsbacher & Hargreaves, 1988) . ", "mid_sen": "Motivated by the theoretical work by Chafe (1976) and Jacobs (2001) , we view the VF as the place for elements which modify the situation described in the sentence, i.e. for so called frame-setting topics (Jacobs, 2001) . ", "after_sen": "For example, temporal or locational constituents, or anaphoric adverbs are good candidates for the VF. "}
{"citeStart": 38, "citeEnd": 53, "citeStartToken": 38, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "Types not in goal-types have to be excised from the subgrammar being extracted. This is carried out for the entries of the systems in a preparatory step. We assume that the entries are given in disjunctive normal form. First, every conjunction containing a type which is not in goal-types is removed. After this deletion of unsatisfiable conjunctions, every type in an entry which is not in goal-types is removed. The restriction of the outputs of every system to the goal-types is done during a simulated depth-first traversal through the entire grammatical type lattice. The procedure works on the type lattice with the revised entries. Starting with the most general type start (and the most general system called rank which is the system with start as entry), a hierarchy traversal looks for systems which although restricted to the type set goal-types actually branch, i.e. have more than one type in their output. These systems constitute the new subgrammar. In essence, each grammatical system s is examined to see how many of its possible subtypes in out (s) are used within the target grammar. Those types which are not used are excised from the subgrammar being extracted. More specific types that are dependent on any excised types are not considered further during the traversal. Grammatical systems where there is only a single remaining unexcised subtype collapse to form a degenerated pseudo-system indicating that no grammatical variation is possible in the considered application domain. For example, in the application described in section 3 the system indicative = declarative I interrogative. collapses into indicative = declarative. because questions do not occur in the application domain. Pseudo-systems of this kind are not kept in the subgrammar. The types on their right-hand side (pseudotypes) are excised accordingly, although they are used for deeper traversal, thus defining a path to more specific systems. Such a path can consist of more than one pseudotype, if the repeated traversal steps find further degenerated systems. Constraints defined for pseudo-types are raised, chooser actions are percolated down--i.e., more precisely, constraints belonging to a pseudo-type are unified with the constraints of the most general not pseudo type at the beginning of the path. Chooser actions from systems on the path are collected and extend the chooser associated with the final (and first not pseudo) system of the path. However, in the case As the recursion criteria in the traversal, we first simply look for a system which has the actual type in its revised entry regardless of the fact if it occurs in a conjunction or not. This on its own, however, oversimplifies the real logical relations between the types and would create an inconsistent subgrammar. The problem is the conjunctive inheritance. If the current type occurs in an entry of another system where it is conjunctively bound, a deeper traversal is in fact only licensed if the other types of the conjunctions are chosen as well. In order to perform such a traversal, a breadth traversal with compilation of all crowns of the lattice (see (A~t-Kaci et al., 1989) ) would be necessary. In order to avoid this potentially computationally very expensive operation, but not to give up the consistency of the subgrammar, the implemented subgrammar extraction procedure sketched in Figure 1 maintains all systems with complex entries (be they conjunctive or disjunctive) for the subgrammar even if they do not really branch and collapse to a single-subtype system. 2 A related approach can be found in (O'Donnell, 1992) for the extraction of smaller systemic subgrammars for analysis.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to avoid this potentially computationally very expensive operation, but not to give up the consistency of the subgrammar, the implemented subgrammar extraction procedure sketched in Figure 1 maintains all systems with complex entries (be they conjunctive or disjunctive) for the subgrammar even if they do not really branch and collapse to a single-subtype system. ", "mid_sen": "2 A related approach can be found in (O'Donnell, 1992) for the extraction of smaller systemic subgrammars for analysis.", "after_sen": "If the lexicon is organized as or under a complex type hierarchy, the extraction of an applicationtuned lexicon is carried out similarly. "}
{"citeStart": 60, "citeEnd": 71, "citeStartToken": 60, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Our computational model is designed to process child-directed speech. The corpus we use to evaluate it is the same corpus used by Yang (2004) . Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah. We obtained the phonetic transcriptions of words from the Carnegie Mellon Pronouncing Dictionary (CMUdict) Version 0.6 (Weide, 1998) , using the first pronunciation of each word. In CMUdict, lexical stress information is preserved by numbers: 0 for unstressed, 1 for primary stress, 2 for secondary stress. For instance, cat is represented as K.AE1.T, catalog is K.AE1.T.AH0.L.AO0.G, and catapult is K.AE1.T.AH0.P.AH2.L.T. We treat primary stress as \"strong\" and secondary or unstressed syllables as \"weak.\"", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our computational model is designed to process child-directed speech. ", "mid_sen": "The corpus we use to evaluate it is the same corpus used by Yang (2004) . ", "after_sen": "Adult utterances were extracted from the Brown (1973) data in the CHILDES corpus (MacWhinney, 2000), consisting of three children's data: Adam, Eve, and Sarah. "}
{"citeStart": 204, "citeEnd": 217, "citeStartToken": 204, "citeEndToken": 217, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexical-Functional Grammar (SLFG) is a stochastic extension of Lexical-Functional Grammar (LFG), a UBG formalism developed by Kaplan and Bresnan (1982) . Given a base LFG, an SLFG is constructed by defining features which identify salient constructions in a linguistic structure (in LFG this is a c-structure/f-structure pair and its associated mapping; see Kaplan (1995) ). Apart from the auxiliary distributions, we based our features on those used in Johnson et al. (1999) , which should be consulted for further details. Most of these feature values range over the natural numbers, counting the number of times that a particular construction appears in a linguistic structure. For example, adjunct and argument features count the number of adjunct and argument attachments, permitting SLFG to capture a general argument attachment preference, while more specialized features count the number of attachments to each grammatical function (e.g., SUB J, OBJ, COMP, etc.). The flexibility of features in stochastic UBGs permits us to include features for relatively complex constructions, such as date expressions (it seems that date interpretations, if possible, are usually preferred), right-branching constituent structures (usually preferred) and non-parallel coordinate structures (usually dispreferred). Johnson et al. remark that they would have liked to have included features for lexical selectional preferences. While such features are perfectly acceptable in a SLFG, they felt that their corpora were so small that the large number of lexical dependency parameters could not be accurately estimated. The present paper proposes a method to address this by using an auxiliary distribution estimated from a corpus large enough to (hopefully) provide reliable estimates for these parameters.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical-Functional Grammar (SLFG) is a stochastic extension of Lexical-Functional Grammar (LFG), a UBG formalism developed by Kaplan and Bresnan (1982) . ", "mid_sen": "Given a base LFG, an SLFG is constructed by defining features which identify salient constructions in a linguistic structure (in LFG this is a c-structure/f-structure pair and its associated mapping; see Kaplan (1995) ). ", "after_sen": "Apart from the auxiliary distributions, we based our features on those used in Johnson et al. (1999) , which should be consulted for further details. "}
{"citeStart": 114, "citeEnd": 124, "citeStartToken": 114, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "As a pre-processing step, the English sentences were tokenized using the maximum-entropy based tokenizer of the OpenNLP toolkit, and case information was removed. For Chinese, the data provided were tokenized according to the output format of ASR systems, and human-corrected (Paul, 2006) . Since segmentations are human-corrected, we are sure that they are good from a monolingual point of view. Table 2 contains the various corpus statistics.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a pre-processing step, the English sentences were tokenized using the maximum-entropy based tokenizer of the OpenNLP toolkit, and case information was removed. ", "mid_sen": "For Chinese, the data provided were tokenized according to the output format of ASR systems, and human-corrected (Paul, 2006) . ", "after_sen": "Since segmentations are human-corrected, we are sure that they are good from a monolingual point of view. "}
{"citeStart": 26, "citeEnd": 41, "citeStartToken": 26, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. Previously, a user study (Lin et al. 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance. We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this. Issuing extremely short queries appears to be an ingrained habit of information seekers today, and the dominance of World Wide Web searches reinforce this behavior. Given these trends, physicians may actually prefer the rapid back-and-forth interaction style that comes with short queries. We believe that if systems can produce noticeably better results with richer queries, users will make more of an effort to formulate them. This, however, presents a chicken-and-egg problem: One possible solution is to develop models that can automatically fill query frames given a couple of keywords-this would serve to kick-start the query generation process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The design and implementation of our current system leaves many open avenues for future exploration, one of which concerns our assumptions about the query interface. ", "mid_sen": "Previously, a user study (Lin et al. 2003) has shown that people are reluctant to type full natural language questions, even after being told that they were using a questionanswering system and that typing complete questions would result in better performance. ", "after_sen": "We have argued that a query interface based on structured PICO frames will yield better-formulated queries, although it is unclear whether physicians would invest the upfront effort necessary to accomplish this. "}
{"citeStart": 123, "citeEnd": 128, "citeStartToken": 123, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "BFP can add the VP and the S onto the end of the forward centers list, as Sidner does in her algorithm for local focusing [Sid79] . This lets BFP get the two examples of event anaphora. Hobbs discusses the fact that his algorithm cannot be modified to get event anaphora in [Hob76b] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This means that in the 15 out of 20 cases where the shift to global focus is identifiably marked with a cue-word such as now, the segment rules will allow BFP to get the global focus examples.", "mid_sen": "BFP can add the VP and the S onto the end of the forward centers list, as Sidner does in her algorithm for local focusing [Sid79] . ", "after_sen": "This lets BFP get the two examples of event anaphora. "}
{"citeStart": 148, "citeEnd": 171, "citeStartToken": 148, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2. We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work. The words that occurred less than 40 times in the data were discarded from the vocabulary. To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014) . Embeddings of 50, 200, 400 and 600 dimensions were trained.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The words that occurred less than 40 times in the data were discarded from the vocabulary. ", "mid_sen": "To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014) . ", "after_sen": "Embeddings of 50, 200, 400 and 600 dimensions were trained."}
{"citeStart": 94, "citeEnd": 113, "citeStartToken": 94, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "The GLR parser is the syntactic engine of the Universal Parser Architecture developed at CMU [Tomita et al., 1988] . The architecture supports grammatical specification in an LFG framework; that consists of contextfree grammar rules augmented with feature bundles that are associated with the non-terminals of the rules. Feature structure computation is, for the most part, specified and implemented via unification operations. This allows the grammar to constrain the applicability of context-free rules. The result of parsing an input sentence consists of both a parse tree and the computed feature structure associated with the non-terminal at the root of the tree.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A process of local ambiguity packing allows the parser to pack subparses that are rooted in the same non-terminal into a single structure that represents them all.", "mid_sen": "The GLR parser is the syntactic engine of the Universal Parser Architecture developed at CMU [Tomita et al., 1988] . ", "after_sen": "The architecture supports grammatical specification in an LFG framework; that consists of contextfree grammar rules augmented with feature bundles that are associated with the non-terminals of the rules. "}
{"citeStart": 19, "citeEnd": 35, "citeStartToken": 19, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Method 1: Using the second eigenvector only The first method is to use only the second eigenvector, e 2 , to partition the points. Besides revealing one of the most important dimensions of the data, this eigenvector induces an intuitively ideal partition of the data -the partition induced by the minimum normalized cut of the similarity graph 2 , where the nodes are the data points and the edge weights are the pairwise similarity values of the points (Shi and Malik, 2000) . Clustering in a onedimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points. However, we follow Ng et al. (2002) and cluster using 2-means in this one-dimensional space.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clustering in a onedimensional space is trivial: since we have a linearization of the points, all we need to do is to determine a threshold for partitioning the points. ", "mid_sen": "However, we follow Ng et al. (2002) and cluster using 2-means in this one-dimensional space.", "after_sen": "Method 2: Using m eigenvectors Recall from Section 2.1 that after eigendecomposing the Laplacian matrix, each data point is represented by m co-ordinates. "}
{"citeStart": 94, "citeEnd": 120, "citeStartToken": 94, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "The second type uses a window to collect training instances by observing how often a pair of nouns cooccur within some fixed number of words. In this study, a variety of window sizes are used. For n > 2, let countn(nl, n2) be the number of times a sequence nlwl...wins occurs in the training corpus where i < n -2. Note that windowed counts are asymmetric. In the case of a window two words wide, this yields the mutual information metric proposed by Liberman and Sproat (1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For n > 2, let countn(nl, n2) be the number of times a sequence nlwl...wins occurs in the training corpus where i < n -2. Note that windowed counts are asymmetric. ", "mid_sen": "In the case of a window two words wide, this yields the mutual information metric proposed by Liberman and Sproat (1992) .", "after_sen": "Using each of these different training schemes to arrive at appropriate counts it is then possible to estimate the parameters. "}
{"citeStart": 175, "citeEnd": 194, "citeStartToken": 175, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006 ). An example annotation for Smadja (1993) is given in Figure  2 , where the first column shows the line number and the second one shows the class label. To compare our work with Athar (2011), we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and Mac-Roberts, 1984) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A total of 1,741 citations were annotated. ", "mid_sen": "Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006 ). ", "after_sen": "An example annotation for Smadja (1993) is given in Figure  2 , where the first column shows the line number and the second one shows the class label. "}
{"citeStart": 58, "citeEnd": 70, "citeStartToken": 58, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "The glue language is the tensor fragment of linear logic (Girard, 1987) . The semantic contribution of each lexical entry, which we will refer to as a meaning constructor, is a linear-logic formula consisting of instructions in the glue language for combining the meanings of the lexical entry's syntactic arguments to obtain the meaning of the f-structure headed by the entry. For instance, the meaning constructor for the verb supported is a glue language formula paraphrasable as: \"If my SUBJ means X and (®) my OBJ means Y, then ( ---o ) my sentence means supported(X, Y)\".", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Expressions of the meaning language (such as Bill) appear on the right side of the meaning relation ---~.", "mid_sen": "The glue language is the tensor fragment of linear logic (Girard, 1987) . ", "after_sen": "The semantic contribution of each lexical entry, which we will refer to as a meaning constructor, is a linear-logic formula consisting of instructions in the glue language for combining the meanings of the lexical entry's syntactic arguments to obtain the meaning of the f-structure headed by the entry. "}
{"citeStart": 155, "citeEnd": 180, "citeStartToken": 155, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "These figures show that genre does indeed provide useful additional control over the expression of task elements, which can be exploited by a text generation system. Neither task structure nor genre alone is sufficient to provide this control, but, taken together, they offer a real prospect of adequate control over the output of a text generator. The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., (Kosseim and Lapalme, 1994; Paris and Scott, 1994) . Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Neither task structure nor genre alone is sufficient to provide this control, but, taken together, they offer a real prospect of adequate control over the output of a text generator. ", "mid_sen": "The results from our linguistic analysis are consistent with other research on sublanguages in the instructions domain, in both French and English, e.g., (Kosseim and Lapalme, 1994; Paris and Scott, 1994) . ", "after_sen": "Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator."}
{"citeStart": 39, "citeEnd": 57, "citeStartToken": 39, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "uses syntactic information from the source and target dependency trees, and orders each local tree of the target dependency tree independently. It follows the order model defined in (Quirk et al., 2005) . The model assigns a probability to the position of each target node (modifier) relative to its parent (head), based on information in both the source and target trees. The probability of an order of the complete target dependency tree decomposes into a product over probabilities of positions for each node in the tree as follows:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "uses syntactic information from the source and target dependency trees, and orders each local tree of the target dependency tree independently. ", "mid_sen": "It follows the order model defined in (Quirk et al., 2005) . ", "after_sen": "The model assigns a probability to the position of each target node (modifier) relative to its parent (head), based on information in both the source and target trees. "}
{"citeStart": 34, "citeEnd": 48, "citeStartToken": 34, "citeEndToken": 48, "sectionName": "UNKNOWN SECTION NAME", "string": "We parallelized the algorithm of (Torisawa, 2001 ) using the Message Passing Interface (MPI), with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary. Applying the parallelized clustering to a large set of dependencies collected from Web documents enabled us to construct gazetteers with up to 500,000 entries and 3,000 classes.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We enabled such large-scale clustering by parallelizing the clustering algorithm, and we demonstrate the usefulness of the gazetteer constructed.", "mid_sen": "We parallelized the algorithm of (Torisawa, 2001 ) using the Message Passing Interface (MPI), with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary. ", "after_sen": "Applying the parallelized clustering to a large set of dependencies collected from Web documents enabled us to construct gazetteers with up to 500,000 entries and 3,000 classes."}
{"citeStart": 126, "citeEnd": 145, "citeStartToken": 126, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 133, "citeEnd": 147, "citeStartToken": 133, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, storing partial (active) constituents in a chart; Carroll (1993) gives a full description. Although pure bottomup parsing is not usually thought of as providing high performance, the actual implementation achieves very good throughput (see section 4) due to a number of significant optimisations, amongst which are:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The bottom-up left-corner (BU-LC) parser operates left-to-right and breadth-first, storing partial (active) constituents in a chart; Carroll (1993) gives a full description. ", "after_sen": "Although pure bottomup parsing is not usually thought of as providing high performance, the actual implementation achieves very good throughput (see section 4) due to a number of significant optimisations, amongst which are:"}
{"citeStart": 65, "citeEnd": 90, "citeStartToken": 65, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "For both applications, a tagger with the highest possible accuracy is required. The debate about which paradigm solves the part-of-speech tagging problem best is not finished. Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996) . They are only surpassed by combinations of different systems, forming a \"voting tagger\".", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The debate about which paradigm solves the part-of-speech tagging problem best is not finished. ", "mid_sen": "Recent comparisons of approaches that can be trained on corpora (van Halteren et al., 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al., 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al., 1996) . ", "after_sen": "They are only surpassed by combinations of different systems, forming a \"voting tagger\"."}
{"citeStart": 37, "citeEnd": 54, "citeStartToken": 37, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "We notice that the relatively simpler HMM model can perform comparable or better than the sophisticated Model-4 when proper constraints are active in guiding word alignment model training. We also try to put constraints in Model-4. As the Equation 1 implies, when a word-to-word generative probability is needed, one should multiply corresponding lexicon entry in the t-table with the word pair constraint. We simply modify the GIZA++ toolkit (Och and Ney, 2003) by always weighting lexicon probabilities with soft constraints during iterative model training, and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As the Equation 1 implies, when a word-to-word generative probability is needed, one should multiply corresponding lexicon entry in the t-table with the word pair constraint. ", "mid_sen": "We simply modify the GIZA++ toolkit (Och and Ney, 2003) by always weighting lexicon probabilities with soft constraints during iterative model training, and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set.", "after_sen": ""}
{"citeStart": 274, "citeEnd": 286, "citeStartToken": 274, "citeEndToken": 286, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, the concept of valency has gained considerable attention. Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990) . Even theories based on phrase structure may have processing models based on relations between lexical items (Rambow & Joshi, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recently, the concept of valency has gained considerable attention. ", "mid_sen": "Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990) . ", "after_sen": "Even theories based on phrase structure may have processing models based on relations between lexical items (Rambow & Joshi, 1994) ."}
{"citeStart": 129, "citeEnd": 149, "citeStartToken": 129, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "We work with sentences, clauses, phrases and words, using syntactic structures generated by a parser. Our system incrementally processes a text, and extracts pairs of text units: two clauses, a verb and each of its arguments, a noun and each of its modifiers. For each pair of units, the system builds a syntactic graph surrounding the main element (main clause, head verb, head noun). It then tries to find among the previously processed instances another main element with a matching syntactic graph. If such a graph is found, then the system maps previously assigned semantic relations onto the current syntactic graph. We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. The list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent (Barker et al., 1997a) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have a list of 47 relations that manifest themselves in compound clauses, inside a simple clause or in noun phrases. ", "mid_sen": "The list, a synthesis of a number of relation lists cited in the literature, has been designed to be general, domainindependent (Barker et al., 1997a) .", "after_sen": "Section 2 overviews research in semantic relation analysis. "}
{"citeStart": 314, "citeEnd": 337, "citeStartToken": 314, "citeEndToken": 337, "sectionName": "UNKNOWN SECTION NAME", "string": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "mid_sen": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . ", "after_sen": "Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . "}
{"citeStart": 265, "citeEnd": 283, "citeStartToken": 265, "citeEndToken": 283, "sectionName": "UNKNOWN SECTION NAME", "string": "Our system maintains a set of beliefs about the domain and about the user's beliefs. Associated with each belief is a strength that represents the agent's confidence in holding that belief. We model the strength of a belief using endorsements, which are explicit records of factors that affect one's certainty in a hypothesis (Cohen, 1985) , following (Galliers, 1992; Logan et al., 1994) . Our endorsements are based on the semantics of the utterance used to convey a befief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. The belief level of the dialogue model consists of mutual beliefs proposed by the agents' discourse actions. When an agent proposes a new belief and gives (optional) supporting evidence for it, this set of proposed beliefs is represented as a belief tree, where the belief represented by a child node is intended to support that represented by its parent. The root nodes of these belief trees (rap-level beliefs) contribute to problem-solving actions and thus affect the domain plan being developed. Given a set of newly proposed beliefs, the system must decide whether to accept the proposal or m initiate a negotiation dialogue to resolve conflicts. The evaluation of proposed beliefs starts at the leaf nodes of the proposed belief trees since acceptance of a piece of proposed evidence may affect acceptance of the parent belief it is intended to support. The process continues until the top-level proposed beliefs are evaluated. Conflict resolution strategies are invoked only if the top-level proposed beliefs are not accepted because if collaborative agents agree on a belief relevant to the domain plan being constructed, it is irrelevant whether they agree on the evidence for that belief (Young et al., 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The process continues until the top-level proposed beliefs are evaluated. ", "mid_sen": "Conflict resolution strategies are invoked only if the top-level proposed beliefs are not accepted because if collaborative agents agree on a belief relevant to the domain plan being constructed, it is irrelevant whether they agree on the evidence for that belief (Young et al., 1994) .", "after_sen": "In determining whether to accept a proposed befief or evidential relationship, the evaluator first constructs an evidence set containing the system's evidence thin supports or attacks _bcl and the evidence accepted by the system that was proposed by the user as support for -bel. "}
{"citeStart": 84, "citeEnd": 96, "citeStartToken": 84, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "The notion of ~head' employed here is connected more closely with processing control than linguistics. In particular, nothing requires that a head of a rule should share any information with the LItS item, although in practice it often will. Heads serve as anchor-points in the input string around which islands may be formed, and are accordingly treated before non-head items (RHS items are re-ordered during compilation-see below). In the central role of heads, LtIIP resembles parsers devised by Kay (1989) and van Noord (1991) ; in other respects, including the use which is made of heads, the approaches are rather different, however.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Heads serve as anchor-points in the input string around which islands may be formed, and are accordingly treated before non-head items (RHS items are re-ordered during compilation-see below). ", "mid_sen": "In the central role of heads, LtIIP resembles parsers devised by Kay (1989) and van Noord (1991) ; in other respects, including the use which is made of heads, the approaches are rather different, however.", "after_sen": ""}
{"citeStart": 315, "citeEnd": 338, "citeStartToken": 315, "citeEndToken": 338, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. ", "mid_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "after_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . "}
{"citeStart": 87, "citeEnd": 98, "citeStartToken": 87, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "1 Introduction \"Unification-based\" Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic constraints. However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999) . Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. ", "mid_sen": "Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999) . ", "after_sen": "Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus."}
{"citeStart": 175, "citeEnd": 190, "citeStartToken": 175, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993) , robust parsing (Abney, 1991) , and general parsing (deMarcken, 1990; Charniak et al., 1994) . The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. ", "mid_sen": "Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993) , robust parsing (Abney, 1991) , and general parsing (deMarcken, 1990; Charniak et al., 1994) . ", "after_sen": "The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. "}
{"citeStart": 32, "citeEnd": 41, "citeStartToken": 32, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993) ) of each sentence. The DRS consists of semantic predicates and named entity tags. We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. We developed the named-entity tagger for the weather domain ourselves. To tag entities in the biography domain, we used OpenCalais (www.opencalais.com). For example, in the biography in (1), the conceptual meaning (semantic predicates and domain-specific entities) of sentences (a-b) are represented in (c-d). The corresponding templates are showing in (e-f).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The DRS consists of semantic predicates and named entity tags. ", "mid_sen": "We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE. ", "after_sen": "In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates. "}
{"citeStart": 229, "citeEnd": 238, "citeStartToken": 229, "citeEndToken": 238, "sectionName": "UNKNOWN SECTION NAME", "string": "1For the experiments described in this paper we have used TiMBL, an MBL software package developed in the ILK-group (Daelemans et al., 1998) , TiMBL is available from: http://ilk.kub.nl/. memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For other", "mid_sen": "1For the experiments described in this paper we have used TiMBL, an MBL software package developed in the ILK-group (Daelemans et al., 1998) , TiMBL is available from: http://ilk.kub.nl/. memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998) .", "after_sen": ""}
{"citeStart": 61, "citeEnd": 75, "citeStartToken": 61, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a method of detecting Japanese homophone errors in Japanese texts. Our method is based on a decision list proposed by Yarowsky (Yarowsky, 1994; Yarowsky, 1995) . We improve the original decision list by using written words in the default evidence. The improved decision list can raise the F-measure of error detection.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a method of detecting Japanese homophone errors in Japanese texts. ", "mid_sen": "Our method is based on a decision list proposed by Yarowsky (Yarowsky, 1994; Yarowsky, 1995) . ", "after_sen": "We improve the original decision list by using written words in the default evidence. "}
{"citeStart": 81, "citeEnd": 105, "citeStartToken": 81, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999) , which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. Section 2 overviews our CLIR system, and Section 3 elaborates on the translation module focusing on compound word translation and transliteration. Section 4 then evaluates the effectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(2) Asian languages often represent loanwords based on their special phonograms (primarily for technical terms and proper nouns), which creates new base words progressively (in the case of Japanese, the phonogram is called katakana).", "mid_sen": "To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999) , which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. ", "after_sen": "For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . "}
{"citeStart": 30, "citeEnd": 50, "citeStartToken": 30, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that the experiments in (Carpuat and Wu, 2005) did not use a state-of-the-art MT system, while the experiments in (Vickrey et al., 2005) were not done using a full-fledged MT system and the evaluation was not on how well each source sentence was translated as a whole. The relatively small improvement reported by Cabezas and Resnik (2005) without a statistical significance test appears to be inconclusive. Considering the conflicting results reported by prior work, it is not clear whether a WSD system can help to improve the performance of a state-of-the-art statistical MT system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They obtained a relatively small improvement, and no statistical significance test was reported to determine if the improvement was statistically significant.", "mid_sen": "Note that the experiments in (Carpuat and Wu, 2005) did not use a state-of-the-art MT system, while the experiments in (Vickrey et al., 2005) were not done using a full-fledged MT system and the evaluation was not on how well each source sentence was translated as a whole. ", "after_sen": "The relatively small improvement reported by Cabezas and Resnik (2005) without a statistical significance test appears to be inconclusive. "}
{"citeStart": 15, "citeEnd": 38, "citeStartToken": 15, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although there are a number of efforts to construct reusable large-coverage testsuites, none has to my knowledge explored how existing grammars could be used for this purpose.", "mid_sen": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . ", "after_sen": "Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. "}
{"citeStart": 83, "citeEnd": 109, "citeStartToken": 83, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "We are particularly interested in how the different phases interact, 1) because one major way to expand the system is to add different exercises and incorporate them into the second phase, and 2) because the results in table 6 show a strong interdependence between phases. We thus performed a set of experiments to gauge the effect of different types of features. By running ablation studies-i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011 ))-we can determine their relative importance and usefulness. We run phase 2 (k = 1) using different combinations of phase 1 classifiers (1-best) as input. Perhaps unsurprisingly, the combination of all feature types results in the highest results of 60.5%. Also, using only one type of features results in the lowest performance, with the global features being the least informative set, on par with the baseline of 34.2%. If we use only two feature sets, removing the global features results in the least deterioration. Since these features do not directly model errors but rather global sentence trends, this is to be expected. However, leaving out inter-token features results in the second-lowest results (36.8%), thus showing that this set is extremely important-again not surprising given that we are working with an exercise designed to test word order skills.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We thus performed a set of experiments to gauge the effect of different types of features. ", "mid_sen": "By running ablation studies-i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011 ))-we can determine their relative importance and usefulness. ", "after_sen": "We run phase 2 (k = 1) using different combinations of phase 1 classifiers (1-best) as input. "}
{"citeStart": 137, "citeEnd": 139, "citeStartToken": 137, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "The ideas presented above have been implemented in an experimental system called Tin (after the woodman of OZ). The intent is to provide a uniform f~amework for the construction and experimentation of chart parsers, somewhat as systems like MCHART [29] , but with a more systematic theoretical foundation. The kernel of the system is a virtual parsing machine with a stack and a set of primitive commands corresponding essentially to the operation of a practical Push-Down Transducer. These commands include for example: push (resp. pop) to push a symbol on the stack (reap. pop one), check~indow to compare the look-ahead symbol(s) to some given symbol, chsckstack to branch depending on the top of the sta~k, scan to read an input word, outpu$ to output a rule number (or a terminal symbol), goto for unconditional jumps, and a few others. However theae commands are never used directly to program parsers. They are used as machine instructions for compilers that compile grammatical definitions into Tin code according to some parsing schema. A characteristic of these commands is that they may all be marked as non-determlnistic. The intuitive interpretation is that there is a non-deterministic choice between a command thus marked and another command whose address in the virtual machine code is then specified. However execution of the virtual machine code is done by an all-paths interpreter that follows the dynamic programming strategy described in section 2.1 and appendix A.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ideas presented above have been implemented in an experimental system called Tin (after the woodman of OZ). ", "mid_sen": "The intent is to provide a uniform f~amework for the construction and experimentation of chart parsers, somewhat as systems like MCHART [29] , but with a more systematic theoretical foundation. ", "after_sen": "The kernel of the system is a virtual parsing machine with a stack and a set of primitive commands corresponding essentially to the operation of a practical Push-Down Transducer. "}
{"citeStart": 73, "citeEnd": 99, "citeStartToken": 73, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "For the entity grid model, we follow the generative approach proposed by Lapata and Barzilay (2005) . A text is converted into a matrix, where rows correspond to sentences, in the order in which they appear in the article. Columns are created one for each entity appearing in the text. Each cell (i,j) is filled with the grammatical role r i,j of the entity j in sentence i. We computed the entity grids using the Brown Coherence Toolkit 4 . The probability of the text (T ) is defined using the likely sequence of grammatical role transitions.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Despite the difference, the content model accuracies for our implementation are quite close to that from the original.", "mid_sen": "For the entity grid model, we follow the generative approach proposed by Lapata and Barzilay (2005) . ", "after_sen": "A text is converted into a matrix, where rows correspond to sentences, in the order in which they appear in the article. "}
{"citeStart": 53, "citeEnd": 72, "citeStartToken": 53, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Supervised algorithms are given training data with all word boundaries marked, and must infer word boundaries in a separate test set. Simple supervised algorithms perform extremely well (Cairns et al., 1997; Teahan et al., 2000) , but don't address our main goal: learning how to segment.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Supervised algorithms are given training data with all word boundaries marked, and must infer word boundaries in a separate test set. ", "mid_sen": "Simple supervised algorithms perform extremely well (Cairns et al., 1997; Teahan et al., 2000) , but don't address our main goal: learning how to segment.", "after_sen": "Notice that phrase boundaries are not randomly selected word boundaries. "}
{"citeStart": 171, "citeEnd": 188, "citeStartToken": 171, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "Our baseline phrase table training method is the ViterbiExtract algorithm. All phrase pairs with respect to the word alignment boundary constraint are identified and pooled to build phrase translation tables with the Maximum Likelihood criterion. We prune phrase translation entries by their probabilities. The maximum number of words in Chinese and English phrases is set to 8 and 25 respectively for all conditions 2 . We perform online style phrase training, i.e., phrase extraction is not particular for any evaluation set. Two different word alignment models are trained as the baseline, one is symmetric HMM word alignment model, the other is IBM Model-4 as implemented in the GIZA++ toolkit (Och and Ney, 2003) . The translation results as measured by BLEU and METEOR scores are presented in Table 3 . We notice that Model-4 based phrase table performs roughly 1% better in terms of both BLEU and METEOR scores than that based on HMM.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We perform online style phrase training, i.e., phrase extraction is not particular for any evaluation set. ", "mid_sen": "Two different word alignment models are trained as the baseline, one is symmetric HMM word alignment model, the other is IBM Model-4 as implemented in the GIZA++ toolkit (Och and Ney, 2003) . ", "after_sen": "The translation results as measured by BLEU and METEOR scores are presented in Table 3 . "}
{"citeStart": 189, "citeEnd": 209, "citeStartToken": 189, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "In the first experiment, the ANLT grammar was loaded and a set of sentences was input to each of the three parsers. In order to provide an independent basis for comparison, the same sentences were also input to the SRI Core Language Engine (CLE) parser (Moore & Alshawi, 1992 ) with the CLARE2.5 grammar (Alshawi et al., 1992) , a state-of-the-art system accessible to the author. The sentences were taken from an initial sample of 175 representative sentences extracted from a corpus of approximately 1500 that form part of the ANLT package. This corpus, implicitly defining the types of construction the grammar is intended to cover, was written by the linguist who developed the ANLT grammar and is used to check for any adverse effects on coverage when the grammar is modified during grammar development. Of the initial 175 sentences, the CLARE2.5 grammar failed to parse 42 (in several cases because punctuation is strictly required but is missing from the corpus). The ANLT grammar also failed to parse three of these, plus an additional four. These sentences were removed from the sample, leaving 129 (mean length 6.7 words) of which 47 were declarative sentences, 38 wh-questions and other sentences with gaps, 20 passives, and 24 sentences containing co-ordination. Table 1 shows the total parse times and storage allocated for the BU-LC parser, the LR parser, and the CE parser, all with ANLT grammar and lexicon. All three parsers have been implemented by the author to a similar high standard: similar implementation techniques are used in all the parsers, the parsers share the same unification module, run in the same Lisp environment, have been compiled with the same optimisation settings, and have all been profiled with the same tools and hand-optimised to a similar extent. (Thus any difference in performance of more than around 15% is likely to stem from algorithmic rather than implementational reasons). Both of the predictive parsers employ one symbol of lookahead, incorporated into the parsing tables by the LALR technique. Table 1 also shows the results for the CLE parser with the CLARE2.5 grammar and lexicon. The figures include garbage collection time, and phrasal (where appropriate) processing, but not parse forest unpacking. Both grammars give a total of around 280 analyses at a similar level of detail.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the first experiment, the ANLT grammar was loaded and a set of sentences was input to each of the three parsers. ", "mid_sen": "In order to provide an independent basis for comparison, the same sentences were also input to the SRI Core Language Engine (CLE) parser (Moore & Alshawi, 1992 ) with the CLARE2.5 grammar (Alshawi et al., 1992) , a state-of-the-art system accessible to the author. ", "after_sen": "The sentences were taken from an initial sample of 175 representative sentences extracted from a corpus of approximately 1500 that form part of the ANLT package. "}
{"citeStart": 209, "citeEnd": 234, "citeStartToken": 209, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "The joint n-gram model is language-independent. An aligned corpus with words and their pronunciations is needed, but no further adaptation is required. Table 1 shows the performance of our model in comparison to alternative approaches on the German and English versions of the CELEX corpus, the English NetTalk corpus, the English Teacher's Word Book (TWB) corpus, the English beep corpus and the French Brulex corpus. The joint n-gram model performs significantly better than the decision tree (essentially based on (Lucassen and Mercer, 1984) ), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005) . For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. Automatic syllabification for our model integrated phonological constraints (as described in section 3.1), and therefore led to an improvement in phoneme accuracy, while the word error rate increased for the PbA approach, which does not incorporate such constraints. (Chen, 2003) also used a joint n-gram model. The two approaches differ in that Chen ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 1 shows the performance of our model in comparison to alternative approaches on the German and English versions of the CELEX corpus, the English NetTalk corpus, the English Teacher's Word Book (TWB) corpus, the English beep corpus and the French Brulex corpus. ", "mid_sen": "The joint n-gram model performs significantly better than the decision tree (essentially based on (Lucassen and Mercer, 1984) ), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005) . ", "after_sen": "For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. "}
{"citeStart": 325, "citeEnd": 336, "citeStartToken": 325, "citeEndToken": 336, "sectionName": "UNKNOWN SECTION NAME", "string": "Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and subjectivity clues listed in (Wiebe, 1990) . Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al., 2003) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Any data used to develop this vocabulary does not overlap with the test sets or the unannotated data used in this paper.", "mid_sen": "Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and subjectivity clues listed in (Wiebe, 1990) . ", "after_sen": "Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al., 2003) ."}
{"citeStart": 138, "citeEnd": 156, "citeStartToken": 138, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "mid_sen": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "after_sen": "Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. "}
{"citeStart": 253, "citeEnd": 270, "citeStartToken": 253, "citeEndToken": 270, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. The Temporal Centering framework (Kameyama et al., 1993) integrates lWe will limit the scope of this paper by restricting the discussion to accomplishments and achievements. aspects of both approaches, but patterns with the first in treating tense as anaphoric.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . ", "mid_sen": "Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . ", "after_sen": "Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. "}
{"citeStart": 239, "citeEnd": 262, "citeStartToken": 239, "citeEndToken": 262, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach is integrated into an English-German morphology-aware SMT system which first translates into a lemmatized representation with a com-ponent to generate fully inflected forms in a second step, an approach similar to the work by Toutanova et al. (2008) and Fraser et al. (2012) . The inflection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function . Weller et al. (2013) describe modeling case in SMT; we want to treat all subcategorized elements of a verb in one step and extend their setup to cover the prediction of prepositions in both PP and NPs (i.e., the \"empty\" preposition).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our approach is integrated into an English-German morphology-aware SMT system which first translates into a lemmatized representation with a com-ponent to generate fully inflected forms in a second step, an approach similar to the work by Toutanova et al. (2008) and Fraser et al. (2012) . ", "after_sen": "The inflection requires the modeling of the grammatical case of noun phrases (among other features), which corresponds to determining the syntactic function . "}
{"citeStart": 95, "citeEnd": 114, "citeStartToken": 95, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "Our experiments are based on a subset of meeting recordings collected and transcribed by ICSI (Morgan et al., 2001) . Seven meetings were segmented (automatically, but with human adjustment) into 9854 total spurts. We define a 'spurt' as a period of speech by one speaker that has no pauses of greater than one half second (Shriberg et al., 2001) . Spurts are used here, rather than sentences, because our goal is to use ASR outputs and unsupervised training paradigms, where hand-labeled sentence segmentations are not available.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our experiments are based on a subset of meeting recordings collected and transcribed by ICSI (Morgan et al., 2001) . ", "after_sen": "Seven meetings were segmented (automatically, but with human adjustment) into 9854 total spurts. "}
{"citeStart": 249, "citeEnd": 263, "citeStartToken": 249, "citeEndToken": 263, "sectionName": "UNKNOWN SECTION NAME", "string": "Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994) , or sense-tagged seed examples (Yarowsky, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. ", "mid_sen": "The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994) , or sense-tagged seed examples (Yarowsky, 1995) .", "after_sen": "Some observations can be made on the previous supervised and semi-supervised methods. "}
{"citeStart": 234, "citeEnd": 258, "citeStartToken": 234, "citeEndToken": 258, "sectionName": "UNKNOWN SECTION NAME", "string": "More and more often, in real-word natural language processing (NLP) applications based upon grammars, these grammars are no more written by hand but are automatically generated, this has several consequences. This paper will consider one of these consequences: the generated grammars may be very large. Indeed, we aim to deal with grammars that have, say, over a million symbol occurrences and several hundred thousands rules. Traditional parsers are not usually prepared to handle them, either because these grammars are simply too big (the parser's internal structures blow up) or the time spent to analyze a sentence becomes prohibitive. This paper will concentrate on context-free grammars (CFG) and their associated parsers. However, virtually all Tree Adjoining Grammars (TAG, see e.g., (Schabes et al., 1988) ) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters, 1995) . Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass. This is indeed what we have achieved with a TAG automatically extracted from (Villemonte de La Clergerie, 2005)'s large-coverage factorized French TAG, as we will see in Section 4. Even (some kinds of) non CFGs may benefit from the ideas described in this paper.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper will concentrate on context-free grammars (CFG) and their associated parsers. ", "mid_sen": "However, virtually all Tree Adjoining Grammars (TAG, see e.g., (Schabes et al., 1988) ) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters, 1995) . ", "after_sen": "Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass. "}
{"citeStart": 13, "citeEnd": 32, "citeStartToken": 13, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "According to Hobbs (1979, p. 67) , these two sentences are incoherent. However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs) becomes coherent. It seems that any analysis of coherence in terms of the relation between subsequent sentences cannot explain this sudden change; after all, the first two sentences didn't change when the third one was added. On the other hand, this change is easily explained when we treat the first two sentences as a paragraph: if the third sentence is not a part of the background knowledge, the paragraph is incoherent. And the paragraph obtained by adding the third sentence is coherent. Moreover, coherence here is clearly the result of the existence of the topic \"John likes spinach.\"", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "He likes spinach.", "mid_sen": "According to Hobbs (1979, p. 67) , these two sentences are incoherent. ", "after_sen": "However, the same fragment, augmented with the third sentence Mary told him yesterday that the French spinach crop failed and Turkey is the only country... (ibid.) suddenly (for Hobbs) becomes coherent. "}
{"citeStart": 64, "citeEnd": 91, "citeStartToken": 64, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "To find such a fragment, we use a simple yet adequate heuristic. We start by parsing the sentence using the link grammar parser (Sleator and Temperley, 1991) . Since the parser is not trained on citation sentences, we replace the references with placeholders before passing the sentence to the parser. Figure 1 shows a portion of the parse tree for Sentence (1) (from Section 1). We extract the scope of the reference from the parse tree as follows. We find the smallest subtree rooted at an S node (sentence clause node) and contains the target reference node. We extract the text that corresponds to this subtree if it is grammatical. Otherwise, we find the second smallest subtree rooted at an S node and so on. For example, the parse tree shown in Figure 1 suggests that the scope of the reference is:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To find such a fragment, we use a simple yet adequate heuristic. ", "mid_sen": "We start by parsing the sentence using the link grammar parser (Sleator and Temperley, 1991) . ", "after_sen": "Since the parser is not trained on citation sentences, we replace the references with placeholders before passing the sentence to the parser. "}
{"citeStart": 83, "citeEnd": 103, "citeStartToken": 83, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "We experiment with two datasets of research paper content. One consists of the headers of research papers. The other consists of pre-segmented citations from the reference sections of research papers. These data sets have been used as standard benchmarks in several previous studies (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The other consists of pre-segmented citations from the reference sections of research papers. ", "mid_sen": "These data sets have been used as standard benchmarks in several previous studies (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003) .", "after_sen": ""}
{"citeStart": 108, "citeEnd": 133, "citeStartToken": 108, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006) . Given a bilingual corpus with the English NE annotated, the system had to discover the NE in target language text. We used the English-Russian news corpus used in the baseline system. NEs were grouped into equivalence classes, each containing different variations of the same NE. We randomly sampled 500 documents from the corpus. Transliteration pairs were mapped into 97 equivalence classes, identified by an expert. In a second experiment, different learning parameters such as selective sampling efficiency and feature weights were checked. 300 English-Russian and English-Hebrew NE pairs were used; negative samples were generated by coupling every English NE with all other target language NEs. Table 2 : Comparison of correctly identified English-Russian transliteration pairs in news corpus. The model trained using selective sampling outperforms models trained using random sampling, even when trained with twice the data. The top one and top two results columns describe the proportion of correctly identified pairs ranked in the first and top two places, respectively.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006) . ", "after_sen": "Given a bilingual corpus with the English NE annotated, the system had to discover the NE in target language text. "}
{"citeStart": 129, "citeEnd": 151, "citeStartToken": 129, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "These empirical results indicate that investigating different similarity measures can lead to improved natural language processing. On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987) , there has been some doubt expressed in that community that the choice of similarity metric has any practical impact:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These empirical results indicate that investigating different similarity measures can lead to improved natural language processing. ", "mid_sen": "On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987) , there has been some doubt expressed in that community that the choice of similarity metric has any practical impact:", "after_sen": "Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised. "}
{"citeStart": 15, "citeEnd": 41, "citeStartToken": 15, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on Soon et al. (2001) . They built a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier (Denis and Baldridge, 2007) , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. ", "mid_sen": "More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. ", "after_sen": "However, when encoding constraints into their ILP solver, they did not enforce transitivity."}
{"citeStart": 35, "citeEnd": 57, "citeStartToken": 35, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "See web: http://www.statmt.org 3 W b is exactly the weight of In-Hiero in Table 1. NIST05 NIST06 NIST08 NIST02 0.665 0.571 0.506 For an efficient tuning, the retrieval process is parallelized as follows: the examples are assigned to 4 CPUs so that each CPU accepts a query and returns its top-100 results, then all these top-100 results are merged into the final top-100 retrieved examples together with their translation candidates. In our experiments, we employ the two incremental training methods, i.e. MBUU and EBUU. Both of the hyperparameters λ are tuned on NIST05 and set as 0.018 and 0.06 for MBUU and EBUU, respectively. In the incremental training step, only one CPU is employed. Table 2 depicts that testing each sentence with local training method takes 2.9 seconds, which is comparable to the testing time 2.0 seconds with global training method 4 . This shows that the local method is efficient. Further, compared to the retrieval, the local training is not the bottleneck. Actually, if we use LSH technique (Andoni and Indyk, 2008) in retrieval process, the local method can be easily scaled to a larger training data. Table 3 shows the main results of our local training methods. The EBUU training method significantly outperforms the MERT baseline, and the improvement even achieves up to 2.0 BLEU points on NIST08. We can also see that EBUU and MBUU are comparable on these three test sets. Both of these two local training methods achieve significant improvements over the MERT baseline, which proves the effectiveness of our local training method over global training method.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Further, compared to the retrieval, the local training is not the bottleneck. ", "mid_sen": "Actually, if we use LSH technique (Andoni and Indyk, 2008) in retrieval process, the local method can be easily scaled to a larger training data. ", "after_sen": "Table 3 shows the main results of our local training methods. "}
{"citeStart": 166, "citeEnd": 192, "citeStartToken": 166, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008) . We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005) , or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error,", "mid_sen": "We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008) . ", "after_sen": "We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005) , or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006) ."}
{"citeStart": 41, "citeEnd": 52, "citeStartToken": 41, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "Our project's long-range goal (see http://www.isp. pitt.edu/'intgen/) is to create a unified architecture for collaborative discourse, accommodating both interpretation and generation. Our computational approach (Thomason and Hobbs, 1997) uses a form of weighted abduction as the reasoning mechanism (Hobbs et al., 1993) and modal operators to model context. In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. From our first annotation trials, we found that the recognition of \"classical\" speech acts (Austin, 1962; Searle, 1975) by coders is fairly reliable, while recognizing contextual relationships (e.g., whether an utterance accepts a proposal) is not as reliable. Thus, we explore other features that can help us recognize how participants coordinate agreement. Our corpus study also provides a preliminary assessment of the Discourse Resource Initiative (DR/) tagging scheme. The DRI is an international \"grassroots\" effort that seeks to share corpora that have been tagged with the core features of interest to the discourse community. In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes. A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html). Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997; Poesio and Traum, 1997) , we have attempted to adapt it to our corpus and particular research questions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our project's long-range goal (see http://www.isp. pitt.edu/'intgen/) is to create a unified architecture for collaborative discourse, accommodating both interpretation and generation. ", "mid_sen": "Our computational approach (Thomason and Hobbs, 1997) uses a form of weighted abduction as the reasoning mechanism (Hobbs et al., 1993) and modal operators to model context. ", "after_sen": "In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. "}
{"citeStart": 91, "citeEnd": 93, "citeStartToken": 91, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "To discus• the above issue• in a uniform way, we need a genera] framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take a• a l~sk a formalism developed by the second author in previous papers [15, 16] . The idea of this approach is to separate the dynamic programming construct• needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Gritfith & Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers 5. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not alway• terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. However ity. This approach may thus be used as a uniform framework for comparing chart parsers s.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To discus• the above issue• in a uniform way, we need a genera] framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. ", "mid_sen": "We shall take a• a l~sk a formalism developed by the second author in previous papers [15, 16] . ", "after_sen": "The idea of this approach is to separate the dynamic programming construct• needed for efficient chart parsing from the chosen parsing schema. "}
{"citeStart": 42, "citeEnd": 54, "citeStartToken": 42, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "normalized entropy decrease of the classification set caused by the presence of the feature. Details of the algorithm can be found in (Daelemans et al., 1998) I.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "normalized entropy decrease of the classification set caused by the presence of the feature. ", "mid_sen": "Details of the algorithm can be found in (Daelemans et al., 1998) I.", "after_sen": ""}
{"citeStart": 83, "citeEnd": 97, "citeStartToken": 83, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2 . The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations. However, the differences are not significant.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This algorithm does not converge on the Large Set in 10000 iterations.", "mid_sen": "We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2 . ", "after_sen": "The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations. "}
{"citeStart": 168, "citeEnd": 189, "citeStartToken": 168, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "A wide range of XML-based tools for NLP applications lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. In processing the HOLJ documents we have built a pipeline using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000) , (Thompson et al., 1997) and the xmlperl program (McKelvie, 1999) . The overall processing stages contained in our pipeline are shown in Figure 1 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A wide range of XML-based tools for NLP applications lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. ", "mid_sen": "In processing the HOLJ documents we have built a pipeline using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000) , (Thompson et al., 1997) and the xmlperl program (McKelvie, 1999) . ", "after_sen": "The overall processing stages contained in our pipeline are shown in Figure 1 ."}
{"citeStart": 65, "citeEnd": 77, "citeStartToken": 65, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Syntactic parsing of natural language is the task of analyzing the grammatical structure of sentences and predicting their most likely parse trees (see Figure 1 ). These parse trees can then be used in many ways to enable natural language processing applications like machine translation, question answering, and information extraction. Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank. The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n 3 ) time. While often ignored, the grammar constant |G| typically dominates the runtime in practice. This is because grammars with high accuracy (Collins, 1999; Charniak, 2000; Petrov et al., 2006) have thousands of nonterminal symbols and millions of context-free rules, while most sentences have on average only about n = 20 words.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most syntactic constituency parsers employ a weighted context-free grammar (CFG), that is learned from a treebank. ", "mid_sen": "The CKY dynamic programming algorithm (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) is then be used to find the most likely parse tree for a given sentence of length n in O(|G|n 3 ) time. ", "after_sen": "While often ignored, the grammar constant |G| typically dominates the runtime in practice. "}
{"citeStart": 67, "citeEnd": 79, "citeStartToken": 67, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. Because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm~, troublesome in actual application environments. On the other hand, the algorithm in this paper has no such requirement, it requires only a minimum of linguistic knowledge, including parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules which lexicon based syntactic theories like HPSG CC etc. normally assume. The parser is not a deterministic parser, but a parser which produces all possible analyses. All of the results are used for calculation ant the system assumes that there is a correct answer among them. The algorithm builds correct structural descriptions of sentences and discovers semantic collocations at the same time. It works as a relaxation process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains.", "mid_sen": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. ", "after_sen": "It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. "}
{"citeStart": 166, "citeEnd": 193, "citeStartToken": 166, "citeEndToken": 193, "sectionName": "UNKNOWN SECTION NAME", "string": "The classifiers generally mimic human judgements in that accuracy is much lower in the threeway classification task -a pattern concurring with past observations (cf. Esuli and Sebastiani (2006) ; Andreevskaia and Bergler (2006) ). Crucially, FA-TAL errors remain below 10% throughout. Further advances can be made by fine-tuning the Φ ntr coefficients, and by learning weights for individual classifiers which can currently mask each other and suppress the correct analysis when run collectively.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "M (N) ) cases.", "mid_sen": "The classifiers generally mimic human judgements in that accuracy is much lower in the threeway classification task -a pattern concurring with past observations (cf. Esuli and Sebastiani (2006) ; Andreevskaia and Bergler (2006) ). ", "after_sen": "Crucially, FA-TAL errors remain below 10% throughout. "}
{"citeStart": 127, "citeEnd": 141, "citeStartToken": 127, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "The possibility of inserting a word into a domain of some trausitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity. From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981) . In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994 ), Becker et al. (1991 ). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. Given this correspondence, it is natural to employ dependencies in the description of discontinuities as fol-3Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only apply to sister nodes. lows: For each modifier of a certain head, a set of dependency types is defined which may link the direct head and the positional head of the modifier (\"gesehen\" and \"hat\", resp.). If this set is empty, both heads are identical and a contiguous attachment results. The impossibility of extraction from, e.g., a finite verb phrase may follow from the fact that the dependency embedding finite verbs, propo, may not appear on any path between a direct and a positional head. 4", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981) . ", "mid_sen": "In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994 ), Becker et al. (1991 ). ", "after_sen": "In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. "}
{"citeStart": 41, "citeEnd": 51, "citeStartToken": 41, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "We illustrate the relevant syntactic and semantic properties of these forms using the version of Categorial Semantics described in Pereira (1990) . In the Montagovian tradition, semantic representations are com-positionaUy generated in correspondence with the constituent modification relationships manifest in the syntax; predicates are curried. Traces are associated with assumptions which are subsequently discharged by a suitable construction. Figure 1 shows the representations for the sentence Bill became upset; this will serve as the initial source clause representation for the examples that follow. 3 For our analysis of gapping, we follow Sag (1976) in hypothesizing that a post-surface-structure level of syntactic representation is used as the basis for interpretation. In source clauses of gapping constructions, constituents in the source that are parallel to the overt constituents in the target are abstracted out of the clause representation. 4 For simplicity, we will assume that 3We will ignore the tense of the predicates for ease of exposition.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figure 1 shows the representations for the sentence Bill became upset; this will serve as the initial source clause representation for the examples that follow. ", "mid_sen": "3 For our analysis of gapping, we follow Sag (1976) in hypothesizing that a post-surface-structure level of syntactic representation is used as the basis for interpretation. ", "after_sen": "In source clauses of gapping constructions, constituents in the source that are parallel to the overt constituents in the target are abstracted out of the clause representation. "}
{"citeStart": 62, "citeEnd": 74, "citeStartToken": 62, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "As a seinant{c representation of words, distltllCe w~ctors are expected to depend very weakly on the particular source dictionary. We eolilpared two sets of distance vectors, one from I,I)OCE (Procter 1978) and the other from COBUILD (Sinclair 1987) , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves (Niwa and Nitta 1993 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a seinant{c representation of words, distltllCe w~ctors are expected to depend very weakly on the particular source dictionary. ", "mid_sen": "We eolilpared two sets of distance vectors, one from I,I)OCE (Procter 1978) and the other from COBUILD (Sinclair 1987) , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves (Niwa and Nitta 1993 ).", "after_sen": "We will now describe some technical details al)Ollt the derivation of distance vectors."}
{"citeStart": 35, "citeEnd": 47, "citeStartToken": 35, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "The predictors in this case are used to decide, for each word, whether it is the first in a phrase, the last in a phrase, both of these, or none of these. . This approach has been studied in (Church, 1988; Argamon et al., 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The predictors in this case are used to decide, for each word, whether it is the first in a phrase, the last in a phrase, both of these, or none of these. . ", "mid_sen": "This approach has been studied in (Church, 1988; Argamon et al., 1998) .", "after_sen": ""}
{"citeStart": 211, "citeEnd": 231, "citeStartToken": 211, "citeEndToken": 231, "sectionName": "UNKNOWN SECTION NAME", "string": "To understand the behavior of this model, we computed the standard alignment error rate (AER) performance metric. 2 We also investigated extractionspecific metrics: the frequency of interior nodes -a measure of how often the alignments violate the constituent structure of English parses -and a variant of the CPER metric of Ayan and Dorr (2006) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To understand the behavior of this model, we computed the standard alignment error rate (AER) performance metric. ", "mid_sen": "2 We also investigated extractionspecific metrics: the frequency of interior nodes -a measure of how often the alignments violate the constituent structure of English parses -and a variant of the CPER metric of Ayan and Dorr (2006) .", "after_sen": "We evaluated the performance of our model on both French-English and Chinese-English manually aligned data sets. "}
{"citeStart": 106, "citeEnd": 117, "citeStartToken": 106, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "where t i is a disambiguated POS tag of the ith word, a i is the abbreviation flag of the ith word, and O i is the observation at the ith position, which in our case is the ambiguity class the word belongs to, its capitalization, and its abbreviation flag (AmbClass i , a i , Cap i ) . Since the abbreviation flag of the previous word strongly influences period disambiguation, it was included in the standard trigram model. We decided to train the tagger with the minimum of preannotated resources. First, we used 20,000 tagged words to \"bootstrap\" the training process, because purely unsupervised techniques, at least for the HMM class of taggers, yield lower precision. We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch [Baum and Petrie 1966] or Brill's [Brill 1995b ]) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns. Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. Periods as many other closed-class words cannot be successfully covered by such technique.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also used our DCA system to assign capitalized words, abbreviations, and sentence breaks, retaining only cases assigned by the strategies with an accuracy not less than 99.8%. ", "mid_sen": "This was done because purely unsupervised techniques (e.g., Baum-Welch [Baum and Petrie 1966] or Brill's [Brill 1995b ]) enable regularities to be induced for word classes which contain many entries, exploiting the fact that individual words that belong to a POS class occur in different ambiguity patterns. ", "after_sen": "Counting all possible POS combinations in these ambiguity patterns over multiple patterns usually produces the right combinations as the most frequent. "}
{"citeStart": 82, "citeEnd": 100, "citeStartToken": 82, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "To predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. This may be sufficient for written discourse. For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories. I investigate this via a phenomenon that, by the strictest interpretation of either centering or intonation theories, should not occur --the case of pitch accented pronominals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. ", "after_sen": "This may be sufficient for written discourse. "}
{"citeStart": 195, "citeEnd": 220, "citeStartToken": 195, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011) . Thus, the classifiers trained on the learner data make use of a discriminative model. Because the Google corpus does not contain complete sentences but only n-gram counts of length up to five, training a discriminative model is not desirable, and we thus use NB (details in Rozovskaya and Roth (2011) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The other system components use the preprocessing tools only as part of candidate generation (e.g., to identify all nouns in the data for the noun classifier).", "mid_sen": "The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks (Rozovskaya and Roth, 2011) . ", "after_sen": "Thus, the classifiers trained on the learner data make use of a discriminative model. "}
{"citeStart": 41, "citeEnd": 59, "citeStartToken": 41, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The above two methods have been used in (Banea et al., 2008) for Romanian subjectivity analysis, but the experimental results are not very promising. As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lastly, we use the classifier to classify the unlabeled Chinese reviews.", "mid_sen": "The above two methods have been used in (Banea et al., 2008) for Romanian subjectivity analysis, but the experimental results are not very promising. ", "after_sen": "As shown in our experiments, the above two methods do not perform well for Chinese sentiment classification, either, because the underlying distribution between the original language and the translated language are different."}
{"citeStart": 95, "citeEnd": 119, "citeStartToken": 95, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994) . Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• We provide evidence that the proposed subclass still captures the vast majority of TAG analyses that have been currently proposed for the syntax of English and of several other languages.", "mid_sen": "Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994) . ", "after_sen": "Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). "}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Little work has been reported on measures of the relationship between dialogue complexity and the semantic structure of a DS application's database. Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database. Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space. Gorin et al. (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS. Semantic complexity is measured by inheritance relations between call types, the number of type labels per call, and how often calls are routed to human agents. Linguistic complexity is measured by utterance length, vocabulary size and perplexity. Popescu et al. (2003) identify a class of \"semantically tractable\" natural language questions that can be mapped to an SQL query to return the question's unique correct answer. Ambiguous questions with multiple correct answers are not considered semantically tractable. Polifroni and Walker (2008) address how to present informative options to users who are exploring a database, for example, to choose a restaurant. When a query returns many options, their system summarizes the return using attribute value pairs shared by many of the members.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database. ", "mid_sen": "Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space. ", "after_sen": "Gorin et al. (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS. "}
{"citeStart": 22, "citeEnd": 43, "citeStartToken": 22, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "Existing selection-based methods can be divided into four categories: cluster based (Hardy et al., 2002) , centroid based (Radev, 2004) , graph based (Erkan and Radev, 2004; Mani and Bloedorn, 1999) , and machine learning based (Neto et al., 2002) . Clusterbased methods first separate a document set into several groups, each representing a subtopic. Then representative sentences are selected from each group, and finally those sentences are put together as the summarization of the whole document set. Centroid-based methods compute the center of a document set, and then use the similarity between the sentence and the center as the sentence salience score. Graph-based methods construct a graph, where a vertex denotes a sentence and the weight of an edge represents the similarity between the two sentences connected by the edge. Then, similar to PageRank (Page et al., 1998) , a Markov Random Walk is performed on the graph to compute the salience score of every sentence. Machine learning based methods model the summarization process as a classification problem: Whether or not a sentence should be selected as summary sentences. A proper classifier, e.g., a Naive Bayes classifier, is learnt statistically from the training data. There are methods between those categories. For example, Wan and Yang (2008) consider cluster level information, i.e., the importance of the cluster and the relevance of sentence to the cluster, for computing sentence salience score. Motivated by LexRank (Erkan and Radev, 2004) , we adopt graph based methods. Differently, our system incorporates rich social network features and considers readability to compute salience score of every tweet.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Wan and Yang (2008) consider cluster level information, i.e., the importance of the cluster and the relevance of sentence to the cluster, for computing sentence salience score. ", "mid_sen": "Motivated by LexRank (Erkan and Radev, 2004) , we adopt graph based methods. ", "after_sen": "Differently, our system incorporates rich social network features and considers readability to compute salience score of every tweet."}
{"citeStart": 138, "citeEnd": 151, "citeStartToken": 138, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "Materials. Stimuli were constructed using 11 verbs of killing, which are widely viewed as prototypical for the semantic properties of interest here (Lemmens, 1998) : X killed Y normally involves conscious, intentional causation by X of a kinetic event that causes a (rather decisive and clearly terminated!) change of state in Y . The verbs comprise two classes: the \"transitive\" class, involving externally caused change-of-state verbs (kill, slaughter, assassinate, shoot, poison) , and the \"ergative\" class (strangle, smother, choke, drown, suffocate, starve) , within which verbs are internally caused (McKoon and MacFarland, 2000) or otherwise emphasize properties of the object. Variation of syntactic description involved two forms: a transitive syntactic frame with a human agent as subject (\"transitive form\", 2a), and a nominalization of the verb as subject and the verb kill as the predicate (\"nominalized form\", 2b). properties as well as Hopper and Thompson's semantic transitivity components, responding via ratings on a 1-to-7 scale. For example, the questions probing volition were: \"In this event, how likely is it that subject chose to be involved?\", where subject was the gunmen and the shooting, for 2(ab), respectively. 6", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Materials. ", "mid_sen": "Stimuli were constructed using 11 verbs of killing, which are widely viewed as prototypical for the semantic properties of interest here (Lemmens, 1998) : X killed Y normally involves conscious, intentional causation by X of a kinetic event that causes a (rather decisive and clearly terminated!) change of state in Y . ", "after_sen": "The verbs comprise two classes: the \"transitive\" class, involving externally caused change-of-state verbs (kill, slaughter, assassinate, shoot, poison) , and the \"ergative\" class (strangle, smother, choke, drown, suffocate, starve) , within which verbs are internally caused (McKoon and MacFarland, 2000) or otherwise emphasize properties of the object. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "The ambiguity factor 2.27 attributed to Dagan and Itai's 1994 experiment is calculated by dividing their average of 3.27 alternative translations by their average of 1.44 correct translations. Furthermore, we calculated the ambiguity factor 3.51 for Resnik's 1997 experiment shows the random baselines cited for the respective experiments, ranging from ca. 11 to 50 . Precision values are given in column 5. In order to compare these results which were computed for di erent a m biguity factors, we standardized the measures to an evaluation for binary ambiguity. This is achieved by calculating p 1= log 2 amb for precision p and ambiguity factor amb. The consistency of this binarization can be seen by a standardization of the di erent random baselines which yields a value of ca. 50 for all approaches 5 . The standardized precision of our approach is ca. 79 on all test corpora. The most direct point of comparison is the method of Dagan and Itai 1994 which gives 91.4 precision 92.7 standardized and 62.1 e ectiveness 66.8 standardized on 103 test examples for target word selection in the transfer of Hebrew to English. However, compensating this high precision measure for the low e ectiveness gives values comparable to our results. Dagan and Itai's 1994 method is based on a large variety of grammatical relations for verbal, nominal, and adjectival predicates, but no class-based information or slot-labeling is used. Resnik 1997 presented a disambiguation method which yields 44.3 precision 63.8 standardized for a test set of 88 verb-object tokens. His approach i s comparable to ours in terms of informedness of the disambiguator. He also uses a class-based selection measure, but based on WordNet classes. However, the task of his evaluation was to select WordNet-senses for the objects rather than the objects themselves, so the results cannot becompared directly. The same is true for the Senseval evaluation exercise Kilgarri and Rosenzweig, 2000there word senses from the Hector-dictionary had to be disambiguated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, compensating this high precision measure for the low e ectiveness gives values comparable to our results. ", "mid_sen": "Dagan and Itai's 1994 method is based on a large variety of grammatical relations for verbal, nominal, and adjectival predicates, but no class-based information or slot-labeling is used. ", "after_sen": "Resnik 1997 presented a disambiguation method which yields 44.3 precision 63.8 standardized for a test set of 88 verb-object tokens. "}
{"citeStart": 143, "citeEnd": 162, "citeStartToken": 143, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998) . The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation. In contrast, (Langkilde and Knight, 1998) uses corpus-derived statistical knowledge to rank plausible hypotheses from a grammarbased surface generation component.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text.", "mid_sen": "The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998) . ", "after_sen": "The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation. "}
{"citeStart": 34, "citeEnd": 54, "citeStartToken": 34, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "Most of this section is based on (Saerens et al., 2002) . Assume we have a set of labeled data D L with n classes and a set of N independent instances (x 1 , . . . , x N ) from a new data set. The likelihood of these N instances can be defined as:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns (Chan and Ng, 2005) .", "mid_sen": "Most of this section is based on (Saerens et al., 2002) . ", "after_sen": "Assume we have a set of labeled data D L with n classes and a set of N independent instances (x 1 , . . . , x N ) from a new data set. "}
{"citeStart": 76, "citeEnd": 99, "citeStartToken": 76, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the things noted by the native speakers was that applying ellipsis sometimes leads to slightly unnatural sentences. The preferred type and degree of ellipsis is different for each type of sentence, but this is not taken into account by Genpex. As a consequence, the system frequently applies too much or too little ellipsis to the generated sentences, with less than ideal (though not ungrammatical) results. The existence of such preferred formulations is in line with the results of Cahill and Forst (2010) , who carried out an experiment in which native speakers of German evaluated a number of alternative realisations of the same sentence. Their subjects accepted some variation in word order, but showed a clear preference for some of the alternatives. Some of the generated question sentences also sounded a bit forced to the native speakers. For example, the question template for joint probabilities (A∧B) uses the formal phrasing \"sowohl... als auch\" (both ... and), whereas a simple \"und\" (and) would be the more natural choice for most questions. However, in some question contexts, in particular those involving negations, using the simpler formulation might lead to the kind of scope ambiguities mentioned in Section 2. Therefore, the choice was made to use \"sowohl... als auch\" in all cases, even in those where it is not strictly necessary. Similarly, questions asking for a conditional probability were found to be somewhat difficult to understand. For these questions, readability might be improved by using two sentences to express them, along the lines of \"Consider the set of bicycles that are not mountain bikes. What is the probability that one of those bicycles is either black or white?\" as an alternative to the more complex formulation given in Figure 4 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a consequence, the system frequently applies too much or too little ellipsis to the generated sentences, with less than ideal (though not ungrammatical) results. ", "mid_sen": "The existence of such preferred formulations is in line with the results of Cahill and Forst (2010) , who carried out an experiment in which native speakers of German evaluated a number of alternative realisations of the same sentence. ", "after_sen": "Their subjects accepted some variation in word order, but showed a clear preference for some of the alternatives. "}
{"citeStart": 198, "citeEnd": 218, "citeStartToken": 198, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti, 2008; .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE.", "mid_sen": "Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti, 2008; .", "after_sen": "ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. "}
{"citeStart": 274, "citeEnd": 290, "citeStartToken": 274, "citeEndToken": 290, "sectionName": "UNKNOWN SECTION NAME", "string": "For generating a personalized summary, traditional methods usually require that a user explicitly provides his interest aspects, such as specifying the categories he prefers (Díaz and Gervás, 2007) or clicking a subset of sentences in a document according to his interests (Yan et al., 2011) . However, most users are reluctant to provide such information, thus it is more meaningful to infer a user's interests implicitly.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All these information can be considered as the potential data source for document understanding and personalization.", "mid_sen": "For generating a personalized summary, traditional methods usually require that a user explicitly provides his interest aspects, such as specifying the categories he prefers (Díaz and Gervás, 2007) or clicking a subset of sentences in a document according to his interests (Yan et al., 2011) . ", "after_sen": "However, most users are reluctant to provide such information, thus it is more meaningful to infer a user's interests implicitly."}
{"citeStart": 125, "citeEnd": 140, "citeStartToken": 125, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Beyond performance values, we believe our formalism and methodology have the following attractive features: first, our models incorporate context and lexical information collected from the whole tree-bank. Information is bundled into abstract heads of higher-order information, which results in a drastically reduced parameter space. In terms of Section 2, our approach does not aim at improving the approximation of rule probabilities p(r|C, h) and dependency probabilities p(d|D, C, h) by smoothing. Rather, our approach induces head classes for the words h and d from the tree-bank and aims at a exact calculation of rule probabilities p(r|C, class(h)) and dependency probabilities p(class(d)|D, C, class(h)). This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data (Magerman (1995) , Collins (1996) , Charniak (1997), etc.). Particularly, class-based dependency probabilities p(class(d)|D, C, class(h)) induced from the tree-bank are not exploited by most of these parsers.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Rather, our approach induces head classes for the words h and d from the tree-bank and aims at a exact calculation of rule probabilities p(r|C, class(h)) and dependency probabilities p(class(d)|D, C, class(h)). ", "mid_sen": "This is in sharp contrast to the smoothed fixed-word statistics in most lexicalized parsing models derived from sparse data (Magerman (1995) , Collins (1996) , Charniak (1997), etc.). ", "after_sen": "Particularly, class-based dependency probabilities p(class(d)|D, C, class(h)) induced from the tree-bank are not exploited by most of these parsers."}
{"citeStart": 137, "citeEnd": 148, "citeStartToken": 137, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "Our system maintains a set of beliefs about the domain and about the user's beliefs. Associated with each belief is a strength that represents the agent's confidence in holding that belief. We model the strength of a belief using endorsements, which are explicit records of factors that affect one's certainty in a hypothesis (Cohen, 1985) , following (Galliers, 1992; Logan et al., 1994) . Our endorsements are based on the semantics of the utterance used to convey a befief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. The belief level of the dialogue model consists of mutual beliefs proposed by the agents' discourse actions. When an agent proposes a new belief and gives (optional) supporting evidence for it, this set of proposed beliefs is represented as a belief tree, where the belief represented by a child node is intended to support that represented by its parent. The root nodes of these belief trees (rap-level beliefs) contribute to problem-solving actions and thus affect the domain plan being developed. Given a set of newly proposed beliefs, the system must decide whether to accept the proposal or m initiate a negotiation dialogue to resolve conflicts. The evaluation of proposed beliefs starts at the leaf nodes of the proposed belief trees since acceptance of a piece of proposed evidence may affect acceptance of the parent belief it is intended to support. The process continues until the top-level proposed beliefs are evaluated. Conflict resolution strategies are invoked only if the top-level proposed beliefs are not accepted because if collaborative agents agree on a belief relevant to the domain plan being constructed, it is irrelevant whether they agree on the evidence for that belief (Young et al., 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Associated with each belief is a strength that represents the agent's confidence in holding that belief. ", "mid_sen": "We model the strength of a belief using endorsements, which are explicit records of factors that affect one's certainty in a hypothesis (Cohen, 1985) , following (Galliers, 1992; Logan et al., 1994) . ", "after_sen": "Our endorsements are based on the semantics of the utterance used to convey a befief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. "}
{"citeStart": 239, "citeEnd": 259, "citeStartToken": 239, "citeEndToken": 259, "sectionName": "UNKNOWN SECTION NAME", "string": "A preliminary version of the Penn Treebenk bracketed corpus was analysed to extract information on the sisters of particular verbs. Although the Penn Treebank data is unreliable since it does not always distinguish complements from adjuncts, it was the only suitable parsed corpus to which the authors had access. Although the distinction between complements and adjuncts is a theoretically interesting one, the process of determining which constructions fill which functional roles in the analysis of real text often creates a number of problems (see [Hindle and Rooth1993] for discussion 2It is of course possible to store these cross-item similarities as lexical rules [Bresnan1978] , but this alone does not entail that the properties axe specific to a category, cff. the theta grids of verbs and their ~related' nouns.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although the Penn Treebank data is unreliable since it does not always distinguish complements from adjuncts, it was the only suitable parsed corpus to which the authors had access. ", "mid_sen": "Although the distinction between complements and adjuncts is a theoretically interesting one, the process of determining which constructions fill which functional roles in the analysis of real text often creates a number of problems (see [Hindle and Rooth1993] for discussion 2It is of course possible to store these cross-item similarities as lexical rules [Bresnan1978] , but this alone does not entail that the properties axe specific to a category, cff. the theta grids of verbs and their ~related' nouns.", "after_sen": "on this issue regarding output of the Fidditch parser [Hindle1993] )."}
{"citeStart": 71, "citeEnd": 85, "citeStartToken": 71, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) , which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author's reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a \"lower level\" of analysis.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . ", "mid_sen": "However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) , which makes the creation of a general sentiment classifier a difficult task. ", "after_sen": "Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. "}
{"citeStart": 354, "citeEnd": 381, "citeStartToken": 354, "citeEndToken": 381, "sectionName": "UNKNOWN SECTION NAME", "string": "Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti, 2008; .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our novel composite kernels, which account for the two syntactic structures, are experimented with the appropriate convolution kernels and show significant improvement with respect to the state-ofthe-art in RE.", "mid_sen": "Regarding future work, there are many research line that may be followed: i) Capturing more features by employing external knowledge such as ontological, lexical resource or WordNet-based features (Basili et al., 2005a; Basili et al., 2005b; Bloehdorn et al., 2006; Bloehdorn and Moschitti, 2007) or shallow semantic trees, (Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti and Bejan, 2004; Moschitti, 2008; .", "after_sen": "ii) Design a new tree-based structures, which combines the information of both constituent and dependency parses. "}
{"citeStart": 114, "citeEnd": 119, "citeStartToken": 114, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "But why should deixis and event anaphors behave differently from the other anaphors? Deixis serves to pick out objects that cannot be selected by the use of standard anaphora, i.e. we should expect the referents for deixis to be outside immediate focus and hence more likely to be outside the current segment[Web86]. The picture is more complex for event anaphora, which seems to serve a number of different functions in the dialogue. It is used to talk about the past events that lead up to the current situation, I did THAT in order to move the place. It is also used to refer to sets of propositions of the preceding discourse, Now THAT'S a little background (cf [Web88]). The most prevalent usei however, was to refer to future events or actions, THAT would be the move that I would make Since the task in the ADs is to develop a plan, speakers use event anaphora as concise references to the plans they have just negotiated and to discuss the status and quality of plans that have been suggested. Thus the frequent cross-speaker references to future events and actions correspond to phases of plan negotiation [PHW82] . More importantly these references are closely related to the control structure. The example above illustrates the clustering of event anaphora at segment boundaries. One discourse participant uses an anaphor to summarize a plan, but when the other participant evaluates this plan there may be a control shift and any reference to the plan will necessarily cross a control boundary. The distribution of event anaphora bears this out, since 23/25 references to future actions are within 2 utterances of a segment boundary (See the example above). More significantly every instance of event anaphora crossing a segment boundary occurs when the speaker is talking about future events or actions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The most prevalent usei however, was to refer to future events or actions, THAT would be the move that I would make Since the task in the ADs is to develop a plan, speakers use event anaphora as concise references to the plans they have just negotiated and to discuss the status and quality of plans that have been suggested. ", "mid_sen": "Thus the frequent cross-speaker references to future events and actions correspond to phases of plan negotiation [PHW82] . ", "after_sen": "More importantly these references are closely related to the control structure. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995) , and support the conclusion that the dependency model is superior to the adjacency model. Lauer and Dras (1994) suggest two improvements to the method used above. These are:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995) , and support the conclusion that the dependency model is superior to the adjacency model. ", "mid_sen": "Lauer and Dras (1994) suggest two improvements to the method used above. ", "after_sen": "These are:"}
{"citeStart": 145, "citeEnd": 168, "citeStartToken": 145, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "The auxiliary distribution we used here is based on the probabilistic model of lexical selectional preferences described in Rooth et al. (1999) . An existing broad-coverage parser was used to find shallow parses (compared to the LFG parses) for the 117 million word British National Corpus (Carroll and Rooth, 1998) . We based our auxiliary distribution on 3.7 million (g, r, a) tuples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a preposition), a is the head of one of its NP arguments and r is the the grammatical relationship between the governor and argument (in the shallow parses r is always OBJ for prepositional governors, and r is either SUBJ or OBJ for verbal governors).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The auxiliary distribution we used here is based on the probabilistic model of lexical selectional preferences described in Rooth et al. (1999) . ", "mid_sen": "An existing broad-coverage parser was used to find shallow parses (compared to the LFG parses) for the 117 million word British National Corpus (Carroll and Rooth, 1998) . ", "after_sen": "We based our auxiliary distribution on 3.7 million (g, r, a) tuples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a preposition), a is the head of one of its NP arguments and r is the the grammatical relationship between the governor and argument (in the shallow parses r is always OBJ for prepositional governors, and r is either SUBJ or OBJ for verbal governors)."}
{"citeStart": 185, "citeEnd": 198, "citeStartToken": 185, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002) . Our task is closer to the work of Teufel and Moens (2000) , who looked at the problem of intellectual attribution in scientific texts.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here.", "mid_sen": "Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002) . ", "after_sen": "Our task is closer to the work of Teufel and Moens (2000) , who looked at the problem of intellectual attribution in scientific texts."}
{"citeStart": 258, "citeEnd": 263, "citeStartToken": 258, "citeEndToken": 263, "sectionName": "UNKNOWN SECTION NAME", "string": "The bottom-up presentation process simulates the unplanned part of proof presentation. Instead of splitting presentation goals into subgoals according to standard schernata, it follows the local derivation relation to find a next proof node or subproof to be presented, in this sense, it is similar to the local organization techniques used in [Sib90] . When no top-down presentation operator applies, I~ROVI'2RB chooses a bottom-up operator.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The bottom-up presentation process simulates the unplanned part of proof presentation. ", "mid_sen": "Instead of splitting presentation goals into subgoals according to standard schernata, it follows the local derivation relation to find a next proof node or subproof to be presented, in this sense, it is similar to the local organization techniques used in [Sib90] . ", "after_sen": "When no top-down presentation operator applies, I~ROVI'2RB chooses a bottom-up operator."}
{"citeStart": 38, "citeEnd": 52, "citeStartToken": 38, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "We wish to learn how our model performance depends on the choice and number of anchor words. Selecting from WordNet synonym lists (Fellbaum, 1998) , we choose five positive anchor words and five negative (Figure 3 ). This produces a total of 25 different possible pairs for use in producing coefficient estimates. Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data. Coefficientsα j are estimated as described in Equation 22. Several different experimental conditions are applied. The methods labeled \"Count\" use the original un-normalized coefficients, while those labeled \"Norm.\" have been normalized so that the number of co-occurrences with each anchor have identical variance. Results are shown when rare words (with three or fewer occurrences in the labeled corpus) are included and omitted. The methods \"pair\" and \"10\" describe whether all ten anchor coefficients are used at once, or just the ones that correspond to a single pair of Models that use the original pair of anchor words, excellent and poor, perform slightly better than the average pair. Whereas mean performance ranges from 37.3% to 39.6%, misclassification rates for this pair of anchors ranges from 37.4% to 38.1%.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We wish to learn how our model performance depends on the choice and number of anchor words. ", "mid_sen": "Selecting from WordNet synonym lists (Fellbaum, 1998) , we choose five positive anchor words and five negative (Figure 3 ). ", "after_sen": "This produces a total of 25 different possible pairs for use in producing coefficient estimates. "}
{"citeStart": 169, "citeEnd": 192, "citeStartToken": 169, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995) , these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). ", "after_sen": "Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995) , these methods are not of practical interest, due to large hidden constants. "}
{"citeStart": 64, "citeEnd": 87, "citeStartToken": 64, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "In the task of relation classification, information that is needed to determine the class of a relation between two target nouns normally comes from words which are close to the target nouns. Zeng et al. (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns. These features are similar to the position features proposed by Collobert et al. (2011) for the Semantic Role Labeling task.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Zeng et al. (2014) propose the use of word position embeddings (position features) which help the CNN by keeping track of how close words are to the target nouns. ", "mid_sen": "These features are similar to the position features proposed by Collobert et al. (2011) for the Semantic Role Labeling task.", "after_sen": "In this work we also experiment with the word position embeddings (WPE) proposed by Zeng et al. (2014) . "}
{"citeStart": 61, "citeEnd": 74, "citeStartToken": 61, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "To summarize: we define ranked alphabets in a standard way, adding the requirements that every sort includes a countable infinity of variables, a finite number of function letters, and at least one ground term. We then define the set of terms in a standard way. All unification in this paper is unification of terms, as in Robinson 1965--not graphs or other structures, as in much recent work (Shieber 1985b) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We then define the set of terms in a standard way. ", "mid_sen": "All unification in this paper is unification of terms, as in Robinson 1965--not graphs or other structures, as in much recent work (Shieber 1985b) .", "after_sen": "A unification grammar is a five-tuple G = (S, (~,r) T, P, Z) where S is a set of sorts, (~,r) an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in (~,r). "}
{"citeStart": 113, "citeEnd": 125, "citeStartToken": 113, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "The direct evaluation of the rule-sets gave us the grounds for the comparison and selection of the best performing guessing rule-sets. The task of unknown word guessing is, however, a subtask of the overall part-of-speech tagging process. Thus we are mostly interested in how the advantage of one rule-set over another will affect the tagging performance. So, we performed an independent evaluation of the impact of the word guessers on tagging accuracy. In this evaluation we tried two different taggers. First, we used a tagger which was a c++ re-implementation of the LISP implemented HMM Xerox tagger described in (Kupiec, 1992) . The other tagger was the rule-based tagger of Brill (Brill, 1995) . Both of the taggers come with data and word-guessing components pre-trained on the Brown Corpus 6. This, actually gave us the search-space of four combinations: the Xerox tagger equipped with the original Xerox guesser, Brill's tagger with its original guesser, the Xerox tagger with our cascading Ps0+S60+E75 guesser and Brill's tagger with the cascading guesser. For words which failed to be guessed by the guessing rules we applied the standard method of classifying them as common nouns (NN) if they are not capitalised inside a sentence and proper nouns (NP) otherwise. As the base-line result we measured the performance of the taggers with all known words on the same word sample.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this evaluation we tried two different taggers. ", "mid_sen": "First, we used a tagger which was a c++ re-implementation of the LISP implemented HMM Xerox tagger described in (Kupiec, 1992) . ", "after_sen": "The other tagger was the rule-based tagger of Brill (Brill, 1995) . "}
{"citeStart": 82, "citeEnd": 110, "citeStartToken": 82, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "Meronyms: For some relations, there is no contradiction when y 1 and y 2 share a meronym, i.e. \"part of\" relation. For example, in the set born in(Mozart,•) there is no contradiction between the y values \"Salzburg\" and \"Austria\", but \"Salzburg\" conflicts with \"Vienna\". Although this is only true in cases where y occurs in an upward monotone context (MacCartney and Manning, 2007) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare. We therefore simply assigned contradictions between meronyms a probability close to zero. We used the Tipster Gazetteer 4 and WordNet to identify meronyms, both of which have high precision but low coverage.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, in the set born in(Mozart,•) there is no contradiction between the y values \"Salzburg\" and \"Austria\", but \"Salzburg\" conflicts with \"Vienna\". ", "mid_sen": "Although this is only true in cases where y occurs in an upward monotone context (MacCartney and Manning, 2007) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare. ", "after_sen": "We therefore simply assigned contradictions between meronyms a probability close to zero. "}
{"citeStart": 65, "citeEnd": 83, "citeStartToken": 65, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "Using part of the Giga-FrEn data -along with the additions to the Europarl, news commentary, and UN document courses released since last year -is beneficial to translation quality, as there is a clear improvement in metric scores between the 2010 and 2011 systems. Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method (Koehn, 2004) with n = 1000 and p < 0.01. They are also larger than the 0.7-to 1.1-point gains reported by Pino et al. (2010) when the full Giga-FrEn was added. The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system. Differences between grammar filtering techniques, on the other hand, are much less significant according to all three metrics. Under paired bootstrap resampling on the newstest2009 set, the grammar variants in both the 2010 and 2011 systems are statistically equivalent according to BLEU score. On newstest2010, the 2k+100k grammar improves over the 10k version (p < 0.01) in the 2010 system, but the situation is reversed in the 2011 system. We investigated differences in grammar use with an analysis of rule applications in the two variants of the 2011 system, the results of which are summarized in Table 3 . Though the configuration with the 2k+100k grammar does apply syntactic rules 20% more frequently than its 10k counterpart, the 10k system uses overall 53% more unique rules. One contributing factor to this situation could be that the fully abtract rule cutoff is set too low compared to the increase in partially lexicalized rules. The effect of the 2k+100k filtering is to reduce the number of abstract rules from 4000 to 2000 while increasing the number of partially lexicalized rules from 6000 to 100,000. However, we find that the 10k system makes heavy use of some short, meaningful abstract rules that were excluded from the 2k+100k system. The 2k+100k grammar, by contrast, includes a long tail of less frequently used partially lexicalized grammar rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap resampling method (Koehn, 2004) with n = 1000 and p < 0.01. ", "mid_sen": "They are also larger than the 0.7-to 1.1-point gains reported by Pino et al. (2010) when the full Giga-FrEn was added. ", "after_sen": "The 2011 system also shows a significant reduction in the out-of-vocabulary (OOV) rate on both test sets: 38% and 47% fewer OOV types, and 44% and 45% fewer OOV tokens, when compared to the 2010 system. "}
{"citeStart": 187, "citeEnd": 199, "citeStartToken": 187, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "(23) Ce livre est triste \"The book is sad\" a. ~ whose reading causes somebody to be sad b. -~ whose writing causes somebody to be sad c. -~ whose writing is the manifestation of somebody's sadness (24) Ce sapin est triste \"The pine tree is sad\" a. ~ whose experiencing causes somebody to be sad In (23), the modification by the adjective is possible as livre (book) contains in its qualia structure two events, namely life (to read) (relic of livre) and derire (to write) (agentive of livre) (see Pustejovsky and Bouillon, 1995, for the qualia representation of livre). Two causative interpretations (23a,b) and one manifestation (23c) are therefore possible. Notice that when the events are defined in the lexical semantics of the word, the experiencing and the manifestation are intentional and controlled (the experiencing is active, following Lehrer, 1990) . In (24) on the other hand, there is nothing contributed by the ~ree per se to how the experiencing is achieved (as the noun has no telic nor agentive), except for it being a physically manifested object with extension. In this case, it is the properties inherited through the formal (and not the lexical semantics of the word) that suggests how it can be experienced. For this reason, the experiencing is not controlled, nor intentional (it is stative). The manifestation sense is impossible as sapin (versus book) has no intellectual act in its qualia.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Two causative interpretations (23a,b) and one manifestation (23c) are therefore possible. ", "mid_sen": "Notice that when the events are defined in the lexical semantics of the word, the experiencing and the manifestation are intentional and controlled (the experiencing is active, following Lehrer, 1990) . ", "after_sen": "In (24) on the other hand, there is nothing contributed by the ~ree per se to how the experiencing is achieved (as the noun has no telic nor agentive), except for it being a physically manifested object with extension. "}
{"citeStart": 77, "citeEnd": 96, "citeStartToken": 77, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "(III) The way in which spans are annotated as ar-guments to connectives also raises a challenge. First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003) , a span can serve as argument to >1 connective. Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005) , discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them. Instead, in annotating arguments, PDTB annotators have selected the minimal clausal text span needed to interpret the relation. This could comprise an embedded, subordinate or coordinate clause, an entire sentence, or a (possibly disjoint) sequence of sentences. As a result, there are fairly complex patterns of spans within and across sentences that serve as arguments to different connectives, and there are parts of sentences that don't appear within the span of any connective, explicit or implicit. The result is that the PDTB provides only a partial but complexly-patterned cover of the corpus. Understanding what's going on and what it implies for discourse structure (and possibly syntactic structure as well) is a challenge we're currently trying to address (Lee et al., 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(III) The way in which spans are annotated as ar-guments to connectives also raises a challenge. ", "mid_sen": "First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003) , a span can serve as argument to >1 connective. ", "after_sen": "Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005) , discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them. "}
{"citeStart": 154, "citeEnd": 165, "citeStartToken": 154, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . ", "mid_sen": "Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "after_sen": ") ( ) ( ) , ( log ) , ( 2 f P w P f w P f w MI ="}
{"citeStart": 53, "citeEnd": 70, "citeStartToken": 53, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiments, we employed the well-known classifier SVM light to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features. 5 Following standard practice in sentiment analysis (Pang et al., 2002) , the input to SVM light consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. The ind value for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our experiments, we employed the well-known classifier SVM light to obtain individual-document classification scores, treating Y as the positive class and using plain unigrams as features. ", "mid_sen": "5 Following standard practice in sentiment analysis (Pang et al., 2002) , the input to SVM light consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. ", "after_sen": "The ind value for each speech segment s was based on the signed distance d(s) from the vector representing s to the trained SVM decision plane:"}
{"citeStart": 109, "citeEnd": 125, "citeStartToken": 109, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Computational approaches fail to account for the cancellation of pragmatic inferences: once presuppositions (Weischedel, 1979) or implicatures (Hirschberg, 1985; Green, 1992) are generated, they can never be cancelled. We are not aware of any formalism or computational approach that offers a unified explanation for the cancellability of pragmatic inferences in general, and of no approach that handles cancellations that occur in sequences of utterances.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mercer (1987) formalizes pre-suppositions in a logical framework that handles defaults (Reiter, 1980) , but this approach is not tractable and it treats natural disjunction as an exclusiveor and implication as logical equivalence.", "mid_sen": "Computational approaches fail to account for the cancellation of pragmatic inferences: once presuppositions (Weischedel, 1979) or implicatures (Hirschberg, 1985; Green, 1992) are generated, they can never be cancelled. ", "after_sen": "We are not aware of any formalism or computational approach that offers a unified explanation for the cancellability of pragmatic inferences in general, and of no approach that handles cancellations that occur in sequences of utterances."}
{"citeStart": 292, "citeEnd": 317, "citeStartToken": 292, "citeEndToken": 317, "sectionName": "UNKNOWN SECTION NAME", "string": "Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011) . One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008) ; see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011) . Only rarely has there been work on detecting errors in more morphologically-complex languages .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several strands of research in intelligent computerassisted language learning (ICALL) focus on determining learner ability (Attali and Burstein, 2006; Yannakoudakis et al., 2011) . ", "mid_sen": "One of the tasks, detecting errors in a range of languages and for a range of types of errors, is becoming an increasingly popular topic (Rozovskaya and Roth, 2011; Tetreault and Chodorow, 2008) ; see, for example, the recent HOO (Helping Our Own) Challenge for Automated Writing Assistance (Dale and Kilgarriff, 2011) . ", "after_sen": "Only rarely has there been work on detecting errors in more morphologically-complex languages ."}
{"citeStart": 137, "citeEnd": 162, "citeStartToken": 137, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "item. However (Daelemans et al., 1999) report that for baseNP recognition better results can be obtained by making the algorithm consider the classification values of the three closest training items. We have tested this by repeating the first experiment series and part of the third experiment series for k=3. In this revised version we have repeated the best experiment of the third series with the results for k=l replaced by the k=3 results whenever the latter outperformed the first in the revised first experiment series. The results can be found in table 5. All formats benefited from this step. In this final experiment series the best results were obtained with IOB1 but the differences with the results of the other formats are not significant. We have used the optimal experiment configurations that we had obtained from the fourth experiment series for processing the complete (Ramshaw and Marcus, 1995) data set. The results can be found in table 6. They are better than the results for section 15 because more training data was used in these experiments. Again the best result was obtained with IOB1 (F~=I =92.37) which is an im-I)rovement of the best reported F,~=1 rate for this data set ((Ramshaw and Marcus, 1995) : 92.03).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They are better than the results for section 15 because more training data was used in these experiments. ", "mid_sen": "Again the best result was obtained with IOB1 (F~=I =92.37) which is an im-I)rovement of the best reported F,~=1 rate for this data set ((Ramshaw and Marcus, 1995) : 92.03).", "after_sen": "We would like to apply our learning approach to the large data set mentioned in (Ramshaw and Marcus, 1995) : Wall Street Journal corpus sections 2-21 as training material and section 0 as test material. "}
{"citeStart": 32, "citeEnd": 47, "citeStartToken": 32, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "ALE signatures Carpenter's ALE (Carpenter, 1993) allows the user to define the type hierarchy of a grammar by writing a collection of clauses which together denote an inheritance hierarchy, a set of features and a set of appropriateness conditions. An example of such a hierarchy is given in ALE syntax in figure 4.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "ALE signatures Carpenter's ALE (Carpenter, 1993) allows the user to define the type hierarchy of a grammar by writing a collection of clauses which together denote an inheritance hierarchy, a set of features and a set of appropriateness conditions. ", "after_sen": "An example of such a hierarchy is given in ALE syntax in figure 4."}
{"citeStart": 89, "citeEnd": 109, "citeStartToken": 89, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "The first directed Applicative CG was proposed by Bar-Hillel (1953) . Functional types included a list of arguments to the left, and a list of arguments to the right. Translating Bar-Hillel's notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994) , we obtain the following category for a ditransitive verb such as put:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Functional types included a list of arguments to the left, and a list of arguments to the right. ", "mid_sen": "Translating Bar-Hillel's notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994) , we obtain the following category for a ditransitive verb such as put:", "after_sen": "r s ] Unp> L r(np, pp>"}
{"citeStart": 138, "citeEnd": 157, "citeStartToken": 138, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "However, such remedies have their drawbacks. Firstly, even when the context is extended, some important influences may still not be modeled. For example, dependencies between words exist at separations greater than those allowed for by trigrams (for which long-distance N-grams [Jelinek et al, 1991] are a partial remedy), and associating scores with parsing table states may not model all the important correlations between grammar rules. Secondly, extending the model may greatly increase the amount of training data required if sparseness problems are to be kept under control, and additional data may be unavailable or expensive to collect. Thirdly, one cannot always know in advance of doing the work whether extending a model in a particular direction will, in practice, improve results. If it turns out not to, considerable ingenuity and effort may have been wasted.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Firstly, even when the context is extended, some important influences may still not be modeled. ", "mid_sen": "For example, dependencies between words exist at separations greater than those allowed for by trigrams (for which long-distance N-grams [Jelinek et al, 1991] are a partial remedy), and associating scores with parsing table states may not model all the important correlations between grammar rules. ", "after_sen": "Secondly, extending the model may greatly increase the amount of training data required if sparseness problems are to be kept under control, and additional data may be unavailable or expensive to collect. "}
{"citeStart": 100, "citeEnd": 114, "citeStartToken": 100, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "A pitch accent is a distinctive intonational contour applied to a word to convey sentential stress (Bolinger, 1958; Pierrehumbert, 1980) . PH90 catalogues six pitch accents, all combinations of high (H) and low (L) pitch targets, and structured as a main tone and an optional leading or trailing tone. The form of the accent --L, H, L+H or H+L --informs about the operation that would relate the salient item to the mutual beliefs 1 of the conversants; the main tone either commits (H*) or fails to commit 1 Mutual beliefs: propositions expressed or implied by the discourse, and which all conversants believe each other to accept as true and relevant same (Clark and Marshall, 1981) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A pitch accent is a distinctive intonational contour applied to a word to convey sentential stress (Bolinger, 1958; Pierrehumbert, 1980) . ", "after_sen": "PH90 catalogues six pitch accents, all combinations of high (H) and low (L) pitch targets, and structured as a main tone and an optional leading or trailing tone. "}
{"citeStart": 43, "citeEnd": 65, "citeStartToken": 43, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation. This stands in contrast to the analysis of Wason and Reich (1979) , which presumes that people are applying some higher-level reasoning to \"correct\" an ill-formed statement in the case of the \"no\" in-terpretation. While such extra-grammatical inference may play a role in support of language understanding when people are faced with noisy data, it seems unlikely to us that a construction that is used quite readily and with a predictable interpretation is nonsensical according to rules of grammar. Our results point to an alternative linguistic analysis, one whose further development may also help to improve automatic disambiguation of instances of No X is too Y to Z. In the next section, we discuss directions for future work that could elaborate on these preliminary findings.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation. ", "mid_sen": "This stands in contrast to the analysis of Wason and Reich (1979) , which presumes that people are applying some higher-level reasoning to \"correct\" an ill-formed statement in the case of the \"no\" in-terpretation. ", "after_sen": "While such extra-grammatical inference may play a role in support of language understanding when people are faced with noisy data, it seems unlikely to us that a construction that is used quite readily and with a predictable interpretation is nonsensical according to rules of grammar. "}
{"citeStart": 75, "citeEnd": 98, "citeStartToken": 75, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "In general, SML tools work with a vector representation of data. First, a relevancy vector of relevant features for each class is computed (Yang and Pedersen, 1997) . In our case the relevant features consist of the user-defined output of the linguistic preprocessor. Then each single document is translated into a vector of numbers isomorphic to the defining vector. Each entry represents the occurrence of the corresponding feature. More details will be given in Section 4", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In general, SML tools work with a vector representation of data. ", "mid_sen": "First, a relevancy vector of relevant features for each class is computed (Yang and Pedersen, 1997) . ", "after_sen": "In our case the relevant features consist of the user-defined output of the linguistic preprocessor. "}
{"citeStart": 114, "citeEnd": 141, "citeStartToken": 114, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems unlikely that these continuously variable aspcct:s of fluent natural language can be captured by a purely combinatoric model. This naturally leads to the qtwstion of how best to introduce quantitative modeli,g into language processing. It is not, of course, nec-,,ssary for the quantities of a quantitative model to be probabilities. For example, we may wish to define realvalued functions on parse trees that reflect the extent to which the trees conform to, .say, minimal attachment and parallelism between conjuncts. Such functions have been used in tandem with statistical functions in experiments on disambiguation (for instance Alshawi and (',a.rter 1994) . Another example is connection strengths i, m~ural network approaches to language processing, th,mgh it. has been shown that certain networks are ~,tfectively computing probabilities (Richard and Lippmann 1991) . Nevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing. The case f.r probability theory is strengthened by a well devel-,,p-d empirical methodology in the form of statistical I,:~ramet.ccr estimation. There is also the strong connecl i,,n between probability theory and the formal theory .1\" i.formation and communication, a connection that has been exploited in speech recognition, for example I~qing tim concept of entropy to provide a motivated way ,.f measuring the complexity of a recognition problem (.h'lim'k et ai. 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, we may wish to define realvalued functions on parse trees that reflect the extent to which the trees conform to, .say, minimal attachment and parallelism between conjuncts. ", "mid_sen": "Such functions have been used in tandem with statistical functions in experiments on disambiguation (for instance Alshawi and (',a.rter 1994) . ", "after_sen": "Another example is connection strengths i, m~ural network approaches to language processing, th,mgh it. "}
{"citeStart": 154, "citeEnd": 174, "citeStartToken": 154, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "Linguistic preprocessing of text documents is carried out by re-using sines, an information extraction core system for real-world German text processing (Neumann et al., 1997) . The fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient STP components and 4Almost all tools we examined build a single multicategorizer except for SVM-Light, which builds multiple binary classifiers. generic linguistic knowledge sources that can easily be customized to deal with different tasks in a flexible manner, sines includes a text tokenizer, a lexical processor and a chunk parser. The chunk parser itself is subdivided into three components. In the first step, phrasal fragments like general nominal expressions and verb groups are recognized. Next, the dependency-based structure of the fragments of each sentence is computed using a set of specific sentence patterns. Third, the grammatical functions are determined for each dependency-based structure on the basis of a large subcategorization lexicon. The present application benefits from the high modularity of the usage of the components. Thus, it is possible to run only a subset of the components and to tailor their output. The experiments described in Section 4 make use of this feature.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Linguistic preprocessing of text documents is carried out by re-using sines, an information extraction core system for real-world German text processing (Neumann et al., 1997) . ", "after_sen": "The fundamental design criterion of sines is to provide a set of basic, powerful, robust, and efficient STP components and 4Almost all tools we examined build a single multicategorizer except for SVM-Light, which builds multiple binary classifiers. "}
{"citeStart": 122, "citeEnd": 141, "citeStartToken": 122, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "Other sentences can be generated with this pattern, for instance, \"The algorithm is based in the Vector Space Model -VSM (Salton et al., 1975) \". This sentence pre-annotated is \"<material>The algorithm</material> is based in the Vector Space Model -VSM <cited> (Salton et al., 1975 )</cited>\". The pattern is the same that the one in the previous example \"MATERIAL is based CITED\", with the equal function type than last example because pattern is identical.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this case, it is clear that citation function has to do with the use other author's material as a base for own work.", "mid_sen": "Other sentences can be generated with this pattern, for instance, \"The algorithm is based in the Vector Space Model -VSM (Salton et al., 1975) \". ", "after_sen": "This sentence pre-annotated is \"<material>The algorithm</material> is based in the Vector Space Model -VSM <cited> (Salton et al., 1975 )</cited>\". "}
{"citeStart": 138, "citeEnd": 158, "citeStartToken": 138, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "Training Set (Labeled English Reviews): There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007) 9 , because the corpus was large-scale and it was within similar domains as the test set. The dataset consisted of 8000 Amazon product reviews (4000 positive reviews + 4000 negative reviews) for four different product types: books, DVDs, electronics and kitchen appliances.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Training Set (Labeled English Reviews): ", "mid_sen": "There are many labeled English corpora available on the Web and we used the corpus constructed for multi-domain sentiment classification (Blitzer et al., 2007) 9 , because the corpus was large-scale and it was within similar domains as the test set. ", "after_sen": "The dataset consisted of 8000 Amazon product reviews (4000 positive reviews + 4000 negative reviews) for four different product types: books, DVDs, electronics and kitchen appliances."}
{"citeStart": 71, "citeEnd": 94, "citeStartToken": 71, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "We performed experiments with five categories to evaluate the effectiveness and generality of our approach: energy, financial, military, vehicles, and weapons. The MUC-4 development corpus (1700 texts) was used as the text corpus (MUC-4 Proceedings, 1992) . We chose these five categories because they represented relatively different semantic classes, they were prevalent in the MUC-4 corpus, and they seemed to be useful categories.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We performed experiments with five categories to evaluate the effectiveness and generality of our approach: energy, financial, military, vehicles, and weapons. ", "mid_sen": "The MUC-4 development corpus (1700 texts) was used as the text corpus (MUC-4 Proceedings, 1992) . ", "after_sen": "We chose these five categories because they represented relatively different semantic classes, they were prevalent in the MUC-4 corpus, and they seemed to be useful categories."}
{"citeStart": 125, "citeEnd": 138, "citeStartToken": 125, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agn~ et al, 1994) . Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). When a sentence was analysed successfully, several semantic analyses were, in general, created, and a selection was made from among these on the basis of trained preference functions (Alshawi and Carter, 1994) . For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agn~ et al, 1994) . ", "after_sen": "Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). "}
{"citeStart": 142, "citeEnd": 162, "citeStartToken": 142, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems (Campana et al., 2001) and to disambiguate speech input (Tanaka, 1999) . In contrast to these earlier studies, our work focuses on a different goal of using eye gaze for automated vocabulary acquisition and interpretation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In research on multimodal interactive systems, recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances (Kaur et al., 2003) .", "mid_sen": "Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems (Campana et al., 2001) and to disambiguate speech input (Tanaka, 1999) . ", "after_sen": "In contrast to these earlier studies, our work focuses on a different goal of using eye gaze for automated vocabulary acquisition and interpretation."}
{"citeStart": 192, "citeEnd": 210, "citeStartToken": 192, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "The texts currently being generated by Genpex are grammatical, but our native speakers reported that some sentences had to be studied carefully before it was possible to get the information needed to solve the problem. No actual misinterpretations occurred, but the increased reading time (as compared to more preferred formulations) may still increase the difficulty of the exercise. A thorough investigation into the effect of textual variations on item difficulty is therefore necessary. Genpex supports this type of research by enabling the systematic application of different variations, while logging all textual operations that have been applied and saving them together with the generated text. The underlying probability problem is saved together with the text as well, so all factors that certainly or potentially influence item difficulty are known. This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al. (2005) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The underlying probability problem is saved together with the text as well, so all factors that certainly or potentially influence item difficulty are known. ", "mid_sen": "This makes it relatively easy to test the influence of those factors on the difficulty of the exercise, for example by carrying out the kind of statistical and cognitive analysis advocated by Graf et al. (2005) .", "after_sen": "The effect of the main parameters of the probability problems in Genpex (i.e., the type of question being asked) was already statistically analyzed by Holling et al. (2009) and Zeuch (In preparation). "}
{"citeStart": 81, "citeEnd": 94, "citeStartToken": 81, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004) . Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004) . Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. ", "mid_sen": "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006) .", "after_sen": "Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. "}
{"citeStart": 88, "citeEnd": 114, "citeStartToken": 88, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "These unseen events generally make up a substantial portion of novel data; for example, Essen and Steinbiss (1992) report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In particular, reasonable events that happen to not occur in the training set may mistakenly be assigned a probability of zero.", "mid_sen": "These unseen events generally make up a substantial portion of novel data; for example, Essen and Steinbiss (1992) report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition.", "after_sen": "We consider here the question of how to estimate the conditional cooccurrence probability P(v[n) of an unseen word pair (n, v) drawn from some finite set N x V. "}
{"citeStart": 28, "citeEnd": 51, "citeStartToken": 28, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "The main difference between Basilisk and Meta-Bootstrapping is that Basilisk scores each noun based on collective information gathered from all patterns that extracted it. In contrast, Meta-Bootstrapping identifies a single best pattern and assumes that everything it extracted belongs to the same semantic class. The second level of bootstrapping smoothes over some of the problems caused by this assumption. In comparative experiments (Thelen and Riloff, 2002) , Basilisk outperformed Meta-Bootstrapping. But since our goal of learning subjective nouns is different from the original intent of the algorithms, we tried them both. We also suspected they might learn different words, in which case using both algorithms could be worthwhile.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second level of bootstrapping smoothes over some of the problems caused by this assumption. ", "mid_sen": "In comparative experiments (Thelen and Riloff, 2002) , Basilisk outperformed Meta-Bootstrapping. ", "after_sen": "But since our goal of learning subjective nouns is different from the original intent of the algorithms, we tried them both. "}
{"citeStart": 168, "citeEnd": 188, "citeStartToken": 168, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to be able to compare our results with the results obtained by other researchers, we worked with the same data sets already used by (Ramshaw and Marcus, 1995; Argamon et al., 1998) for NP and SV detection. These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993) . For NP, the training and test corpus was prepared from sections 15 to 18 and section 20, respectively; the SV corpus was prepared from sections 1 to 9 for training and section 0 for testing. Instead of using the NP bracketing information present in the tagged Treebank data, Ramshaw and Marcus modified the data so as to include bracketing information related only to the non-recursive, base NPs present in each sentence while the subject verb phrases were taken as is. The data sets include POS tag information generated by Ramshaw and Marcus using Brill's transformational part-of-speech tagger (Brill, 1995) . The sizes of the training and test data are summarized in Table 1 and Table 2 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In order to be able to compare our results with the results obtained by other researchers, we worked with the same data sets already used by (Ramshaw and Marcus, 1995; Argamon et al., 1998) for NP and SV detection. ", "after_sen": "These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993) . "}
{"citeStart": 183, "citeEnd": 205, "citeStartToken": 183, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "We argue that the various cliques in which a word appears represent different axes of similarity and help to identify the different senses of that word. For instance, in the whole set of words connected to etude (study) in a strongly connected component of the NTC graph (analyse, evaluation, resultat, presentation, principe, calcul, travail...) , some subsets form cliques with etude. dle, 1990) shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning. Hindle uses the observed frequencies within a specific syntactic pattern (sub-ject/verb, and verb/object) to derive a cooccu,> rence score which is an estimate of mutual information (Church and Hanks, 1990) . We adapted this score to noun phrase patterns) However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis. The subgraph of the chirurgical acts words, which is easy to identify from the SYCLADE graph ( fig. 5a ), is split in different parts in the similarity graph ( fig. 5b) . This difference stems from the fact that this cooccurrence score overestimates rare events and underlines the collocations specific to each form. 1° For instance, it appears that the relationship between stenose and lesion, which was central in figure   3 , with 8 shared contexts, almost diseappears if one considers the number of shared cooccurrences. Therefore, similarity measures based on cooccurrences and similarity estimation based on shared contexts must not be used in place of each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "dle, 1990) shows that SYCLADE similarity indicator is specifically relevant for ontology bootstrap and tuning. ", "mid_sen": "Hindle uses the observed frequencies within a specific syntactic pattern (sub-ject/verb, and verb/object) to derive a cooccu,> rence score which is an estimate of mutual information (Church and Hanks, 1990) . ", "after_sen": "We adapted this score to noun phrase patterns) However the similarity measures based on cooccurrence scores and nominal phrase patterns are less relevant for an ontological analysis. "}
{"citeStart": 273, "citeEnd": 306, "citeStartToken": 273, "citeEndToken": 306, "sectionName": "UNKNOWN SECTION NAME", "string": "The notion of head However, not all mental adjectives will be able to project the two types they denote (i.e. state and event), depending on the event headeduess, in GL, the notion of head provides a way of indicating a type of foregronnding and baekgronnding of event arguments. In doing this, it specifies how to project the qualia representation and acts as a filter to constrain the set of projectable qualia: the headed event projects the tbrnlula associated with that event and it; is this formula which needs to be saturated at the syntax level (Pustejovsky, 1995, (Jhapter 6.2.5 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The notion of head However, not all mental adjectives will be able to project the two types they denote (i.e. state and event), depending on the event headeduess, in GL, the notion of head provides a way of indicating a type of foregronnding and baekgronnding of event arguments. ", "mid_sen": "In doing this, it specifies how to project the qualia representation and acts as a filter to constrain the set of projectable qualia: the headed event projects the tbrnlula associated with that event and it; is this formula which needs to be saturated at the syntax level (Pustejovsky, 1995, (Jhapter 6.2.5 ).", "after_sen": "2 For ~L similar view see Anscombres (1995) who distinguishes internal feeling and external attitude. "}
{"citeStart": 220, "citeEnd": 232, "citeStartToken": 220, "citeEndToken": 232, "sectionName": "UNKNOWN SECTION NAME", "string": "Tracking Agreement Our corpus consists of 24 computer-mediated dialogues 1 in which two participants collaborate on a simple task of buying furniture for the living and dining rooms of a house (a variant of the task in (Walker, 1993) ). The participants' main goal is to negotiate purchases; the items of highest priority are a sofa for the living room and a table and four chairs for the dining room. The problem solving task is complicated by several secondary goals: 1) Match colors within a room, 2) Buy as much furniture as you can, 3) Spend all your money. A point system is used to motivate participants to try to achieve as many goals as possible. Each subject has a budget and inventory of furniture that lists the quantities, colors, and prices for each available item. By sharing this initially private information, the participants can combine budgets and select furniture from either's inventory. The problem is collaborative in that all decisions have to be consensual; funds are shared and purchasing decisions are joint.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Tracking Agreement Our corpus consists of 24 computer-mediated dialogues 1 in which two participants collaborate on a simple task of buying furniture for the living and dining rooms of a house (a variant of the task in (Walker, 1993) ). ", "after_sen": "The participants' main goal is to negotiate purchases; the items of highest priority are a sofa for the living room and a table and four chairs for the dining room. "}
{"citeStart": 78, "citeEnd": 100, "citeStartToken": 78, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "paradigmatic relation: how the words are associated with each other. Similarity between words can be defined by either a syntagmatic or a paradigmatic relation. Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. This paper concentrates on paradigmatic similarity, because a paradigmatic relation can be established both inside a sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside a sentence --like syntax deals with sentence structure. The rest of this section focuses on two related works on measuring paradigmatic similarity --a psycholinguistic approach and a thesaurus-based approach.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. ", "mid_sen": "Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. ", "after_sen": "This paper concentrates on paradigmatic similarity, because a paradigmatic relation can be established both inside a sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside a sentence --like syntax deals with sentence structure. "}
{"citeStart": 109, "citeEnd": 130, "citeStartToken": 109, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "Our experiments are based on a subset of meeting recordings collected and transcribed by ICSI (Morgan et al., 2001) . Seven meetings were segmented (automatically, but with human adjustment) into 9854 total spurts. We define a 'spurt' as a period of speech by one speaker that has no pauses of greater than one half second (Shriberg et al., 2001) . Spurts are used here, rather than sentences, because our goal is to use ASR outputs and unsupervised training paradigms, where hand-labeled sentence segmentations are not available.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Seven meetings were segmented (automatically, but with human adjustment) into 9854 total spurts. ", "mid_sen": "We define a 'spurt' as a period of speech by one speaker that has no pauses of greater than one half second (Shriberg et al., 2001) . ", "after_sen": "Spurts are used here, rather than sentences, because our goal is to use ASR outputs and unsupervised training paradigms, where hand-labeled sentence segmentations are not available."}
{"citeStart": 194, "citeEnd": 214, "citeStartToken": 194, "citeEndToken": 214, "sectionName": "UNKNOWN SECTION NAME", "string": "candi SubjNP rank top in the decision tree. That is, for the reference determination, the subject roles of the candidate's referent within a discourse segment will be checked in the first place. This finding supports well the suggestion in centering theory that the grammatical relations should be used as the key criteria to rank forward-looking centers in the process of focus tracking (Brennan et al., 1987; Grosz et al., 1995) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "That is, for the reference determination, the subject roles of the candidate's referent within a discourse segment will be checked in the first place. ", "mid_sen": "This finding supports well the suggestion in centering theory that the grammatical relations should be used as the key criteria to rank forward-looking centers in the process of focus tracking (Brennan et al., 1987; Grosz et al., 1995) .", "after_sen": "3.) candi Pron and candi NoAntecedent are to be examined in the cases when the subject-role checking fails, which confirms the hypothesis in the S-List model by Strube (1998) that co-refereing candidates would have higher preference than other candidates in the pronoun resolution."}
{"citeStart": 192, "citeEnd": 215, "citeStartToken": 192, "citeEndToken": 215, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations, such as (multi-component) tree-adjoining grammars (Joshi and Schabes, 1997; Weir, 1988) , IO macro grammars (Fisher, 1968) , and (parallel) multiple contextfree grammars (Seki et al., 1991) . For instance, the TAG in Figure 3 is represented by the Datalog program in Figure 4 . Moreover, the method of reduc- Figure 3 : A TAG with one initial tree (left) and one auxiliary tree (right) tion extends to the problem of tactical generation (surface realization) for these grammar formalisms coupled with Montague semantics (under a certain restriction). Our method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars (de Groote, 2001) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "N(i, j) :− unicorn(i, j).", "mid_sen": "In this paper, we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations, such as (multi-component) tree-adjoining grammars (Joshi and Schabes, 1997; Weir, 1988) , IO macro grammars (Fisher, 1968) , and (parallel) multiple contextfree grammars (Seki et al., 1991) . ", "after_sen": "For instance, the TAG in Figure 3 is represented by the Datalog program in Figure 4 . "}
{"citeStart": 53, "citeEnd": 67, "citeStartToken": 53, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "The implementation described in this paper is based on the most recent model, that of (Gorrell (in press) ). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980) , Marcus et al (1983) ), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987 (Abney ( , 1989 , Pritehett (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that \"the parser attempts incrementally to satisfy the principles of grammar\". For the purposes of this implementation, I have interpreted this to mean that each word must be attached into a fullyconnected phrase marker as it is found in the input. 1 The psychological desirability of such a 1In fact, GorreU conjectures that, where there is insufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue & Fodor (1991) , Frazier (1987) ). Such models have also been explored computationally (Milward (1995) , Crocker (1991) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that, since we do not deal with such Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue & Fodor (1991) , Frazier (1987) ). ", "mid_sen": "Such models have also been explored computationally (Milward (1995) , Crocker (1991) ).", "after_sen": ""}
{"citeStart": 91, "citeEnd": 103, "citeStartToken": 91, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P (t|s). Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004) , in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input. These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P (t|s). ", "mid_sen": "Portage implements a dynamic programming beam search decoding algorithm similar to that of Koehn (2004) , in which translation hypotheses are constructed by combining in various ways the target-language part of phrase pairs whose sourcelanguage part matches the input. ", "after_sen": "These phrase pairs come from large phrase tables constructed by collecting matching pairs of contiguous text segments from word-aligned bilingual corpora."}
{"citeStart": 55, "citeEnd": 73, "citeStartToken": 55, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999) . Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. ", "mid_sen": "Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999) . Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity.", "after_sen": "In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. "}
{"citeStart": 46, "citeEnd": 73, "citeStartToken": 46, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "It is essentially ill addressing the issue of ovelgenerality that Mel'~:uk introduces sub-and superscripts to lexical functions, enhancing their precision and making them sensitive to meaning aspects of tile lcxical items over which they operate. Superscripts are illtended to make the nleaning of tile I,F nlore precise and he|me |nero likely to imply unary inappings between argu|nents and vahlcs, subscripts a|e used to reference a particular semautic COlllpOUellt of a keyword. The introduclion of such devices into tile account of l,Fs demtmstrates hoth the need tk)r precision and the fact lbat it does seeul necessary to address semantic aspects of lexemes stand| ng it| co-occurrence relatio|ls. Ill fact it has been asserted by sonm (e.g., (Anick and Pustciovsky, 1990) , (lteid and Raab, 1989) ) that collocational systems are systematically predictable from the lexical Selllantics Of nt)tUlS, it) till atteln]Jt to explore this notion furthel; we have investigated the appr(lach to nolninal semantics known as Qualia structure (Pustejovsky, 1991) and conside|ed how this lnay ct)tnple-u|ent the LF notion to inlprove its descriptive powe| r.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The introduclion of such devices into tile account of l,Fs demtmstrates hoth the need tk)r precision and the fact lbat it does seeul necessary to address semantic aspects of lexemes stand| ng it| co-occurrence relatio|ls. ", "mid_sen": "Ill fact it has been asserted by sonm (e.g., (Anick and Pustciovsky, 1990) , (lteid and Raab, 1989) ) that collocational systems are systematically predictable from the lexical Selllantics Of nt)tUlS, it) till atteln]Jt to explore this notion furthel; we have investigated the appr(lach to nolninal semantics known as Qualia structure (Pustejovsky, 1991) and conside|ed how this lnay ct)tnple-u|ent the LF notion to inlprove its descriptive powe| r.", "after_sen": "alnoDg tile prolnising avenues that occur to tlS are, firstly, tile postulation of I,F subscripts based on the four Qualia roles (assuming thal these are tim lexically hies) relevant aspects of noun selnantics) and, secondly, the application of l,Fs to senlaulic (Qualia) structures rather titan monolithic lexenles; cg: tile I ,l; Ibm is used in delivering evahlative qualitiers which are standard expressions of praise or approval. "}
{"citeStart": 171, "citeEnd": 182, "citeStartToken": 171, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "For our second set of experiments, we use the three-class annotation scheme. We merge the text of the sentences in the context windows as well as their dependency triplets to obtain the features. The results are reported in Table 3 with best results in bold. Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011) . ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results are reported in Table 3 with best results in bold. ", "mid_sen": "Although these results are not better than the context-less baseline, the reason might be data sparsity since existing work on citation sentiment analysis uses more data (Athar, 2011) . ", "after_sen": ""}
{"citeStart": 109, "citeEnd": 117, "citeStartToken": 109, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ([Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes \"these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 56, "citeEnd": 74, "citeStartToken": 56, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "To rescore the N -best lists, we use the method of . But the results shown here are different from that work due to a better optimization of the overall ASR system, using a better MT system, and generating a larger N -best list from the ASR word graphs. We rescore the ASR N -best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models. The development and evaluation sets Nbest lists sizes are sufficiently large to achieve almost the best possible results, on average 1738 hypotheses per each source sentence are extracted from the ASR word graphs. The recognition results are summarized in Table 2. In this table, the translation results of the MT system are shown first, which are obtained using the phrase-based approach. Then the recognition results of the ASR system are shown. Afterwards, the results of combined speech recognition and translation models are presented.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But the results shown here are different from that work due to a better optimization of the overall ASR system, using a better MT system, and generating a larger N -best list from the ASR word graphs. ", "mid_sen": "We rescore the ASR N -best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models. ", "after_sen": "The development and evaluation sets Nbest lists sizes are sufficiently large to achieve almost the best possible results, on average 1738 hypotheses per each source sentence are extracted from the ASR word graphs. "}
{"citeStart": 260, "citeEnd": 270, "citeStartToken": 260, "citeEndToken": 270, "sectionName": "UNKNOWN SECTION NAME", "string": "Nodes in the input layer of the network represent simple relations on the input sentence and are being used as the input features. Target nodes represent words that are of interest; in the case studied here, each of the word candidates for prediction is represented as a target node. An input sentence, along with a designated word of interest in it, is mapped into a set of features which are active in it; this representation is presented to the input layer of SNoW and propagates to the target nodes. Target nodes are linked via weighted edges to (some of) the input features. Let At = {Q,... , i,~} be the set of features that are active in an example and are linked to the target node t. Then the linear unit corresponding to t is active iff t E w i > Ot, iEAt where w~ is the weight on the edge connecting the ith feature to the target node t, and Ot is the threshold A given example is treated autonomously by each target subnetwork; an example labeled t may be treated as a positive example by the subnetwork for t and as a negative example by the rest of the target nodes. The learning policy is on-line and mistake-driven; several update rules can be used within SNOW. The most successful update rule is a variant of Littlestone's Winnow update rule (Littlestone, 1988 ), a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992) . This mechanism is implemented via the sparse architecture of SNOW. That is, (1) input features are allocated in a data driven way -an input node for the feature i is allocated only if the feature i was active in any input sentence and (2) a link (i.e., a non-zero weight) exists between a target node t and a feature i if and only if i was active in an example labeled t.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The learning policy is on-line and mistake-driven; several update rules can be used within SNOW. ", "mid_sen": "The most successful update rule is a variant of Littlestone's Winnow update rule (Littlestone, 1988 ), a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992) . ", "after_sen": "This mechanism is implemented via the sparse architecture of SNOW. "}
{"citeStart": 44, "citeEnd": 68, "citeStartToken": 44, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Ps,.l(Xwitfl = 1) are to be compared (c.f. (Hindle and Rool:.h, 1991) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For slot-based models, with tile independence assumption, P.~(X,~,ith = 1) and", "mid_sen": "Ps,.l(Xwitfl = 1) are to be compared (c.f. (Hindle and Rool:.h, 1991) ).", "after_sen": "Assuming that random variables (case slots) are mutually independent would drastically reduce tile number of parameters. "}
{"citeStart": 13, "citeEnd": 38, "citeStartToken": 13, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "If the world were ideal, we would have an oracle, able to always select the right parser. In such situation our selection of parsers would grant the accuracy as high as 95.8 %. We attempt to imitate the oracle by a second-level classifier that learns from the Tune set, which parser is right in which situations. Such technique is usually called classifier stacking. Parallel to (van Halteren et al., 1998) , we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based. This approach roughly corresponds to (Henderson and Brill, 1999) 's Naïve Bayes parse hybridization.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such technique is usually called classifier stacking. ", "mid_sen": "Parallel to (van Halteren et al., 1998) , we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based. ", "after_sen": "This approach roughly corresponds to (Henderson and Brill, 1999) 's Naïve Bayes parse hybridization."}
{"citeStart": 250, "citeEnd": 270, "citeStartToken": 250, "citeEndToken": 270, "sectionName": "UNKNOWN SECTION NAME", "string": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . ", "mid_sen": "In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) .", "after_sen": "Besides, an increasing concern in current projects is that of linguistic relevance of the analysis t)erformed by the grammar correction system. "}
{"citeStart": 289, "citeEnd": 296, "citeStartToken": 289, "citeEndToken": 296, "sectionName": "UNKNOWN SECTION NAME", "string": "Despite the fact that almost all running EBMT systems employ the sentence as the text unit, it is believed that the potential of EBMT lies on the exploitation of fragments of text snualler that sentences and the combination of such fragments to produce the translation of whole sentences [Sato 90 ]. Automatic sub-sentential alignment is, however, a problem yet to be solved.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If sub-sentence alignment is available, the approach is fully automated but is quite vulnerable to the problem of luw quality as mentioned above, as well as to ambiguity problems when the produced segments are rather small.", "mid_sen": "Despite the fact that almost all running EBMT systems employ the sentence as the text unit, it is believed that the potential of EBMT lies on the exploitation of fragments of text snualler that sentences and the combination of such fragments to produce the translation of whole sentences [Sato 90 ]. ", "after_sen": "Automatic sub-sentential alignment is, however, a problem yet to be solved."}
{"citeStart": 235, "citeEnd": 267, "citeStartToken": 235, "citeEndToken": 267, "sectionName": "UNKNOWN SECTION NAME", "string": "word sense disambiguation, information retrieval, natural language generation and so on. Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). ", "mid_sen": "However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "after_sen": "We have been concerned with investigating the lexical . ['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates. "}
{"citeStart": 167, "citeEnd": 181, "citeStartToken": 167, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "• We developed a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet report on parsing English text (Magerman, 1995) . The fact that its recall and precision are both in the high 80s represents not just a quantitative improvement in parser performance, but also a qualitative improvement.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We believe that development of other broadly applicable information extraction functionality such as NE and TE will be a win, maximizing the value of defining reusable knowledge bases for information extraction.", "mid_sen": "• We developed a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet report on parsing English text (Magerman, 1995) . ", "after_sen": "The fact that its recall and precision are both in the high 80s represents not just a quantitative improvement in parser performance, but also a qualitative improvement."}
{"citeStart": 128, "citeEnd": 140, "citeStartToken": 128, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "A new L1-regularized Maxent algorithms was proposed for density estimation (Dudik et al., 2004) and we adapted it to classification. We found this algorithm to converge faster than the current state-ofthe-art in Maxent training, which is L2-regularized L-BFGS (Malouf, 2002) 1 . Moreover, the number of trained parameters is considerably smaller.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A new L1-regularized Maxent algorithms was proposed for density estimation (Dudik et al., 2004) and we adapted it to classification. ", "mid_sen": "We found this algorithm to converge faster than the current state-ofthe-art in Maxent training, which is L2-regularized L-BFGS (Malouf, 2002) 1 . Moreover, the number of trained parameters is considerably smaller.", "after_sen": ""}
{"citeStart": 77, "citeEnd": 90, "citeStartToken": 77, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The Alvey features are mapped on to the CLAWS tagset used in the LOB corpus (Garside, 1987) . Tag transitions are checked against an occurrence matrix of the tagged LOB corpus using positional binary trigrams similar to those used in the spelling checks mentioned above. Tag checks though the current set of categories stop when one category passes, but backtrack and continue if parsing then fails.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Alvey features are mapped on to the CLAWS tagset used in the LOB corpus (Garside, 1987) . ", "after_sen": "Tag transitions are checked against an occurrence matrix of the tagged LOB corpus using positional binary trigrams similar to those used in the spelling checks mentioned above. "}
{"citeStart": 76, "citeEnd": 96, "citeStartToken": 76, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "Classifier design and feature selection. The overall approach to classifying spurts uses a decision tree classifier (Breiman et al., 1984) to combine the word based and prosodic cues. In order to facilitate learning of cues for the less frequent classes, the data was upsampled (duplicated) so that there were the same number of training points per class. The decision tree size was determined using error-based cost-complexity pruning with 4-fold cross validation. To reduce our initial candidate feature set, we used an iterative feature selection algorithm that involved running multiple decision trees (Shriberg et al., 2000) . The algorithm combines elements of brute-force search (in a leave-one-out paradigm) with previously de-termined heuristics for narrowing the search space. We used entropy reduction of the tree after cross-validation as a criterion for selecting the best subtree.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Classifier design and feature selection. ", "mid_sen": "The overall approach to classifying spurts uses a decision tree classifier (Breiman et al., 1984) to combine the word based and prosodic cues. ", "after_sen": "In order to facilitate learning of cues for the less frequent classes, the data was upsampled (duplicated) so that there were the same number of training points per class. "}
{"citeStart": 143, "citeEnd": 156, "citeStartToken": 143, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "Although the explanatory relation extraction task has been studied from the view of linguistic and discourse representation by existing works (Carston, 1993; Lascarides and Asher, 1993) , the automatic extraction task is still an open question. Consider the following examples extracting from online reviews:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Edges describe the explanatory relationships between them, of which the heads are explanatory clauses.", "mid_sen": "Although the explanatory relation extraction task has been studied from the view of linguistic and discourse representation by existing works (Carston, 1993; Lascarides and Asher, 1993) , the automatic extraction task is still an open question. ", "after_sen": "Consider the following examples extracting from online reviews:"}
{"citeStart": 194, "citeEnd": 207, "citeStartToken": 194, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "We argue that this customization bottleneck can be overcome by the automatic extraction of application-tuned consistent generation subgrammars from proved given large-scale grammars. In this paper we present such an automatic subgrammar extraction tool. The underlying procedure is valid for grammars written in typed unification formalisms; it is here carried out for systemic grammars within the development environment for text generation KPML (Bateman, 1997) . The input is a set of semantic specifications covering the intended application. This can either be provided by generating a predefined test suite or be automatically produced by running the particular application during a training phase.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we present such an automatic subgrammar extraction tool. ", "mid_sen": "The underlying procedure is valid for grammars written in typed unification formalisms; it is here carried out for systemic grammars within the development environment for text generation KPML (Bateman, 1997) . ", "after_sen": "The input is a set of semantic specifications covering the intended application. "}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "The criteria for assignment of capital letters to verbs is not made explicit, but is influenced by the syntactic and semantic relations which hold between the verb and its arguments; for example, I5, L5 and T5 can all be assigned to verbs which take a NP subject and a sentential complement, but L5 will only be assigned if there is a fairly close semantic link between the two arguments and T5 will be used in preference to I5 if the verb is felt to be semantically two place rather than one place, such as know versus appear. On the other hand, both believe and promise are assigned V3 which means they take a NP object and infinitival complement, yet there is a similar semantic distinction to be made between the two verbs; so the criteria for the assignment of the V code seem to be purely syntactic. Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Michiels (1982) and Akkerman et al. (1985) provide a more detailed analysis of the information encoded by the LDOCE grammar codes and discuss their efficacy as a system of linguistic description. ", "mid_sen": "Ingria (1984) comprehensively compares different approaches to complementation within grammatical theory providing a touchstone against which the LDOCE scheme can be evaluated.", "after_sen": "Most automated parsing systems employ grammars which carefully distinguish syntactic and semantic information, therefore, if the information provided by the Longman grammar code system is to be of use, we need to be able to separate out this information and map it into a representation scheme compatible with the type of lexicon used by such parsing systems."}
{"citeStart": 30, "citeEnd": 55, "citeStartToken": 30, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous computational models have demanded tremendous memory and computational capacity from human learners. For example, the algorithm of Brent & Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible. It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation. Work in this tradition makes no claims, however, that these methods are actually the ones used by human learners.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Previous computational models have demanded tremendous memory and computational capacity from human learners. ", "mid_sen": "For example, the algorithm of Brent & Cartwright (1996) produces a set of possible lexicons that describe the learning corpus, each of which is evaluated as the learner iterates until no further improvement is possible. ", "after_sen": "It is unlikely that an algorithm of this type is something a human learner is capable of using given the requirement to remember at the very least a long history of recent utterances encountered and constantly reanalyze them to find a optimal segmentation. "}
{"citeStart": 109, "citeEnd": 130, "citeStartToken": 109, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, the graph-based method (LexRank) is applied successfully to generic, multi-document summarization (Erkan and Radev, 2004) . A topicsensitive LexRank is proposed in (Otterbacher et al., 2005) . In this method, a sentence is mapped to a vector in which each element represents the occurrence frequency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information thus cannot distinguish between \"The hero killed the villain\" and \"The villain killed the hero\". The task like answering complex questions that requires the use of more complex syntactic and semantics, the approaches with only TF*IDF are often inadequate to perform fine-level textual analysis.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Natural Language Processing (NLP), this information synthesis can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information.", "mid_sen": "Recently, the graph-based method (LexRank) is applied successfully to generic, multi-document summarization (Erkan and Radev, 2004) . ", "after_sen": "A topicsensitive LexRank is proposed in (Otterbacher et al., 2005) . "}
{"citeStart": 143, "citeEnd": 156, "citeStartToken": 143, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "The popularity of natural language search is evidenced by the growing number of search engines, such as AskJeeves, Electric Knowledge, and Northern Light, 1 that offer such functionality. For most sites, we were only able to perform a cursory examination of their proprietary techniques. Adopting a similar approach as FAQFinder (Hammond et al., 1995) , AskJeeves maintains a database of questions and webpages that provide answers to them. User questions are compared against those in the database, and links to webpages for the closest matches are returned. Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001 ). However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "User questions are compared against those in the database, and links to webpages for the closest matches are returned. ", "mid_sen": "Similar to our approach, Electric Knowledge transforms a natural language question into a series of increasingly more general keyword queries (Bierner, 2001 ). ", "after_sen": "However, their query formulation process utilizes hard-crafted regular expressions, while we adopt a more general machine learning approach for transformation rule application."}
{"citeStart": 45, "citeEnd": 62, "citeStartToken": 45, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "SNoW uses the Open Close model, described in Muñoz et al. 1999 Table 1 : The e ects of system-internal combination by using di erent output representations. A straight-forward majority v ote of the output yields better bracket accuracies and F =1 rates than any included individual classi er. The bracket accuracies in the columns O and C show what percentage of words was correctly classi ed as baseNP start, baseNP end or neither. model produced better results than the other paradigm evaluated there, the Inside Outside paradigm. The Open Close model consists of two SNoW predictors, one of which predicts the beginning of baseNPs Open predictor, and the other predicts the end of the phrase Close predictor. The Open predictor is learned using SNoW Carlson et al., 1999; Roth, 1998 as a function of features that utilize words and POS tags in the sentence and, given a new sentence, will predict for each w ord whether it is the rst word in the phrase or not. For each Open, the Close predictor is learned using SNoW as a function of features that utilize the words in the sentence, the POS tags and the open prediction. It will predict, for each word, whether it can be the end of the phrase, given the previously predicted Open. Each pair of predicted Open and Close forms a candidate of a baseNP. These candidates may con ict due to overlapping; at this stage, a graph-based constraint satisfaction algorithm that uses the con dence values SNoW associates with its predictions is employed. This algorithm \"the combinator\" produces the list of the nal baseNPs for each sentence. Details of SNoW, its application in shallow parsing and the combinator's algorithm are in Muñoz et al. 1999.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results presented here were obtained by optimizing MBSL parameters based on 5-fold CV on the training data.", "mid_sen": "SNoW uses the Open Close model, described in Muñoz et al. 1999 Table 1 : The e ects of system-internal combination by using di erent output representations. ", "after_sen": "A straight-forward majority v ote of the output yields better bracket accuracies and F =1 rates than any included individual classi er. "}
{"citeStart": 161, "citeEnd": 178, "citeStartToken": 161, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "A class of technique that can handle all kinds of functions of random variables without the above problems is the computationally-intensive randomization tests (Noreen, 1989, Ch. 2) (Cohen, 1995, Sec. 5.3 ). These tests have previously used on such functions during the \"message understanding\" (MUC) evaluations (Chinchor et al., 1993) . The randomization test we use is like a randomization version of the paired sample (matched-pair) t test (Cohen, 1995, Sec. 5.3.2) . This is a type of stratified shuffling (Noreen, 1989, Sec. 2.7) . When comparing two techniques, we gather-up all the responses (whether actually of interest or not) produced by one of the two techniques when examining the test data, but not both techniques. Under the null hypothesis, the two techniques are not really different, so any response produced by one of the techniques could have just as likely come from the other. So we shuffle these responses, reassign each response to one of the two techniques (equally likely to either technique) and see how likely such a shuffle produces a difference (new technique minus old technique) in the metric(s) of interest (in our case, precision and F-score) that is at least as large as the difference observed when using the two techniques on the test data. n responses to shuffle and assign 4 leads to 2 n different ways to shuffle and assign those responses. So when n is small, one can try each of the different shuffles once and produce an exact randomization. When n gets large, the number of different shuffles gets too large to be exhaustively evaluated. Then one performs an approximate randomization where each shuffle is performed with random assignments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A class of technique that can handle all kinds of functions of random variables without the above problems is the computationally-intensive randomization tests (Noreen, 1989, Ch. 2) (Cohen, 1995, Sec. 5.3 ). ", "after_sen": "These tests have previously used on such functions during the \"message understanding\" (MUC) evaluations (Chinchor et al., 1993) . "}
{"citeStart": 35, "citeEnd": 58, "citeStartToken": 35, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "Coding Scheme We will present our coding scheme by first describing the core DR/ scheme, followed by the adaptations for our corpus and research issues. For details about our scheme, see (Di Eugenio et al., 1997) ; for details about features we added to DR/, but that are not relevant for this paper, see (Di Eugenio et al., 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Coding Scheme We will present our coding scheme by first describing the core DR/ scheme, followed by the adaptations for our corpus and research issues. ", "mid_sen": "For details about our scheme, see (Di Eugenio et al., 1997) ; for details about features we added to DR/, but that are not relevant for this paper, see (Di Eugenio et al., 1998) .", "after_sen": ""}
{"citeStart": 185, "citeEnd": 210, "citeStartToken": 185, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "3.1 Ontology Various authors (including Link, 1983 , Bach, 1986 , Krifka, 1989 , Eberle, 1990 have proposed modeltheoretic treatments in which a parallel ontological distinction is made between substances and things, processes and events, etc. A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount. As such, the approach developed here may be seen as building upon the work of Carlson (1977) and his successors; it also represents one way to further formalize the intuitions found in Moens and Steedman (1988) and Jackendoff (1991) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount. ", "mid_sen": "As such, the approach developed here may be seen as building upon the work of Carlson (1977) and his successors; it also represents one way to further formalize the intuitions found in Moens and Steedman (1988) and Jackendoff (1991) .", "after_sen": "Following Schubert and Pelletier (1987) , the present account distinguishes individuals from kinds, but not from stages or quantities. "}
{"citeStart": 139, "citeEnd": 152, "citeStartToken": 139, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "If the word is unknown, the system reconsiders analysis from the point where it broke down with the added possibility of an error rule. There are currently four error rules, corresponding to the four Damerau transformations: omission, insertion, transposition, substitution (Damerau, 1964) -considered in that order (Pollock, 1983) . The error rules are in two level format and integrate seamlessly into morphological analysis.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the word is unknown, the system reconsiders analysis from the point where it broke down with the added possibility of an error rule. ", "mid_sen": "There are currently four error rules, corresponding to the four Damerau transformations: omission, insertion, transposition, substitution (Damerau, 1964) -considered in that order (Pollock, 1983) . ", "after_sen": "The error rules are in two level format and integrate seamlessly into morphological analysis."}
{"citeStart": 301, "citeEnd": 323, "citeStartToken": 301, "citeEndToken": 323, "sectionName": "UNKNOWN SECTION NAME", "string": "Features We extracted the following features from each sentence and used them in the featurebased classifiers: (1) Discourse features: location in the article/section/paragraph. For this feature each text batch was divided to ten equal size parts and the corresponding feature value identifies the relevant part; (2) Lexical features: number of citations and references to tables and figures (0, 1, or more), word, bi-gram, verb, and verb class (obtained by spectral clustering (Sun and Korhonen, 2009) ); (3) Syntactic features: tense and voice (POS tags of main and auxiliary verbs), grammatical relation, subject and object. The lexical and the syntactic features were extracted for the represented sentence as well as for its surrounding sentences. We used the C&C POS tagger and parser (Curran et al., 2007) for extracting the lexical and the syntactic features. Note that all the information encoded into our constraints is also encoded in the features and is thus available to the feature-based model. This enables us to properly evaluate the impact of our modeling decision which augments a feature-based model with constraints.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Features We extracted the following features from each sentence and used them in the featurebased classifiers: (1) Discourse features: location in the article/section/paragraph. ", "mid_sen": "For this feature each text batch was divided to ten equal size parts and the corresponding feature value identifies the relevant part; (2) Lexical features: number of citations and references to tables and figures (0, 1, or more), word, bi-gram, verb, and verb class (obtained by spectral clustering (Sun and Korhonen, 2009) ); (3) Syntactic features: tense and voice (POS tags of main and auxiliary verbs), grammatical relation, subject and object. ", "after_sen": "The lexical and the syntactic features were extracted for the represented sentence as well as for its surrounding sentences. "}
{"citeStart": 149, "citeEnd": 166, "citeStartToken": 149, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "After the last iteration, we use these collected counts as weights and compute maximum directed spanning trees using the Chu-Liu/Edmonds' algorithm (Chu and Liu, 1965) . Therefore, the resulting trees consist of edges maximizing the sum of individual counts:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After each iteration is finished (all the trees in the corpus are re-sampled), we increment the counter of all directed pairs of nodes which are connected by a dependency edge in the current trees.", "mid_sen": "After the last iteration, we use these collected counts as weights and compute maximum directed spanning trees using the Chu-Liu/Edmonds' algorithm (Chu and Liu, 1965) . ", "after_sen": "Therefore, the resulting trees consist of edges maximizing the sum of individual counts:"}
{"citeStart": 399, "citeEnd": 416, "citeStartToken": 399, "citeEndToken": 416, "sectionName": "UNKNOWN SECTION NAME", "string": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "mid_sen": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "after_sen": "Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. "}
{"citeStart": 138, "citeEnd": 161, "citeStartToken": 138, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "c5.0 3 , a commercial version of c4.5 Quinlan, 1993, performs top-down induction of decision trees tdidt. On the basis of an instance base of examples, c5.0 constructs a decision tree which compresses the classi cation information in the instance base by exploiting differences in relative importance of di erent features. Instances are stored in the tree as paths A demo of the NP and VP chunker is available at http: www.sfb441.unituebingen.de ~dejean chunker.h tml 3 Available from http: www.rulequest.com of connected nodes ending in leaves which contain classi cation information. Nodes are connected via arcs denoting feature values. Feature information gain mutual information between features and class is used to determine the order in which features are employed as tests at all levels of the tree Quinlan, 1993 . With the full input representation words and POS tags, we were not able to run complete experiments. We therefore experimented only with the POS tags with a context of two left and right. We h a ve used the default parameter setting with decision trees combined with value grouping. We have used a nearest neighbor algorithm ib1-ig, here listed as MBL and a decision tree algorithm IGTree from the TiMBL learning package Daelemans et al., 1999b . Both algorithms store the training data and classify new items by choosing the most frequent classi cation among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. Each feature receives a weight which is based on the amount of information which it provides for computing the classi cation of the items in the training data. ib1-ig uses these weights for computing the distance between a pair of data items and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree Daelemans et al., 1999b . We will use their default parameters except for the ib1-ig parameter for the numberof examined nearest neighbors k which we have set to 3 Daelemans et al., 1999a . The classi ers use a left and right context of four words and partof-speech tags. For the four IO representations we have used a second processing stage which used a smaller context but which included information about the IO tags predicted by the rst processing phase Tjong Kim Sang, 2000. When building a classi er, one must gather evidence for predicting the correct class of an item from its context. The Maximum Entropy MaxEnt framework is especially suited for integrating evidence from various information sources. Frequencies of evidence class combinations called features are extracted from a sample corpus and considered to beproperties of the classi cation process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active i.e. a feature is active when the context meets the requirements given by the feature. For every class the weights of the active features are combined and the best scoring class is chosen Berger et al., 1996 . For the classier built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features consisting of one of the mentioned information sources and complex features combinations thereof were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software Dehaspe, 1997 . MBSL Argamon et al., 1999 uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP including complete phrases. These sequences may include a few context tags, up to a prespeci ed max context. During inference, MBSL tries to 'tile' each POS string with parts of noun-phrases from the memory. If the string could befully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm. Adding a context extends the possibilities for tiling, thereby giving more opportunities to better candidates. The approach of MBSL to the problem of identifying baseNPs is sequence-based rather than word-based, that is, decisions are taken per POS sequence, or per candidate, but not for a single word. In addition, the tiling process gives no preference to any direction in the sentence. The tiles may b e of any length, up to the maximal length of a phrase in the training data, which gives MBSL a generalization power that compensates for the setup of using only POS tags. The results presented here were obtained by optimizing MBSL parameters based on 5-fold CV on the training data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ib1-ig uses these weights for computing the distance between a pair of data items and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree Daelemans et al., 1999b . ", "mid_sen": "We will use their default parameters except for the ib1-ig parameter for the numberof examined nearest neighbors k which we have set to 3 Daelemans et al., 1999a . ", "after_sen": "The classi ers use a left and right context of four words and partof-speech tags. "}
{"citeStart": 105, "citeEnd": 128, "citeStartToken": 105, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same way that tags are allocated to words, or to punctuation marks, they can represent the boundaries of syntactic constituents, such as noun phrases and verb phrases. Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do. (Atwell, 1987) and (Church, 1989) have used this approach. If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994) . Our approach uses a similar concept, but differs in that embedded syntactic constituents are detected one at a time in separate steps. There are only 2 hypertags -the opening and closing brackets marking the possible location(s) of the syntactic constituent in question. Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Atwell, 1987) and (Church, 1989) have used this approach. ", "mid_sen": "If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994) . ", "after_sen": "Our approach uses a similar concept, but differs in that embedded syntactic constituents are detected one at a time in separate steps. "}
{"citeStart": 50, "citeEnd": 80, "citeStartToken": 50, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Meaning-Text Theory (Melc'fik, 1988) assumes seven strata of representation. The rules mapping fi'om the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Melc'~tk & Pertsov, 1987p.1870 . Word Grammar (WG, Hudson (1990 ) is based on general graphs instead of trees. The ordering of two linked words is specified together with their dependency relation, as in the proposition \"object of verb follows it\". Extraction of, e.g., objects is analyzed by establishing an additional dependency called visitor between the verb and the extractee, which requires the reverse order, as in \"visitor of verb precedes it\". This results in inconsistencies, since an extracted object must follow the verb (being its object) and at the same time precede it (being its visitor). The approach compromises the semantic motivation of dependencies by adding purely order-induced dependencies. WG is similar to our proposal in that it also distinguishes a propositional meta language describing the graph-based analysis structures.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The rules mapping fi'om the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. ", "mid_sen": "These rules have not yet been formally specified (Melc'~tk & Pertsov, 1987p.1870 . Word Grammar (WG, Hudson (1990 ) is based on general graphs instead of trees. ", "after_sen": "The ordering of two linked words is specified together with their dependency relation, as in the proposition \"object of verb follows it\". "}
{"citeStart": 183, "citeEnd": 195, "citeStartToken": 183, "citeEndToken": 195, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances (Almuallim and Dietterich, 1991, Langley and Sage, in press ). Unfortunately, the task of designing an appropriate instance representation --also known as feature set selection --can be extraordinarily difficult, time-consuming, and knowledge-intensive (Quinlan, 1983) . This poses a problem for current statistical and machine learning approaches to natural language understanding where a new instance representation is typically required for each linguistic task tackled.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). ", "mid_sen": "Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . ", "after_sen": "In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. "}
{"citeStart": 10, "citeEnd": 29, "citeStartToken": 10, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "We follow Stone et al. (2003) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches, we use the semantic roles in t as the arguments of these atoms. For instance, the semantic content of the \"likes\" tree in Fig. 1 is {like(self, ag, pat)} (see also the semcon entries in Fig. 4) . The knowledge base is some finite set of ground atoms; in the example, it could contain such entries as like(e, m, r) and rabbit(r). Finally, the communicative goal is some subset of the knowledge base, such as {like(e, m, r)}.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also use a knowledge base that specifies the speaker's knowledge, and require that we can only use trees that express information in this knowledge base.", "mid_sen": "We follow Stone et al. (2003) in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches, we use the semantic roles in t as the arguments of these atoms. ", "after_sen": "For instance, the semantic content of the \"likes\" tree in Fig. 1 is {like(self, ag, pat)} (see also the semcon entries in Fig. "}
{"citeStart": 41, "citeEnd": 53, "citeStartToken": 41, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "In the Penn treebank tree-tagged corpus (Marcus, 1991) , for instance, about 80 percents of the rules are concerned with peculiar sentences which include inversive, elliptic, parenthetic, or emphatic phrases. For example, we can drive a rule VP ---, vb NP comma rb comma PP from the following sentence.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So, the bulk of written sentences are open to the extragrammaticality.", "mid_sen": "In the Penn treebank tree-tagged corpus (Marcus, 1991) , for instance, about 80 percents of the rules are concerned with peculiar sentences which include inversive, elliptic, parenthetic, or emphatic phrases. ", "after_sen": "For example, we can drive a rule VP ---, vb NP comma rb comma PP from the following sentence."}
{"citeStart": 371, "citeEnd": 386, "citeStartToken": 371, "citeEndToken": 386, "sectionName": "UNKNOWN SECTION NAME", "string": "Northwest Airlines settled the remaining lawsuits filed on behalf of 156 people killed in a 1987 crash, but claims against the jetliner's maker are being pursued, a federal judge said. (\"Northwest Airlines Settles Rest of Suits,\" Wall Street Journal, November 1, 1989) A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues. It is most fully presented in Wiebe and Rapaport (1986 , 1988 , 1991 and Wiebe (1990 Wiebe ( , 1994 . It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory. The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973 Kuroda ( , 1976 ) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory. ", "mid_sen": "The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973 Kuroda ( , 1976 ) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) ", "after_sen": ""}
{"citeStart": 0, "citeEnd": 20, "citeStartToken": 0, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) , which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author's reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a \"lower level\" of analysis.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) , which makes the creation of a general sentiment classifier a difficult task. ", "mid_sen": "Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. ", "after_sen": "While the authors did make use of the context in their annotation, their focus was on the task of determining the author's reason for citing a given paper. "}
{"citeStart": 102, "citeEnd": 120, "citeStartToken": 102, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004) , the Polarity data set 5 created by (Pang and Lee, 2004) , and the MPQA data set created by (Wiebe et al., 2005) . 6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves sentence-level classification.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004) , the Polarity data set 5 created by (Pang and Lee, 2004) , and the MPQA data set created by (Wiebe et al., 2005) . 6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves sentence-level classification.", "after_sen": "The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993) . "}
{"citeStart": 295, "citeEnd": 333, "citeStartToken": 295, "citeEndToken": 333, "sectionName": "UNKNOWN SECTION NAME", "string": "We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carlson et al., 1999) , 1 which learns a linear classifier in feature space, and has been successful in several NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005) . In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N, E}. In the second case, E has been incremented with emotional distinctions according to the valence, i.e. E = {N, P E, N E}. Experiments used 10-fold cross-validation, with 90% train and 10% test data. 2", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The goal here is to get a good understanding of the nature of the TEP problem and explore features which may be useful.", "mid_sen": "We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carlson et al., 1999) , 1 which learns a linear classifier in feature space, and has been successful in several NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005) . ", "after_sen": "In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N, E}. In the second case, E has been incremented with emotional distinctions according to the valence, i.e. E = {N, P E, N E}. Experiments used 10-fold cross-validation, with 90% train and 10% test data. "}
{"citeStart": 183, "citeEnd": 199, "citeStartToken": 183, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "Information structure analysis The information structure of scientific documents (e.g. journal articles, abstracts, essays) can be analyzed in terms of patterns of topics, functions or relations observed in multi-sentence scientific text. Computational approaches have mainly focused on analysis based on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2006; Hachey and Grover, 2006; Teufel et al., 2009) , discourse structure (Burstein et al., 2003; Webber et al., 2011) , qualitative dimensions (Shatkay et al., 2008) , scientific claims (Blake, 2009) , scientific concepts and information status (Markert et al., 2012) . Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012) . Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga et al., 2012; Reichart and Korhonen, 2012) . Unfortunately, these approaches do not reach the performance level of fully supervised models, let alone exceed it. Our novel method addresses this problem.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most existing methods for analyzing scientific text according to information structure use full supervision in the form of thousands of manually annotated sentences (Teufel and Moens, 2002; Burstein et al., 2003; Mizuta et al., 2006; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012; Markert et al., 2012) . ", "mid_sen": "Because manual annotation is prohibitively expensive, approaches based on light supervision are now emerging for the task, including those based on active learning and self-training (Guo et al., 2011) and unsupervised methods (Varga et al., 2012; Reichart and Korhonen, 2012) . ", "after_sen": "Unfortunately, these approaches do not reach the performance level of fully supervised models, let alone exceed it. "}
{"citeStart": 106, "citeEnd": 123, "citeStartToken": 106, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section, we describe an EM-based algorithm that was introduced by Saerens et al. (2002) , which can be used to estimate the sense priors, or a priori probabilities of the different senses in a new dataset. We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns (Chan and Ng, 2005) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section, we describe an EM-based algorithm that was introduced by Saerens et al. (2002) , which can be used to estimate the sense priors, or a priori probabilities of the different senses in a new dataset. ", "mid_sen": "We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns (Chan and Ng, 2005) .", "after_sen": "Most of this section is based on (Saerens et al., 2002) . "}
{"citeStart": 72, "citeEnd": 84, "citeStartToken": 72, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. The Temporal Centering framework (Kameyama et al., 1993) integrates lWe will limit the scope of this paper by restricting the discussion to accomplishments and achievements. aspects of both approaches, but patterns with the first in treating tense as anaphoric.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. ", "mid_sen": "This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. ", "after_sen": "The Temporal Centering framework (Kameyama et al., 1993) integrates lWe will limit the scope of this paper by restricting the discussion to accomplishments and achievements. "}
{"citeStart": 52, "citeEnd": 64, "citeStartToken": 52, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing. In the last decade, research in speech recognition (Jelinek 1985) , noun classification (Hindle 1988) , predicate argument relations (Church & Hanks 1989) , and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing. ", "mid_sen": "In the last decade, research in speech recognition (Jelinek 1985) , noun classification (Hindle 1988) , predicate argument relations (Church & Hanks 1989) , and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.", "after_sen": ""}
{"citeStart": 106, "citeEnd": 129, "citeStartToken": 106, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "A class of technique that can handle all kinds of functions of random variables without the above problems is the computationally-intensive randomization tests (Noreen, 1989, Ch. 2) (Cohen, 1995, Sec. 5.3 ). These tests have previously used on such functions during the \"message understanding\" (MUC) evaluations (Chinchor et al., 1993) . The randomization test we use is like a randomization version of the paired sample (matched-pair) t test (Cohen, 1995, Sec. 5.3.2) . This is a type of stratified shuffling (Noreen, 1989, Sec. 2.7) . When comparing two techniques, we gather-up all the responses (whether actually of interest or not) produced by one of the two techniques when examining the test data, but not both techniques. Under the null hypothesis, the two techniques are not really different, so any response produced by one of the techniques could have just as likely come from the other. So we shuffle these responses, reassign each response to one of the two techniques (equally likely to either technique) and see how likely such a shuffle produces a difference (new technique minus old technique) in the metric(s) of interest (in our case, precision and F-score) that is at least as large as the difference observed when using the two techniques on the test data. n responses to shuffle and assign 4 leads to 2 n different ways to shuffle and assign those responses. So when n is small, one can try each of the different shuffles once and produce an exact randomization. When n gets large, the number of different shuffles gets too large to be exhaustively evaluated. Then one performs an approximate randomization where each shuffle is performed with random assignments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These tests have previously used on such functions during the \"message understanding\" (MUC) evaluations (Chinchor et al., 1993) . ", "mid_sen": "The randomization test we use is like a randomization version of the paired sample (matched-pair) t test (Cohen, 1995, Sec. 5.3.2) . ", "after_sen": "This is a type of stratified shuffling (Noreen, 1989, Sec. 2.7) . "}
{"citeStart": 46, "citeEnd": 58, "citeStartToken": 46, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "The Monte Carlo estimates of the KL divergence from several document collection pair are listed in of the pair (Israeli, Palestinian) is not necessarily the same as (Palestinian, Israeli). KL divergence is greater than zero (Cover and Thomas, 1991) and equal to zero only when document collections A and B are exactly the same. Here (ACQ, ACQ) is close to but not exactly zero because they are different samples of documents in the ACQ category.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Monte Carlo estimates of the KL divergence from several document collection pair are listed in of the pair (Israeli, Palestinian) is not necessarily the same as (Palestinian, Israeli). ", "mid_sen": "KL divergence is greater than zero (Cover and Thomas, 1991) and equal to zero only when document collections A and B are exactly the same. ", "after_sen": "Here (ACQ, ACQ) is close to but not exactly zero because they are different samples of documents in the ACQ category."}
{"citeStart": 218, "citeEnd": 246, "citeStartToken": 218, "citeEndToken": 246, "sectionName": "UNKNOWN SECTION NAME", "string": "The qualla structure (QUALIA) encodes the basic semantic type of a word (its Lexical Coilceptual Paradigm, or LCP) and specifics how it is linked to other events and arguments of the event and argument structures (see Pustejovsky, 1995, chapter 6 ). To do this, it call use four possible different roles: the FORMAL role encoding the basic semantic type(s) of the word, the CONsTrru'rIw,; role its constitutive elements, the TELIC role its purpose or function and the AGENTIVE role the factors involved in bringing it about. In terms of temporal relations, the qualia encode specific constraints on the relative temporal ordering of the wducs of tile quMia. That is, the event involved in the AGENTIVE role precedes that state existing ill the FORMAL) alld the associated CONSTITUT1VI,) vMne, shonhl there be one. Finally, the T]~LIC role is inherently a tcinporal consequence of the FOlt-MAI,, cf. (15).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I,'ollowing Croft (1990) , we think that there are two processes implied in a causal emotional state: an cxperiencer must direct his or her attention to a stimulus and this causes the experiencer to enter ill a mental state.", "mid_sen": "The qualla structure (QUALIA) encodes the basic semantic type of a word (its Lexical Coilceptual Paradigm, or LCP) and specifics how it is linked to other events and arguments of the event and argument structures (see Pustejovsky, 1995, chapter 6 ). ", "after_sen": "To do this, it call use four possible different roles: the FORMAL role encoding the basic semantic type(s) of the word, the CONsTrru'rIw,; role its constitutive elements, the TELIC role its purpose or function and the AGENTIVE role the factors involved in bringing it about. "}
{"citeStart": 47, "citeEnd": 66, "citeStartToken": 47, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "An algorithm that applies the technique of bagging to parsing is given in Algorithm 2. Previous work on combining independent parsers is leveraged to produce the combined parser. The rest of the algorithm is a straightforward transformation of bagging for classifiers. Exploratory work in this vein was described by HajiC et al. (1999) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The rest of the algorithm is a straightforward transformation of bagging for classifiers. ", "mid_sen": "Exploratory work in this vein was described by HajiC et al. (1999) .", "after_sen": ""}
{"citeStart": 105, "citeEnd": 129, "citeStartToken": 105, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed 'lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991) .", "mid_sen": "In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. ", "after_sen": "The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. "}
{"citeStart": 78, "citeEnd": 100, "citeStartToken": 78, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor. We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013) , which is based on scikitlearn (Pedregosa et al., 2011) , with F1 optimization (\"metaphor\" class). Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (\"metaphor\") class.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this study, each content-word token in a text is an instance that is classified as either a metaphor or not a metaphor. ", "mid_sen": "We use the logistic regression classifier as implemented in the SKLL package (Blanchard et al., 2013) , which is based on scikitlearn (Pedregosa et al., 2011) , with F1 optimization (\"metaphor\" class). ", "after_sen": "Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (\"metaphor\") class."}
{"citeStart": 58, "citeEnd": 74, "citeStartToken": 58, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. Many investigators (e.g. Allen 1976; Elowitz et al. 1976; Luce et al. 1983; Cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech. And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. As we will show, our results so far indicate that our experimental system, which assigns a discourse neutral prosoclic phrasing on the level of sentences, provides a significant improvement in the quality of synthesized speech. We believe that one reason for the improvement has to do with the increased pitch range that our system uses. Textto-speech systems that lack sentence-level phrasing must take a conservative approach to pitch settings in order to avoid misleading and inappropriate pitch modulations. Correct phrase identification makes it possible to adopt an expanded pitch range that greatly enhances the naturalness of the final speech. In constructing the system, we focused on two core questions: (i) what kind of parser is needed for the p:rosody rules? and (ii) how should prosodic phrasing, i.e. boundary location and strength, be represented?", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Existing text-to-speech systems perform well on word pronunciation and short sentences, 12 but when it comes to long sentences and paragraphs, synthetic speech tends to be difficult to listen to and understand. ", "mid_sen": "Many investigators (e.g. Allen 1976; Elowitz et al. 1976; Luce et al. 1983; Cahn 1988 ) have suggested that the poor prosody of synthetic speech, in comparison with natural speech, is the primary factor leading to difficulties in the comprehension of fluent synthetic speech. ", "after_sen": "And while researchers in text-tospeech synthesis have adopted a variety of approaches to prosodic phrase generation--from the simple punctuationbased rules and function word listings of existing commercial systems to the sophisticated prosodic heuristics described in Emorine and Martin (1988) and O'Shaughnessy (1989)---the generation of appropriate prosodic phrasing in unres~tricted text has remained a problem. "}
{"citeStart": 0, "citeEnd": 16, "citeStartToken": 0, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. Gerdemann's generator follows a head-driven strategy in order to avoid inefficient evaluation orders. More specifically, the head of the right-hand side of each grammar rule is distinguished, and distinguished categories are scanned or predicted upon first. The resulting evaluation strategy is similar to that of the head-corner approach (Shieber et al., 1990 ; Gerdemann and IIinrichs, in press): prediction follows the main flow of semantic information until a lexical pivot is reached, and only then are the headdependent subparts of the construction built up in a bottom-up fashion. This mixture of top-down and bottom-up information flow is crucial since the topdown semantic information from the goal category must be integrated with the bottom-up subcategorization information from the lexicon. A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing. Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. ", "mid_sen": "Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. ", "after_sen": "Gerdemann's generator follows a head-driven strategy in order to avoid inefficient evaluation orders. "}
{"citeStart": 109, "citeEnd": 126, "citeStartToken": 109, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set. This can be done by smoothing the observed frequencies 7 (Church and Mercer 1993) or by class-based methods (Brown et al. 1991; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993; Hirschman 1986; Resnik 1992; Brill et al. 1990; Dagan, Marcus, and Markovitch 1993) . In comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics. This allows the system to learn successfully from very sparse data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set. ", "mid_sen": "This can be done by smoothing the observed frequencies 7 (Church and Mercer 1993) or by class-based methods (Brown et al. 1991; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993; Hirschman 1986; Resnik 1992; Brill et al. 1990; Dagan, Marcus, and Markovitch 1993) . ", "after_sen": "In comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics. "}
{"citeStart": 58, "citeEnd": 67, "citeStartToken": 58, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984) . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. ", "mid_sen": "Another dialogue acquisition system has been developed by Ho (1984) . ", "after_sen": "However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. "}
{"citeStart": 119, "citeEnd": 131, "citeStartToken": 119, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . A number of other systems have addressed part of the task. Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. ", "mid_sen": "Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . ", "after_sen": "In most of this and other related work the treatment is some variant of the following. "}
{"citeStart": 260, "citeEnd": 284, "citeStartToken": 260, "citeEndToken": 284, "sectionName": "UNKNOWN SECTION NAME", "string": "where For instance, a particular feature/class function might fire if and only if the bigram \"still hate\" appears and the document's sentiment is hypothesized to be negative. 7 Importantly, unlike Naive Bayes, MaxEnt makes no assumptions about the relationships between features, and so might potentially perform better when conditional independence assumptions are not met. The λ i,c 's are feature-weight parameters; inspection of the definition of P ME shows that a large λ i,c means that f i is considered a strong indicator for class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier's name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The λ i,c 's are feature-weight parameters; inspection of the definition of P ME shows that a large λ i,c means that f i is considered a strong indicator for class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier's name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. ", "mid_sen": "We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000) .", "after_sen": "Z(d) is a normalization function. "}
{"citeStart": 86, "citeEnd": 100, "citeStartToken": 86, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French. These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals. Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals. ", "mid_sen": "Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990) .", "after_sen": "We also note that the patterns of realisations uncovered in our analysis follow the principle of good technical writing practice known as the minimalist approach, e.g., (Carroll, 1994; Hammond, 1994) . "}
{"citeStart": 197, "citeEnd": 217, "citeStartToken": 197, "citeEndToken": 217, "sectionName": "UNKNOWN SECTION NAME", "string": "While our work is in part motivated by the above research, other developmental research supports certain ;assumptions we make. The input to our system is represented as a sequence of i)houenms, so we implicitly assume that infants are aisle I.o ,'ouv('rl, from acoustic inl)ut to phoneme sequem:es; research i)y Kuhl (e.g., Gricser & Kuhl, 1989) suggests tha.t this assmnl)tion is remsonal)h,. Since sentence I)oundaries provide informal.ion ahout word I)oumlaries (the end of a sentence is also the end of a word), our input contains sentence I~oumhu'ik~s; several studies (13ernstein-II.atm 'r, 1985; Ilirsh-lh~sek et al., 1987; Kemler Nels~m, I lirsh-I'asek, ,lusczyk & Wright C; msidy, 1989; ,I usczyk et al., 1992) have shown that infimts can perceive senl,cncc I)oundarics using prosodic cues. Ih)wever, FiSher and 'lbkura (in press) found m) evidence that prosody can accurately predict word boundaries, .so the task of finding words remains. Finally, one might question whether in-Ikmts have the ability we are trying to model--that is, whether they can identify words embedded in sentences; Jusczyk and Aslin (submitted) found that 7 I/2-month-olds can do so.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While our work is in part motivated by the above research, other developmental research supports certain ;assumptions we make. ", "mid_sen": "The input to our system is represented as a sequence of i)houenms, so we implicitly assume that infants are aisle I.o ,'ouv('rl, from acoustic inl)ut to phoneme sequem:es; research i)y Kuhl (e.g., Gricser & Kuhl, 1989) suggests tha.t this assmnl)tion is remsonal)h,. ", "after_sen": "Since sentence I)oundaries provide informal.ion ahout word I)oumlaries (the end of a sentence is also the end of a word), our input contains sentence I~oumhu'ik~s; several studies (13ernstein-II.atm 'r, 1985; Ilirsh-lh~sek et al., 1987; Kemler Nels~m, I lirsh-I'asek, ,lusczyk & Wright C; msidy, 1989; ,I usczyk et al., 1992) have shown that infimts can perceive senl,cncc I)oundarics using prosodic cues. "}
{"citeStart": 121, "citeEnd": 152, "citeStartToken": 121, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "Our language is called Z: and its primitive symbols (with respect to a given signature (Cat, Atom, Feat)) consists of (1) all items in Cat and Atom (2) two constants, c-struct and f-struct, and (4) if n > 0, and ¢1,...,¢n are wffs, then so is *(¢1,...,¢n)-Nothing else is a wff. Now for the satisfaction definition. We inductively define a three place relation ~ which holds between models M, nodes n and wffs ¢. Intuitively, M, n ~ ¢ means that the constraint ¢ holds at (is true at, is satisfied at) the node n in model M. The required inductive definition is as follows: For the most part the import of these clauses should be clear. The constants true and false play their usual role, c-struct and f-struct give us 'labels' for our two domains, while the elements of Cat and Atom enable us to talk about syntactic categories and atomic f-structure information respectively. The clauses for --, and A are the usual definitions of classical logic, thus we have all propositional calculus at our disposal; as we shall see, this gives us the flexibility required to formulate non-trivial general constraints. More interesting are the clauses for the modalities. The unary modalities (a), (up), (down), and (zoomin) and the variable arity modality * give us access to the binary relations important in formulating LFG grammars. Incidentally, • is essentially a piece of syntactic sugar; it could be replaced by a collection of unary modalities (see Blackburn and Meyer-Viol (1994) ). However, as the * operator is quite a convenient piece of syntax for capturing the effect of phrase structure rules, we have included it as a primitive in/3.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The unary modalities (a), (up), (down), and (zoomin) and the variable arity modality * give us access to the binary relations important in formulating LFG grammars. ", "mid_sen": "Incidentally, • is essentially a piece of syntactic sugar; it could be replaced by a collection of unary modalities (see Blackburn and Meyer-Viol (1994) ). ", "after_sen": "However, as the * operator is quite a convenient piece of syntax for capturing the effect of phrase structure rules, we have included it as a primitive in/3."}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "Whatever term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser. Shieber (1985 Shieber ( , 1992 follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm. His central notion of restriction, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of complex categories, but Shieber's restrictors are specified manually.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Whatever term one takes, an important aspect of the relation is that it can be used to reduce the search space of possible syntactic analyses at an earlier point in parsing and thus serves to improve the efficiency of a parser. ", "mid_sen": "Shieber (1985 Shieber ( , 1992 follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm. ", "after_sen": "His central notion of restriction, whereby a restrictor is a finite subset of the paths specified in a feature structure, is related to the technique we introduce here, since both guarantee the finiteness of an otherwise possibly infinite domain of complex categories, but Shieber's restrictors are specified manually."}
{"citeStart": 36, "citeEnd": 50, "citeStartToken": 36, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "BU is almost, as efficient as TD/I, eventhough it works with a generic grammar, and thus produces (significantly) more chart-items. Once we replace the generic grammar by an instantiated grammar, and add left-corner relationships (BU/LC), the predictive capacities of the parser are maximal, and a sharp decrease in the number of chart items and parse times occurs. Gap-threading For the grammar with gap-threading (table 2) , we used a restrictor R = {< 1 ° > II = eat,val, arg,dir, gap, in or out}. The TD parser encounters serious difficulties in this case, whereas TD/I performs significantly better, but still is rather inefficient. There is a distinct difference between BU and BU/LC if we look at the number of chart items, although the difference is less marked than in the case of German. In terms of parse times the two algorithms are almost equivalent. Comparing our results with those of Shieber (1985) and Haas (1989) , we see that in all cases top-down filtering may reduce the size of the chart significantly. Whereas Haas (1989) found that top-down filtering never helps to actually decrease parse times in a bottom-up parser, we have found at least one example (German) where top-down filtering is useful.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In terms of parse times the two algorithms are almost equivalent. ", "mid_sen": "Comparing our results with those of Shieber (1985) and Haas (1989) , we see that in all cases top-down filtering may reduce the size of the chart significantly. ", "after_sen": "Whereas Haas (1989) found that top-down filtering never helps to actually decrease parse times in a bottom-up parser, we have found at least one example (German) where top-down filtering is useful."}
{"citeStart": 164, "citeEnd": 180, "citeStartToken": 164, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "A module realizing this scheme has been implemented and applied to the very setup used for the previous experiments with the hand-coded tree-cutting criteria, see [Samuelsson 1994a ]. 2100 of the verified parse trees constituted the training set, while 230 of them were used for the test set. The table below summarizes the results for some grammars of different coverage extracted using:1. Hand-coded tree-cutting criteria.2. Induced tree-cutting criteria where the node entropy was taken to be the phrase entropy of the RHIS phrase of the dominating grammar rule.3. Induced tree-cutting criteria where the node entropy was the sum of the phrase entropy of the RHS phrase of the dominating grammar rule and the weighted sum of the phrase entropies of the LHSs of the alternative choices of grammar rules to resolve on.In the latter two cases experiments were carried out both with and without the restrictions on neighbouring cutnodes discussed in the previous section. With the mixed entropy scheme it seems important to include the restrictions on neighbouring cutnodes, while this does not seem to be the case with the RHS phrase entropy scheme. A potential explanation for the significantly higher average parsing times for all grammars extracted using the induced tree-cutting criteria is that these are in general recursive, while the handcoded criteria do not allow recursion, and thus only produce grammars that generate finite languages.Although the hand-coded tree-cutting criteria are substantially better than the induced ones, we must remember that the former produce a grammar that in median allows 60 times faster processing than the original grammar and parser do. This means that even if the induced criteria produce grammars that are a factor two or three slower than this, they are still approximately one and a half order of magnitude faster than the original setup. Also, this is by no means a closed research issue, but merely a first attempt to realize the scheme, and there is no doubt in my mind that it can be improved on most substantially.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A module realizing this scheme has been implemented and applied to the very setup used for the previous experiments with the hand-coded tree-cutting criteria, see [Samuelsson 1994a ]. 2100 of the verified parse trees constituted the training set, while 230 of them were used for the test set. ", "after_sen": "The table below summarizes the results for some grammars of different coverage extracted using:1. "}
{"citeStart": 82, "citeEnd": 105, "citeStartToken": 82, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. Since we are not generating from the model, this does not introduce difficulties (Klein and Manning, 2002) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Like CCM, this model is deficient since the same supertags are generated multiple times, and parses with conflicting supertags are not valid. ", "mid_sen": "Since we are not generating from the model, this does not introduce difficulties (Klein and Manning, 2002) .", "after_sen": "One additional complication that must be addressed is that left-frontier non-terminal categories -those whose subtree span includes the first word of the sentence -do not have a left-side supertag to use as context. "}
{"citeStart": 177, "citeEnd": 199, "citeStartToken": 177, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "Collocations have been studied by computational linguists in different contexts. For instance, there is a substantial body of papers on the extraction of \"frequently co-occurring words\" from corpora using statistical methods (e.g., (Choueka et al., 1983) , (Church and Hanks, 1989) , (Smadja, 1993) to list only a few). These authors focus on techniques for providing material that can be used in other processing tasks such as", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Collocations have been studied by computational linguists in different contexts. ", "mid_sen": "For instance, there is a substantial body of papers on the extraction of \"frequently co-occurring words\" from corpora using statistical methods (e.g., (Choueka et al., 1983) , (Church and Hanks, 1989) , (Smadja, 1993) to list only a few). ", "after_sen": "These authors focus on techniques for providing material that can be used in other processing tasks such as"}
{"citeStart": 67, "citeEnd": 89, "citeStartToken": 67, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "As a baseline, we use the best performing feature set from Beigman Klebanov et al. (2014) , who investigated supervised word-level identification of metaphors. We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Performance will be evaluated using Precision, Recall, and F-1 score, for the positive (\"metaphor\") class.", "mid_sen": "As a baseline, we use the best performing feature set from Beigman Klebanov et al. (2014) , who investigated supervised word-level identification of metaphors. ", "after_sen": "We investigate the effect of reweighting of examples, as well as the effectiveness of features related to the notion of concreteness."}
{"citeStart": 56, "citeEnd": 73, "citeStartToken": 56, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "We ran our system on the ACE-2004 Corpus as a baseline to prove that the system worked properly and could approximately duplicate Zhou et al.'s results. Using 5-fold cross validation on the newswire and broadcast news documents in the dataset we achieved an average overall F-Measure of 50.6 on the fine-grained relations. Although a bit lower than Zhou et al.'s result of 55.5 (Zhou et al., 2005) , we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using 5-fold cross validation on the newswire and broadcast news documents in the dataset we achieved an average overall F-Measure of 50.6 on the fine-grained relations. ", "mid_sen": "Although a bit lower than Zhou et al.'s result of 55.5 (Zhou et al., 2005) , we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.", "after_sen": ""}
{"citeStart": 129, "citeEnd": 142, "citeStartToken": 129, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "We extend the baseline model with lexical linguistic representations (supertags) both in the language model as well as in the phrase translation model. Before we describe how our model extends the baseline, we shortly review the supertagging approaches in Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Modern linguistic theory proposes that a syntactic parser has access to an extensive lexicon of wordstructure pairs and a small, impoverished set of operations to manipulate and combine the lexical entries into parses. Examples of formal instantiations of this idea include CCG and LTAG. The lexical entries are syntactic constructs (graphs) that specify information such as POS tag, subcategorization/dependency information and other syntactic constraints at the level of agreement features. One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks (Bangalore & Joshi, 1999; Clark & Curran, 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The lexical entries are syntactic constructs (graphs) that specify information such as POS tag, subcategorization/dependency information and other syntactic constraints at the level of agreement features. ", "mid_sen": "One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks (Bangalore & Joshi, 1999; Clark & Curran, 2004) .", "after_sen": ""}
{"citeStart": 22, "citeEnd": 35, "citeStartToken": 22, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data.", "after_sen": "This treebank contains detailed syntactic derivations represented as trees, but the node labeling is incompatible with the system grammar. "}
{"citeStart": 73, "citeEnd": 92, "citeStartToken": 73, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "We hypothesize that an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993; Hanks, 2013) . This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012) , which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012) , can be seamlessly integrated into our scheme.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. ", "mid_sen": "This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012) , which successfully utilized such language models to represent word window contexts of target words. ", "after_sen": "However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012) , can be seamlessly integrated into our scheme."}
{"citeStart": 75, "citeEnd": 105, "citeStartToken": 75, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Prefer is misclassified as Object Raising, rather than as Object Equi, because the relevant code field contains a T5 code, as well as a V3 code. The T5 code is marked as 'rare', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal: I help (,~ yo,,, wo~k) This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as \"would\". This deficiency is rectified in the verb classification system employed by Jackendoff and Grimshaw (1985) in the Brandeis verb catalogue.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The T5 code is marked as 'rare', and the occurrence of prefer with a tensed sentential complement, as opposed to with an infinitive, is certainly marginal: I help (,~ yo,,, wo~k) This example also highlights a deficiency in the LDOCE coding system since prefer occurs much more naturally with a sentential complement if it collocates with a modal such as \"would\". ", "mid_sen": "This deficiency is rectified in the verb classification system employed by Jackendoff and Grimshaw (1985) in the Brandeis verb catalogue.", "after_sen": "The main source of error comes from the misclassification of Object Raising into Object Equi verbs. "}
{"citeStart": 205, "citeEnd": 218, "citeStartToken": 205, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "In theory, WordNet nouns and verbs are arranged as a set of hierarchies (Fellbaum 1998) , each with an unique beginner, but multiple inheritance is not ruled out, particularly where the second hypernym of a synset is in a different semantic category. The semantic categories in WordNet are based, according to Fellbaum (1998) on a standard work on psycholinguistics (Miller & Johnson-Laird, 1976) . The latter discusses in detail verbs of motion, possession, vision (WordNet perception) and communication, which are the basis for the corresponding WordNet categories. Other semantic fields mentioned are contact, bodily activity (WN body), thought (WN cognition) and affect (WN emotion). Miller & Johnson-Laird, (1976) acknowledge that these categories overlap, but WordNet does not allow a verb to belong to more than one category. No theoretical basis has been found for the remaining categories. Competition is subsumed by social, and consumption is subsumed by body. Weather would seem selfcontained, but change, creation and stative are not semantic fields at all. Stative belongs to the Aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields (Vendler, 1967 , Moens & Steedman 1988 , Amaro, 2006 . Moreover, a verb can belong to more than one Aktionsart category, as these apply to verbs in contexts.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Weather would seem selfcontained, but change, creation and stative are not semantic fields at all. ", "mid_sen": "Stative belongs to the Aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields (Vendler, 1967 , Moens & Steedman 1988 , Amaro, 2006 . ", "after_sen": "Moreover, a verb can belong to more than one Aktionsart category, as these apply to verbs in contexts."}
{"citeStart": 367, "citeEnd": 379, "citeStartToken": 367, "citeEndToken": 379, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. ", "mid_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "after_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . "}
{"citeStart": 5, "citeEnd": 39, "citeStartToken": 5, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992) , we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c ; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). ", "mid_sen": "Like Cowie, Guthrie, and Guthrie (1992) , we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c ; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions.", "after_sen": "In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2)."}
{"citeStart": 109, "citeEnd": 129, "citeStartToken": 109, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, the concept of valency has gained considerable attention. Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990) . Even theories based on phrase structure may have processing models based on relations between lexical items (Rambow & Joshi, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Not only do all linguistic theories refer to some reformulation of the traditional notion of valency (in the form of 0grid, subcategorization list, argument list, or extended domain of locality); there is a growing number of parsers based on binary relations between words (Eisner, 1997; Maruyama, 1990) . ", "mid_sen": "Even theories based on phrase structure may have processing models based on relations between lexical items (Rambow & Joshi, 1994) .", "after_sen": "Against the background of this interest in the valency concept, and the fact that word order is one of the main difference between phrasestructure based approaches (henceforth PSG) and dependency grammar (DG), this paper will propose a word order description for DG and describe its implementation. "}
{"citeStart": 11, "citeEnd": 35, "citeStartToken": 11, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Selecting the mean that gives the best correlation could be considered as training on test data. However, were we simply to have selected a value somewhere in the middle of the graph, as was our original intuition, it would have given an unfair advantage to either version of Roget's Thesaurus over Word-Net. Our system shows good results for both versions of Roget's Thesauri and WordNet. The 1987 Thesaurus once again performs better than the 1911 version and than WordNet. Much like (Miller and Charles, 1991) , the data set used here is not large enough to determine if any system's improvement is statistically significant.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The 1987 Thesaurus once again performs better than the 1911 version and than WordNet. ", "mid_sen": "Much like (Miller and Charles, 1991) , the data set used here is not large enough to determine if any system's improvement is statistically significant.", "after_sen": ""}
{"citeStart": 37, "citeEnd": 68, "citeStartToken": 37, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Sense disambiguation methods require a decision model that evaluates the relevant statistics. Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works. These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991) ; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper). At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others. 21 Yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them. As will be explained below, many of the differences are correlated with the different information sources employed by these models.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works. ", "mid_sen": "These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991) ; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper). ", "after_sen": "At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others. "}
{"citeStart": 65, "citeEnd": 88, "citeStartToken": 65, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "We consider lexical cohesion as semantic similarity between words. Similarity is Computed by spreading activation (or association) [Waltz and Pollack, 1985] on a semantic network constructed systematically from an English dictionary. Whereas it is edited by some lexicographers, a dictionary is a set of associative relation shared by the people in a linguistic community.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We consider lexical cohesion as semantic similarity between words. ", "mid_sen": "Similarity is Computed by spreading activation (or association) [Waltz and Pollack, 1985] on a semantic network constructed systematically from an English dictionary. ", "after_sen": "Whereas it is edited by some lexicographers, a dictionary is a set of associative relation shared by the people in a linguistic community."}
{"citeStart": 97, "citeEnd": 117, "citeStartToken": 97, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "• For an independent category CatI, order sentences by decreasing order of confidence score. The confidence score is the average confidence score of the SVM and CRF classifiers reported in (Liakata et al., 2012 ) for a sentence. • For a dependent category Cat, for which we need n sentences, given the selected sentences m from the corresponding independent category CatI we do the following:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• For an independent category CatI, order sentences by decreasing order of confidence score. ", "mid_sen": "The confidence score is the average confidence score of the SVM and CRF classifiers reported in (Liakata et al., 2012 ) for a sentence. ", "after_sen": "• For a dependent category Cat, for which we need n sentences, given the selected sentences m from the corresponding independent category CatI we do the following:"}
{"citeStart": 105, "citeEnd": 126, "citeStartToken": 105, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The head word is a useful but sparse feature. In our corpus, of the roles, 1016 head words (type) are used, in which are used only once. The top 20 words are given in Table 4 . In the top 20 words, 4 are prepositions (\" /in /at /than /for\") and 3 are temporal nouns(\" /today /present /recently\") and 2 are adverbs(\" /already, /will\"). These closed class words are highly correlated with specific semantic roles. For example,\" /for\" occurs 195 times as the head of a constituent, of which 172 are non-roles, 19 are argM-BFYs, 3 are arg1s and 1 is an argM-TPC.\" /in\" occurs 644 times as a head, of which 430 are nonroles, 174 are argM-LOCs, 24 are argM-TMPs, 9 are argM-RNGs, and 7 are argM-CND. \" /already\" occurs 135 times as a head, of which 97 are non-roles and 38 are argM-ADVs. \" /today\" occurs 69 times as a head, of which 41 are argM-TMPs and 28 are nonroles. Within the open class words, some are closely correlated to the target verb. For example, \" /meeting; conference\" occurs 43 times as a head for roles, of which 24 are for the target \" /take place\" and 19 for \" /pass\". \" /ceremony\" occurs 28 times and all are arguments of \" \"(take place).\" /statement\" occurs 19 times, 18 for \" /release; publish\" and one for \" /hope\". These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments. Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003) . For example, \"7 26 /July 26\" may not be seen in the training, but its POS, NT(temporal noun) , is a good indicator that it is a temporal.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments. ", "mid_sen": "Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003) . ", "after_sen": "For example, \"7 26 /July 26\" may not be seen in the training, but its POS, NT(temporal noun) , is a good indicator that it is a temporal."}
{"citeStart": 131, "citeEnd": 151, "citeStartToken": 131, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "Magic compilation does not limit the information that can be used for filtering. This can lead to nontermination as the tree fragments enumerated in bottom-up evaluation of magic compiled grammars are connected (Johnson, forthcoming) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Magic compilation does not limit the information that can be used for filtering. ", "mid_sen": "This can lead to nontermination as the tree fragments enumerated in bottom-up evaluation of magic compiled grammars are connected (Johnson, forthcoming) .", "after_sen": "More specifically, 'magic generation' falls prey to non-termination in the face of head recursion, i.e., the generation analog of left recursion in parsing."}
{"citeStart": 295, "citeEnd": 307, "citeStartToken": 295, "citeEndToken": 307, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . However none of these methods has considered the way of dealing both phenomena in the same concrete system. We propose in this paper an algorithm that deals with both phenomena, in the same analyser. The anaphora module pertains to the recent methods, uses a set of resolution rules based on the focusing approach, see (Sidner, 1983) . These rules are applied to the conceptual representation and their output is a set of candidate antecedents. Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1999) . The disambiguation procedure alms at filling the empty roles using attachment rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . ", "after_sen": "However none of these methods has considered the way of dealing both phenomena in the same concrete system. "}
{"citeStart": 15, "citeEnd": 39, "citeStartToken": 15, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory. In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also \"remembers\" exceptional, low-frequency cases which are useful to extrapolate from. Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997) . We have used the following MBL algorithms1: IB1 : A variant of the k-nearest neighbor (k-NN) algorithm. The distance between a test item and each memory item is defined as the number of features for which they have a different value (overlap metric).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory. ", "mid_sen": "In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also \"remembers\" exceptional, low-frequency cases which are useful to extrapolate from. ", "after_sen": "Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997) . "}
{"citeStart": 92, "citeEnd": 122, "citeStartToken": 92, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Motivated by the argument structure of discourse relations used in Penn Discourse Treebank (Rashmi Prasad and Webber, 2008) , in this work, we adopt the clause unit-based definition. It means that clauses are treated as the basic units of opinion expressions and explanations. Let d = {c 1 , c 2 , ...c n } be the clauses of document d. Directed graph G = (V, E) is used to represent the subjectivity of clauses and explanatory relationships between them. In the graph, vertices represent clauses, whose categories are specified by the vertex attributes. Directed edges describe the explanatory relationships between them, of which the heads are explanatory clauses. If clause c a describes a set of facts which clarify the causes, context, situation, or consequences of another clause c b , c a −→ c b is used to indicate that clause c a explains c b .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Motivated by the argument structure of discourse relations used in Penn Discourse Treebank (Rashmi Prasad and Webber, 2008) , in this work, we adopt the clause unit-based definition. ", "after_sen": "It means that clauses are treated as the basic units of opinion expressions and explanations. "}
{"citeStart": 237, "citeEnd": 243, "citeStartToken": 237, "citeEndToken": 243, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to take steps towards establishing a methodology for doing this type of comparison, we conducted a case study. We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the cospecifiers of pronouns in naturally occurring texts and dialogues [Hob76b, BFP87] . Thus there are two parts to this paper: we present the quantitative results of hand-simulating these algorithms (henceforth Hobbs algorithm and BFP algorithm), but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. We illustrate the general difficulties encountered with quantitative evaluation. These are problems with: (a) allowing for underlying assumptions, (b) determining how to handle underspecifications, and (c) evaluating the contribution of false positives and error chaining.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to take steps towards establishing a methodology for doing this type of comparison, we conducted a case study. ", "mid_sen": "We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the cospecifiers of pronouns in naturally occurring texts and dialogues [Hob76b, BFP87] . ", "after_sen": "Thus there are two parts to this paper: we present the quantitative results of hand-simulating these algorithms (henceforth Hobbs algorithm and BFP algorithm), but this analysis naturally gives rise to both a qualitative evaluation and recommendations for performing such evaluations in general. "}
{"citeStart": 49, "citeEnd": 58, "citeStartToken": 49, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "The lexicai level maintains three lexieai tapes (Kay, 1987; Kiraz, 1994) : pattern tape, root tape and vocalism tape; each tape scans a lexical tree. Exampies of pattern morphemes are:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The lexicai level maintains three lexieai tapes (Kay, 1987; Kiraz, 1994) : pattern tape, root tape and vocalism tape; each tape scans a lexical tree. ", "after_sen": "Exampies of pattern morphemes are:"}
{"citeStart": 90, "citeEnd": 108, "citeStartToken": 90, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (Church 1988) , and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM (Weischedel et al. 1993) and NYU Proteus (Grishman and Sterling 1993) . More recently, statistical methods have been applied to domain-specific semantic parsing (Miller et al. 1994) , and to the more difficult problem of wide-coverage syntactic parsing (Magerman 1995) . Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as AT&T Chronus (Levin and Pieraccini 1995) , continue to require a significant rule based component.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A recent trend in natural language processing has been toward a greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (Church 1988) , and continuing with other work using statistical part-of-speech tagging programs, such as BBN PLUM (Weischedel et al. 1993) and NYU Proteus (Grishman and Sterling 1993) . ", "mid_sen": "More recently, statistical methods have been applied to domain-specific semantic parsing (Miller et al. 1994) , and to the more difficult problem of wide-coverage syntactic parsing (Magerman 1995) . ", "after_sen": "Nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as AT&T Chronus (Levin and Pieraccini 1995) , continue to require a significant rule based component."}
{"citeStart": 116, "citeEnd": 134, "citeStartToken": 116, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Our project's long-range goal (see http://www.isp. pitt.edu/'intgen/) is to create a unified architecture for collaborative discourse, accommodating both interpretation and generation. Our computational approach (Thomason and Hobbs, 1997) uses a form of weighted abduction as the reasoning mechanism (Hobbs et al., 1993) and modal operators to model context. In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. From our first annotation trials, we found that the recognition of \"classical\" speech acts (Austin, 1962; Searle, 1975) by coders is fairly reliable, while recognizing contextual relationships (e.g., whether an utterance accepts a proposal) is not as reliable. Thus, we explore other features that can help us recognize how participants coordinate agreement. Our corpus study also provides a preliminary assessment of the Discourse Resource Initiative (DR/) tagging scheme. The DRI is an international \"grassroots\" effort that seeks to share corpora that have been tagged with the core features of interest to the discourse community. In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes. A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html). Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997; Poesio and Traum, 1997) , we have attempted to adapt it to our corpus and particular research questions.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our project's long-range goal (see http://www.isp. pitt.edu/'intgen/) is to create a unified architecture for collaborative discourse, accommodating both interpretation and generation. ", "mid_sen": "Our computational approach (Thomason and Hobbs, 1997) uses a form of weighted abduction as the reasoning mechanism (Hobbs et al., 1993) and modal operators to model context. ", "after_sen": "In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. "}
{"citeStart": 95, "citeEnd": 110, "citeStartToken": 95, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "In future research, we intend to deal with syntagmatic relations between words. Meaning of a text lies in the texture of paradigmatic and syntagmatic relations between words [Hjelmslev, 1943] . Paradigme provides the former dimension --an associative system of words --as a screen onto which the meaning of a word is projected like a still picture. The latter dimension --syntactic process --will be treated as a film projected dynamically onto Paradigme. This enables us to measure the similarity between texts as a syntactic process, not as word lists.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In future research, we intend to deal with syntagmatic relations between words. ", "mid_sen": "Meaning of a text lies in the texture of paradigmatic and syntagmatic relations between words [Hjelmslev, 1943] . ", "after_sen": "Paradigme provides the former dimension --an associative system of words --as a screen onto which the meaning of a word is projected like a still picture. "}
{"citeStart": 185, "citeEnd": 197, "citeStartToken": 185, "citeEndToken": 197, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of skipgrams is a technique whereby n-grams are formed (bigrams, trigrams, etc.), but in addition to using adjacent sequences of words, it also allows some words to be skipped [Guthrie 2006 ]. More generally, in a k-skip-n-gram, n determines the number of terms, and k the maximum number of skips allowed. In this way skipgrams are new terms that retain part of the sequentiality of the terms, but in a more flexible way than n-grams [Fernandez 2014] . Note that an n-gram can be defined as a 0-skip-n-gram, a skipgram where k=0. For example, the sentence \"I love healthy food\" has two word level trigrams: \"I love healthy\" and \"love healthy food\". However, there is one important trigram implied by the sentence that was not captured: \"I love food\". The use of skipgrams allows the word \"health\" be skipped, providing the mentioned trigram.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is the reason why we decided to use of skipgrams in sentiment analysis.", "mid_sen": "The use of skipgrams is a technique whereby n-grams are formed (bigrams, trigrams, etc.), but in addition to using adjacent sequences of words, it also allows some words to be skipped [Guthrie 2006 ]. ", "after_sen": "More generally, in a k-skip-n-gram, n determines the number of terms, and k the maximum number of skips allowed. "}
{"citeStart": 5, "citeEnd": 26, "citeStartToken": 5, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Like Cimiano et al. (2005) , we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet. However, instead of performing top-down or bottom-up clustering, we pose ontology learning as a k-partite graph construction problem. We use term frequency to determine the position of a concept in the hierarchy. Ryu and Choi (2006) also used term frequency as a measure of domain specificity, but instead of partitioning they combined term frequency and distributional similarity to construct hierarchy. Other method similar to our work is proposed in Fountain and Lapata (2012) . Fountain and Lapata (2012) proposed a graph based approach that does not require a separate term extraction step. However, their approach works with a predefined set of seed terms. Our approach is completely unsupervised and does not require any human intervention or predefined seed terms. Term frequency based partition provides early detection of the top level concepts and provides an additional level of term filtering.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Domínguez García et al. (2012) used wikipedia to extract ontology for different languages.", "mid_sen": "Like Cimiano et al. (2005) , we follow a hybrid approach and construct a concept hierarchy using distributional similarity, patterns and WordNet. ", "after_sen": "However, instead of performing top-down or bottom-up clustering, we pose ontology learning as a k-partite graph construction problem. "}
{"citeStart": 20, "citeEnd": 32, "citeStartToken": 20, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data. have been proposed (Hindle, 1990; Brown et al., 1992; Pereira et al., 1993; Tokunaga et al., 1995) . The realization of such an automatic construction method would make it possible to a) save the cost of constructing a thesaurus by hand, b) do away with subjectivity inherent in a hand made thesaurus, and c) make it easier to adapt a natural language processing system to a new domain. In this paper, we propose a new method for automatic construction of thesauri. Specifically, we view the problem of automatically clustering words as that of estimating a joint distributiofl over the Cartesian product of a partition of a set of nouns (in general, any set of words) and a partition of a set of w:rbs (in general, any set of words), and propose an est.imation *Real World Computing Partership algorithm using simulated annealing with an energy function based on the blinimum Description Length (MDL) Principle. The MDL Principle is a well-motivated and theoretically sound principle for data compression and estimation in information theory and statistics. As a method of sta-tisticM estimation MDL is guaranteed to be near optimal.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recently various methods for automatically constructing a thesaurus (hierarchically clustering words) based on corpus data. ", "mid_sen": "have been proposed (Hindle, 1990; Brown et al., 1992; Pereira et al., 1993; Tokunaga et al., 1995) . ", "after_sen": "The realization of such an automatic construction method would make it possible to a) save the cost of constructing a thesaurus by hand, b) do away with subjectivity inherent in a hand made thesaurus, and c) make it easier to adapt a natural language processing system to a new domain. "}
{"citeStart": 74, "citeEnd": 87, "citeStartToken": 74, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "This topic-based approach is in contrast to Kameyama's ,Japanese version (Kameyama 1985 , Kameyama 1986 of\" tbcus-based spproach to anaphora by Grosz et al. 1983 . In her framewock, subjecthood and predicate deixis play the principal role, and the fact that topic provides the most important clue to anaphora identification in actual spoken Japanese discourse is not utilized explicitly.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Its identification needs ~peech act cal;egorization of sentences.)", "mid_sen": "This topic-based approach is in contrast to Kameyama's ,Japanese version (Kameyama 1985 , Kameyama 1986 of\" tbcus-based spproach to anaphora by Grosz et al. 1983 . ", "after_sen": "In her framewock, subjecthood and predicate deixis play the principal role, and the fact that topic provides the most important clue to anaphora identification in actual spoken Japanese discourse is not utilized explicitly."}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. ", "mid_sen": "Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. ", "after_sen": "They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. "}
{"citeStart": 105, "citeEnd": 131, "citeStartToken": 105, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ( (Turney, 2002; Pang et al., 2002) ). Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999) . Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in arti-cles that are not editorials or reviews) were subjective.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. ", "mid_sen": "Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). ", "after_sen": "In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999) . "}
{"citeStart": 110, "citeEnd": 121, "citeStartToken": 110, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Much research has been done on knowledge acquisition from large-scale annotated corpora as a rich source of linguistic knowledge. Major works done to create English POS taggers (henceforth, \"taggers\"), for example, include (Church 1988) , (Kupiec 1992) , (Brill 1992) and (Voutilainen et al. 1992) . The problem with this framework, however, is that such reliable corpora are hardly available due to a huge amount of the labor-intensive work required. In case of the acquisition of non-core knowledge, such as specific, lexically or domain dependent knowledge, preparation of annotated corpora becomes more serious problem. One viable approach then is to utilize plain text corpora instead, as in (Mikheev 1996) . But The method proposed by (Mikheev 1996) has its own weaknesses, in that it is restricted in scope. That is, it aims to acquire rules for unknown words in corpora from their ending characters without looking at the context. In the meantime, (Brill 1995a ) (Brill 1995b) proposed a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning. The weakness of his method is that the effect of unsupervised learning decreases as the training corpus size increases. The problem in using plain text corpora for knowledge acquisition is that we need a human supervisor who can evaluate and sift the obtained knowledge. An alternative to this would be to use a number of modules of a welldeveloped NLP system which stores most of the highly reliable general rules. Here, one module functions as a supervisor for other modules, since all these modules are designed to work cooperatively and the knowledges stored in each module are correlated. Keeping this idea in mind, we propose a new unsupervised learning method for obtaining linguistic rules from plain text corpora using the existing linguistic knowledge. This method has been implemented in the rule extraction system APRAS (Automatic POS Rule Acquisition System), which automatically acquires rules for refining the morphological analyzer (tagger) in our English-Japanese MT system ASTRANSAC (Hirakawa et al. 1991) through the interaction between the system's tagger and parser on the assumption that they are considerably accurate. This paper is organized as follows: Section 2 illustrates the basic idea of our method; Section 3 gives the outline of APRAS; Sections 4 and 5 describe our experiments.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much research has been done on knowledge acquisition from large-scale annotated corpora as a rich source of linguistic knowledge. ", "mid_sen": "Major works done to create English POS taggers (henceforth, \"taggers\"), for example, include (Church 1988) , (Kupiec 1992) , (Brill 1992) and (Voutilainen et al. 1992) . ", "after_sen": "The problem with this framework, however, is that such reliable corpora are hardly available due to a huge amount of the labor-intensive work required. "}
{"citeStart": 61, "citeEnd": 80, "citeStartToken": 61, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994) . Our unit of analysis is a pair of adjacent sentences (S 1 , S 2 ) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. We enumerate all productions that appear in the syntactic parse of any sentence and exclude those that appear less than 25 times, resulting in a list of 197 unique productions. Then all ordered pairs 2 (p 1 , p 2 ) of productions are formed. For each pair, we compute (p1, p2) and (p2, p1) are considered as different pairs. is significantly (95% confidence level) greater or lesser than the expected value if occurrences of p 1 and p 2 were independent. Of the 38,809 production pairs, we found that 1,168 pairs occurred in consecutive sentences significantly more often than chance and 172 appeared significantly fewer times than expected. In Table 2 we list, grouped in three simple categories, the 25 pairs of the first kind with most significant p-values.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We analyze all co-occurrence patterns rather than just repetitions.", "mid_sen": "We use the gold standard parse trees from the Penn Treebank (Marcus et al., 1994) . ", "after_sen": "Our unit of analysis is a pair of adjacent sentences (S 1 , S 2 ) and we choose to use Section 0 of the corpus which has 99 documents and 1727 sentence pairs. "}
{"citeStart": 83, "citeEnd": 102, "citeStartToken": 83, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "Translation Parameters '1'o bc practical, a model for P(CtIC,) needs to decompose the source and target graphs C~ and Ct into subgraphs small enough that subgraph translation parameters can be estimated. We do this with the help of 'node a.lignment relations' between the nodes of these graphs. 'l'lmse alignment relations are similar in some respects to the alignments used by Brown et al. (1990) in their surface translation model. The translation probability is then the sum of probabilities over different alignments .t: I'(C, ICo) = ~s P(C. flC,).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do this with the help of 'node a.lignment relations' between the nodes of these graphs. ", "mid_sen": "'l'lmse alignment relations are similar in some respects to the alignments used by Brown et al. (1990) in their surface translation model. ", "after_sen": "The translation probability is then the sum of probabilities over different alignments .t: I'(C, ICo) = ~s P(C. flC,)."}
{"citeStart": 96, "citeEnd": 107, "citeStartToken": 96, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "The aims of this paper are to compare implementations of pronoun resolution algorithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution. Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999) , but these have the drawback of being carried out on small corpora. While manual evaluations have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reliable, broadly based statistics. With a system that can run various pronoun resolution algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this study, this ability to alter an algorithm slightly and test its performance is central.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aims of this paper are to compare implementations of pronoun resolution algorithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution. ", "mid_sen": "Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999) , but these have the drawback of being carried out on small corpora. ", "after_sen": "While manual evaluations have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reliable, broadly based statistics. "}
{"citeStart": 66, "citeEnd": 87, "citeStartToken": 66, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Other studies which view IR as a query generation process include Maron and Kuhns, 1960; Hiemstra and Kraaij, 1999; Ponte and Croft, 1998; Miller et al, 1999 . Our work has focused on cross-lingual retrieval.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Other studies which view IR as a query generation process include Maron and Kuhns, 1960; Hiemstra and Kraaij, 1999; Ponte and Croft, 1998; Miller et al, 1999 . ", "after_sen": "Our work has focused on cross-lingual retrieval."}
{"citeStart": 16, "citeEnd": 41, "citeStartToken": 16, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "We use several metrics to evaluate the word classes. First, we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap, and then computing tagging accuracy (Smith and Eisner, 2005; Haghighi and Klein, 2006) . Additionally, we compute the mutual information of the learned clusters with the gold tags, and we compute the cluster F-score (Ghosh, 2003) . See Table 1 for results of the different models, parameter settings, and metrics. Given the variance in the number of classes learned it is a little difficult to interpret these results, but it is clear that the Markov child model is the best; it achieves superior performance to the independent child model on all metrics, while learning fewer word classes. The poor performance of the simultaneous model warrants further investigation, but we observed that the distributions learned by that 8 The advantage of this metric is that it's comprehensible. The disadvantage is that it's easy to inflate by adding classes. model are far more spiked, potentially due to double counting of tags, since the sequence probabilities are already based on the local probabilities. For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. However, they train on less data, and learn fewer word classes.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "model are far more spiked, potentially due to double counting of tags, since the sequence probabilities are already based on the local probabilities. ", "mid_sen": "For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. ", "after_sen": "However, they train on less data, and learn fewer word classes."}
{"citeStart": 161, "citeEnd": 183, "citeStartToken": 161, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "Several approaches have been suggested to account for this behavior. [Litman and Allen, 1987] introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991 ] or Shared Plans [Grosz and Sidner, 1990] . While these accounts do help explain some discourse phenomena more satisfactorily, they still require a strong degree of cooperativity to account for dialogue coherence, and do not provide easy explanations of why an agent might act in cases which do not support high-level mutual goals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "[Litman and Allen, 1987] introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. ", "mid_sen": "Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991 ] or Shared Plans [Grosz and Sidner, 1990] . ", "after_sen": "While these accounts do help explain some discourse phenomena more satisfactorily, they still require a strong degree of cooperativity to account for dialogue coherence, and do not provide easy explanations of why an agent might act in cases which do not support high-level mutual goals."}
{"citeStart": 108, "citeEnd": 120, "citeStartToken": 108, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "Even better accuracy can be achieved with a more fine-grained link class structure. Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992) . If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. In this manner, the model can account for a wider range of translation phenomena.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy . ", "mid_sen": "Another interesting extension is to broaden the definition of a \"word\" to include multi-word lexical units (Smadja, 1992) . ", "after_sen": "If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model. "}
{"citeStart": 237, "citeEnd": 248, "citeStartToken": 237, "citeEndToken": 248, "sectionName": "UNKNOWN SECTION NAME", "string": "Many European languages are of the inflecting type, and occupy still another region of the space of difficulty. They are too complex morphologically to yield easily to the simpler techniques that can work for English. The phonological or orthographic changes involved in affixation may be quite complex, so dimension (a) can be laige, and a feature mechanism may be needed to handle such varied but interrelated morphosyn-tactic phenomena such as umlaut (Trost, 1991) , case, number, gender, and different morphological paradigms. On the other hand, while there may be many different affixes, their possibilities for combination within a word are fairly limited, so dimension (b) is quite manageable.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They are too complex morphologically to yield easily to the simpler techniques that can work for English. ", "mid_sen": "The phonological or orthographic changes involved in affixation may be quite complex, so dimension (a) can be laige, and a feature mechanism may be needed to handle such varied but interrelated morphosyn-tactic phenomena such as umlaut (Trost, 1991) , case, number, gender, and different morphological paradigms. ", "after_sen": "On the other hand, while there may be many different affixes, their possibilities for combination within a word are fairly limited, so dimension (b) is quite manageable."}
{"citeStart": 187, "citeEnd": 207, "citeStartToken": 187, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "We generate a sample of 160 random utterances by varying the parameters in Table 2 with a uniform distribution. This sample is intended to provide enough training material for estimating all 67 parameters for each personality dimension. Following , two expert judges (not the authors) familiar with the Big Five adjectives (Table 1) evaluate the personality of each utterance using the Ten-Item Personality Inventory (TIPI; Gosling et al., 2003) , and also judge the utterance's naturalness. Thus 11 judgments were made for each utterance for a total of 1760 judgments. The TIPI outputs a rating on a scale from 1 (low) to 7 (high) for each Big Five trait. The expert judgments are approximately nor-mally distributed; Figure 1 shows the distribution for agreeableness.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This sample is intended to provide enough training material for estimating all 67 parameters for each personality dimension. ", "mid_sen": "Following , two expert judges (not the authors) familiar with the Big Five adjectives (Table 1) evaluate the personality of each utterance using the Ten-Item Personality Inventory (TIPI; Gosling et al., 2003) , and also judge the utterance's naturalness. ", "after_sen": "Thus 11 judgments were made for each utterance for a total of 1760 judgments. "}
{"citeStart": 35, "citeEnd": 53, "citeStartToken": 35, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "Functional Generative Description (Sgall et al., 1986 ) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary categorial operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Functional Generative Description (Sgall et al., 1986 ) assumes a language-independent underlying order, which is represented as a projective dependency tree. ", "after_sen": "This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. "}
{"citeStart": 79, "citeEnd": 98, "citeStartToken": 79, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we present a learning approach to the full-scale ST task of extracting information from free texts. The task we tackle is considerably more complex than that of (Soderland, 1999; Chieu and Ng, 2002a ), since we need to deal with merging information from multiple sentences to fill one template. We evaluated our learning approach on the MUC-4 task of extracting terrorist events from free texts. We chose the MUC-4 task since manually prepared templates required for training are available. 1 When trained and tested on the official benchmark data of MUC-4, our learning approach achieves accuracy competitive with the best MUC-4 systems, which were all built using manually engineered rules. To our knowledge, our work is the first learning-based approach to have achieved performance competitive with the knowledge-engineering approach on the full-scale ST task.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we present a learning approach to the full-scale ST task of extracting information from free texts. ", "mid_sen": "The task we tackle is considerably more complex than that of (Soderland, 1999; Chieu and Ng, 2002a ), since we need to deal with merging information from multiple sentences to fill one template. ", "after_sen": "We evaluated our learning approach on the MUC-4 task of extracting terrorist events from free texts. "}
{"citeStart": 143, "citeEnd": 156, "citeStartToken": 143, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "A number of questions immediately arise, some of which will hopefully be answered until the time of the workshop. On the theoretical side, this work has argued for a strict separation of precedence and categorial information in LFG (or PSG in general, see (BrSker, 1998a) ). Can these analyses and insights be transferred? On the practical side, can the conversion we sketched be used to create efficient large-scale DGs? Or will the amount of f-structural indeterminacy introduced by our use of functional uncertainty lead to overly long processing? And, last and most challenging, when will the first large treebank with dependency annotation be available, and will it be derived from XLE's fstructure output?", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A number of questions immediately arise, some of which will hopefully be answered until the time of the workshop. ", "mid_sen": "On the theoretical side, this work has argued for a strict separation of precedence and categorial information in LFG (or PSG in general, see (BrSker, 1998a) ). ", "after_sen": "Can these analyses and insights be transferred? "}
{"citeStart": 113, "citeEnd": 115, "citeStartToken": 113, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "This representation of mutual belief differs from the common representation in terms of an iterated eonjunction [11] in that: (1) it relocates information from mental states to the environment in which utteranees occur; (2) it allows one to represent the different kinds of evidence for mutual belief; (3) it controls reasoning when discrepancies in mutual beliefs are discovered since evidence and assumptions can be inspected; (4) it does not consist of an infinite list of statements.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This seems intuitively plausible and means that the strength of belief depends on the strength of underlying assumptions, and that for all inference rules that depend on multiple premises, the strength of an inferred belief is the weakest of the supporting beliefs.", "mid_sen": "This representation of mutual belief differs from the common representation in terms of an iterated eonjunction [11] in that: (1) it relocates information from mental states to the environment in which utteranees occur; (2) it allows one to represent the different kinds of evidence for mutual belief; (3) it controls reasoning when discrepancies in mutual beliefs are discovered since evidence and assumptions can be inspected; (4) it does not consist of an infinite list of statements.", "after_sen": ""}
{"citeStart": 87, "citeEnd": 96, "citeStartToken": 87, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "The language chosen for semantic representation is a flat semantics along the line of (Bos, 1995; Copestake et al., 1999; Copestake et al., 2001 ). However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. Thus the semantic representations we assume are simply set of literals of the form P n (x 1 , . . . , x n ) where P n is a predicate of arity n and x i is either a constant or a unification variable whose value will be instantiated during processing.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, the bottom feature structure encodes information that remains local to the node at which adjunction takes place.", "mid_sen": "The language chosen for semantic representation is a flat semantics along the line of (Bos, 1995; Copestake et al., 1999; Copestake et al., 2001 ). ", "after_sen": "However because we are here focusing on paraphrases rather than fine grained semantic distinctions, the underspecification and the description of the scope relations permitted by these semantics will here be largely ignored and flat semantics will be principally used as a convenient way of describing predicate/arguments and modifiers/modified relationships. "}
{"citeStart": 200, "citeEnd": 212, "citeStartToken": 200, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996) ), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. ", "mid_sen": "Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). ", "after_sen": "These studies represent the context in which an ambiguous word occurs with a wide variety of features. "}
{"citeStart": 113, "citeEnd": 127, "citeStartToken": 113, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(NP (NN lung) (NN cancer) (NNS deaths))", "mid_sen": "CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. ", "after_sen": "However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. "}
{"citeStart": 204, "citeEnd": 220, "citeStartToken": 204, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SE-MAFOR (Das et al., 2010) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f . A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame appears within an element of the second frame. Finally, frame ngram features are a variant of word n-grams. For each word n-gram in the sentence, a frame n-gram feature is created by replacing one or more words in the word n-gram with the name of the frame or the frame element in which the word appears. A detailed description of these features can be found in Hasan and Ng (2013c).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. ", "mid_sen": "Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SE-MAFOR (Das et al., 2010) . ", "after_sen": "Frame-word interaction features encode whether two words appear in different elements of the same frame. "}
{"citeStart": 184, "citeEnd": 194, "citeStartToken": 184, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "Many approaches to cross-lingual IR have been published. One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents to the language of the queries (Gey et al, 1999; Oard, 1998) . For most languages, there are no MT systems at all. Our focus is on languages where no MT exists, but a bilingual dictionary may exist or may be derived.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many approaches to cross-lingual IR have been published. ", "mid_sen": "One common approach is using Machine Translation (MT) to translate the queries to the language of the documents or translate documents to the language of the queries (Gey et al, 1999; Oard, 1998) . ", "after_sen": "For most languages, there are no MT systems at all. "}
{"citeStart": 114, "citeEnd": 149, "citeStartToken": 114, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "(after Lakoff, 1971) is felicitously understood to mean that after a slanderous introduction, Bill retaliated in kind against John. What makes (1) felicitous is that the pitch accents on the pronominals contribute attentional information that cannot be gleaned from text alone. This suggests an attentional component to pitch accents, in addition to the propositional component explicated in Pierrehumbert and Hirschberg (1990) . In this paper, I combine their account of pitch accent semantics with Grosz, Joshi and Weinstein's (1989) account of centering to yield insights into the phenomenon of pitch accented pronominals, and the attentional consequences of pitch accents in general. The relevant claims in PH90 and GJW89 are reviewed in the next two sections.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "What makes (1) felicitous is that the pitch accents on the pronominals contribute attentional information that cannot be gleaned from text alone. ", "mid_sen": "This suggests an attentional component to pitch accents, in addition to the propositional component explicated in Pierrehumbert and Hirschberg (1990) . ", "after_sen": "In this paper, I combine their account of pitch accent semantics with Grosz, Joshi and Weinstein's (1989) account of centering to yield insights into the phenomenon of pitch accented pronominals, and the attentional consequences of pitch accents in general. "}
{"citeStart": 19, "citeEnd": 33, "citeStartToken": 19, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. Moreover, although Schabes (1992) and others have proposed 'lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The close connection between sense and subcategorization and between subject domain and sense makes it likely that a fully accurate 'static' subcategorization dictionary of a language is unattainable in any case. ", "mid_sen": "Moreover, although Schabes (1992) and others have proposed 'lexicalized' probabilistic grammars to improve the accuracy of parse ranking, no wide-coverage parser has yet been constructed incorporating probabilities of different subcategorizations for individual predicates, because of the problems of accurately estimating them.", "after_sen": "These problems suggest that automatic construction or updating of subcategorization dictionaries from textual corpora is a more promising avenue to pursue. "}
{"citeStart": 99, "citeEnd": 124, "citeStartToken": 99, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "The joint n-gram model is language-independent. An aligned corpus with words and their pronunciations is needed, but no further adaptation is required. Table 1 shows the performance of our model in comparison to alternative approaches on the German and English versions of the CELEX corpus, the English NetTalk corpus, the English Teacher's Word Book (TWB) corpus, the English beep corpus and the French Brulex corpus. The joint n-gram model performs significantly better than the decision tree (essentially based on (Lucassen and Mercer, 1984) ), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005) . For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. Automatic syllabification for our model integrated phonological constraints (as described in section 3.1), and therefore led to an improvement in phoneme accuracy, while the word error rate increased for the PbA approach, which does not incorporate such constraints. (Chen, 2003) also used a joint n-gram model. The two approaches differ in that Chen ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 1 shows the performance of our model in comparison to alternative approaches on the German and English versions of the CELEX corpus, the English NetTalk corpus, the English Teacher's Word Book (TWB) corpus, the English beep corpus and the French Brulex corpus. ", "mid_sen": "The joint n-gram model performs significantly better than the decision tree (essentially based on (Lucassen and Mercer, 1984) ), and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm (Marchand and Damper, 2005) . ", "after_sen": "For the Nettalk data, we also compared the influence of syllable boundary annotation from a) automatically learnt and b) manually annotated syllabification information on phoneme accuracy. "}
{"citeStart": 95, "citeEnd": 109, "citeStartToken": 95, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model); and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given tbe sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word. while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency. For an introduction to the algorithms, see Cutting et al. (1992) , or the lucid description by Sharman (1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency. ", "mid_sen": "For an introduction to the algorithms, see Cutting et al. (1992) , or the lucid description by Sharman (1990) .", "after_sen": "There are two principal sources for the parameters of the model. "}
{"citeStart": 95, "citeEnd": 105, "citeStartToken": 95, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "This section deals with morphosyntactic errors which are independent of the two-level analysis. The data described below was obtained from Daniel Ponsford (personal communication), based on (Wehr, 1971) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This section deals with morphosyntactic errors which are independent of the two-level analysis. ", "mid_sen": "The data described below was obtained from Daniel Ponsford (personal communication), based on (Wehr, 1971) .", "after_sen": "Recall that a Semitic stems consists of a root morpheme and a vocalism morpheme arranged according to a canonical pattern morpheme. "}
{"citeStart": 141, "citeEnd": 157, "citeStartToken": 141, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "Word Sense Disambiguation (WSD) is wellknown as one of the more difficult problems in the field of natural language processing, as noted in (Gale et al, 1992; Kilgarriff, 1997; Ide and Véronis, 1998) , and others. The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms. To address the last problem, (Gale et al, 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. The lower bound should not drop below the baseline usage of the algorithm (in which every word that is disambiguated is assigned the most frequent sense) whereas the upper bound should not be too restrictive\" when the word in question is hard to disambiguate even for human judges (a measure of this difficulty is the computation of the agreement rates between human annotators). Identification and formalization of the determining contextual parameters for a word used in a given sense is the focus of WSD work that treats texts in a monolingual setting-that is, a setting where translations of the texts in other languages either do not exist or are not considered. This focus is based on the assumption that for a given word w and two of its contexts C 1 and C 2 , if C 1 ≡ C 2 (are perfectly equivalent), then w is used with the same sense in C 1 and C . A formalized definition of context for a given sense would then enable a WSD system to accurately assign sense labels to occurrences of w in unseen texts. Attempts to characterize context for a given sense of a word have addressed a variety of factors:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Word Sense Disambiguation (WSD) is wellknown as one of the more difficult problems in the field of natural language processing, as noted in (Gale et al, 1992; Kilgarriff, 1997; Ide and Véronis, 1998) , and others. ", "after_sen": "The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms. "}
{"citeStart": 148, "citeEnd": 169, "citeStartToken": 148, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to overcome these issues, we explore the application of direct search methods (Wright, 1995) to SMT. To do this, we integrate the decoder and the evaluation metric inside the objective function, which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences. Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Nelder and Mead, 1965 ) and Powell's method (Powell, 1964) , which generally handle such difficult search conditions relatively well. This approach confers several benefits over MERT. First, the function being optimized is the true error rate. Second, integrating the decoder inside the objective function forces the optimizer to account for possible search errors. Third, contrary to MERT, our approach does not require input parameters to be those of a linear model, so our approach can tune a broader range of features, including non-linear and hidden-state parameters (e.g., distortion limit, beam size, and weight vector applied to future cost estimates).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To do this, we integrate the decoder and the evaluation metric inside the objective function, which takes source sentences and a set of weights as inputs, and outputs the evaluation score (e.g., BLEU score) computed on the decoded sentences. ", "mid_sen": "Since it is impractical to calculate derivatives of this function, we use derivative-free optimization methods such as the downhill simplex method (Nelder and Mead, 1965 ) and Powell's method (Powell, 1964) , which generally handle such difficult search conditions relatively well. ", "after_sen": "This approach confers several benefits over MERT. "}
{"citeStart": 22, "citeEnd": 34, "citeStartToken": 22, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "The schema in (5) makes the arguments available in higher types, and allows lower (NPn) types only if higher types fail (as in NP2 in (8)). There are two reasons for this: Higher types carry more information about surface order of the language, and they are sufficient to cover bounded phenomena. §3 shows how higher types correctly derive the PAS in various word orders. Lower types are indispensable for unbounded constructions such as relativization and coordination. The choice is due to a concern for economy. If lower types were allowed freely, they would yield the correct PAS as well: 4 (1996) . Grammar rewriting can be done using predictive combinators (Wittenburg, 1987) , but they cannot handle crossing compositions that are essential to our method. Other normal form parsers, e.g. that of Hepple and Morrill (1989) , have the same problem. All grammar rules in (4) in fact check the labels of the constituent categories, which show how the category is derived. The labels are as in (Eisner, 1996) . -FC: Output of forward composition, of which forward crossing composition is a special case. -BC: Output of backward composition, of which backward crossing composition is a special case. -OT: Lexical or type shifted category. The goal is to block e.g., X/Y-FC Y/Z-{FC, BC, OT} =~B> X/Z and X/Y-FC Y-{FC, BC, OT} =~A> X in (10a). S/TV composition would have the label -FC, which cannot be an input to forward application. In (10b), the backward composition follows through since it has the category-label S/TV-BC, which the forward application rule does not block. We use Eisner's method to rewrite all rules in (4).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All grammar rules in (4) in fact check the labels of the constituent categories, which show how the category is derived. ", "mid_sen": "The labels are as in (Eisner, 1996) . ", "after_sen": "-FC: Output of forward composition, of which forward crossing composition is a special case. "}
{"citeStart": 97, "citeEnd": 106, "citeStartToken": 97, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "We seek to overcome these difficulties by generating TOEFL-like tests automatically from Word-Net (Fellbaum, 1998) . While WordNet has been used before to evaluate corpus-analytic approaches to lexical similarity (Lin, 1998) , the metric proposed in that study, while useful for comparative purposes, lacks an intuitive interpretation. In contrast, we emulate the TOEFL using WordNet and inherit the TOEFL's easy interpretability.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We seek to overcome these difficulties by generating TOEFL-like tests automatically from Word-Net (Fellbaum, 1998) . ", "mid_sen": "While WordNet has been used before to evaluate corpus-analytic approaches to lexical similarity (Lin, 1998) , the metric proposed in that study, while useful for comparative purposes, lacks an intuitive interpretation. ", "after_sen": "In contrast, we emulate the TOEFL using WordNet and inherit the TOEFL's easy interpretability."}
{"citeStart": 207, "citeEnd": 224, "citeStartToken": 207, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas (Doan et al. 2002) . Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999) , (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003) . Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993) , or require human-annotated training data with relation information for each domain (Craven et al. 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas (Doan et al. 2002) . ", "mid_sen": "Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999) , (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003) . ", "after_sen": "Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993) , or require human-annotated training data with relation information for each domain (Craven et al. 1998) ."}
{"citeStart": 93, "citeEnd": 104, "citeStartToken": 93, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994) . In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991) . Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time. However, in the context of unification-based parsing, left-recursive grammars have the formal power of a Turing machine, therefore detection of all infinite loops due to left-recursion is impossible (Shieber, 1992) . So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994) . ", "mid_sen": "In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991) . ", "after_sen": "Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time. "}
{"citeStart": 0, "citeEnd": 22, "citeStartToken": 0, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "For the problem with multi-sentence discourses, and the \"threads\" that sentences continue, we use an implementation of tempo-rM centering (Kameyama et al., 1993; Poesio, 1994 ). This is a technique similar to the type of centering used for nominal anaphora (Sidner, 1983; Grosz et al., 1983) . Centering assumes that discourse understanding requires some notion of \"aboutness.\" While nominal centering assumes there is one object that the current discourse is \"about,\" temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread. Kameyama et al. (1993) confirmed these preferences when testing their ideas on the Brown corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Centering assumes that discourse understanding requires some notion of \"aboutness.\" While nominal centering assumes there is one object that the current discourse is \"about,\" temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread. ", "mid_sen": "Kameyama et al. (1993) confirmed these preferences when testing their ideas on the Brown corpus.", "after_sen": "As an example of how the temporal centering preference techniques can reduce ambiguity, recall example (9) and the possible continuations shown in (10). "}
{"citeStart": 21, "citeEnd": 41, "citeStartToken": 21, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "However, different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997) , on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal communication), this is fine for information retrieval. Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. ", "mid_sen": "The SPARKLE project (Carroll et al., 1997) , on the other hand, does not differentiate between these types of modifiers. ", "after_sen": "As has been mentioned by John Carroll (personal communication), this is fine for information retrieval. "}
{"citeStart": 152, "citeEnd": 170, "citeStartToken": 152, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "1. Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values (Brown et al., 1990; Dagan et al., 1993; Chen, 1996) . To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. Conversely, the threshold can be lowered to buy more coverage at the cost of a larger model that will converge more slowly.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Discard all likelihood scores for word types deemed unlikely to be mutual translations, i.e. all L(u,v) < 1. This step significantly reduces the computational burden of the algorithm. ", "mid_sen": "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values (Brown et al., 1990; Dagan et al., 1993; Chen, 1996) . ", "after_sen": "To retain word type pairs that are at least twice as likely to be mutual translations than not, the threshold can be raised to 2. "}
{"citeStart": 165, "citeEnd": 176, "citeStartToken": 165, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such as Czech (Veselá et al., 2004) . Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999) , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997) . Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005) , but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, most formal results on non-projectivity are discouraging: ", "mid_sen": "While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999) , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997) . ", "after_sen": "Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005) , but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006) ."}
{"citeStart": 65, "citeEnd": 77, "citeStartToken": 65, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Coreference is an inherently complex phenomenon. Some of the limitations of the traditional rulebased approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Coreference is an inherently complex phenomenon. ", "mid_sen": "Some of the limitations of the traditional rulebased approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.", "after_sen": "This task will promote the development of linguistic resources -annotated corpora -and machine-learning techniques oriented to coreference resolution. "}
{"citeStart": 85, "citeEnd": 103, "citeStartToken": 85, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Additionally, supervised methods suffer from the \"knowledge acquisition bottleneck\" (Gale et al., 1992a) . (Ng, 1997b) estimates that the manual annotation effort necessary to build a broad coverage semantically annotated English corpus is about 16 personyears. This overhead for supervision could be much greater if a costly tuning procedure is required before applying any existing system to each new domain.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to corroborate the previous hypotheses, this paper explores the portability and tuning of four different ML algorithms (previously applied to WSD) by training and testing them on different corpora.", "mid_sen": "Additionally, supervised methods suffer from the \"knowledge acquisition bottleneck\" (Gale et al., 1992a) . ", "after_sen": "(Ng, 1997b) estimates that the manual annotation effort necessary to build a broad coverage semantically annotated English corpus is about 16 personyears. "}
{"citeStart": 85, "citeEnd": 107, "citeStartToken": 85, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "Our work is an extension of the continuous relevance annotation model put forward in Lavrenko et al. (2003) . Unlike other unsupervised approaches where a set of latent variables is introduced, each defining a joint distribution on the space of keywords and image features, the relevance model captures the joint probability of images and annotated words directly, without requiring an intermediate clustering stage. This model is a good point of departure for our task for several reasons, both theoretical and empirical. Firstly, expectations are computed over every single point in the training set and therefore parameters can be estimated without EM. Indeed, Lavrenko et al. achieve competitive performance with latent variable models. Secondly, the generation of feature vectors is modeled directly, so there is no need for quantization. Thirdly, as we show below the model can be easily extended to incorporate information outside the image and its keywords.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our work is an extension of the continuous relevance annotation model put forward in Lavrenko et al. (2003) . ", "after_sen": "Unlike other unsupervised approaches where a set of latent variables is introduced, each defining a joint distribution on the space of keywords and image features, the relevance model captures the joint probability of images and annotated words directly, without requiring an intermediate clustering stage. "}
{"citeStart": 49, "citeEnd": 61, "citeStartToken": 49, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been some attempts to capture the behavior of semantic categories in a distributional setting, despite the unavailability of sense-annotated corpora. For example, Hearst and Schtltze (1993) take steps toward a distributional treatment of WordNet-based classes, using Schtltze's (1993) approach to constructing vector representations from a large co-occurrence matrix. Yarowsky's (1992) algorithm for sense disambiguation can be thought of as a way of determining how Roget's thesaurus categories behave with respect to contextual features. And my own treatment of selectional constraints (Resnik, 1993) provides a way to describe the plausibility of co-occuffence in terms of WordNet's semantic categories, using co-occurrence relationships mediated by syntactic structure. In each case, one begins with known semantic categories (WordNet synsets, Roget's numbered classes) and non-sense-annotated text, and proceeds to a distributional characterization of semantic category behavior using co-occurrence relationships.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Yarowsky's (1992) algorithm for sense disambiguation can be thought of as a way of determining how Roget's thesaurus categories behave with respect to contextual features. ", "mid_sen": "And my own treatment of selectional constraints (Resnik, 1993) provides a way to describe the plausibility of co-occuffence in terms of WordNet's semantic categories, using co-occurrence relationships mediated by syntactic structure. ", "after_sen": "In each case, one begins with known semantic categories (WordNet synsets, Roget's numbered classes) and non-sense-annotated text, and proceeds to a distributional characterization of semantic category behavior using co-occurrence relationships."}
{"citeStart": 27, "citeEnd": 40, "citeStartToken": 27, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "It is a remarkable fact that humans can often understand sentences containing unknown words, infer their grammatical properties and incrementally refine hypotheses about these words when encountering later instances. In contrast, many current NLP systems still presuppose a complete lexicon. Notable exceptions include Zernik (1989) , Erbach (1990) , Hastings & Lytinen (1994) . See Zernik for an introduction to the general issues involved.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, many current NLP systems still presuppose a complete lexicon. ", "mid_sen": "Notable exceptions include Zernik (1989) , Erbach (1990) , Hastings & Lytinen (1994) . ", "after_sen": "See Zernik for an introduction to the general issues involved."}
{"citeStart": 187, "citeEnd": 201, "citeStartToken": 187, "citeEndToken": 201, "sectionName": "UNKNOWN SECTION NAME", "string": "to detransform the parse trees produced using the PCFG estimated from trees transformed by T . By calculating the labelled precision and recall scores for the detransformed trees in the usual manner, we can systematically compare the parsing accuracy of di erent kinds of stochastic generalized left-corner parsers. Table 5 presents the results of this comparison. As reported previously, the standard left-corner grammar embeds su cient non-local information in its productions to signi cantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransformed trees Manning and Carpenter, 1997; Roark and Johnson, 1999 . Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which t o d escribe this non-local information. There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy Johnson, 1998b . Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak 1997 and Collins 1997 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which t o d escribe this non-local information. ", "mid_sen": "There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy Johnson, 1998b . ", "after_sen": "Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak 1997 and Collins 1997 ."}
{"citeStart": 53, "citeEnd": 72, "citeStartToken": 53, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper presents a maximum entropy approach to the NER task, where NER not only made use of local context within a sentence, but also made use of other occurrences of each word within the same document to extract useful features (global features). Such global features enhance the performance of NER (Chieu and Ng, 2002b) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper presents a maximum entropy approach to the NER task, where NER not only made use of local context within a sentence, but also made use of other occurrences of each word within the same document to extract useful features (global features). ", "mid_sen": "Such global features enhance the performance of NER (Chieu and Ng, 2002b) .", "after_sen": ""}
{"citeStart": 67, "citeEnd": 88, "citeStartToken": 67, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Following White et al. (2007) , we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2-21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005) , the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates a word trigram model with a trigram model that chains a POS model with a supertag model, where the POS model conditions on the previous two POS tags, and the supertag model conditions on the previous two POS tags as well as the current one.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2-21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. ", "mid_sen": "While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005) , the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). ", "after_sen": "The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. "}
{"citeStart": 89, "citeEnd": 102, "citeStartToken": 89, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the original KL distance is asymmetric and is not defined when zero frequency occurs. Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991) , which introducing a probabilistic variable m, or α -Skew Divergence (Lee, 1999) , by adopting adjustable variable α. Research shows that Skew Divergence achieves better performance than other measures. (Lee, 2001) To convert distance to similarity value, we adopt the formula inspired by Mochihashi, and Matsumoto 2002.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Due to the original KL distance is asymmetric and is not defined when zero frequency occurs. ", "mid_sen": "Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991) , which introducing a probabilistic variable m, or α -Skew Divergence (Lee, 1999) , by adopting adjustable variable α. Research shows that Skew Divergence achieves better performance than other measures. ", "after_sen": "(Lee, 2001) To convert distance to similarity value, we adopt the formula inspired by Mochihashi, and Matsumoto 2002."}
{"citeStart": 82, "citeEnd": 105, "citeStartToken": 82, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "In the group-and-abstract paradigm, working with feature sets of a word, as in De Pauw and Wagacha (2007) , is an ingenious generalization that holds numerous advantages over string edit distances. Feature set comparisons are naturally defined over arbitrary collections, whereas string edit distances work on pairs of strings. Many morphologically related words differ in several characters and are therefore not particularly close in edit distance. Features instead of edit distances provide a neat framework, based on global properties of the feature distribution, of capturing the fact that some character mismatches do not really matter, whereas some character matches (although not necessarily long) are very significant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The recent increased interest in Bayesian generative models in general in NLP may possibly serve as a catalyst.", "mid_sen": "In the group-and-abstract paradigm, working with feature sets of a word, as in De Pauw and Wagacha (2007) , is an ingenious generalization that holds numerous advantages over string edit distances. ", "after_sen": "Feature set comparisons are naturally defined over arbitrary collections, whereas string edit distances work on pairs of strings. "}
{"citeStart": 279, "citeEnd": 301, "citeStartToken": 279, "citeEndToken": 301, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated our approaches for word-phrase semantic relatedness on the SemEval task of evaluating phrasal semantics, and more specifically on the sub-task of evaluating the semantic similarity between words and phrases. The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative. To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) , in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one. The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: If the semantic relatedness of the word-phrase is over 61% then the similarity is positive, otherwise it is negative. So for the example Interview -Formal meeting, which resulted in a semantic relatedness of 66.7% in the semantic network approach, it will be classified positively by the generated rule. This method was our first submitted test run to this task, which resulted in a recall of 63.79%, a precision of 91.01%, and an F-measure of 75.00% on the testing set.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The task provided an English dataset of 15,628 word-phrases, 60% annotated for training and 40% for testing, with the goal of classifying each word-phrase as either positive or negative. ", "mid_sen": "To transform the semantic relatedness measure to a semantic similarity classification one, we first calculated the semantic relatedness of each word-phrase in the training set, and used JRip, WEKA's (Witten et al., 1999) implementation of Cohen's RIPPER rule learning algorithm (Cohen and Singer, 1999) , in order to learn a set of rules that can differentiate between a positive semantic similarity and a negative one. ", "after_sen": "The classifier resulted in rules for the semantic network model based relatedness that could be summarized as follows: "}
{"citeStart": 0, "citeEnd": 19, "citeStartToken": 0, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "ing. Ferro et al. (1999) and Buchholz et al. (1999) both describe learning systems to find GRs. The former (TR) uses transformation-based error-driven learning (Brill and Resnik, 1994) and the latter (MB) uses memory-based learning . In addition, there are other differences. The TR system includes several types of information not used in the MB system (some because memory-based systems have a harder time handling set-valued attributes): possible syntactic (Comlex) and semantic (Wordnet) classes of a chunk headword, the stem(s) and named-entity category (e.g., person, location), if any, of a chunk headword, lexemes in a chunk besides the headword, pp-attachment estimate and certain verb chunk properties (e.g., passive, infinitive).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ing. ", "mid_sen": "Ferro et al. (1999) and Buchholz et al. (1999) both describe learning systems to find GRs. ", "after_sen": "The former (TR) uses transformation-based error-driven learning (Brill and Resnik, 1994) and the latter (MB) uses memory-based learning . "}
{"citeStart": 64, "citeEnd": 81, "citeStartToken": 64, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent advances in recognizing spontaneous speech with repairs (Hale et al., 2006; Johnson and Charniak, 2004) have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. For instance, in the repair . . . a flight to Boston, uh, I mean, to Denver on Friday . . . , there is an exact match of the word 'to' between reparandum and repair, and a part of speech match between the words 'Boston' and 'Denver'.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The alteration contains the words that are meant to replace the words in the reparandum.", "mid_sen": "Recent advances in recognizing spontaneous speech with repairs (Hale et al., 2006; Johnson and Charniak, 2004) have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above. ", "after_sen": "One salient aspect of structure is the fact that there is often a good deal of overlap in words between the reparandum and the alteration, as speakers may trace back several words when restarting after an error. "}
{"citeStart": 9, "citeEnd": 23, "citeStartToken": 9, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "The stochastic approach, on the other hand, has the potential to overcome these difficulties. Because it. induces stochastic rules to maximize overall performance against training data, it not only adapts to any application domain but. also may avoid overfitting to the data. In the late 80s and early 90s, the induction and parameter estimation of probabilistic context free grammars (PCFGs) from corpora were intensively studied. Because these grammars comprise only nonterminal and part-of-speech tag symbols, their performances were not enough to be used in practical applications (Charniak, 1993) . A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993) . The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice. The other type of highprecision parser, which is based on dependency analysis was introduced by Collins (Collins, 1996) . Dependency analysis first segments a sentence into syntactically meaningful sequences of words and then considers the modification of each segment. Collins' parser computes the likelihood that each segment modifies the other (2 term relation) by using large corpora. These modification probabilities are conditioned by head words of two segments, distance between the two segments and other syntactic features. Although these two parsers have shown similar performance, the keys of their success are slightly different. SPATTER parser performance greatly depends on the feature selection ability of the decision tree algorithm rather than its linguistic representation. On the other hand, dependency analysis plays an essential role in Collins' parser for efficiently extracting information from corpora.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. ", "mid_sen": "SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. ", "after_sen": "The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993) . "}
{"citeStart": 231, "citeEnd": 243, "citeStartToken": 231, "citeEndToken": 243, "sectionName": "UNKNOWN SECTION NAME", "string": "The word in focus is first passed through a twolevel morphological analysis stage, based on an adaption of (Pulman, 1991) . Two purposes are served here: checking the word is lexica] (i.e. in the lexicon or a permissible inflection of a word in the lexicon) and collecting the possible categories, which are represented as sets of feature specifications (Grover, 1993) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The word in focus is first passed through a twolevel morphological analysis stage, based on an adaption of (Pulman, 1991) . ", "mid_sen": "Two purposes are served here: checking the word is lexica] (i.e. in the lexicon or a permissible inflection of a word in the lexicon) and collecting the possible categories, which are represented as sets of feature specifications (Grover, 1993) .", "after_sen": "This morphological lookup operates over a character trie which has been compressed into a (directed) graph. "}
{"citeStart": 175, "citeEnd": 194, "citeStartToken": 175, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence, and before the end of phrasal constituents (e.g. Marslen-Wilson 1973 , Tanenhaus et al. 1990 ). There is also recent evidence suggesting that, during speech processing, partial interpretations can be built extremely rapidly, even before words are completed (Spivey-Knowlton et al. 1994) 1. There are also potential computational applications for incremental interpretation, including early parse filtering using statistics based on logical form plausibility, and interpretation of fragments of dialogues (a survey is provided by Milward and Cooper, 1994 , henceforth referred to as M&:C).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "There is a large body of psycholinguistic evidence which suggests that meaning can be extracted before the end of a sentence, and before the end of phrasal constituents (e.g. Marslen-Wilson 1973 , Tanenhaus et al. 1990 ). ", "after_sen": "There is also recent evidence suggesting that, during speech processing, partial interpretations can be built extremely rapidly, even before words are completed (Spivey-Knowlton et al. 1994) 1. There are also potential computational applications for incremental interpretation, including early parse filtering using statistics based on logical form plausibility, and interpretation of fragments of dialogues (a survey is provided by Milward and Cooper, 1994 , henceforth referred to as M&:C)."}
{"citeStart": 238, "citeEnd": 252, "citeStartToken": 238, "citeEndToken": 252, "sectionName": "UNKNOWN SECTION NAME", "string": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies. Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models. 2", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies. ", "mid_sen": "Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models. ", "after_sen": "2"}
{"citeStart": 210, "citeEnd": 224, "citeStartToken": 210, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996) , a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998) . The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? What is the effect of cascading? Will errors at a lower level percolate to higher modules?", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. ", "mid_sen": "Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996) , a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998) . ", "after_sen": "The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? "}
{"citeStart": 99, "citeEnd": 121, "citeStartToken": 99, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. ", "mid_sen": "TAGs have been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies.", "after_sen": "Consistency of probabilistic TAGs has practical significance for the following reasons:"}
{"citeStart": 130, "citeEnd": 156, "citeStartToken": 130, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; Tür et al., 2001) . Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; Tür et al., 2001 ) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997) .", "mid_sen": "In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; Tür et al., 2001) . ", "after_sen": "Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. "}
{"citeStart": 96, "citeEnd": 114, "citeStartToken": 96, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "To evaluate our coreference-chain extraction method we compare it with a cue-phrases technique (Nanba et al., 2004) and two baselines. Baseline 1 extracts only the c-site anchor sentence as the c-site; baseline 2 includes sentences before/after the c-site anchor sentence as part of the c-site with a 50/50 probability -it tosses a coin for each consecutive sentence to decide its inclusion. We also created two hybrid meth-ods that combine the results of the cue-phrases and coreference-chain techniques, one the union of their results (includes the extracted sentences of both methods), and the other the intersection (includes sentences only for which both methods agree), to measure their mutual compatibility.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To evaluate our coreference-chain extraction method we compare it with a cue-phrases technique (Nanba et al., 2004) and two baselines. ", "after_sen": "Baseline 1 extracts only the c-site anchor sentence as the c-site; baseline 2 includes sentences before/after the c-site anchor sentence as part of the c-site with a 50/50 probability -it tosses a coin for each consecutive sentence to decide its inclusion. "}
{"citeStart": 17, "citeEnd": 38, "citeStartToken": 17, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "Multi-document summarization has been well studied, and a couple of systems have been developed. We test LexRank (Erkan and Radev, 2004) , a state-of-the-art summarization system and find a sharp drop of ROUGE-2, from 0.3894 on news to 0.2871 on tweets. This can be largely attributed to the short and noise prone nature of tweets, which causes a single tweet to be insufficient to provide reliable information to compute its salience score. We develop a graph-based summarization system that aggregates social signals, i.e., re-tweeted times and follower numbers to handle this challenge. More specifically, the translation probability from one tweet to the other depends on both the similarity between the two tweets and the social network features associated with the second tweet. This largely differentiates our system from existing studies, such as the work of Sharifi et al. (2010) , which uses only tweet-level content features (e.g., keywords) to select representative sentences.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Multi-document summarization has been well studied, and a couple of systems have been developed. ", "mid_sen": "We test LexRank (Erkan and Radev, 2004) , a state-of-the-art summarization system and find a sharp drop of ROUGE-2, from 0.3894 on news to 0.2871 on tweets. ", "after_sen": "This can be largely attributed to the short and noise prone nature of tweets, which causes a single tweet to be insufficient to provide reliable information to compute its salience score. "}
{"citeStart": 42, "citeEnd": 53, "citeStartToken": 42, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "The UG used in our experiments consists of 700 lexical entries and 60 rules. We used a variant of inside-outside training to estimate a model of UG derivations. It is a rule bigram model similar to PCFG with special extensions for UG type operations. The probability of future unifications is made dependent from the result type of earlier unifications. The model is described in more detail in (Weber 1994a; Weber 1995) ; it is very similar to (Brew 1995) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The probability of future unifications is made dependent from the result type of earlier unifications. ", "mid_sen": "The model is described in more detail in (Weber 1994a; Weber 1995) ; it is very similar to (Brew 1995) .", "after_sen": ""}
{"citeStart": 362, "citeEnd": 378, "citeStartToken": 362, "citeEndToken": 378, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . ", "after_sen": "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. "}
{"citeStart": 6, "citeEnd": 24, "citeStartToken": 6, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "The technique originally used for aligning texts was to link regions of texts according to regularity of word cooccurrences across texts [Catizone et al., 1989] . Pairs of words were linked if they have similar distributions in their home texts. This strategy doesn't always work well because in many languages a good writer does not use the exact same word many times in a text. Similarly, a good translator does not always translate a word exactly the same way every time it occurs. Clearly this algorithm is heavily text dependent. For texts with limited vocabularies this might work extremely well, but in \"free\" text it falls. Currently we are experimenting with assorted algorithms; a major problem is having good test texts to run them on. So far the best results on reasonable text come from the Gale-Church algorithm . It has been tested on English, German, French, Czech, and Italian parallel texts. The Gale-Church algorithm relies on the length of regions, where the character is the unit of measurement. (For details see their paper.) We have experienced three problems with this method. First, the implementation of the algorithm published in Church-Gale severely limits the size of the input file . This is, however, merely an implementation problem. Second, there is no way to set \"anchor points\" and align around them. That is, one cannot pick two anchor points, one in each text, and have the program align the corresponding regions above and below the anchor points. (See [Brown et al., 1991] for discussion of an alternative.) This is not necessarily a problem either, and can be worked around. Lastly, it does not give usable results on texts which are not absolutely parallel. That is to say, on texts which do not have exactly the same number of large regions, with the same hierarchical structure. A single extra line of characters in one text will cause a complete failure of the alignment algorithm. This is a major difficulty.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "That is, one cannot pick two anchor points, one in each text, and have the program align the corresponding regions above and below the anchor points. ", "mid_sen": "(See [Brown et al., 1991] for discussion of an alternative.) This is not necessarily a problem either, and can be worked around. ", "after_sen": "Lastly, it does not give usable results on texts which are not absolutely parallel. "}
{"citeStart": 116, "citeEnd": 144, "citeStartToken": 116, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005) . While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005) , which convert the inputs into logical forms and then attempt to 'prove' H from T plus a set of axioms. For instance, (Braz et al., 2005) represents T , H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin-ear program derived from that representation. These approaches and related ones (e.g., (Van Durme and Schubert, 2008)) use highly expressive representations, enabling them to express negation, temporal information, and more. HOLMES's representation is much simpler-Markov Logic Horn Clauses for inference rules coupled with a massive database of ground assertions. However, this simplification allows HOLMES to tackle a \"text\" of enormously larger size: 117 million Web pages versus a single paragraph. A second, if smaller, difference stems from the fact that instead of determining whether a single hypothesis sentence, H, follows from the text, HOLMES tries to find all consequents that match a conjunctive query.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Textual Entailment systems are given two textual fragments, text T and hypothesis H, and attempt to decide if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005) . ", "mid_sen": "While many approaches have addressed this problem, our work is most closely related to that of (Raina et al., 2005; MacCartney and Manning, 2007; Tatu and Moldovan, 2006; Braz et al., 2005) , which convert the inputs into logical forms and then attempt to 'prove' H from T plus a set of axioms. ", "after_sen": "For instance, (Braz et al., 2005) represents T , H, and a set of rewrite rules in a description logic framework, and determines entailment by solving an integer lin-ear program derived from that representation. "}
{"citeStart": 82, "citeEnd": 98, "citeStartToken": 82, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "The evaluation picture would not be accurate even if we compared anaphora resolution systems on the basis of the same data since the pre-processing errors which would be carried over to the systems' outputs might vary. As a way forward we have proposed the idea of the evaluation workbench (Mitkov, 2000b ) -an open-ended architecture which allows the incorporation of different algorithms and their comparison on the basis of the same pre-processing tools and the same data. Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \"knowledge-poor philosophy\": Kennedy and Boguraev's (1996) parser-free algorithm, Baldwin's (1997) CogNiac and Mitkov's (1998b) knowledge-poor approach.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common \"knowledge-poor philosophy\": ", "mid_sen": "Kennedy and Boguraev's (1996) parser-free algorithm, Baldwin's (1997) CogNiac and Mitkov's (1998b) knowledge-poor approach.", "after_sen": ""}
{"citeStart": 141, "citeEnd": 161, "citeStartToken": 141, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we have shown how a CRF-based approach for opinion target extraction performs in a single-and cross-domain setting. We have presented a comparative evaluation of our approach on datasets from four different domains. In the single-domain setting, our CRF-based approach outperforms a supervised baseline on all four datasets. Our error analysis indicates that additional features, which can capture opinions in more complex sentences, are required to improve the performance of the opinion target extraction. Our CRF-based approach also yields promising results in the crossdomain setting. The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (Blitzer et al., 2007; Jiang and Zhai, 2007) , perform in comparison to our approach. Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. We observe similar challenges as Choi et al. (2005) regarding the analysis of complex sentences. Although our data is user-generated from Web 2.0 communities, a manual inspection has shown that the documents were of relatively high textual quality. It is to investigate to which extent the approaches taken in the analysis of newswire, such as identifying targets with coreference resolution, can also be applied to our task on user-generated discourse.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The features we employ scale well across domains, given that the opinion target vocabularies are substantially different. ", "mid_sen": "For future work, we might investigate how machine learning algorithms, which are specifically designed for the problem of domain adaptation (Blitzer et al., 2007; Jiang and Zhai, 2007) , perform in comparison to our approach. ", "after_sen": "Since three of the features we employed in our CRF-based approach are based on the respective opinion expressions, it is to investigate how to mitigate the possible negative effects introduced by errors in the opinion expression identification if they are not annotated in the gold standard. "}
{"citeStart": 43, "citeEnd": 60, "citeStartToken": 43, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By repeating the above processes, it can create an accurate classifier for word translation disambiguation.", "mid_sen": "For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999) .", "after_sen": ""}
{"citeStart": 371, "citeEnd": 400, "citeStartToken": 371, "citeEndToken": 400, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of the NLU systems developed in the 70's indu(le(l a kind of error recovery Inechanisln ranging flom the treatment only of spelling e.rrors, PARRY (1)arkinson c 't al., 1977) , to tile inclusion also of incomplete int)ut containing some kind of ellipsis, LAD-DEll,/LIFEll (Hendrix et al., 1977) .", "mid_sen": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . ", "after_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. "}
{"citeStart": 52, "citeEnd": 75, "citeStartToken": 52, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous semantic role classifiers always did the classification problem in one-step. However, in this paper, we did SRC in two steps. The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. be found in figure 2, which is similar with that in Moschitti et al. (2005) . As what has been shown in figure 2, a semantic role will first be determined whether it is a numbered argument or an ARGM by a binary-category classifier. And, then if the semantic role is a numbered argument, it will be determined by a 5category classifier designed for ARGX, i.e. the numbered arguments. If it is an ARGM, the functional tag will be assigned by a 17-category classifier built for ARGMs. Accordingly, with this hierarchical architecture, the SRC problem is divided into 3 sub tasks, each of which has an independent classifier.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The architectures of hierarchical semantic role classifiers can 2 Extra features e.g. predicate may be still useful because that the information, provided by the high-level description of selfdescriptive features, e.g. phrase type, are limited. ", "mid_sen": "be found in figure 2, which is similar with that in Moschitti et al. (2005) . ", "after_sen": "As what has been shown in figure 2, a semantic role will first be determined whether it is a numbered argument or an ARGM by a binary-category classifier. "}
{"citeStart": 143, "citeEnd": 174, "citeStartToken": 143, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "Note: In our translation from English to logic we are assuming that \"it\" is anaphoric (with the pronoun following the element that it refers to), not cataphoric (the other way around). This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3. This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This means that the \"it\" that brought the disease in P1 will not be considered to refer to the infection \"i\" or the death \"d\" in P3. ", "mid_sen": "This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329) .", "after_sen": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. "}
{"citeStart": 33, "citeEnd": 52, "citeStartToken": 33, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "For German, there are several resources with derivational information. We use version 1.3 of DERIVBASE (Zeller et al., 2013) , 1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage compared to CELEX (Baayen et al., 1996) and IMSLEX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For German, there are several resources with derivational information. ", "mid_sen": "We use version 1.3 of DERIVBASE (Zeller et al., 2013) , 1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families. ", "after_sen": "It has a precision of 84% and a recall of 71%. "}
{"citeStart": 361, "citeEnd": 378, "citeStartToken": 361, "citeEndToken": 378, "sectionName": "UNKNOWN SECTION NAME", "string": "Sense disambiguation methods require a decision model that evaluates the relevant statistics. Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works. These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991) ; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper). At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others. 21 Yet, these decision models can be characterized by several criteria, which clarify the similarities and differences between them. As will be explained below, many of the differences are correlated with the different information sources employed by these models.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sense disambiguation thus resembles many other decision tasks, and not surprisingly, several common decision algorithms were employed in different works. ", "mid_sen": "These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991) ; and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper). ", "after_sen": "At the present stage of research on sense disambiguation, it is difficult to judge whether a certain decision algorithm is significantly superior to others. "}
{"citeStart": 113, "citeEnd": 126, "citeStartToken": 113, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "For purposes of simplicity and because oh the whole is it likely that words will contain no more than one error (Damerau, 1964; Pollock and Zamora, 1983) , normal 'no error' analysis usually resumes if an error rule succeeds. The exception occurs with a vowel shift error ( §3.2.1). If this error rule succeeds, an expectation of further shifted vowels is set up, but no other error rule is allowed in the subsequent partitions. For this reason rules are marked as to whether they can occur more than once.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If no error rules succeed, or lead to a successful partition of the word, analysis backtracks to try the error rules at successively earlier points in the word.", "mid_sen": "For purposes of simplicity and because oh the whole is it likely that words will contain no more than one error (Damerau, 1964; Pollock and Zamora, 1983) , normal 'no error' analysis usually resumes if an error rule succeeds. ", "after_sen": "The exception occurs with a vowel shift error ( §3.2.1). "}
{"citeStart": 151, "citeEnd": 165, "citeStartToken": 151, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "At one time the Gaelic languag~ group was spoken throughout Ireland, from where it spread to the Isle of Man and to much of Scotland. Currently fully native use of Gaelic is limited to a few discontiguous areas in the westernmost reaches of Ireland and Scotland. In the case of Ireland, everyone agrees that Gaelic is nowadays found in three main dialects: that of Ulster, that of Connacht, and that of Munster (0 Siadhail, 1989) . But several questions are raised that are less easily answered. Do the three provinces separate out so neatly for intrinsic linguistic reasons, or simply because their speakers have become so widely separated from each other geographically as speakers in intervening areas have adopted English? Does the language of Connacht naturally group with that of Ulster or with that of Munster? And looking beyond Ireland, many have commented that the language of Ulster in general is similar to that of Scotland. Are Irish, Manx, and Scottish Gaelic considered three separate languages for intrinsic linguistic reasons, or because they are spoken in different countries? To a large extent, dialectologists have found these questions difficult to answer because they accepted Paris's conundrum. For 6 Siadhail, the ultimate scientific justification ill adopting the three-dialect account is the fact that the Gaeltacht (Irish-speaking territory) is so fragmented nowadays that it no longer forms a continuum. O Cuiv (1951:4-49) felt that there can be no dialect boundaries because transitions are gradual. Elsie (1986:240 ) considers a dialect to be an area where all communities are linguistically more similar to each other than any community is to any site outside the dialect. Such notions provide a very firm, absolute notion of dialecthood: a set of communities either constitutes a dialect area, or it does not. But as the dialectometrists have shown, other notions of clustering are equally scientific and may more accurately correspond to intuitive notions of what it means to be a dialect.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Currently fully native use of Gaelic is limited to a few discontiguous areas in the westernmost reaches of Ireland and Scotland. ", "mid_sen": "In the case of Ireland, everyone agrees that Gaelic is nowadays found in three main dialects: that of Ulster, that of Connacht, and that of Munster (0 Siadhail, 1989) . ", "after_sen": "But several questions are raised that are less easily answered. "}
{"citeStart": 106, "citeEnd": 125, "citeStartToken": 106, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Three of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253) . Therein, the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable. It is reproduced here for reference:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I will therefore briefly describe these algorithms.", "mid_sen": "Three of the algorithms use what I will call the ADJACENCY MODEL, an analysis procedure that goes back to Marcus (1980, p253) . ", "after_sen": "Therein, the procedure is stated in terms of calls to an oracle which can determine if a noun compound is acceptable. "}
{"citeStart": 155, "citeEnd": 168, "citeStartToken": 155, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "Text is automatically tagged using the first modules of the CLAWS program (1985 version) , in which words are allocated one or more tags from 134 classes (Garside, 1987) . These 134 tags are then mapped onto the small customised tagsets. Tag disambiguation is part of the parsing task, handled by the neural net and its pre-processor. This version of CLAWS has a dictionary of about 6,300 words only. Other words are tagged using suffix information, or else defaults are invoked. The correct tag is almost always included in the set allocated, but more tags than necessary are often proposed. A larger dictionary in later versions will address this problem.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus the pair \"most <adjective>\" is taken as a single superlative adjective.", "mid_sen": "Text is automatically tagged using the first modules of the CLAWS program (1985 version) , in which words are allocated one or more tags from 134 classes (Garside, 1987) . ", "after_sen": "These 134 tags are then mapped onto the small customised tagsets. "}
{"citeStart": 47, "citeEnd": 67, "citeStartToken": 47, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "To test the performance of the combined task on automatic parse trees, we employ two different configurations. First, we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak's reranking parser (Charniak and Johnson, 2005) , and test them on section 23 with automatic parse trees. This is the same configuration as reported in (Pradhan et al., 2005; Jiang and Ng, 2006) . The scores are presented in the fourth row auto parse (t&t) in Table 3 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak's reranking parser (Charniak and Johnson, 2005) , and test them on section 23 with automatic parse trees. ", "mid_sen": "This is the same configuration as reported in (Pradhan et al., 2005; Jiang and Ng, 2006) . ", "after_sen": "The scores are presented in the fourth row auto parse (t&t) in Table 3 ."}
{"citeStart": 85, "citeEnd": 94, "citeStartToken": 85, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "The concept of a domain, however, is not precisely defined across existing domain adaptation methods. Different domains typically correspond to different subcorpora, in which documents exhibit a particular combination of genre and topic, and optionally other textual characteristics such as dialect and register. This definition, however, has two major shortcomings. First, subcorpusbased domains depend on provenance information, which might not be available, or on manual grouping of documents into subcorpora, which is labor intensive and often carried out according to arbitrary criteria. Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006) . While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002) , most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009) ), making it unclear whether the proposed solutions address topic or genre differences.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006) . ", "mid_sen": "While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002) , most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009) ), making it unclear whether the proposed solutions address topic or genre differences.", "after_sen": "In this work, we follow text classification literature for definitions of the concepts topic and genre. "}
{"citeStart": 316, "citeEnd": 331, "citeStartToken": 316, "citeEndToken": 331, "sectionName": "UNKNOWN SECTION NAME", "string": "Else if Predlct(.bel, ..bel.u-evid + .bel.s-attack) = -,.bel (i.e., the system's evidence against .bel will cause the user to reject _bel), .bel.focus ~--. McKeown's focusing rules suggest that continuing a newly introduced topic (about which there is more to be said) is preferable to returning to a previous topic OVIcKcown, 1985) . Thus the algorithm first considers whether or not attacking the user's support for ..bel is sufficient to convince him of--,-bel (step 4.2). It does so by gathering (in cand-set) evidence proposed by the user as direct support for _bel but which was not accepted by the system and which the system predicts it can successfully refute (i.e., =beli.focus is not nil). The algorithm then hypothesizes that the user has changed his mind about each belief in cand-set and predicts how this will affect the user's belief about .bel (step 4.2). If the user is predicted to accept --,..bel under this hypothesis, the algorithm invokes Select-Min-Set to select a minimum subset of cand-set as the unaccepted beliefs that it would actually pursue, and the focus of modification (..bel.focus) will be the union of the focus for each of the beliefs in this minimum subset.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Else if Predlct(.bel, ..bel.u-evid + .bel.s-attack) = -,.bel (i.e., the system's evidence against .bel will cause the user to reject _bel), .bel.focus ~--. McKeown's focusing rules suggest that continuing a newly introduced topic (about which there is more to be said) is preferable to returning to a previous topic OVIcKcown, 1985) . ", "after_sen": "Thus the algorithm first considers whether or not attacking the user's support for ..bel is sufficient to convince him of--,-bel (step 4.2). "}
{"citeStart": 5, "citeEnd": 24, "citeStartToken": 5, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Like Rooth et al. (1999) we evaluate selectional preference induction approaches in a pseudodisambiguation task. In a test set of pairs (r p , w), each headword w is paired with a confounder w chosen randomly from the BNC according to its frequency . Noun headwords are paired with noun confounders in order not to disadvantage Resnik's model, which only works with nouns. The headword/confounder pairs are only computed once and reused in all crossvalidation runs. The task is to choose the more likely role headword from the pair (w, w ).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Like Rooth et al. (1999) we evaluate selectional preference induction approaches in a pseudodisambiguation task. ", "after_sen": "In a test set of pairs (r p , w), each headword w is paired with a confounder w chosen randomly from the BNC according to its frequency . "}
{"citeStart": 306, "citeEnd": 326, "citeStartToken": 306, "citeEndToken": 326, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems unlikely that these continuously variable aspcct:s of fluent natural language can be captured by a purely combinatoric model. This naturally leads to the qtwstion of how best to introduce quantitative modeli,g into language processing. It is not, of course, nec-,,ssary for the quantities of a quantitative model to be probabilities. For example, we may wish to define realvalued functions on parse trees that reflect the extent to which the trees conform to, .say, minimal attachment and parallelism between conjuncts. Such functions have been used in tandem with statistical functions in experiments on disambiguation (for instance Alshawi and (',a.rter 1994) . Another example is connection strengths i, m~ural network approaches to language processing, th,mgh it. has been shown that certain networks are ~,tfectively computing probabilities (Richard and Lippmann 1991) . Nevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing. The case f.r probability theory is strengthened by a well devel-,,p-d empirical methodology in the form of statistical I,:~ramet.ccr estimation. There is also the strong connecl i,,n between probability theory and the formal theory .1\" i.formation and communication, a connection that has been exploited in speech recognition, for example I~qing tim concept of entropy to provide a motivated way ,.f measuring the complexity of a recognition problem (.h'lim'k et ai. 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The case f.r probability theory is strengthened by a well devel-,,p-d empirical methodology in the form of statistical I,:~ramet.ccr estimation. ", "mid_sen": "There is also the strong connecl i,,n between probability theory and the formal theory .1\" i.formation and communication, a connection that has been exploited in speech recognition, for example I~qing tim concept of entropy to provide a motivated way ,.f measuring the complexity of a recognition problem (.h'lim'k et ai. 1992) .", "after_sen": "I\",v,'n if probability t|wory remains, as it currently is, th,, m~.l.llod of clloicc in making language processing qu.ntitative, this still h~aw:s the fieht wide open in terms .,f carving up languag~ processing into an appropriate set ,,f ,,wmts tbr probability theory to work with. "}
{"citeStart": 30, "citeEnd": 41, "citeStartToken": 30, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "To estimate the inter-annotator agreement, we picked 400 sentences from the training/testing dataset and assigned them to two different annotators. We use the Kappa coefficient (Cohen, 1968) to measure the agreement. The Kappa coefficient is defined as follows:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To estimate the inter-annotator agreement, we picked 400 sentences from the training/testing dataset and assigned them to two different annotators. ", "mid_sen": "We use the Kappa coefficient (Cohen, 1968) to measure the agreement. ", "after_sen": "The Kappa coefficient is defined as follows:"}
{"citeStart": 82, "citeEnd": 97, "citeStartToken": 82, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "The concept of packed forest has been previously used in translation rule extraction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007) . However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest. Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial. The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007) . The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007) . ", "mid_sen": "The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below). ", "after_sen": "This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding."}
{"citeStart": 64, "citeEnd": 89, "citeStartToken": 64, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The way I'ERFORMANCI! is defined reflects the litct that agents m'e me,'mt to collaborate on the tlksk. The costs that ~u'e deducted liom tbe RAW SCORE are the costs for both agents' communication, inference, m~d retrieval. Thus PERFORMANCE is a measnre of LEAS'I\" COLI,ABORATIVE EFFORT [Cl~u'k and Scbaeler, 1989; Brennan, 19901 . Since the par~uneters for cognitive ef-lk~rt iu'e fixed while discom'se strategy and AWM settings iue vltried, we can directly test the benefits of different discourse strategies under different assumptions ~d)ont cognitive effort and Ihe cognitive demands of the task. This is impossible to do with corpus imldysis alone.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The costs that ~u'e deducted liom tbe RAW SCORE are the costs for both agents' communication, inference, m~d retrieval. ", "mid_sen": "Thus PERFORMANCE is a measnre of LEAS'I\" COLI,ABORATIVE EFFORT [Cl~u'k and Scbaeler, 1989; Brennan, 19901 . ", "after_sen": "Since the par~uneters for cognitive ef-lk~rt iu'e fixed while discom'se strategy and AWM settings iue vltried, we can directly test the benefits of different discourse strategies under different assumptions ~d)ont cognitive effort and Ihe cognitive demands of the task. "}
{"citeStart": 73, "citeEnd": 99, "citeStartToken": 73, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "3 An HPSG implementation of a discourse grammar Following Scha ~ Polanyi (1988) To allow the above-mentioned types of information to mutually constrain each other, we employ a hierarchy of rhetorical and temporal relations (illustrated in Figure 1 ), using the ALE system in such a way that clues such as tense and cue words work together to reduce the number of possible temporal structures. This approach improves upon earlier work on discourse structure such as (Lascarides and Asher, 1991) and (Kehler, 1994) in reducing the number of possible ambiguities; it is also more precise than the Kamp/Hinrichs/Partee approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities, which behave differently in narrative progression.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "3 An HPSG implementation of a discourse grammar Following Scha ~ Polanyi (1988) To allow the above-mentioned types of information to mutually constrain each other, we employ a hierarchy of rhetorical and temporal relations (illustrated in Figure 1 ), using the ALE system in such a way that clues such as tense and cue words work together to reduce the number of possible temporal structures. ", "mid_sen": "This approach improves upon earlier work on discourse structure such as (Lascarides and Asher, 1991) and (Kehler, 1994) in reducing the number of possible ambiguities; it is also more precise than the Kamp/Hinrichs/Partee approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities, which behave differently in narrative progression.", "after_sen": "Tense, aspect, rhetorical relations and temporal expressions affect the value of the RHET..RELN type that expresses the relationship between two I)CVs: cue words are lexicMly marked according to what rhetorical relation they specify, and this rel.ation is passed on to the DCU. "}
{"citeStart": 132, "citeEnd": 153, "citeStartToken": 132, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003) , Shen et al. (2003) . However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001) . UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS. For example, for the phrase immediate systemic anticoagulants, MetaMap identifies immediate as a TEMPORAL CONCEPT, systemic as a FUNCTIONAL CONCEPT, and anticoagulants as a PHARMACOLOGIC SUBSTANCE. More than one semantic category in UMLS may correspond to MED-ICATION or DISEASE. For example, either a PHAR-MACOLOGIC SUBSTANCE or a THERAPEUTIC OR PREVENTIVE PROCEDURE can be a MEDICATION; either a DISEASE OR SYNDROME or a PATHOLOGIC FUNCTION can be a DISEASE.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. ", "mid_sen": "To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001) . UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. ", "after_sen": "Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS. "}
{"citeStart": 70, "citeEnd": 87, "citeStartToken": 70, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Therefore, we should rely on a semantic context alternative to the frame (Giuglea and Moschitti, 2004) . Such context should have a wide coverage and should be easily derivable from FN data. A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998 ) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such context should have a wide coverage and should be easily derivable from FN data. ", "mid_sen": "A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al., 1998 ) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al., 2000) .", "after_sen": "In this paper we have investigated the above claim by designing a semi-automatic algorithm that assigns ILCs to FN verb predicates and by carrying out several semantic role labeling (SRL) experiments in which we replace the frame with the ILC information. "}
{"citeStart": 154, "citeEnd": 167, "citeStartToken": 154, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "(1 ((((he:l PPHS1)) (VSUBCAT NP_PP) ((attribute:6 VVD)) ((failure:8 NN1)) ((PSUBCAT SING) ((to:9 II)) ((no<blank>one:lO PN)) ((buy: The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977) , by Chomsky-adjunction to maximal projections of adjuncts (XP --* XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within X1 projections; X1 ~ X0 Argl... ArgN). Furthermore, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar--the proportion of sentences for which at least one analysis is found--is 79% when applied to the Susanne corpus (Sampson, 1995) , a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. ", "mid_sen": "Currently, the coverage of this grammar--the proportion of sentences for which at least one analysis is found--is 79% when applied to the Susanne corpus (Sampson, 1995) , a 138K word treebanked and balanced subset of the Brown corpus. ", "after_sen": "Wide coverage is important since information is acquired only from successful parses. "}
{"citeStart": 104, "citeEnd": 128, "citeStartToken": 104, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Another test that both makes this assumption and uses a form of equation 1 is a test for binomial data (Harnett, 1982, Sec. 8.11 ) which uses the \"fact\" that binomial distributions tend to approximate normal distributions. In this test, the x i 's being compared are the fraction of the items of interest that are recovered by the ith technique. In this test, the denominator s d of equation 1 also has a complicated form, both due to the reasons mentioned for the t test above and to the fact that with a binomial distribution, the standard deviation is a function of the number of samples and the mean value.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2 , the form we had earlier for using the independence assumption.", "mid_sen": "Another test that both makes this assumption and uses a form of equation 1 is a test for binomial data (Harnett, 1982, Sec. 8.11 ) which uses the \"fact\" that binomial distributions tend to approximate normal distributions. ", "after_sen": "In this test, the x i 's being compared are the fraction of the items of interest that are recovered by the ith technique. "}
{"citeStart": 272, "citeEnd": 286, "citeStartToken": 272, "citeEndToken": 286, "sectionName": "UNKNOWN SECTION NAME", "string": "The training of our probabilistic CFG proceeds in three steps: (i) unlexicalized training with the supar parser, (ii) bootstrapping a lexicalized model from the trained unlexicalized one with the ultra parser, and finally (iii) lexicalized training with the hypar parser (Carroll, 1997b) . Each of the three parsers uses the insideoutside algorithm, supar and ultra use an unlexicalized weighting of trees, while hypar uses a lexicalized weighting of trees, ultra and hypar both collect frequencies for lexicalized rule and lexical choice events, while supar collects only unlexicalized rule frequencies.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The training of our probabilistic CFG proceeds in three steps: (i) unlexicalized training with the supar parser, (ii) bootstrapping a lexicalized model from the trained unlexicalized one with the ultra parser, and finally (iii) lexicalized training with the hypar parser (Carroll, 1997b) . ", "after_sen": "Each of the three parsers uses the insideoutside algorithm, supar and ultra use an unlexicalized weighting of trees, while hypar uses a lexicalized weighting of trees, ultra and hypar both collect frequencies for lexicalized rule and lexical choice events, while supar collects only unlexicalized rule frequencies."}
{"citeStart": 87, "citeEnd": 105, "citeStartToken": 87, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "The implementation described in this paper is based on the most recent model, that of (Gorrell (in press) ). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980) , Marcus et al (1983) ), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987 (Abney ( , 1989 , Pritehett (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that \"the parser attempts incrementally to satisfy the principles of grammar\". For the purposes of this implementation, I have interpreted this to mean that each word must be attached into a fullyconnected phrase marker as it is found in the input. 1 The psychological desirability of such a 1In fact, GorreU conjectures that, where there is insufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue & Fodor (1991) , Frazier (1987) ). Such models have also been explored computationally (Milward (1995) , Crocker (1991) ).", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) ).", "mid_sen": "The implementation described in this paper is based on the most recent model, that of (Gorrell (in press) ). ", "after_sen": "This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980) , Marcus et al (1983) ), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987 (Abney ( , 1989 , Pritehett (1992)). "}
{"citeStart": 0, "citeEnd": 28, "citeStartToken": 0, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of nonlocal context and dynamic adaptation have been studied in language modeling for speech recognition. Kuhn and de Mori (1998) proposed a cache model that works as a kind of short-term memory by which the probability of the most recent n words is increased over the probability of a general-purpose bigram or trigram model. Within certain limits, such a model can adapt itself to changes in word frequencies, depending on the topic of the text passage. The DCA system is similar in spirit to such dynamic adaptation: it applies word n-grams collected on the fly from the document under processing and favors them more highly than the default assignment based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted for not propagating it from one document for processing of another. For handling very long documents with our method, however, the information decay strategy seems to be the right way to proceed. Mani and MacMillan (1995) pointed out that little attention had been paid in the named-entity recognition field to the discourse properties of proper names. They proposed that proper names be viewed as linguistic expressions whose interpretation often depends on the discourse context, advocating text-driven processing rather than reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal discourse context and does not heavily rely on pre-existing word lists. It has been applied not only to the identification of proper names, as described in this article, but also to their classification (Mikheev, Grover, and Moens 1998) . Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit only one sense in a document or discourse (\"one sense per discourse\"). Since then this idea has been applied to several tasks, including word sense disambiguation (Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999) . Gale, Church, and Yarowsky's observation is also used in our DCA, especially for the identification of abbreviations. In capitalized-word disambiguation, however, we use this assumption with caution and first apply strategies that rely not just on single words but on words together with their local contexts (n-grams). This is similar to \"one sense per collocation\" idea of Yarowsky (1993) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But unlike the cache model, it uses a multipass strategy. ", "mid_sen": "Clarkson and Robinson (1997) developed a way of incorporating standard n-grams into the cache model, using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word's last occurrence. ", "after_sen": "In our experiments we applied simple linear interpolation to incorporate the DCA system into a POS tagger. "}
{"citeStart": 110, "citeEnd": 120, "citeStartToken": 110, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "To prove that our method is effective, we also make a comparison between the performances of our system and , Xue (2008) . Xue (2008) is the best SRL system until now and it has the same data setting with ours. The results are presented in Table 6 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So the ARGX sub task is improved.", "mid_sen": "To prove that our method is effective, we also make a comparison between the performances of our system and , Xue (2008) . ", "after_sen": "Xue (2008) is the best SRL system until now and it has the same data setting with ours. "}
{"citeStart": 118, "citeEnd": 160, "citeStartToken": 118, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Here we compared two discourse strategies: Allhnplicit ~ul(l Explicit-Warr~mh Explicit-W;uranl is a type of discourse stralegy called ~m Attelition strategy in [W;dkel; 1993] because its main lunction is to manipulate agents' altenlional slate. Elsewhere we show that (1) some IRU strategies are only beneficial when inferential complexity is higher Ihall in the Standard \"l,u,~k [R~unbow and Walker, 1994; Walker, 1994al;  (2) IRUs that make intL'rences explicit can help inlbrence limited agents perlorm as well as logic;ally omniscient ones I Walker, 199311.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here we compared two discourse strategies: Allhnplicit ~ul(l Explicit-Warr~mh Explicit-W;uranl is a type of discourse stralegy called ~m Attelition strategy in [W;dkel; 1993] because its main lunction is to manipulate agents' altenlional slate. ", "mid_sen": "Elsewhere we show that (1) some IRU strategies are only beneficial when inferential complexity is higher Ihall in the Standard \"l,u,~k [R~unbow and Walker, 1994; Walker, 1994al;  (2) IRUs that make intL'rences explicit can help inlbrence limited agents perlorm as well as logic;ally omniscient ones I Walker, 199311.", "after_sen": "Although much work remains to be done, there is reason to believe that these results are (Iomsdn independent. "}
{"citeStart": 30, "citeEnd": 56, "citeStartToken": 30, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "To ensure that our coding scheme leads to less biased annotation than some of the other resources available for building summarisation systems, and to ensure that other researchers besides ourselves can use it to replicate our results on different types of texts, we wanted to examine two properties of our scheme: stability and reproducibility (Krippendorff, 1980) . Stability is the extent to which an annotator will produce the same classifications at different times. Reproducibility is the extent to which different annotators will produce the same classification. We use the Kappa coefficient (Siegel and Castellan, 1988) to measure stability and reproducibility. The rationale for using Kappa is explained in (Carletta, 1996) . The studies used to evaluate stability and reproducibility we describe in more detail in (Teufel et al., To Appear) . In brief, 48 papers were annotated by three extensively trained annotators. The training period was four weeks consisting of 5 hours of annotation per week. There were written instructions (guidelines) of 17 pages. Skim-reading and annotation of an average length (3800 word) paper typically took 20-30 minutes. The studies show that the training material is reliable. In particular, the basic annotation scheme is stable (K=.82, .81, .76; N=1220; k=2 for all three annotators) and reproducible (K=.71, N=4261, k=3), where k denotes the number of annotators, N the number of sentences annotated, and K gives the Kappa value. The full annotation scheme is stable (K=.83, .79, .81; N=1248; k-2 for all three annotators) and reproducible (K=.78, N=4031, k=3). Overall, reproducibility and stability for trained annotators does not quite reach the levels found for, for instance, the best dialogue act coding schemes, which typically reach Kappa values of around K=.80 (Carletta et al., 1997; Jurafsky et al., 1997) . Our annotation requires more subjective judgements and is possibly more cognitively complex. Our reproducibility and stability results are in the range which Krippendorff (1980) describes as giving marginally significant results for reasonable size data sets when correlating two coded variables which would show a clear correlation if there were perfect agreement. As our requirements are less stringent than Krippendorff's, we find the level of agreement which we achieved acceptable. Figure 3 , which gives the overall distribution of categories, shows that OWN is by far the most frequent category. Figure 4 reports how well the four non-basic categories could be distinguished from all other categories, measured by Krippendorff's diagnostics for category distinctions (i.e. collapsing all other distinctions). When compared to the overall reproducibility of .71, we notice that the annotators were good at distinguishing AIM and TEX-TUAL, and less good at determining BASIS and CON-TRAST. This might have to do with the location of those types of sentences in the paper: AIM and TEX-TUAL are usually found at the beginning or end of the introduction section, whereas CONTRAST, and even more so BASIS, are usually interspersed within longer stretches of OWN. As a result, these categories are more exposed to lapses of attention during annotation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Reproducibility is the extent to which different annotators will produce the same classification. ", "mid_sen": "We use the Kappa coefficient (Siegel and Castellan, 1988) to measure stability and reproducibility. ", "after_sen": "The rationale for using Kappa is explained in (Carletta, 1996) . "}
{"citeStart": 306, "citeEnd": 320, "citeStartToken": 306, "citeEndToken": 320, "sectionName": "UNKNOWN SECTION NAME", "string": "The stochastic model generating dependency trees is very similar to other statistical dependency models, e.g., to that of Alshawi, 1996. Formulating it using Gorn's notation and the L and F variables, though, is concise, elegant and novel. Nothing prevents conditioning the random variables on arbitrary portions of the partial tree generated this far, using, e.g., maximum-entropy or decision-tree models to extract relevant features of it; there is no di erence in principle between our model and history-based parsing, see Black et al., 1993; Magerman, 1995. The proposed treatment of string realization through the use of the S and M variables is also both truly novel and important. While phrase-structure grammars overemphasize word order by making the processes generating the S variables deterministic, Tesni ere treats string realization as a secondary issue. We nd a middle ground by using stochastic processes to generate the S and M variables, thus reinstating word order as a parameter of equal importance as, say, lexical collocation statistics. It is however not elevated to the hard-constraint status it enjoys in phrase-structure grammars.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Formulating it using Gorn's notation and the L and F variables, though, is concise, elegant and novel. ", "mid_sen": "Nothing prevents conditioning the random variables on arbitrary portions of the partial tree generated this far, using, e.g., maximum-entropy or decision-tree models to extract relevant features of it; there is no di erence in principle between our model and history-based parsing, see Black et al., 1993; Magerman, 1995. ", "after_sen": "The proposed treatment of string realization through the use of the S and M variables is also both truly novel and important. "}
{"citeStart": 50, "citeEnd": 66, "citeStartToken": 50, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "In the experiments reported below, two settings are considered. In the first one, denoted NEWSCO, Moses was trained only on a small data set taken from the News Commentary corpus. Using a small sized corpus reduces both training time and decoding time, which allows us to quickly test different configurations of the decoder. In a second setting, denoted EUROPARL, Moses was trained on a larger corpora containing the entirety of the Europarl Corpus, but no in-domain data, to provide results on more realistic conditions. Statistics regarding the different corpora used are reported in Table 1 . These statistics are computed on the lowercase cleaned corpora. Finding the oracle alignment amounts to solving the ILP problems introduced above. Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible. In our experiments, we used the free solver SCIP (Achterberg, 2007) . An optimal solution was found for all problems we considered. Decoding the 3, 027 sentences of WMT'09 test set takes about 10 minutes (wall time) for the NEWSCO setting, and several hours for the EUROPARL setting 13 . Table 2 reports, for all considered settings, the BLEU-4 scores 14 achieved by our oracle decoder, as well as the number of source words used to generate the oracle hypothesis and the number of target words that are reachable. In these experiments, two objective functions were considered: first, we only consider the objective function corresponding to the relaxed problem defined by Eq. (6); second, we introduced an extra term in the objective to penalize distortion, as described by Eq. (7). Unless explicitly stated otherwise, we always used the exact match strategy.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Even though ILP problems are NP-hard in general, there exist several off-theshelf ILP solvers able to efficiently find an optimal solution or decide that the problem is infeasible. ", "mid_sen": "In our experiments, we used the free solver SCIP (Achterberg, 2007) . ", "after_sen": "An optimal solution was found for all problems we considered. "}
{"citeStart": 92, "citeEnd": 113, "citeStartToken": 92, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Word Sense Disambiguation (WSD) is concerned with the identification of the meaning of ambiguous words in context. For example, among the possible senses of the verb \"run\" are \"to move fast by using one's feet\" and \"to direct or control\". WSD can be useful for many applications, including information retrieval, information extraction and machine translation. Sense ambiguity has been recognized as one of the most important obstacles to successful language understanding since the early 1960's and many techniques have been proposed to solve the problem. Recent approaches focus on the use of various lexical resources and corpus-based techniques in order to avoid the substantial effort required to codify linguistic knowledge. These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al., 2004 for an overview of state-ofthe-art systems). However, current approaches rely on limited knowledge representation and modeling techniques: traditional machine learning algorithms and attribute-value vectors to represent disambiguation instances. This has made it difficult to exploit deep knowledge sources in the generation of the disambiguation models, that is, knowledge that goes beyond simple features extracted directly from the corpus, like bags-of-words and collocations, or provided by shallow natural language tools like part-of-speech taggers.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recent approaches focus on the use of various lexical resources and corpus-based techniques in order to avoid the substantial effort required to codify linguistic knowledge. ", "mid_sen": "These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al., 2004 for an overview of state-ofthe-art systems). ", "after_sen": "However, current approaches rely on limited knowledge representation and modeling techniques: traditional machine learning algorithms and attribute-value vectors to represent disambiguation instances. "}
{"citeStart": 111, "citeEnd": 132, "citeStartToken": 111, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Baldridge observed is that, cross-linguistically, grammars prefer simpler syntactic structures when possible, and that due to the natural correspondence of categories and syntactic structure, biasing toward simpler categories encourages simpler structures. In previous work, we were able to incorporate this preference into a Bayesian parsing model, biasing PCFG productions toward sim-pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015) . Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger (Garrette et al., 2014) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. ", "mid_sen": "We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger (Garrette et al., 2014) .", "after_sen": "In this paper, we present a novel parsing model that is designed specifically for the capacity to capture both of these universal, intrinsic properties of CCG. "}
{"citeStart": 130, "citeEnd": 140, "citeStartToken": 130, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "As we are using the conceptual graph formalism to represent our definitions, we can use the graph matching operations defined in (Sowa, 1984) . The t, wo operations we will need are the maximal common subgraph algorithm and the maximal join algorithm.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A rich hierarchical structure between the set of relations is essential to the graph matching operations we use for the integration phase.", "mid_sen": "As we are using the conceptual graph formalism to represent our definitions, we can use the graph matching operations defined in (Sowa, 1984) . ", "after_sen": "The t, wo operations we will need are the maximal common subgraph algorithm and the maximal join algorithm."}
{"citeStart": 84, "citeEnd": 98, "citeStartToken": 84, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": ". The context windows do not cut across sentence boundaries. Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992) . We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The context windows do not cut across sentence boundaries. ", "mid_sen": "Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992) . ", "after_sen": "We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category."}
{"citeStart": 134, "citeEnd": 146, "citeStartToken": 134, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "In terms of computational experimentation, work by Thomas et al. (2006) , predicting yes and no votes in corpus of United States Congressional floor debate speeches, is quite relevant. They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congressional debates, e.g. the fact that the same individual rarely gives one speech in favor of a bill and another opposing it. We have extend their method to use OPUS features in the SVM and obtained significant improvements over their classification accuracy (Greene, 2007; Greene and Resnik, in preparation) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congressional debates, e.g. the fact that the same individual rarely gives one speech in favor of a bill and another opposing it. ", "mid_sen": "We have extend their method to use OPUS features in the SVM and obtained significant improvements over their classification accuracy (Greene, 2007; Greene and Resnik, in preparation) .", "after_sen": ""}
{"citeStart": 121, "citeEnd": 141, "citeStartToken": 121, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007) . Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. ", "mid_sen": "Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007) . ", "after_sen": "Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. "}
{"citeStart": 1, "citeEnd": 30, "citeStartToken": 1, "citeEndToken": 30, "sectionName": "UNKNOWN SECTION NAME", "string": "There are a large number of strategies that may be used to incorporate obligations into the deliberative process, based on how much weight they are given compared to the agents goals. [Conte and Castelfranchi, 1993] present several strategies of moving from obligations to actions, including: automatically performing an obligated action, adopting all obligations as goals, or adopting an obligated action as a goal only when performing the action results in a state desired by the agent. In the latter cases, these goals still might conflict with other goals of the agent, and so are not guaranteed to be performed.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are a large number of strategies that may be used to incorporate obligations into the deliberative process, based on how much weight they are given compared to the agents goals. ", "mid_sen": "[Conte and Castelfranchi, 1993] present several strategies of moving from obligations to actions, including: automatically performing an obligated action, adopting all obligations as goals, or adopting an obligated action as a goal only when performing the action results in a state desired by the agent. ", "after_sen": "In the latter cases, these goals still might conflict with other goals of the agent, and so are not guaranteed to be performed."}
{"citeStart": 307, "citeEnd": 322, "citeStartToken": 307, "citeEndToken": 322, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense in VP-ellipsis illustrates how categories can be put to work. In (24) I enjoyed it. And so will you the ellipsis is contained within a form expression whose category is vp_ellipsis It ense=inf ,modalffivill ,perf ectffi_, progressive=_,pol=pos .... ] This states the syntactic tense, aspect and polarity marked on the ellipsis (underscores indicate lack of specification). The category constrains resolution to look for verb phrase/sentence sources, which come wrapped in forms with categories like [t ease=past, modalffino, pexf ectffino, Heuristics simi]ar to those described by Hardt (1992) may be used for this. The category also says that, for this kind of VP match 9, the term in the antecedent whose category identifies it as being the subject should be treated as parallel to the explicit term in the ellipsis. As this example illustrates, tense and aspect on ellipsis and antecedent do not have to agree. When Sforns axe described in . 9Not all VP ellipses have VP antecedents. this is so, the antecedent and ellipsis categories are both used to determine what fozm should be substituted for the antecedent form. This comprises the restriction of the antecedent form and a new category constructed by taking the features of the antecedent category, unless overridden by those on the ellipsis--a kind of (monotonic) priority union (Grover et ai., 1994) except using skeptical as opposed to credulous default unification (Carpenter, 1993) . When a new category is constructed for the antecedent, any tense resolutions also need to be undone, since the original ones may no longer be appropriate for the revised category. One thus merges the category information from source and antecedent to determine what verb phrase form should be substituted for the original. In this case, it will have a category", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "this is so, the antecedent and ellipsis categories are both used to determine what fozm should be substituted for the antecedent form. ", "mid_sen": "This comprises the restriction of the antecedent form and a new category constructed by taking the features of the antecedent category, unless overridden by those on the ellipsis--a kind of (monotonic) priority union (Grover et ai., 1994) except using skeptical as opposed to credulous default unification (Carpenter, 1993) . ", "after_sen": "When a new category is constructed for the antecedent, any tense resolutions also need to be undone, since the original ones may no longer be appropriate for the revised category. "}
{"citeStart": 104, "citeEnd": 113, "citeStartToken": 104, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Vector-assisted logic The first class of approaches seeks to use distributional models of word semantics to enhance logic-based models of textual inference. The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al. (2013) . This line of research converts logical representations obtained from syntactic parses using Bos' Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006) , and uses distributional semantics-based models such as that of Erk and Padó (2008) to deal with issues polysemy and ambiguity.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al. (2013) . ", "mid_sen": "This line of research converts logical representations obtained from syntactic parses using Bos' Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006) , and uses distributional semantics-based models such as that of Erk and Padó (2008) to deal with issues polysemy and ambiguity.", "after_sen": "As this class of approaches deals with improving logic-based models rather than giving a distributional account of logical function words, we view such models as orthogonal to the effort presented in this paper."}
{"citeStart": 50, "citeEnd": 60, "citeStartToken": 50, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words. This is in contrast to many other measures, e.g., Lin (1998) , which use the co-occurrences of features with other words to compute a weighting function such as mutual information (MI) (Church and Hanks, 1989 ). Since we only have corpus data for the target phrases, it is not possible for us to use such a measure. However, the α-skew divergence measure has been shown (Weeds, 2003) to perform comparably with measures which use MI, particularly for lower frequency target words.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words. ", "mid_sen": "This is in contrast to many other measures, e.g., Lin (1998) , which use the co-occurrences of features with other words to compute a weighting function such as mutual information (MI) (Church and Hanks, 1989 ). ", "after_sen": "Since we only have corpus data for the target phrases, it is not possible for us to use such a measure. "}
{"citeStart": 256, "citeEnd": 281, "citeStartToken": 256, "citeEndToken": 281, "sectionName": "UNKNOWN SECTION NAME", "string": "We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L. Our HDP extension is also inspired from the Bayesian model proposed by Haghighi and Klein (2007) . However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We present an extension of the hierarchical Dirichlet process (HDP) model which is able to represent each observable object (i.e., event mention) by a finite number of feature types L. Our HDP extension is also inspired from the Bayesian model proposed by Haghighi and Klein (2007) . ", "after_sen": "However, their model is strictly customized for entity coreference resolution, and therefore, extending it to include additional features for each observable object is a challenging task (Ng, 2008; Poon and Domingos, 2008) ."}
{"citeStart": 93, "citeEnd": 105, "citeStartToken": 93, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the A1vey NL Tools (ANLT; Briscoe et al., 1987a) . Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987) . In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et al., 1992; .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the A1vey NL Tools (ANLT; Briscoe et al., 1987a) . ", "mid_sen": "Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987) . ", "after_sen": "In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. "}
{"citeStart": 53, "citeEnd": 86, "citeStartToken": 53, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "The automatic annotation of nouns and verbs in the corpus has been done by matching them with the WordNet database files. Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet. The changes made were inspired by those described in Stetina and Nagao (1997, page 75) . To lemmatize the words we used \"morpha,\" a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html. Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task. a more informative measure of the dispersion of a distribution, which depends both on the range and on the shape of a distribution.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Before doing the annotation, though, some preprocessing of the data was required to maximize the matching between our corpus and WordNet. ", "mid_sen": "The changes made were inspired by those described in Stetina and Nagao (1997, page 75) . ", "after_sen": "To lemmatize the words we used \"morpha,\" a lemmatizer developed by John A. Carroll and freely available at the address: http://www.informatics.susx.ac.uk./research/nlp/carroll/morph.html. Upon simple observation, it showed a better performance than the frequently used Porter Stemmer for this task. "}
{"citeStart": 95, "citeEnd": 116, "citeStartToken": 95, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993) , but differing in detail. The task is to judge which of two verbs v and v ~ is more likely to take a given noun n as its argument where the pair (v, n) has been cut out of the original corpus and the pair (v ~, n) is constructed by pairing n with a randomly chosen verb v ~ such that the combination (v ~, n) is completely unseen. Thus this test evaluates how well the models generalize over unseen verbs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993) , but differing in detail. ", "after_sen": "The task is to judge which of two verbs v and v ~ is more likely to take a given noun n as its argument where the pair (v, n) has been cut out of the original corpus and the pair (v ~, n) is constructed by pairing n with a randomly chosen verb v ~ such that the combination (v ~, n) is completely unseen. "}
{"citeStart": 171, "citeEnd": 184, "citeStartToken": 171, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus, we do not require Th(T) to be closed under substitution instances of tautologies. Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985) . But we won't pursue this topic further here.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, we do not require Th(T) to be closed under substitution instances of tautologies. ", "mid_sen": "Although in this paper we take modus ponens as the main rule of inference, in general one can consider deductive closures with respect to weaker, nonstandard logics, (cf. Levesque 1984; Frisch 1987; Patel-Schneider 1985) . ", "after_sen": "But we won't pursue this topic further here."}
{"citeStart": 101, "citeEnd": 113, "citeStartToken": 101, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Usually this feature-clash situation creates the problem of which constituent to give preference to (Langer, 1990) . Here the vocalism indicates the inflection (e.g. broken plural) and the preferance of vocalism pattern for that type of inflection belongs to the root. For example *(kidaa~)would be analysed as root {kd~} with a broken plural vocalism. The pattern type of the vocalism clashes with the broken plural pattern that the root expects. To correct, the morphological analyser is executed in generation mode to generate the broken plural form of {kd~} in the normal way.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In such a case, the two-level model succeeds in finding two-level analyses of the word in question, but fails when parsing the word morphosyntactically: at this stage, the parser is passed a root, vocalism and pattern whose feature structures do not unify.", "mid_sen": "Usually this feature-clash situation creates the problem of which constituent to give preference to (Langer, 1990) . ", "after_sen": "Here the vocalism indicates the inflection (e.g. broken plural) and the preferance of vocalism pattern for that type of inflection belongs to the root. "}
{"citeStart": 74, "citeEnd": 86, "citeStartToken": 74, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. ", "mid_sen": "The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "after_sen": "Reichenbach's well -known account of the interpretation of the different tense forms uses the temporal relations between three temporal indices: the utterance time, event time and reference time. "}
{"citeStart": 54, "citeEnd": 77, "citeStartToken": 54, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Predictable meaning shift. A set of lexical implication rules were developed by (Ostler and Atkins, 1992) for relating word senses. Based on them, we are developing a set of graph matching rules. Figure 2 exemplifies one of theln where two graphs containing the same word (or morphologically related), here draw and drawing, used as different parts of speech can be related.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Predictable meaning shift. ", "mid_sen": "A set of lexical implication rules were developed by (Ostler and Atkins, 1992) for relating word senses. ", "after_sen": "Based on them, we are developing a set of graph matching rules. "}
{"citeStart": 134, "citeEnd": 159, "citeStartToken": 134, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The biggest difference between this work and theirs is in what the links represent linguistically. ", "mid_sen": "Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). ", "after_sen": "Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . "}
{"citeStart": 167, "citeEnd": 186, "citeStartToken": 167, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "The parse trees produced by our parser are binarybranching and rather deep. In order to compute the probability of a parse tree, it is transformed to a flat dependency tree similar to the syntax graph representation used in the TIGER treebank Brants et al (2002) . An inner node of such a dependency tree represents a constituent or phrase. Typically, it directly connects to a leaf node representing the most important word of the phrase, the head child. The other children represent phrases or words directly depending on the head child. To give an example, the immediate children of a sentence node are the finite verb (the head child), the adverbials, the subject and the all other (verbal and non-verbal) complements. This flat structure has the advantage that the information which is most relevant for the head child is represented within the locality of an inner node. Assuming statistical independence between the internal structures of the inner nodes n i , we can factor P (T ) much like it is done for probabilistic contextfree grammars:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The parse trees produced by our parser are binarybranching and rather deep. ", "mid_sen": "In order to compute the probability of a parse tree, it is transformed to a flat dependency tree similar to the syntax graph representation used in the TIGER treebank Brants et al (2002) . ", "after_sen": "An inner node of such a dependency tree represents a constituent or phrase. "}
{"citeStart": 177, "citeEnd": 222, "citeStartToken": 177, "citeEndToken": 222, "sectionName": "UNKNOWN SECTION NAME", "string": "To this we now add the assumption that each auxiliary tree can have only one complement adjunction site projecting from y0, where y0 is the lexical category that projects ymax. This 1is justified in order to prevent projections of y0 from receiving more than one theta role from complement adjuncts, which would violate the underlying theta criterion in Government and Binding Theory (Chomsky, 1981) .We also assume that an auxiliary tree can not have complement adjunetion sites on its spine projecting from lexical heads other than y0 in order to preserve the minimality of elementary trees (Kroch, 1989; Frank, 1992) . Thus there (2) can be no more than one complement adjunction site on the spine of any complement auxiliary tree, and no complement adjunction site on the spine of any athematie auxiliary tree, since the foot node of an athematie tree lies outside of the maximal projection of the head. 4", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To this we now add the assumption that each auxiliary tree can have only one complement adjunction site projecting from y0, where y0 is the lexical category that projects ymax. ", "mid_sen": "This 1is justified in order to prevent projections of y0 from receiving more than one theta role from complement adjuncts, which would violate the underlying theta criterion in Government and Binding Theory (Chomsky, 1981) .", "after_sen": "We also assume that an auxiliary tree can not have complement adjunetion sites on its spine projecting from lexical heads other than y0 in order to preserve the minimality of elementary trees (Kroch, 1989; Frank, 1992) . "}
{"citeStart": 132, "citeEnd": 143, "citeStartToken": 132, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above. However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties. Sites which logically should be marked with multiple punctuation marks will often only have one ( (Nunberg, 1990) as summarized in (White, 1995) ). For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D. C is followed by only a single . in The president lives in Washington, D.C.).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties. ", "mid_sen": "Sites which logically should be marked with multiple punctuation marks will often only have one ( (Nunberg, 1990) as summarized in (White, 1995) ). ", "after_sen": "For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D. C is followed by only a single . in The president lives in Washington, D.C.)."}
{"citeStart": 86, "citeEnd": 110, "citeStartToken": 86, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The results suggest that neither version of Roget's is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget's 1987. Even on the largest set (Finkelstein et al., 2001) , however, the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4 . The difference between the 1911 Thesaurus and Vector would be statistically signifi- (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri, even at p < 0.1 for a two-tailed test. These data sets are too small for a meaningful comparison of systems with close correlation scores.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Even on the largest set (Finkelstein et al., 2001) , however, the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4 . ", "mid_sen": "The difference between the 1911 Thesaurus and Vector would be statistically signifi- (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri, even at p < 0.1 for a two-tailed test. ", "after_sen": "These data sets are too small for a meaningful comparison of systems with close correlation scores."}
{"citeStart": 61, "citeEnd": 87, "citeStartToken": 61, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "For each system, all 1200 events from the test set (not just the highlights) are indexed. Queries are generated artificially using a method similar to Berger and Lafferty (1999) and used in . First, each highlight is labeled with the event's type (e.g. fly ball), the event's location (e.g. left field) and the event's result (e.g. double play): 13 labels total. Log likelihood ratios are then used to find the phrases (unigram, trigram, and bigram) most indicative of each label (e.g. \"fly ball\" for category fly ball). For each label, the three most indicative phrases are issued as queries to the system, which ranks its results using the language modeling approach of Ponte and Croft (1998) . Precision is measured on how many of the top five returned events are of the correct category. Figure 4 shows the precision of the video IR systems based on ASR with the grounded language model, ASR with the text-only interpolated language model, and closed captioning transcriptions. As with our previous evaluations, the IR results show that the system using ASR with the grounded language model performed better than the one using ASR with the text-only language model (5.1% absolute improvement). More notably, though, Figure 4 shows that the system using the grounded language model performed better than the system using the hand generated closed captioning transcriptions (4.6% absolute improvement). Although this is somewhat counterintuitive given that hand transcriptions are typically considered gold standards, these results follow from a limitation of using text-based methods to index video.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each system, all 1200 events from the test set (not just the highlights) are indexed. ", "mid_sen": "Queries are generated artificially using a method similar to Berger and Lafferty (1999) and used in . ", "after_sen": "First, each highlight is labeled with the event's type (e.g. fly ball), the event's location (e.g. left field) and the event's result (e.g. double play): 13 labels total. "}
{"citeStart": 92, "citeEnd": 108, "citeStartToken": 92, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "The features can be easily obtained by modifying the TAT extraction algorithm described in (Liu et al., 2006) . When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. Then we use the toolkit implemented by Zhang (2004) to train MERS models for the ambiguous source syntactic trees separately. We set the iteration number to 100 and Gaussian prior to 1.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that for lexicalized tree, features do not include the information of sub-trees since there is no nonterminals.", "mid_sen": "The features can be easily obtained by modifying the TAT extraction algorithm described in (Liu et al., 2006) . ", "after_sen": "When a TAT is extracted from a word-aligned, source-parsed parallel sentence, we just record the contextual features and the features of the sub-trees. "}
{"citeStart": 152, "citeEnd": 169, "citeStartToken": 152, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "Grosz and Sidner proposed a theory which accounted for interactions between three notions on discourse: linguistic structure, intention, and attention [C, rosz et al. 86] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ltobbs developed a theory in which lie arranged three kinds of relationships between sentences from the text coherency viewpoint [Hobbs 79] .", "mid_sen": "Grosz and Sidner proposed a theory which accounted for interactions between three notions on discourse: linguistic structure, intention, and attention [C, rosz et al. 86] .", "after_sen": "l,itman and Allen described a model in which a discourse structure of conversation was built by recognizing a participanUs plans [Litman et al. 87] . 'l'hese theories all depend on extra-linguistic knowledge, the accumulation of which presents a problem in the realization of a practical analyzer."}
{"citeStart": 96, "citeEnd": 108, "citeStartToken": 96, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "In our earlier work, we built on Sidner's proposal/acceptance and proposal/rejection sequences (Sitnet, 1994 ) and developed a model tha¢ captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll and Carberry, 1994) . This model views coll~tive planning as agent A proposing a set of actions and beliefs to be i~ted into the plan being developed, agent B evaluating the proposal to determine whether or not he accepts the proposal and, ff not, agent B proposing a set of modifications to A's original proposal. The proposed modifications will again be evaluated by A, and if conflicts arise, she may propose modifications to B's previously proposed modifications, resulting in a recursive process. However, our research did not specify, in cases where multiple conflicts arise, how an agent should identify which pm of an unaccept~ proposal to address or how to select evidence to support the proposed modification. This paper extends that work by i~ting into the modification process a slrategy to determine the aspect of the proposal that the agent will address in her pursuit of conflict resolution, as well as a means of selecting appropriate evidence to justify the need for such modification.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although agents involvedin argumentation and non-collaborative negotiation take other agents' beliefs into consideration, they do so mainly to find weak points in their opponents' beliefs and attack them to win the argument.", "mid_sen": "In our earlier work, we built on Sidner's proposal/acceptance and proposal/rejection sequences (Sitnet, 1994 ) and developed a model tha¢ captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll and Carberry, 1994) . ", "after_sen": "This model views coll~tive planning as agent A proposing a set of actions and beliefs to be i~ted into the plan being developed, agent B evaluating the proposal to determine whether or not he accepts the proposal and, ff not, agent B proposing a set of modifications to A's original proposal. "}
{"citeStart": 139, "citeEnd": 157, "citeStartToken": 139, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "Another well recognized distinction is that of event types, such as state, process, transition, exemplified by the following examples: While verbs have an intrinsic type (e.g. wait is a process and catch is a transition), these types also apply to whole phrases, since tense, aspect, adjuncts and arguments can compose with the type of the lexical head to form a new type: Four kinds of temporal adverbials can be distinguished and are linked to the event types. Duration modifies processes, as in example (lla), but not transitions (llb); frame adverbials modify accomplishments, as in (12a), but not processes (12b); point adverbials modify achievements, as in (13); and frequency adverbials modify iterative events, as in (14) . It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases. Moens & Steedman (1988) identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas Nakhimovsky (1988) identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15. When the children crossed the road, a) they waited for the teacher to give a signal b) they stepped onto its concrete surface as if it were about to swallow them up. c) they were nearly hit by a car d) they reached the other side stricken with fear. e) they found themselves surrounded by strangers. Pustejovsky (1991) offers a much more compositional notion of event structure, where a transition is the composition of a process and a state. This analysis is more closely tied to the lexicon than Moens and Steedman's or Nakhimovsky's (and is offered in the context of a generative theory of lexical semantics). It not only accounts for the semantics of verbs, but also their compositions with adjuncts to form new types, as in 7above.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases. ", "mid_sen": "Moens & Steedman (1988) identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas Nakhimovsky (1988) identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15. ", "after_sen": "When the children crossed the road, a) they waited for the teacher to give a signal b) they stepped onto its concrete surface as if it were about to swallow them up. c) they were nearly hit by a car d) they reached the other side stricken with fear. e) they found themselves surrounded by strangers. "}
{"citeStart": 26, "citeEnd": 41, "citeStartToken": 26, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "Recall Black el al. (1993:7) use the crossing brackets measure to define a notion of structural consistency, where the structural consistency rate for the grammar is defined as the proportion of sentences for which at least one analysis--from the many typically returned by the grammar--contains no crossing brackets, and report a rate of around 95% for the IBM grammar tested on the computer manual corpus. However, a problem with the GEIG scheme and with structural consistency is that both are still weak measures (designed to avoid problems of parser/treebank representational compatibility) which lead to unintuitive numbers whose significance still depends heavily on details of the relationship between the representations compared (e.g. between structure assigned by a grammar and that in a treebank). One particular problem with the crossing bracket measure is that a single attachment mistake embedded n levels deep (and perhaps completely innocuous, such as an \"aside\" delimited by dashes) can lead to n crossings being assigned, whereas incorrect identification of arguments and adjuncts can go unpunished in some cases. Schabes et al. (1993) and Magerman (1995) report results using the GEIG evaluation scheme which are numerically similar in terms of parse selection to those reported here, but achieve 100% coverage. However, their experiments are not strictly comparable because they both utilise more homogeneous and probably simpler corpora. (The appendix gives an indication of the diversity of the sentences in our corpus). In addition, Schabes et al. do not recover tree labelling, whilst", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One particular problem with the crossing bracket measure is that a single attachment mistake embedded n levels deep (and perhaps completely innocuous, such as an \"aside\" delimited by dashes) can lead to n crossings being assigned, whereas incorrect identification of arguments and adjuncts can go unpunished in some cases. ", "mid_sen": "Schabes et al. (1993) and Magerman (1995) report results using the GEIG evaluation scheme which are numerically similar in terms of parse selection to those reported here, but achieve 100% coverage. ", "after_sen": "However, their experiments are not strictly comparable because they both utilise more homogeneous and probably simpler corpora. "}
{"citeStart": 79, "citeEnd": 104, "citeStartToken": 79, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "We turn now to the problem of reducing the size of the grammars produced by left-corner transforms. Many of the productions generated by schemata 1 are useless, i.e., they never appear in any terminating derivation. While they can be removed by standard methods for deleting useless productions Hopcroft and Ullman, 1979 , the relationship between the parse trees of G and LC L G depicted in Figure 1 shows how to determine ahead of time the new nonterminals D X that can appear in useful productions of LC L G. This is known as a link constraint.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of the productions generated by schemata 1 are useless, i.e., they never appear in any terminating derivation. ", "mid_sen": "While they can be removed by standard methods for deleting useless productions Hopcroft and Ullman, 1979 , the relationship between the parse trees of G and LC L G depicted in Figure 1 shows how to determine ahead of time the new nonterminals D X that can appear in useful productions of LC L G. This is known as a link constraint.", "after_sen": "For PCFGs there is a particularly simple link constraint: D X appears in useful productions of LC L G only if 9 2 V T ? : D ? L X . "}
{"citeStart": 128, "citeEnd": 141, "citeStartToken": 128, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used. But there is no question that semantic knowledge is essential for many problems in natural language processing. Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically. Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994) ). Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993) ). Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994) ). ", "mid_sen": "Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993) ). ", "after_sen": "Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context."}
{"citeStart": 149, "citeEnd": 160, "citeStartToken": 149, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . However none of these methods has considered the way of dealing both phenomena in the same concrete system. We propose in this paper an algorithm that deals with both phenomena, in the same analyser. The anaphora module pertains to the recent methods, uses a set of resolution rules based on the focusing approach, see (Sidner, 1983) . These rules are applied to the conceptual representation and their output is a set of candidate antecedents. Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1999) . The disambiguation procedure alms at filling the empty roles using attachment rules.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These rules are applied to the conceptual representation and their output is a set of candidate antecedents. ", "mid_sen": "Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1999) . ", "after_sen": "The disambiguation procedure alms at filling the empty roles using attachment rules."}
{"citeStart": 173, "citeEnd": 186, "citeStartToken": 173, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hampie, 1985) . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988) . On the other hand, Cn'ice's maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is required, s Thus, it is important that a collaborative agent selects suffmient and effective, but not excessive, evidence to justify an intended mutual belief.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hampie, 1985) . ", "mid_sen": "Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988) . ", "after_sen": "On the other hand, Cn'ice's maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is required, s Thus, it is important that a collaborative agent selects suffmient and effective, but not excessive, evidence to justify an intended mutual belief."}
{"citeStart": 66, "citeEnd": 90, "citeStartToken": 66, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008) . This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We optimised the Gaussian Process hyperparameters every 20 new instances, for both tasks.", "mid_sen": "As a measure of informativeness we used Information Density (ID) (Settles and Craven, 2008) . ", "after_sen": "This measure leverages between the variance among instances and how dense the region (in the feature space) where the instance is located is:"}
{"citeStart": 373, "citeEnd": 392, "citeStartToken": 373, "citeEndToken": 392, "sectionName": "UNKNOWN SECTION NAME", "string": "but this led to unwanted effects elsewhere in the grammar (Bouma 1987) . Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \\ for normal args, ? for wh-args (Moortgat 1988) , or have introduced so called modal operators on the whargument (Morrill et al. 1990 ). Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted effects elsewhere in the grammar. However, there are problems with having just composition, the most basic of the non-applicative operations. In CGs which contain functions of functions (such as very, or slowly), the addition of composition adds both new analyses of sentences, and new strings to the language. This is due to the fact that composition can be used to form a function, which can then be used as an argument to a function of a function. For example, if the two types, n/n and n/n are composed to give the type n/n, then this can be modified by an adjectival modifier of type (n/n)/(n/n).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "but this led to unwanted effects elsewhere in the grammar (Bouma 1987) . ", "mid_sen": "Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \\ for normal args, ? for wh-args (Moortgat 1988) , or have introduced so called modal operators on the whargument (Morrill et al. 1990 ). ", "after_sen": "Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted effects elsewhere in the grammar. "}
{"citeStart": 105, "citeEnd": 127, "citeStartToken": 105, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "Unless the desired set of GRs matches the set already annotated in some large training corpus (e.g., the Buchholz et al. (1999) GR finder used the GRs annotated in the Penn Treebank (Marcus et al., 1993) ), one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set. Manually writing rules is expensive, as is annotating a large corpus. We have performed experiments on learning to find GRs with just a small annotated training set. Our starting point is the work described in Ferro et al. (1999) , which used a fairly small training set.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999) .", "mid_sen": "Unless the desired set of GRs matches the set already annotated in some large training corpus (e.g., the Buchholz et al. (1999) GR finder used the GRs annotated in the Penn Treebank (Marcus et al., 1993) ), one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set. ", "after_sen": "Manually writing rules is expensive, as is annotating a large corpus. "}
{"citeStart": 166, "citeEnd": 186, "citeStartToken": 166, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "The head gestures in the DanPASS data have been coded by non expert annotators (one annotator per video) and subsequently controlled by a second annotator, with the exception of one video which was annotated independently and in parallel by two annotators. The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions. In the case of gestures we also measured agreement on gesture segmentation. The figures obtained are given in Table 3 . Table 3 : Inter-coder agreement on head gesture annotation These results are slightly worse than those obtained in previous studies using the same annotation scheme (Jokinen et al., 2008) , but are still sat-isfactory given the high number of categories provided by the scheme.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The figures obtained are given in Table 3 . ", "mid_sen": "Table 3 : Inter-coder agreement on head gesture annotation These results are slightly worse than those obtained in previous studies using the same annotation scheme (Jokinen et al., 2008) , but are still sat-isfactory given the high number of categories provided by the scheme.", "after_sen": "A distinction that seemed particularly difficult was that between nods and jerks: although the direction of the two movement types is different (down-up and up-down, respectively), the movement quality is very similar, and makes it difficult to see the direction clearly. "}
{"citeStart": 268, "citeEnd": 279, "citeStartToken": 268, "citeEndToken": 279, "sectionName": "UNKNOWN SECTION NAME", "string": "The grammar employed is a partial characterisation of Chomsky's Government-Binding theory [Chomsky1981, Chomsky1986] and only takes account of very local constralnts (i.e. X-bar, Theta and Case); a way of encoding all constraints in the proper branch formalism (e.g. [Crocker1992] ) will be needed before a grammar of sufficient coverage to be useful in corpora analysis can be formulated. The problem with using results obtained from the implementation given here is that the grammar is sufficiently underspecified and so leaves too great a task for the probabilistic information.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The grammar employed is a partial characterisation of Chomsky's Government-Binding theory [Chomsky1981, Chomsky1986] and only takes account of very local constralnts (i.e. X-bar, Theta and Case); a way of encoding all constraints in the proper branch formalism (e.g. [Crocker1992] ) will be needed before a grammar of sufficient coverage to be useful in corpora analysis can be formulated. ", "after_sen": "The problem with using results obtained from the implementation given here is that the grammar is sufficiently underspecified and so leaves too great a task for the probabilistic information."}
{"citeStart": 111, "citeEnd": 144, "citeStartToken": 111, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "One salient property of our language model is that it is strongly lexical: it consists of statistical parameters associated with relations between lexical items and the number and ordering of dependents of lexical heads. This lexical anchoring facilitates statistical training and sensitivity to lexical variation and collocations. In order to gain the benefits of probabilistic modeling, we replace the task of developing large rule sets with the task of estimating large numbers of statistical parameters for the monolingual and translation models. This gives rise to a new cost trade-off in human annotation/judgement versus barely tractable fully automatic training. It also necessitates further research on lexical similarity and clustering (e.g. Pereira, Tishby and Lee 1993, Dagan, Marcus and Markovitch 1993) to improve parameter estimation from sparse data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This gives rise to a new cost trade-off in human annotation/judgement versus barely tractable fully automatic training. ", "mid_sen": "It also necessitates further research on lexical similarity and clustering (e.g. Pereira, Tishby and Lee 1993, Dagan, Marcus and Markovitch 1993) to improve parameter estimation from sparse data.", "after_sen": ""}
{"citeStart": 26, "citeEnd": 47, "citeStartToken": 26, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . This idea was inspired by Delisle et al. (1993) , who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles (case relations). In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004; Carreras and Marquez, 2005) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To process a pair P not encountered previously, the system builds a graph centered on the main element (often the head) of P . ", "mid_sen": "This idea was inspired by Delisle et al. (1993) , who used a list of arguments surrounding the main verb together with the verb's subcategorization information and previously processed examples to analyse semantic roles (case relations). ", "after_sen": "In recent approaches, syntactic information is translated into features which, together with information from FrameNet, WordNet or VerbNet, will be used with ML tools to make predictions for each example in the test set (Carreras and Marquez, 2004; Carreras and Marquez, 2005) ."}
{"citeStart": 215, "citeEnd": 237, "citeStartToken": 215, "citeEndToken": 237, "sectionName": "UNKNOWN SECTION NAME", "string": "The eld of Computational Linguistics CL has both moved towards applications and towards large data sets. These developments call for a rigorous methodology for creating so-called lingware: linguistic data such as lexica, grammars, tree-banks, as well as software processing it. Experience from Software Engineering has shown that the earlier de ciencies are detected, the less costly their correction is. Rather than being a post-development e ort, quality evaluation must be an integral part of development to make the construction of lingware more e cient e.g., cf. EAGLES, 1996 for a general evaluation framework and Ciravegna et al., 1998 for the application of a particular software design methodology to linguistic engineering. This paper presents the adaptation of a particular Software Engineering SE method, instrumentation, to Grammar Engineering GE. Instrumentation allows to determine which test item exercises a certain piece of software or grammar code.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Experience from Software Engineering has shown that the earlier de ciencies are detected, the less costly their correction is. ", "mid_sen": "Rather than being a post-development e ort, quality evaluation must be an integral part of development to make the construction of lingware more e cient e.g., cf. EAGLES, 1996 for a general evaluation framework and Ciravegna et al., 1998 for the application of a particular software design methodology to linguistic engineering. ", "after_sen": "This paper presents the adaptation of a particular Software Engineering SE method, instrumentation, to Grammar Engineering GE. "}
{"citeStart": 99, "citeEnd": 117, "citeStartToken": 99, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. In our example above, however, such an approach would lead to the wrong prediction of praise or neutral sentiment.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given that most citations are neutral (Spiegel-Rosing, 1977; Teufel et al., 2006) , this makes it ever more important to recover what explicit sentiment there is from the context of the citation.", "mid_sen": "However, the dominant assumption in current citation identification methods (Ritchie et al., 2008; Radev et al., 2009) is that the sentiment present in the citation sentence represents the true sentiment of the author towards the cited paper. ", "after_sen": "This is due to the difficulty of determining the relevant context, whereas it is substantially easier to identify the citation sentence. "}
{"citeStart": 102, "citeEnd": 112, "citeStartToken": 102, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "We adopted Longman Dictionary of Contemporary English (LDOCE) [1987] as such a closed system of English. LDOCE has a unique feature that each of its 56,000 headwords is defined by using the words in Longman Defining Vocabulary (hereafter, LDV) and their derivations. LDV consists of 2,851 words (as the headwords in LDOCE) based on the survey of restricted vocabulary [West, 1953] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "LDOCE has a unique feature that each of its 56,000 headwords is defined by using the words in Longman Defining Vocabulary (hereafter, LDV) and their derivations. ", "mid_sen": "LDV consists of 2,851 words (as the headwords in LDOCE) based on the survey of restricted vocabulary [West, 1953] .", "after_sen": "We made a reduced version of LDOCE, called Glossdme. "}
{"citeStart": 163, "citeEnd": 183, "citeStartToken": 163, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives. Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994) . This is not necessary in our approach, which drastically reduces the search space for parsing.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives. ", "mid_sen": "Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994) . ", "after_sen": "This is not necessary in our approach, which drastically reduces the search space for parsing."}
{"citeStart": 493, "citeEnd": 523, "citeStartToken": 493, "citeEndToken": 523, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to capture the agents' intentions conveyed by their utterances, our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in (Lambert and Carberry, 1991) to represent the current status of the interaction. The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for the user's later execution, the problem-solving level which contains the actions being performed to construct the do-n~n plan, the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions, and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994) . This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for engaging in collaborative negotiations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to capture the agents' intentions conveyed by their utterances, our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in (Lambert and Carberry, 1991) to represent the current status of the interaction. ", "mid_sen": "The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for the user's later execution, the problem-solving level which contains the actions being performed to construct the do-n~n plan, the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions, and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994) . ", "after_sen": "This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for engaging in collaborative negotiations."}
{"citeStart": 70, "citeEnd": 96, "citeStartToken": 70, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al., 2003) , as described in Clark and Curran (2007a) . Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before this was unlikely.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. ", "mid_sen": "To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al., 2003) , as described in Clark and Curran (2007a) . ", "after_sen": "Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. "}
{"citeStart": 282, "citeEnd": 308, "citeStartToken": 282, "citeEndToken": 308, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition (by the use of Part-of-Speech lters hand-crafted by a linguist) (Oueslati, 1999) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri and Tugwell, 2001 , for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. The original acquisition methodology we present in the next section will allow us to overcome this limitation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the other hand, other work has been carried out in order to acquire collocations. ", "mid_sen": "Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition (by the use of Part-of-Speech lters hand-crafted by a linguist) (Oueslati, 1999) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri and Tugwell, 2001 , for example). ", "after_sen": "It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Up to now, we only looked at the overall results. Table 4 also shows individual FZ=i values for four selected common grammatical relations: subject NP, (in)direct object NP, locative PP adjunct and temporal PP adjunct. Note that the steps have different effects on the different relations: Adding NPs increases FZ=i by 11.3% for subjects resp. 16.2% for objects, but only 3.9% resp. 3.7% for locatives and temporals. Adverbial functions are more important for the two adjuncts (+6.3% resp. +15%) than for the two complements (+0.2% resp. +0.7%). Argamon et al. (1998) report FZ=i for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper. Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier. For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a) . That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Adverbial functions are more important for the two adjuncts (+6.3% resp. +15%) than for the two complements (+0.2% resp. +0.7%). ", "mid_sen": "Argamon et al. (1998) report FZ=i for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper. ", "after_sen": "Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier. "}
{"citeStart": 132, "citeEnd": 146, "citeStartToken": 132, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "Mama and Thompson proposed a linguistic structure of text describing relationships between sentences and their relative importance [Mann et al. 87] . llowever, no method for extracting the relationships from superficial linguistic expressions was described in their paper.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, since only relationships between successive sentences were considered, the scope which the relationships cover cannot be analyzed, even if explicit connectives are detected.", "mid_sen": "Mama and Thompson proposed a linguistic structure of text describing relationships between sentences and their relative importance [Mann et al. 87] . llowever, no method for extracting the relationships from superficial linguistic expressions was described in their paper.", "after_sen": "We have developed a computational rnodel of discourse for Japanese expository writings, and implemented a practical procedure for extracting discourse structure [Sumita 92 ]. "}
{"citeStart": 98, "citeEnd": 115, "citeStartToken": 98, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper proposes an approach to improve domain-specific word alignment through alignment model adaptation. Our approach first trains two alignment models with a large-scale out-of-domain corpus and a small-scale domain-specific corpus. Second, we build a new adaptation model by linearly interpolating these two models. Third, we apply the new model to the domain-specific corpus and improve the word alignment results. In addition, with the training data, an interpolated translation dictionary is built to select the word alignment links from different alignment results. Experimental results indicate that our approach achieves a precision of 84.90% and a recall of 75.99% for word alignment in a specific domain. Our method achieves a relative error rate reduction of 17.43% as compared with the method directly combining the out-of-domain corpus and the in-domain corpus as training data. It also achieves a relative error rate reduction of 6.56% as compared with the previous work in ( Wu and Wang, 2004) . In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004) , our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our method achieves a relative error rate reduction of 17.43% as compared with the method directly combining the out-of-domain corpus and the in-domain corpus as training data. ", "mid_sen": "It also achieves a relative error rate reduction of 6.56% as compared with the previous work in ( Wu and Wang, 2004) . ", "after_sen": "In addition, when we train the model with a smaller-scale in-domain corpus as described in (Wu and Wang, 2004) , our method achieves an error rate reduction of 10.15% as compared with the method in (Wu and Wang, 2004) ."}
{"citeStart": 133, "citeEnd": 145, "citeStartToken": 133, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "Northwest Airlines settled the remaining lawsuits filed on behalf of 156 people killed in a 1987 crash, but claims against the jetliner's maker are being pursued, a federal judge said. (\"Northwest Airlines Settles Rest of Suits,\" Wall Street Journal, November 1, 1989) A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues. It is most fully presented in Wiebe and Rapaport (1986 , 1988 , 1991 and Wiebe (1990 Wiebe ( , 1994 . It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory. The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973 Kuroda ( , 1976 ) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory. ", "mid_sen": "The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973 Kuroda ( , 1976 ) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) ", "after_sen": ""}
{"citeStart": 103, "citeEnd": 116, "citeStartToken": 103, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed an SVM-based NE system by following our NE system based on maximum entropy (ME) modeling (Isozaki, 2001) . We simply replaced the ME model with SVM classifiers. The above datasets are processed by a morphological analyzer ChaSen 2.2.1 2 . It tokenizes a sentence into words and adds POS tags. ChaSen uses about 90 POS tags such as common-noun and location-name. Since most unknown words are proper nouns, ChaSen's parameters for unknown words are modified for better results. Then, a character type tag is added to each word. It uses 17 character types such as all-kanji and smallinteger. See Isozaki (2001) for details. Now, Japanese NE recognition is solved by the classification of words (Sekine et al., 1998; Borthwick, 1999; Uchimoto et al., 2000) . For instance, the words in \"President George Herbert Bush said Clinton is . . . \" are classified as follows: \"President\" = OTHER, \"George\" = PERSON-BEGIN, \"Herbert\" = PERSON-MIDDLE, \"Bush\" = PERSON-END, \"said\" = OTHER, \"Clinton\" = PERSON-SINGLE, \"is\" = OTHER. In this way, the first word of a person's name is labeled as PERSON-BEGIN. The last word is labeled as PERSON-END. Other words in the name are PERSON-MIDDLE. If a person's name is expressed by a single word, it is labeled as PERSON-SINGLE. If a word does not belong to any named entities, it is labeled as OTHER. Since IREX defines eight NE classes, words are classified into 33", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Systems are compared in terms of GENERAL's F-measure Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.", "mid_sen": "We developed an SVM-based NE system by following our NE system based on maximum entropy (ME) modeling (Isozaki, 2001) . ", "after_sen": "We simply replaced the ME model with SVM classifiers. "}
{"citeStart": 15, "citeEnd": 26, "citeStartToken": 15, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "recent papers (Brill, 1994 have reported 96.5% tagging accuracy on the Wall St. Journal corpus. The experiments in this paper test the hypothesis that better use of context will improve the accuracy. A Maximum Entropy model is well-suited for such experiments since it corn-bines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data. Previous uses of this model include language modeling (Lau et al., 1993) , machine translation (Berger et al., 1996) , prepositional phrase attachment , and word morphology (Della Pietra et al., 1995) . This paper briefly describes the maximum entropy and maximum likelihood properties of the model, features used for POS tagging, and the experiments on the Penn Treebank Wall St. Journal corpus. It then discusses the consistency problems discovered during an attempt to use specialized features on the word context. Lastly, the results in this paper are compared to those from previous work on POS tagging.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "recent papers (Brill, 1994 have reported 96.5% tagging accuracy on the Wall St. Journal corpus. ", "after_sen": "The experiments in this paper test the hypothesis that better use of context will improve the accuracy. "}
{"citeStart": 60, "citeEnd": 71, "citeStartToken": 60, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "A lot of work has been done in Japanese pronoun resolution (Kameyama 86) (Yamamuraet al. 92) (Walker et al. 94) (Takada & Doi 94) (Nakaiwa & Ikehara 95) . The main distinguishing features of our work are as follows:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When the word order of sentences is changed and the pronominalized words are changed in translation into English, the system must detect the referents of the pronouns.", "mid_sen": "A lot of work has been done in Japanese pronoun resolution (Kameyama 86) (Yamamuraet al. 92) (Walker et al. 94) (Takada & Doi 94) (Nakaiwa & Ikehara 95) . ", "after_sen": "The main distinguishing features of our work are as follows:"}
{"citeStart": 54, "citeEnd": 80, "citeStartToken": 54, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "For citation extraction, the most relevant work is by Qazvinian and Radev (2010) who proposed a framework of Markov Random Fields to extract only the non-explicit citations for a given paper. They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes weighted by lexical similarity between nodes. However, their dataset consists of only 569 citations from 10 papers and their annotation scheme deals with neither acronyms nor sentiment.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the corpus that they use consists of only 94 citations to 4 papers and is likely to be too small to be representative.", "mid_sen": "For citation extraction, the most relevant work is by Qazvinian and Radev (2010) who proposed a framework of Markov Random Fields to extract only the non-explicit citations for a given paper. ", "after_sen": "They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes weighted by lexical similarity between nodes. "}
{"citeStart": 40, "citeEnd": 61, "citeStartToken": 40, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008) , sentence compression (Knight and Marcu, 2002) , and reformulation (Barzilay et al., 2001; Saggion, 2011) ; while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. News-Blaster 3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring language generation to produce a grammatical and coherent summary, and better suites the scenario of tweet summarization. Note that our method considers each tweet as the unit for summarization, which often cannot provide reliable information to compute the salience. This is one main difference between our system and the existing studies.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Abstraction and selection are two strategies employed for multi-document summarization. ", "mid_sen": "The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008) , sentence compression (Knight and Marcu, 2002) , and reformulation (Barzilay et al., 2001; Saggion, 2011) ; while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. ", "after_sen": "News-Blaster 3 and our method are examples of abstraction and selection based methods, respectively. "}
{"citeStart": 116, "citeEnd": 139, "citeStartToken": 116, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model (Weischedel et al., 1993 , Merialdo, 1994 or Statistical Decision Tree (Jelinek et al., 1994 The Maximum Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods. It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques. 5The mapping from article to annotator is in the file doc/wsj .wht on the Treebank CDROM.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model (Weischedel et al., 1993 , Merialdo, 1994 or Statistical Decision Tree (Jelinek et al., 1994 The Maximum Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods. ", "after_sen": "It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques. "}
{"citeStart": 62, "citeEnd": 82, "citeStartToken": 62, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "to facilitate the annotation process. The citing papers were downloaded from CiteSeer-X; 3 see Table 1 for details. We then proceeded to manually annotate the corpus using SLAT (Noguchi et al., 2008) , a browser-based multi-purpose annotation tool. We devised the following guidelines for annotation. Since the tool allows for two types of annotation, namely segments that demarcate a region of text, and links, that allow an annotator to assign relationships between them, we created four segment types and three link types. Segments were used to mark c-site anchors, c-sites, background information (explained presently), and references. We used the term background information to refer to any running-text that elaborates on a c-site but is not strictly part of the c-site itself (refer to Figure 2 for an example). Even during annotation, however, we encountered situations that felt ambiguous, making this a rather contentious issue.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The citing papers were downloaded from CiteSeer-X; 3 see Table 1 for details. ", "mid_sen": "We then proceeded to manually annotate the corpus using SLAT (Noguchi et al., 2008) , a browser-based multi-purpose annotation tool. ", "after_sen": "We devised the following guidelines for annotation. "}
{"citeStart": 174, "citeEnd": 179, "citeStartToken": 174, "citeEndToken": 179, "sectionName": "UNKNOWN SECTION NAME", "string": "The key claim of this section is that agents monitor the effects of their utterance actions and that the next action by the addressee is taken as evidence of the effect of the speaker's utterance 4. That the utterance will have the intended effect is only a hypothesis at the point where the utterance has just been made, irrespective of the intentions of the speaker. This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed othecwise [11, 8] .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "That the utterance will have the intended effect is only a hypothesis at the point where the utterance has just been made, irrespective of the intentions of the speaker. ", "mid_sen": "This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed othecwise [11, 8] .", "after_sen": "I adopt the assumption that the participants in a dialogue are trying to achieve some purpose [7] . "}
{"citeStart": 21, "citeEnd": 40, "citeStartToken": 21, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "Unification Grammar (DUG, Hellwig (1986) ) defines a tree-like data structure for the representation of syntactic analyses. 'Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head. The approach requires that the parser interpretes several features specially, and it cannot restrict the scope of discontinuities.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Unification Grammar (DUG, Hellwig (1986) ) defines a tree-like data structure for the representation of syntactic analyses. ", "after_sen": "'Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. "}
{"citeStart": 68, "citeEnd": 87, "citeStartToken": 68, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of this paper is to give a detailed account of the techniques used in TnT. ", "mid_sen": "Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . ", "after_sen": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . "}
{"citeStart": 0, "citeEnd": 27, "citeStartToken": 0, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "Let us first consider optimization error in terms of prediction error. The first observation is that there is a gap between the prediction accuracies ofθ gen-EM andθ EM , but this gap shrinks considerably as we increase the number of examples. Figures 4(a,b,c ) support this for all three model families: for the HMM, bothθ gen-EM andθ EM eventually achieve around 90% accuracy; for the DMV, 85%. For the PCFG,θ EM still lagsθ gen-EM by 10%, but we believe that more data can further reduce this gap. Figure 4( betweenθ gen-EM andθ EM also diminishes for the HMM. To reaffirm the trends, we also measure distance D µ . Figure 4 (e) shows that the distance from θ EM to the true parameters θ * decreases, but the gap betweenθ gen-EM andθ EM does not close as decisively as it did for prediction error. It is quite surprising that by simply running EM with a neutral initialization, we can accurately learn a complex model with thousands of parameters. Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data. Carroll and Charniak (1992) report that EM fared poorly with local optima. We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples. With more examples, there is less noise in the aggregate statistics, so it might be easier for EM to pick out the salient patterns. Srebro et al. (2006) made a similar observation in the context of learning Gaussian mixtures. They characterized three regimes: one where EM was successful in recovering the true clusters (given lots of data), another where EM failed but the global optimum was successful, and the last where both failed (without much data).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figures 4(f,g) show how both likelihood and accuracy, which both start quite low, improve substantially over time for the HMM on artificial data. ", "mid_sen": "Carroll and Charniak (1992) report that EM fared poorly with local optima. ", "after_sen": "We do not claim that there are no local optima, but only that the likelihood surface that EM is optimizing can become smoother with more examples. "}
{"citeStart": 61, "citeEnd": 73, "citeStartToken": 61, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "After writing the verse, the system can evaluate if it is technically correct, i.e. if the overall structure is correct and if each line in the form abides by the required syllable count and rhyming scheme. The syllable counter is implemented using the foma software (Hulden, 2009) , and the implementation (Hulden, 2006) can be found on the homepage of foma. 3 Separately, we have also developed a rhyme checker, which extracts special patterns in the lines that must rhyme and checks their conformity.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After writing the verse, the system can evaluate if it is technically correct, i.e. if the overall structure is correct and if each line in the form abides by the required syllable count and rhyming scheme. ", "mid_sen": "The syllable counter is implemented using the foma software (Hulden, 2009) , and the implementation (Hulden, 2006) can be found on the homepage of foma. ", "after_sen": "3 Separately, we have also developed a rhyme checker, which extracts special patterns in the lines that must rhyme and checks their conformity."}
{"citeStart": 69, "citeEnd": 84, "citeStartToken": 69, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "(4) is in fact the n-gram likelihood of the token pair , i i t s 〈 〉 sequence and Eq. (5) approximates this probability using a bigram language model. This model is conceptually similar to the joint sourcechannel model (Li et al., 2004) where the target token i t depends on not only its source token i s but also the history 1 i t − and 1 i s − . Each character in the target name forms a token. To obtain the source tokens, the source and target names in the training data are aligned using the EM algorithm. This yields a set of possible source tokens and a mapping between the source and target tokens. During testing, each source name is first segmented into all possible token sequences given the token set. These source token sequences are mapped to the target sequences to yield an N-best list of transliteration candidates. Each candidate is scored using an n-gram language model given by Eqs. (4) or (5).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(4) is in fact the n-gram likelihood of the token pair , i i t s 〈 〉 sequence and Eq. (5) approximates this probability using a bigram language model. ", "mid_sen": "This model is conceptually similar to the joint sourcechannel model (Li et al., 2004) where the target token i t depends on not only its source token i s but also the history 1 i t − and 1 i s − . ", "after_sen": "Each character in the target name forms a token. "}
{"citeStart": 172, "citeEnd": 200, "citeStartToken": 172, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003) . We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. ", "mid_sen": "We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003) . ", "after_sen": "We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. "}
{"citeStart": 31, "citeEnd": 46, "citeStartToken": 31, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to offer any obvious way to distinguish between the semantic relations of synonymy, antonymy and hyponymy. Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., \"Xs and other Ys\" is used as evidence that X is a hyponym of Y. Our work explores the connection between relative frequency, distributional generality and semantic generality with promising results.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to offer any obvious way to distinguish between the semantic relations of synonymy, antonymy and hyponymy. ", "mid_sen": "Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., \"Xs and other Ys\" is used as evidence that X is a hyponym of Y. ", "after_sen": "Our work explores the connection between relative frequency, distributional generality and semantic generality with promising results."}
{"citeStart": 56, "citeEnd": 83, "citeStartToken": 56, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "A second possibility is to factor out recursive structures from a grammar. Thompson et al. (1991) show how this can be done for a phrase structure grammar (creating an equivalent 'Pree Adjoining (;rammar (,Ioshi I987)). The parser for the resulting grammar allows linear parsing tbr an (infinitely) parallel system, with Cite absorption of each word performed in constant time. At each choice point, there are only a finite number of possible new partial TAG trees (the TAG trees represents the possibly inlinite nmnbet of trees which can be forlned using adjunct|on). It should agMn be possible to extract 'default' semantic values, by taking the semantics from the TA(I tree (i.e. by assuming that there are to be ,to adj unctions). A somewhat similar system has recently been proposed by Shieber and Johnson (191t3) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It should agMn be possible to extract 'default' semantic values, by taking the semantics from the TA(I tree (i.e. by assuming that there are to be ,to adj unctions). ", "mid_sen": "A somewhat similar system has recently been proposed by Shieber and Johnson (191t3) .", "after_sen": "The third possibility is suggested by considering the semantic representations which are appropria.te during a word by word parse. "}
{"citeStart": 123, "citeEnd": 142, "citeStartToken": 123, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "For the next set of experiments we used a supervised machine learning approach for the two-way classification (Agree/Disagree). We use Support Vector Machines (SVM) as our machine-learning algorithm for classification as implemented in Weka (Hall et al., 2009) and ran 10-fold cross validation. As a SVM baseline, we first use all unigrams in Callout and Target as features (Table  6 , Row 2). We notice that the recall improves significantly when compared with the rule-based method. To further improve the classification accuracy, we use Mutual Information (MI) to select the words in the Callouts and Targets that are likely to be associated with the categories Agree and Disagree, respectively. Specifically, we sort each word based on its MI value and then select the first 180 words in each of the two categories to represent our new vocabulary set of 360 words. The feature vector includes only words present in the MI list. Compared to the all unigrams baseline, the MI-based unigrams improve the F1 by 4% (Agree) and 2% (Disagree) ( Table  6 ). The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification. In addition, we consider several types of lexical features (LexF) inspired by previous work on agreement and disagreement (Galley et al., 2004; Misra and Walker, 2013) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The MI approach discovers the words that are highly associated with Agree/Disagree categories and these words turn to be useful features for classification. ", "mid_sen": "In addition, we consider several types of lexical features (LexF) inspired by previous work on agreement and disagreement (Galley et al., 2004; Misra and Walker, 2013) .", "after_sen": "• Sentiment Lexicon (SL): "}
{"citeStart": 80, "citeEnd": 99, "citeStartToken": 80, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al., 1993) . The test set was section 23. The parser induction algorithm used in all of the experiments in this paper was a distribution of Collins's model 2 parser (Collins, 1997) . All comparisons made below refer to results we obtained using Collins's parser. The results for bagging are shown in Figure 2 and Table 1 . The row of figures are (from left-to-right) training set F-measure ~, test set F-measure, percent perfectly parsed sentences in training set, and percent perfectly parsed sentences in test set. An ensemble of bags was produced one bag at a time. In the table, the Initial row shows the performance achieved when the ensemble contained only one bag, Final(X) shows the performance when the ensemble contained X bags, BestF gives the performance of the ensemble size that gave the best F-measure score. TrainBestF and TestBestF give the test set performance for the ensemble size that performed the best on the training and test sets, respectively.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The training set for these experiments was sections 01-21 of the Penn Treebank (Marcus et al., 1993) . ", "after_sen": "The test set was section 23. "}
{"citeStart": 201, "citeEnd": 223, "citeStartToken": 201, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "Process neighbours if (vj, aj) / ∈ A then aj = 0 end if if vj / ∈ P and |P| ≤ Lp,max then a * j = aj + ai * wij * d Replace (vj, aj) ∈ A with (vj, a * j ) SPREAD UNIDIR(vj , A, P) end if end for return end function 50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores. To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005) :", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Process neighbours if (vj, aj) / ∈ A then aj = 0 end if if vj / ∈ P and |P| ≤ Lp,max then a * j = aj + ai * wij * d Replace (vj, aj) ∈ A with (vj, a * j ) SPREAD UNIDIR(vj , A, P) end if end for return end function 50 word-pairs from the WordSimilarity-353 dataset (Gabrilovich, 2002) and correlating our method's scores with the human-assigned scores. ", "mid_sen": "To reduce the possibility of overestimating the performance of our technique on a sample set that happens to be favourable to our technique, we furthermore implemented a technique of repeated holdout (Witten and Frank, 2005) :", "after_sen": "Given a sample test set of N pairs of words with human-assigned ratings of relatedness, randomly divide this set into k parts of roughly equal size 3 . "}
{"citeStart": 60, "citeEnd": 70, "citeStartToken": 60, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "This section describes how TINA handles several issues that are often considered to be part of the task of a parser. These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in \"(which article)/do you think I should read (ti)?\") (Chomsky 1977) . The gap mechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment of bounded domination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ft.) , but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These include agreement constraints, semantic restrictions, subject-tagging for verbs, and long distance movement (often referred to as gaps, or the trace, as in \"(which article)/do you think I should read (ti)?\") (Chomsky 1977) . ", "mid_sen": "The gap mechanism resembles the Hold register idea of ATNs (Woods 1970) and the treatment of bounded domination metavariables in lexical functional grammars (LFGs) (Bresnan 1982, p. 235 ft.) , but it is different from these in that the process of filling the Hold register equivalent involves two steps separately initiated by two independent nodes.", "after_sen": "Our approach to the design of a constraint mechanism is to establish a framework general enough to handle syntactic, semantic, and, ultimately, phonological constraints using identical functional procedures applied at the node level. "}
{"citeStart": 96, "citeEnd": 108, "citeStartToken": 96, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "An interesting future exercise would be to explore whether it is feasible to rewrite Eq. 5 as a linear integer program. If it is, the whole scheme of ours would fall under what is known as 'Linear Programming CRFs' (Tasker, 2004; Roth and Yih, 2005) . What remains to be seen, however, is whether GST is transferrable to languages other than Japanese, notably, English. The answer is likely to be yes, but details have yet to be worked out.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An interesting future exercise would be to explore whether it is feasible to rewrite Eq. 5 as a linear integer program. ", "mid_sen": "If it is, the whole scheme of ours would fall under what is known as 'Linear Programming CRFs' (Tasker, 2004; Roth and Yih, 2005) . ", "after_sen": "What remains to be seen, however, is whether GST is transferrable to languages other than Japanese, notably, English. "}
{"citeStart": 66, "citeEnd": 84, "citeStartToken": 66, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "As a consequence, we use the same type of binary classifier for the sequential and the BOW models. The excellent results recently obtained with the SEARN algorithm (Daume et al., 2007) also suggest that binary classifiers, when properly trained and combined, seem to be capable of matching more complex structured output approaches.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a consequence, we use the same type of binary classifier for the sequential and the BOW models. ", "mid_sen": "The excellent results recently obtained with the SEARN algorithm (Daume et al., 2007) also suggest that binary classifiers, when properly trained and combined, seem to be capable of matching more complex structured output approaches.", "after_sen": ""}
{"citeStart": 59, "citeEnd": 77, "citeStartToken": 59, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Another avenue of further research is the inclusion of yet more levels of annotation. For example, we are currently experimenting with the annotation of semantic frames on top of the treebanks. We use the SALSA tool developed at Saarbrücken University (Erk and Pado, 2004) which also assumes TIGER-XML input. So, TIGER-XML has become the lingua franca of treebank annotation which allows for the addition of arbitrary layers.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, we are currently experimenting with the annotation of semantic frames on top of the treebanks. ", "mid_sen": "We use the SALSA tool developed at Saarbrücken University (Erk and Pado, 2004) which also assumes TIGER-XML input. ", "after_sen": "So, TIGER-XML has become the lingua franca of treebank annotation which allows for the addition of arbitrary layers."}
{"citeStart": 126, "citeEnd": 143, "citeStartToken": 126, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Having discussed linguistic motivation, empirical validation, and practical approximation of semantically relevant features, we now present two studies demonstrating their value in sentiment classification. For the first study, we have constructed a new data set particularly well suited for testing our approach, based on writing about the death penalty. In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al. (2006) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For the first study, we have constructed a new data set particularly well suited for testing our approach, based on writing about the death penalty. ", "mid_sen": "In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al. (2006) .", "after_sen": ""}
{"citeStart": 83, "citeEnd": 104, "citeStartToken": 83, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "'Fo date, psychologists have focused on two aspects of the speech segmentation problem. The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched; both psychologists (e.g., Cutler & Carter, 1987; Cutler & Butterliel (I, 1992) and designers of speech-recognition systems (e.g., (]hur (:h, 1987) have examined I~his problem. However, the problem we examined is dilferent---we want to know how infants segment speech before knowing which phonemic se-qllelW,('s form words. '1' he second aspect psychologists liaw~ focnsed (ill is the lirobleln of dcternihiilig the ill['Orluatioll SOllr(:(~s t() which ilifants are SCllSil,ive. Priluarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress. II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. Sensitivity to native-language phonotactics in 9-month-olds was re(:ently reported by Jusczyk, I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993) . 'i'lles~ sl, udi(~s deruoilstratcd infants' perceptive abilil.il's wil,hout deiilonsl.ral,hig tlw usefuhicss of hli'alil,s > ll(~rcel)l,ioils. I low do childl'(,n coiubine l,li(: iiiforiii;d,ion I, hey i)crc~,iw; froln dilrerenl, SOlll'l;es'. ? Aslili el, al. Sl)(~c-Illate that infants first learn words heard in isolation, then use distribution and prosody to refine and expand their w)cabulary; however, Jusczyk (1(,)93) sliggests that sound sequences learned in isolation dill~r too greatly from those in contexi. to bc useful. He goes on to say, \"just how far inforniation in the sound structure of the input can pies, we see that Hypothesis 1 uses 48 characters and Hypothesis 2 uses 75. However, this simplistic method is inefficient; for instance, the length of lexical indices are arbitrary with respect to properties of the words themselves (e.g., in Hypothesis 2, there is no reason why/jul/was assigned tile index '10'--length two--instead of '9'--length one). Our system improves upon this simple size metri(: I)y coml)uting sizes based on ;t ('Onll)act rel)rcs(,ntat.ion motivated I)y informati(m theory. W(: inmginc hypothes(:s r(qu'(~sented ;~ a string of ones and zeros. This binary string must r(,present not only the lexical entries, their indices (called code words) and the coded sample, but also overhead information specifying the number of items coded and their arrangement in the string (information implicitly given by spacing and sl)atial placement in the introductory cxamples). Furtherrnore, the string and its components must be self-delimiting, so that a decoder could identify the endpoints of components by itself. The next section describes the binary representation and the length formulm derived from it in detail; readers satisfied with the intuitive descriptions presented so far should skip ahead to the Phonotactics subsection.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . ", "mid_sen": "Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. ", "after_sen": "Sensitivity to native-language phonotactics in 9-month-olds was re(:ently reported by Jusczyk, I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993) . 'i'lles~ sl, udi(~s deruoilstratcd infants' perceptive abilil."}
{"citeStart": 113, "citeEnd": 133, "citeStartToken": 113, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "lrFor the definition of the term ~crossing brackets\" used in Table 1 : see (Harrison et al., 1991) . 62.3'~o I 1.2% ~ 3.6% 11.3% 25.6% I percentage of parses which exactly match one of the humanproduced parses (\"exact match\") or which match bracket locations, role names, and syntactic part-of--speech tags only (\"syntactic exact match\"). suggests any candidates which are as likely as the correct un~wer. If not, the parser has erred by \"omrn~sion\" rather than by \"commission': it has ommitted the correct parse from consideration, but not because it seemed ,mJ~lrely. It is entirely possible that the correct parse is in fact among the highest-scoring parses. These types of search error are non--existent for exhaustive search, but become important for sentences between 11 and 15 words in length, and dominate the results for longer sentences. The results in Table 2 reflect tagging accuracy as well as the pefformaace of the parser models per se. Note that tagging accuracy is quoted on a per-word basis, as is customary. From previous work, we estimate the accuracy of the tagger on the syntactic portion of tags to be about 94%. Thus there is typically at least one error in semantic assignment in each sentence, and an error in syntactic assignment in one of every two sentences. It is not surprising, .then, that the per-sentence parsing acclzracy suffers when parses are predicted from raw text. Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in (Jelinek et al., 1994; Magerman, 1995; Black et al., 1993b) , which would seem to be the closest work to ours, and any comparison between this work and ours must be approached with extreme caution. Table 3 shows the differences between the treebank~ utilized in (Jelinek et al., 1994) on the one hand, and in the work reported here, on the other, is Table 4 shows relevant lSFigures for Average Sentence Length ('l~raLuing Corpus) and Training Set Size, for the IBM ManuaLs Corpus, are approximate, and cz~e fzom (Black et aL, 1993a Table 3 above   I  i  I  I  I  I  I  i  I  I  I parsing results by (Jelinek et al., 1994) . Even starker contrasts obtain between the present results and those of e.g. (Magerman, 1995; Black et at., 1993b) , who do not employ an exact-match evaluation criterion, further obscuring possible performance comparisons. Obviously, no direct comparisons of the results of Tables 1-2 with previous parsing work is possible, as we are the first to parse using the Treebank. In our current research, we are emphasizing the creation of decision-tree questions for predicting semantic categories in tagging, as well as continuing to develop questions for syntactic tag prediction, and for our nile-name-prediction model.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is not surprising, .then, that the per-sentence parsing acclzracy suffers when parses are predicted from raw text. ", "mid_sen": "Clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in (Jelinek et al., 1994; Magerman, 1995; Black et al., 1993b) , which would seem to be the closest work to ours, and any comparison between this work and ours must be approached with extreme caution. ", "after_sen": "Table 3 shows the differences between the treebank~ utilized in (Jelinek et al., 1994) on the one hand, and in the work reported here, on the other, is Table 4 shows relevant lSFigures for Average Sentence Length ('l~raLuing Corpus) and Training Set Size, for the IBM ManuaLs Corpus, are approximate, and cz~e fzom (Black et aL, 1993a Table 3 above   I  i  I  I  I  I  I  i  I  I  I parsing results by (Jelinek et al., 1994) . "}
{"citeStart": 75, "citeEnd": 90, "citeStartToken": 75, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The machine-readable thesaurus we used in this study was derived from GETESS , an ontology for the tourism domain. Each concept in the ontology is associated with one lexical item, which expresses this concept. From this ontology, word classes were derived in the following manner. A class was formed by words lexicalizing all child concepts of a given concept. For example, the concept CULTURAL_EVENT in the ontology has successor concepts PERFORMANCE, OPERA, FESTIVAL, associated with words performance, opera, festival correspondingly. Though these words are not synonyms in the traditional sense, they are taken to constitute one semantic class, since out of all words of the ontology's lexicon their meanings are closest. The thesaurus thus derived contained 1052 words and phrases (the corpus used in the study had data on 756 of them). Out of the 756 concepts, 182 were non-final; correspondingly, 182 word classes were formed. The average depth level of the thesaurus is 5.615, the maximum number of levels is 9. The corpus from which distributional data was obtained was extracted from a web site advertising hotels around the world . It contained around 1 million words. Collection of distributional data was carried out in the following settings. The preprocessing of corpus included a very simple stemming (most common inflections were chopped off; irregular forms of verbs, adjectives and nouns were changed to their first forms). The context of usage was delineated by a window of 3 words on either side of the target word, without transgressing sentence boundaries. In case a stop word other than a proper noun appeared inside the window, the window was accordingly expanded. The stoplist included 50 most frequent words of the British National Corpus, words listed as function words in the BNC, and proper nouns not appearing in the sentence-initial position. The obtained frequencies of cooccurrence were weighted by the 1+log weight function. The distributional similarity was measured by means of three different similarity measures: the Jaccard's coefficient, L1 distance, and the skew divergence. This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999 ) which compared several well known measures on similar tasks and found these three to be superior to many others. Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The distributional similarity was measured by means of three different similarity measures: the Jaccard's coefficient, L1 distance, and the skew divergence. ", "mid_sen": "This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999 ) which compared several well known measures on similar tasks and found these three to be superior to many others. ", "after_sen": "Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999) ."}
{"citeStart": 151, "citeEnd": 164, "citeStartToken": 151, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005) . Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004) . Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied (Ratnaparkhi, 1999; Charniak, 2000) . Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005) , which demonstrates the state of the art performance in English dependency parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005) . ", "mid_sen": "Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . ", "after_sen": "Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004) . "}
{"citeStart": 112, "citeEnd": 124, "citeStartToken": 112, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "The literature boom in the life sciences over the past few years has sparked increasing interest into text mining tools, which facilitate the automatic extraction of useful knowledge from text Ananiadou & McNaught, 2006; Zweigenbaum et al., 2007; Cohen & Hunter, 2008) . Most of these tools have focussed on entity recognition and relation extraction and with few exceptions, e.g., (Hyland, 1996; Light et al., 2004; Sándor, 2007; Vincze et al., 2008) , do not take into account the discourse context of the knowledge extracted. However, failure to take this context into account results in the loss of information vital for the correct interpretation of extracted knowledge, e.g. the scope of the relations, or the level of certainty with which they are expressed. A particular piece of knowledge may represent, e.g., an accepted fact, hypothesis, results of an experiment, analysis based on experimental results, factual or speculative statements etc. Furthermore, this knowledge may represent the author's current work, or work reported elsewhere. The ability to recognise different discourse elements automatically provides crucial information for the correct interpretation of extracted knowledge, allowing scientific claims to be linked to experimental evidence, or newly reported experimental knowledge to be isolated. The importance of categorising such knowledge becomes more pronounced as analysis moves from abstracts to full papers, where the content is richer and linguistic constructions are more complex (Cohen et al., 2010) . Analysis of full papers is extremely important, since less than 8% of scientific claims occur in abstracts (Blake, 2010) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The literature boom in the life sciences over the past few years has sparked increasing interest into text mining tools, which facilitate the automatic extraction of useful knowledge from text Ananiadou & McNaught, 2006; Zweigenbaum et al., 2007; Cohen & Hunter, 2008) . ", "mid_sen": "Most of these tools have focussed on entity recognition and relation extraction and with few exceptions, e.g., (Hyland, 1996; Light et al., 2004; Sándor, 2007; Vincze et al., 2008) , do not take into account the discourse context of the knowledge extracted. ", "after_sen": "However, failure to take this context into account results in the loss of information vital for the correct interpretation of extracted knowledge, e.g. the scope of the relations, or the level of certainty with which they are expressed. "}
{"citeStart": 107, "citeEnd": 131, "citeStartToken": 107, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "In the worst case, the dominance chart of a dominance graph with n fragments has O(2 n ) production rules (Koller and Thater, 2005b) , i.e. charts may be exponential in size; but note that this is still an 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 improvement over the n! configurations that these worst-case examples have. In practice, RTGs that are computed by converting the USR computed by a grammar remain compact: Fig. 2 compares the average number of configurations and the average number of RTG production rules for USRs of increasing sizes in the Rondane treebank (see Sect. 4.3); the bars represent the number of sentences for USRs of a certain size. Even for the most ambiguous sentence, which has about 4.5 × 10 12 scope readings, the dominance chart has only about 75 000 rules, and it takes only 15 seconds on a modern consumer PC (Intel Core 2 Duo at 2 GHz) to compute the grammar from the graph. Computing the charts for all 999 MRSnets in the treebank takes about 45 seconds.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The grammar in Section 3.2 is simply f (G) for the chart above (up to consistent renaming of nonterminals).", "mid_sen": "In the worst case, the dominance chart of a dominance graph with n fragments has O(2 n ) production rules (Koller and Thater, 2005b) , i.e. charts may be exponential in size; but note that this is still an 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 improvement over the n! ", "after_sen": "configurations that these worst-case examples have. "}
{"citeStart": 242, "citeEnd": 254, "citeStartToken": 242, "citeEndToken": 254, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been surprisingly little work on precisely the problem that we tackle in this paper. The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001 ) and from free text (Hearst, 1992; Caraballo, 1999) . Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. ", "mid_sen": "There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001 ) and from free text (Hearst, 1992; Caraballo, 1999) . Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. ", "after_sen": "For two terms x and y, x is said to subsume y if the following conditions hold:"}
{"citeStart": 148, "citeEnd": 172, "citeStartToken": 148, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "Dealing with quantifiers incrementally is a rather simila.r problem to dealing with h'aglnents of trees incrementally, a,st as it is in-,possible to predict the level of embedding of ~r noun phrase such as John from tile fragment Mary thinL's John, it is also impossible to predict the scope of a quantifier in a fragment with respect ~o the arbitrarily large number of quantiliers which might appear later in the sentence. Again the problem can be avoided by a tbrm of pacldng. A particularly simple way of doing this is to use unseoped logical forms where qmmtifiers are left in situ (silnilar to the representations used by Hobbs and Shieber (1987) , or to Quasi Logical Form (Alshawi 1990)). For example, the fl'agment Every man gives a boot\" can be given the tbllowing representation:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Again the problem can be avoided by a tbrm of pacldng. ", "mid_sen": "A particularly simple way of doing this is to use unseoped logical forms where qmmtifiers are left in situ (silnilar to the representations used by Hobbs and Shieber (1987) , or to Quasi Logical Form (Alshawi 1990)). ", "after_sen": "For example, the fl'agment Every man gives a boot\" can be given the tbllowing representation:"}
{"citeStart": 153, "citeEnd": 174, "citeStartToken": 153, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "The BEETLE II system we present was built to serve as a platform for research in computational linguistics and tutoring, and can be used for taskbased evaluation of algorithms developed for other domains. We are currently developing an annotation scheme for the data we collected to identify student paraphrases of correct answers. The annotated data will be used to evaluate the accuracy of existing paraphrasing and textual entailment approaches and to investigate how to combine such algorithms with the current deep linguistic analysis to improve system robustness. We also plan to annotate the data we collected for evidence of misunderstandings, i.e., situations where the system arrived at an incorrect interpretation of a student utterance and took action on it. Such annotation can provide useful input for statistical learning algorithms to detect and recover from misun-derstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010) . ", "mid_sen": "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006) . ", "after_sen": "Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail."}
{"citeStart": 24, "citeEnd": 43, "citeStartToken": 24, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "One unconventional step we took was to attempt to model the potentially important contextual effect of negation: clearly \"good\" and \"not very good\" indicate opposite sentiment orientations. Adapting a technique of Das and Chen (2001) , we added the tag NOT to every word between a negation word (\"not\", \"isn't\", \"didn't\", etc.) and the first punctuation mark following the negation word. (Preliminary experiments indicate that removing the negation tag had a negligible, but on average slightly harmful, effect on performance.)", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One unconventional step we took was to attempt to model the potentially important contextual effect of negation: clearly \"good\" and \"not very good\" indicate opposite sentiment orientations. ", "mid_sen": "Adapting a technique of Das and Chen (2001) , we added the tag NOT to every word between a negation word (\"not\", \"isn't\", \"didn't\", etc.) and the first punctuation mark following the negation word. ", "after_sen": "(Preliminary experiments indicate that removing the negation tag had a negligible, but on average slightly harmful, effect on performance.)"}
{"citeStart": 213, "citeEnd": 233, "citeStartToken": 213, "citeEndToken": 233, "sectionName": "UNKNOWN SECTION NAME", "string": "Participants were asked to schedule a health care appointment with each of the 9 systems, yielding a total of 9 dialogues per participant. System utterances were generated using a simple template-based algorithm and synthesised using the speech synthesis system Cerevoice (Aylett et al., 2006) , which has been shown to be intelligible to older users (Wolters et al., 2007) . The human wizard took over the function of the speech recognition, language understanding, and dialogue management components.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Participants were asked to schedule a health care appointment with each of the 9 systems, yielding a total of 9 dialogues per participant. ", "mid_sen": "System utterances were generated using a simple template-based algorithm and synthesised using the speech synthesis system Cerevoice (Aylett et al., 2006) , which has been shown to be intelligible to older users (Wolters et al., 2007) . ", "after_sen": "The human wizard took over the function of the speech recognition, language understanding, and dialogue management components."}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "Carroll & Weir (1997)--without actually building a parsing system--address the issue of how frequency information can be associated with lexicalised grammar formalisms, using Lexicalized Tree Adjoining Grammar (Joshi & Schabes, 1991) as a unifying framework. They consider systematically a number of alternative probao bilistic formulations, including those of Resnik (1992) and and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks. Magerman (1995) , Collins (1996) , Ratnaparkhi (1997) , Charniak (1997) and others describe implemented systems with impressive accuracy on parsing unseen data from the Penn Treebank (Marcus, Santorini & Marcinkiewicz, 1993 ). These parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur. The accuracies reported for these systems are substantially better than their (non-lexicalised) probabilistic context-free grammar analogues, demonstrating clearly the value of lexico-statistical information. However, since the grammatical descriptions are induced from atomic-labeled constituent structures in the training treebank, rather than coming from an explicit generative grammar, these systems do not make contact with traditional notions of argument structure (i.e. subcategorisation, selectional preferences of predicates for complements) in any direct sense. So although it is now possible to extract at least subcategorisation data from large corpora 2 with some degree of reliability, it would be difficult to integrate the data into this type of parsing system. present a small-scale experiment in which subcategorisation class frequency information for individual verbs w~us integrated into a robust statistical (non-lexicalised) parser. The experiment used a test corpus of 250 sentences, and used the standard GEIG bracket precision, recall and crossing measures (Grishman, Macleod & Sterling, 1992) for evaluation. While bracket precision and recall were virtually unchanged, the crossing bracket score for the lexicalised parser showed a 7% improvement. However, this difference turned out not to be statistically significant at the 95% level: some analyses got better while others got worse.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They consider systematically a number of alternative probao bilistic formulations, including those of Resnik (1992) and and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks. ", "mid_sen": "Magerman (1995) , Collins (1996) , Ratnaparkhi (1997) , Charniak (1997) and others describe implemented systems with impressive accuracy on parsing unseen data from the Penn Treebank (Marcus, Santorini & Marcinkiewicz, 1993 ). ", "after_sen": "These parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur. "}
{"citeStart": 106, "citeEnd": 115, "citeStartToken": 106, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "However, such methods so far are usually designed for generic summarization and do not take into account the impact of users' interests on summary generation. Besides, in the existing studies, personalized summarization is often conducted with the help of a query (Sun, 2008; You et al., 2011) or a static user profile (Díaz and Gervás, 2007) , and most studies only use the local content from target document(s) or the user profile, with little attention paid to the rich social contextual information affiliated with them.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, such methods so far are usually designed for generic summarization and do not take into account the impact of users' interests on summary generation. ", "mid_sen": "Besides, in the existing studies, personalized summarization is often conducted with the help of a query (Sun, 2008; You et al., 2011) or a static user profile (Díaz and Gervás, 2007) , and most studies only use the local content from target document(s) or the user profile, with little attention paid to the rich social contextual information affiliated with them.", "after_sen": "Currently, an increasing number of social websites allow users to enrich the source content. "}
{"citeStart": 90, "citeEnd": 110, "citeStartToken": 90, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "Our method has certain aspects in common with other approaches to domain adaptation. For example, Koo et al. (2008) train a dependency parser on features deriving from distributional clusters, with two words having similar cluster features if they have similar bigram distributions. Thus, these clusters engender a form of distributional similarity comparable to that used in our KNN algorithm. KNN algorithms are also commonly used in Graph-Based Semi-Supervised Learning approaches (Das and Petrov, 2011; Altun et al., 2006; Subramanya et al., 2010) , with the knearest-neighbour sets determining the edges that structure the graph. POS tags are then propagated through the graph from labelled to unlabelled data. Although similarity in these cases is commonly being assessed between token sequences, as opposed to word types, the features used are similar to the ngram templates used here and the bigram distributions used by Koo et al. (2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, these clusters engender a form of distributional similarity comparable to that used in our KNN algorithm. ", "mid_sen": "KNN algorithms are also commonly used in Graph-Based Semi-Supervised Learning approaches (Das and Petrov, 2011; Altun et al., 2006; Subramanya et al., 2010) , with the knearest-neighbour sets determining the edges that structure the graph. ", "after_sen": "POS tags are then propagated through the graph from labelled to unlabelled data. "}
{"citeStart": 206, "citeEnd": 230, "citeStartToken": 206, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "The algorithm presented here has extended previous algorithms for rewrite rules by adding a limited version of backreferencing. This allows the output of rewriting to be dependent on the form of the strings which are rewritten. This new feature brings techniques used in Perl-like languages into the finite state calculus. Such an integration is needed in practical applications where simple text processing needs to be combined with more sophisticated computational linguistics techniques. One particularly interesting example where backreferences are essential is cascaded deterministic (longest match) finite state parsing as described for example in Abney (Abney, 1996) and various papers in (Roche and Schabes, 1997a) . Clearly, the standard rewrite rules do not apply in this domain. If NP is an NP recognizer, it would not do to.say NP ~ [NP]/A_p. Nothing would force the string matched by the NP to the left of the arrow to be the same as the string matched by the NP to the right of the arrow.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such an integration is needed in practical applications where simple text processing needs to be combined with more sophisticated computational linguistics techniques. ", "mid_sen": "One particularly interesting example where backreferences are essential is cascaded deterministic (longest match) finite state parsing as described for example in Abney (Abney, 1996) and various papers in (Roche and Schabes, 1997a) . ", "after_sen": "Clearly, the standard rewrite rules do not apply in this domain. "}
{"citeStart": 29, "citeEnd": 49, "citeStartToken": 29, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "The header of a research paper is defined to be all of the words from the beginning of the paper up to either the first section of the paper, usually the introduction, or to the end of the first page, whichever occurs first. It contains 15 fields to be extracted: title, author, affiliation, address, note, email, date, abstract, introduction, phone, keywords, web, degree, publication number, and page (Seymore et al., 1999) . The header dataset contains 935 headers. Following previous research (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003) , for each trial we randomly select 500 for training and the remaining 435 for testing. We refer this dataset as H.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The header dataset contains 935 headers. ", "mid_sen": "Following previous research (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003) , for each trial we randomly select 500 for training and the remaining 435 for testing. ", "after_sen": "We refer this dataset as H."}
{"citeStart": 58, "citeEnd": 79, "citeStartToken": 58, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "To extract such patterns, we first used the Genia tagger (Tsuruoka et al., 2005) , a Part-of-Speech tagger trained on a biomedical corpus, to tag every word in the sentence. Then, we used the rules to extract all phrases and terminologies that correspond to the above mentioned patterns. The results are presented in the Result section.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To extract such patterns, we first used the Genia tagger (Tsuruoka et al., 2005) , a Part-of-Speech tagger trained on a biomedical corpus, to tag every word in the sentence. ", "after_sen": "Then, we used the rules to extract all phrases and terminologies that correspond to the above mentioned patterns. "}
{"citeStart": 91, "citeEnd": 103, "citeStartToken": 91, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, the field of dialectometry, as introduced by S~guy (1971, 1973) , has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. There is however no firm agreement on just how to compute the distance matrices. Sfiguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make. The need to figure out such systems as the comparative phonology of various linguistic sites can be very time-consuming and fraught with arbitrary choices.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. ", "mid_sen": "Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. ", "after_sen": "Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. "}
{"citeStart": 41, "citeEnd": 59, "citeStartToken": 41, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement. Future work will pursue at least two directions for improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally, distributional clustering techniques (Schütze, 1992; Pereira et al., 1993) could be applied to extract semantic classes from the corpus itself. Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results. The second area where the methods described here could be improved is in the way that multiple information sources are integrated. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997) . In particular, boosting (Schapire, 1999; Abney et al., 1999) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997) . ", "mid_sen": "In particular, boosting (Schapire, 1999; Abney et al., 1999) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly.", "after_sen": ""}
{"citeStart": 45, "citeEnd": 59, "citeStartToken": 45, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990; Emele 1994 ). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another common approach to lexical rules is to encode them as unary phrase structure rules. ", "mid_sen": "This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31) . ", "after_sen": "A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990; Emele 1994 ). "}
{"citeStart": 60, "citeEnd": 84, "citeStartToken": 60, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "We have explored an information theoretical neural network (Gorin and Levinson, 1989 ) that can acquire the verbal associations in the dual-coding theory. It provides a learnable lexical selection sub-system for a conneetionist transfer project in machine translation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It depicts the verbal representations for two different languages as two separate but connected logogen systems, characterizes the translation process as the activation along the connections between the logogen systems, and attributes the acquisition of the representation to some unspecified statistical processes.", "mid_sen": "We have explored an information theoretical neural network (Gorin and Levinson, 1989 ) that can acquire the verbal associations in the dual-coding theory. ", "after_sen": "It provides a learnable lexical selection sub-system for a conneetionist transfer project in machine translation."}
{"citeStart": 66, "citeEnd": 85, "citeStartToken": 66, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "The two senses of well also differ in their word class. The word class has been studied as a difficulty indicator by several researchers but with mixed results. Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. Sigott (1995) could not confirm any effect of the word class on C-test difficulty.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The word class has been studied as a difficulty indicator by several researchers but with mixed results. ", "mid_sen": "Brown (1989) finds that function words are easier to solve, while Klein-Braley (1996) claims that prepositions are often harder for learners. ", "after_sen": "Sigott (1995) could not confirm any effect of the word class on C-test difficulty."}
{"citeStart": 37, "citeEnd": 62, "citeStartToken": 37, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "Importantly, this simple story breaks down on the Treebank-Chunk task for the eight sections of the Brown corpus. For these, our AUGMENT technique performs rather poorly. Moreover, there is no clear winning approach on this task. Our hypothesis is that the common feature of these examples is that these are exactly the tasks for which SRCONLY outperforms TGTONLY (with one exception: CoNLL). This seems like a plausible explanation, since it implies that the source and target domains may not be that different. If the domains are so similar that a large amount of source data outperforms a small amount of target data, then it is unlikely that blow- ing up the feature space will help. We additionally ran the MEGAM model (Daumé III and Marcu, 2006) on these data (though not in the multi-conditional case; for this, we considered the single source as the union of all sources). The results are not displayed in Table 2 to save space. For the majority of results, MEGAM performed roughly comparably to the best of the systems in the table. In particular, it was not statistically significantly different that AUGMENT on: ACE-NER, CoNLL, PubMed, Treebank-chunk-wsj, Treebank-chunk-swbd3, CNN and Treebank-brown. It did outperform AUGMENT on the Treebank-chunk on the Treebank-chunk-br-* data sets, but only outperformed the best other model on these data sets for br-cg, br-cm and br-cp. However, despite its advantages on these data sets, it was quite significantly slower to train: a single run required about ten times longer than any of the other models (including AUGMENT), and also required five-to-ten iterations of cross-validation to tune its hyperparameters so as to achieve these results.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the domains are so similar that a large amount of source data outperforms a small amount of target data, then it is unlikely that blow- ing up the feature space will help. ", "mid_sen": "We additionally ran the MEGAM model (Daumé III and Marcu, 2006) on these data (though not in the multi-conditional case; for this, we considered the single source as the union of all sources). ", "after_sen": "The results are not displayed in Table 2 to save space. "}
{"citeStart": 153, "citeEnd": 174, "citeStartToken": 153, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear (1995) . Adjunct and argument features indicate adjunct and argument attachment respectively, and permit the model to capture a general argument attachment preference. In addition, there are specialized adjunct and argument features corresponding to each grammatical function used in LFG (e.g., SUB J, OBJ, COMP, XCOMP, ADJUNCT, etc.).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For any given UBG there are a large (usually infinite) number of SUBGs that can be constructed from it, differing only in the features that each SUBG uses.", "mid_sen": "In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear (1995) . ", "after_sen": "Adjunct and argument features indicate adjunct and argument attachment respectively, and permit the model to capture a general argument attachment preference. "}
{"citeStart": 399, "citeEnd": 409, "citeStartToken": 399, "citeEndToken": 409, "sectionName": "UNKNOWN SECTION NAME", "string": "Maximum likelihood estimation is consistent: under broad conditions the sequence of distributions P0 , associated with the maximum r~ likelihood estimator for 0o given the samples Wl,...wn, converges to P0o. Pseudo-likelihood is also consistent, but in the present implementation it is consistent for the conditional distributions P0o (w[y(w)) and not necessarily for the full distribution P0o (see Chi (1998) ). It is not hard to see that pseudo-likelihood will not always correctly estimate P0o-Suppose there is a feature fi which depends only on yields: fi(w) = fi(y(w)). (Later we will refer to such features as pseudo-constant.) In this case, the derivative of PL0 (~) with respect to Oi is zero; PL0(~) contains no information about Oi. In fact, in this case any value of Oi gives the same conditional distribution Po(wly(w)); Oi is irrelevant to the problem of choosing good parses.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So that maximizing pseudo-likelihood (at large samples) amounts to minimizing the average (over yields) divergence between the true and estimated conditional distributions of analyses given yields.", "mid_sen": "Maximum likelihood estimation is consistent: under broad conditions the sequence of distributions P0 , associated with the maximum r~ likelihood estimator for 0o given the samples Wl,...wn, converges to P0o. Pseudo-likelihood is also consistent, but in the present implementation it is consistent for the conditional distributions P0o (w[y(w)) and not necessarily for the full distribution P0o (see Chi (1998) ). ", "after_sen": "It is not hard to see that pseudo-likelihood will not always correctly estimate P0o-Suppose there is a feature fi which depends only on yields: fi(w) = fi(y(w)). "}
{"citeStart": 19, "citeEnd": 36, "citeStartToken": 19, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. ", "mid_sen": "Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. ", "after_sen": "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. "}
{"citeStart": 130, "citeEnd": 156, "citeStartToken": 130, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "To take advantage of semantic representations, we work with two types of semantic structures; first, the Word Sequence Kernel applied to both question and answer; given s 0 , sample substrings are: What is autism, What is, What autism, is autism, etc. Then, two PAS-based trees: Shallow Semantic Trees for SSTK and Shallow Semantic Trees for PTK, both based on PropBank structures (Kingsbury and Palmer, 2002) are automatically generated by our SRL system (Moschitti et al., 2005) . As an example, let us consider an automatically annotated sentence from our TREC-QA corpus: Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. It can be observed here that, although autism is a different disease from panic disorder, the structure of both definitions and the latent semantics they contain (inherent to behavior, disorder, anxiety) are similar. So for instance, s 2 appears as a definition even to someone who only knows what the definition of autism looks like.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "What is autism, What is, What autism, is autism, etc. ", "mid_sen": "Then, two PAS-based trees: Shallow Semantic Trees for SSTK and Shallow Semantic Trees for PTK, both based on PropBank structures (Kingsbury and Palmer, 2002) are automatically generated by our SRL system (Moschitti et al., 2005) . ", "after_sen": "As an example, let us consider an automatically annotated sentence from our TREC-QA corpus: "}
{"citeStart": 60, "citeEnd": 80, "citeStartToken": 60, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun. The idea is similar to that used in the centering approach (Brennan et al., 1987) where a continued topic is the highest-ranked candidate for pronominalization.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In effect, we use this probability information to identify the topic of the segment with the belief that the topic is more likely to be referred to by a pronoun. ", "mid_sen": "The idea is similar to that used in the centering approach (Brennan et al., 1987) where a continued topic is the highest-ranked candidate for pronominalization.", "after_sen": "Given the above possible sources of informar tion, we arrive at the following equation, where F(p) denotes a function from pronouns to their antecedents: ( A(p) = alp, h, l~', t, l, so, d~ A~') where A(p) is a random variable denoting the referent of the pronoun p and a is a proposed antecedent. "}
{"citeStart": 0, "citeEnd": 20, "citeStartToken": 0, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "Our explicit division of V-space into various support regions has been implicitly considered in other work. Smadja et al. (1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X's occurring with a translation other than Y decreases one's belief in their association; but the absence of both X and Y yields no information. In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema. The Figure 4 : Performance of the skew divergence with respect to the best functions from Figure 2 . definition of commonality is left to the user (several different definitions are proposed for different tasks).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our explicit division of V-space into various support regions has been implicitly considered in other work. ", "mid_sen": "Smadja et al. (1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X's occurring with a translation other than Y decreases one's belief in their association; but the absence of both X and Y yields no information. ", "after_sen": "In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. "}
{"citeStart": 74, "citeEnd": 95, "citeStartToken": 74, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "In this work, we will base our work on Conditional Random Fields (CRF's) (Lafferty et al., 2001) , which are now one of the most preferred sequential models for many natural language processing tasks.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In this work, we will base our work on Conditional Random Fields (CRF's) (Lafferty et al., 2001) , which are now one of the most preferred sequential models for many natural language processing tasks.", "after_sen": "The parametric form of the CRF for a sentence of length n is given as follows:"}
{"citeStart": 216, "citeEnd": 234, "citeStartToken": 216, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "Some studies have tried to incorporate syntactic information into vector-based models. In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of interest. This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998) . Although syntactic information is now used to select a word's appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. ", "mid_sen": "However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998) . ", "after_sen": "Although syntactic information is now used to select a word's appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing."}
{"citeStart": 159, "citeEnd": 178, "citeStartToken": 159, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001 ) and subsequently extended by (Ng and Cardie, 2002) . Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, annotated with coreference relations, for both training and testing. They also outlined a list of features to extract for training the resolver to recognize the coreference relations. Specifically, (Soon et al., 2001 ) established a list of 12 features that compare a given anaphor with a candidate antecedent, e.g. gender agreement, number agreement, both being pronouns, both part of the same semantic class (i.e. WordNet synset hyponyms/hypernyms), etc. For training the resolver, a corpus annotated with anaphors and their antecedents is processed, and pairs of anaphor and candidate antecedents are created so as to have only one positive instance per anaphor (the annotated antecedent). Negative examples are created by taking all occurrences of noun phrases that occur between the anaphor and its antecedent in the text. The antecedent in these steps is also always considered to be to the left of, or preceding, the anaphor; cataphors are not addressed in this technique.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001 ) and subsequently extended by (Ng and Cardie, 2002) . ", "after_sen": "Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, annotated with coreference relations, for both training and testing. "}
{"citeStart": 76, "citeEnd": 104, "citeStartToken": 76, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. Where TagPair used to be significantly better than MBL, the roles are now well reversed. It appears that our hypothesis at the time, that the stacked systems were plagued by a lack of training data, is correct, since they can now hold their own. In order to see at which point TagPair is overtaken, we have trained several systems on increasing amounts of training data from LOB. 29 Each increment is one of the 10% training corpus parts described above. The results are shown in Figure 5 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL. ", "after_sen": "Where TagPair used to be significantly better than MBL, the roles are now well reversed. "}
{"citeStart": 175, "citeEnd": 198, "citeStartToken": 175, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "Based on the above typology, we can systematically construct a testsuite for developing and evaluating a paraphrastic grammar. Indeed, when developing a grammar, it is necessary to have some means of assessing both the coverage of the grammar (does it generate all the sentences of the described language?) and its degree of overgeneration (does it generate only the sentences of the described language?) While corpus driven efforts along the PAR-SEVAL lines (Black et al., 1991) are good at giving some measure of a grammar coverage, they are not suitable for finer grained analysis and in particular, for progress evaluation, regression testing and comparative report generation. Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. For english, there is for instance the 15 year old Hewlett-Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987) ; and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998 ). Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. To remedy this, we propose to develop a paraphrase test suite based on the paraphrase typology described in the previous section. In such a testsuite, test items pair a semantic representation with a set of paraphrases verbalising this semantics. The construction and annotation of the paraphrases reflects the paraphrase typology. In a first phase, we concentrate on simple, non-recursive predicate/argument structure. Given such a structure, the construction and annotation of a test item proceeds as follows.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another known method consists in developing and using a test suite that is, a set of negative and positive items against which the grammar can be systematically tested. ", "mid_sen": "For english, there is for instance the 15 year old Hewlett-Packard test suite, a simple text file listing test sentences and grouping them according to linguistics phenomena (Flickinger et al., 1987) ; and more recently, the much more sophisticated TSNLP (Test Suite for Natural Language Processing) which includes some 9500 test items for English, French and German, each of them being annotated with syntactic and application related information (Oepen and Flickinger, 1998 ). ", "after_sen": "Yet because they do not take into account the semantic dimension, none of these tools are adequate for evaluating the paraphrastic power of a grammar. "}
{"citeStart": 303, "citeEnd": 324, "citeStartToken": 303, "citeEndToken": 324, "sectionName": "UNKNOWN SECTION NAME", "string": "The results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of nondeterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley & Sharman, 1991) , and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore & Dowding, 1991) . However, on the assumption that incorrect prediction of gaps is the main avoidable source of performance degradation (c.f. Moore & Dowding) , further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. ", "mid_sen": "The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of nondeterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley & Sharman, 1991) , and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore & Dowding, 1991) . ", "after_sen": "However, on the assumption that incorrect prediction of gaps is the main avoidable source of performance degradation (c.f. Moore & Dowding) , further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%)."}
{"citeStart": 21, "citeEnd": 40, "citeStartToken": 21, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "The results for Arabic (Hajič et al., 2004; Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). By contrast, Turkish Atalay et al., 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). It is noteworthy that Arabic and Turkish, being \"typological outliers\", show patterns that are different both from each other and from most of the other languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results for Arabic (Hajič et al., 2004; Smrž et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2). ", "mid_sen": "By contrast, Turkish Atalay et al., 2003) exhibits high root accuracy but consistently low attachment scores (about 88% for length 1 and 68% for length 2). ", "after_sen": "It is noteworthy that Arabic and Turkish, being \"typological outliers\", show patterns that are different both from each other and from most of the other languages."}
{"citeStart": 104, "citeEnd": 115, "citeStartToken": 104, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar extraction algorithm Systemic Functional Grammar (SFG) (Halliday, 1985) is based on the assumption that the differentiation of syntactic phenomena is always deter-mined by its function in the communicative context. This functional orientation has lead to the creation of detailed linguistic resources that are characterized by an integrated treatment of content-related, textual and pragmatic aspects. Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects--such as, for example, PENMAN (Mann, 1983) , COMMUNAL (Fawcett and Tucker, 1990) , TECHDOC (KSsner and Stede, 1994) , Drafter (Paris and Vander Linden , 1996) , and Gist (Not and Stock, 1994) . For our present purposes, however, it is the formal characteristics of systemic grammar and its implementations that are more important. Systemic grammar assumes multifunctional constituent structuresrepresentable as feature structures with coreferences. As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: ", "mid_sen": "Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. ", "after_sen": "The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):"}
{"citeStart": 45, "citeEnd": 63, "citeStartToken": 45, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann & Xia, 2003) . However, unlike in rule-and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, unlike in rule-and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. ", "mid_sen": "Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. ", "after_sen": "While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table."}
{"citeStart": 13, "citeEnd": 28, "citeStartToken": 13, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995) , Collins (1996) , and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995) , Collins (1996) , and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lcxical/semantic collocational knowledge of verbs which is useful in syntactic analysis.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. ", "mid_sen": "For example, Magerman (1995) , Collins (1996) , and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. ", "after_sen": "In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. "}
{"citeStart": 109, "citeEnd": 134, "citeStartToken": 109, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "In other words, mother tongue interference is so strong that one can reconstruct a language fam-ily tree from non-native texts. One of the major contributions of this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis. This becomes important in native language identification 1 which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners. As we will see in Sect. 6, this paper reveals several crucial findings that contribute to improving native language identification. In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbançon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) , which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 discusses the methods in detail. Sect. 4 describes experiments conducted to investigate the hypothesis. Sect. 5 discusses the experimental results. Sect. 6 discusses implications for work in related domains.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As we will see in Sect. 6, this paper reveals several crucial findings that contribute to improving native language identification. ", "mid_sen": "In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbançon et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) , which is one of the central tasks in historical linguistics. ", "after_sen": "The rest of this paper is structured as follows. "}
{"citeStart": 112, "citeEnd": 125, "citeStartToken": 112, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "• Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992) . EM based algorithms begin with an initial (usually random) value for each parameter. If the initial assignment causes the grammar to be inconsistent, then iterative re-estimation might converge to an inconsistent grammar 1.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency.", "mid_sen": "• Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992) . ", "after_sen": "EM based algorithms begin with an initial (usually random) value for each parameter. "}
{"citeStart": 13, "citeEnd": 28, "citeStartToken": 13, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. For example, Magerman (1995) , Collins (1996) , and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. This paper also proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Magerman (1995) , Collins (1996) , and Charniak (1997), we assume that syntactic and lexical/semantic features are independent. Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In empirical approaches to parsing, lexical/semantic collocation extracted from corpus has been proved to be quite useful for ranking parses in syntactic analysis. ", "mid_sen": "For example, Magerman (1995) , Collins (1996) , and Charniak (1997) proposed statistical parsing models which incorporated lexical/semantic information. ", "after_sen": "In their models, syntactic and lexical/semantic features are dependent on each other and are combined together. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "After the shared task, several researchers tackled the problem using the CRFs and their extensions. Okanohara et al. (2006) applied semi-CRFs (Sarawagi and Cohen, 2004) , which can treat multiple words as corresponding to a single state. Friedrich et al. (2006) used CRFs with features from the external gazetteer. Current state-of-the-art for the shared-task is achieved by Tsai et al. (2006) , whose improvement depends on careful design of features including the normalization of numeric expressions, and use of post-processing by automatically extracted patterns. ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After the shared task, several researchers tackled the problem using the CRFs and their extensions. ", "mid_sen": "Okanohara et al. (2006) applied semi-CRFs (Sarawagi and Cohen, 2004) , which can treat multiple words as corresponding to a single state. ", "after_sen": "Friedrich et al. (2006) used CRFs with features from the external gazetteer. "}
{"citeStart": 204, "citeEnd": 230, "citeStartToken": 204, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "Accordingly, we try to extract a hierarchical relation of words automatically and statistically. In previous research, ways of extracting from definition sentences in dictionaries (Tsurumaru et al., 1986; Shoutsu et al., 2003) or from a corpus by using patterns such as \"a part of\", \"is-a\", or \"and\" (Berland and Charniak, 1999; Caraballo, 1999) have been proposed. Also, there is a method that uses the dependence relation between words taken from a corpus (Matsumoto et al., 1996) . In contrast, we propose a method based on the inclusion relation of appearance patterns from corpora.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Accordingly, we try to extract a hierarchical relation of words automatically and statistically. ", "mid_sen": "In previous research, ways of extracting from definition sentences in dictionaries (Tsurumaru et al., 1986; Shoutsu et al., 2003) or from a corpus by using patterns such as \"a part of\", \"is-a\", or \"and\" (Berland and Charniak, 1999; Caraballo, 1999) have been proposed. ", "after_sen": "Also, there is a method that uses the dependence relation between words taken from a corpus (Matsumoto et al., 1996) . "}
{"citeStart": 11, "citeEnd": 37, "citeStartToken": 11, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008) . However, these do not capture discourse-level relations. Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008) , Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. ", "mid_sen": "Similar to Somasundaran et al. (2008) , Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. ", "after_sen": "However, these works do not provide an implementation for their insights. "}
{"citeStart": 61, "citeEnd": 70, "citeStartToken": 61, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "It is worth emphasizing that we do not necessarily expect the dependency representations produced by the training method to be traditional dependency structures for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see below) dependency representations that are appropriate to performing the translation task for a specific language pair or specific bilingual corpus. For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. This contrasts with one of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the translation problem, i.e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, headwords in both languages are chosen to force a synchronized alignment (for better or worse) in order to simplify cases involving so-called head-switching. ", "mid_sen": "This contrasts with one of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the translation problem, i.e., the approach in which translation problems are seen in terms of bridging the gap between the most natural monolingual representations underlying the sentences of each language.", "after_sen": "The training method has four stages: (i) Compute co-occurrence statistics from the training data. "}
{"citeStart": 178, "citeEnd": 189, "citeStartToken": 178, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 119, "citeEnd": 138, "citeStartToken": 119, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "Whereas previous finite-state-model-based toolkits place many severe restrictions on domain descriptions, WIT has enough descriptive power to build a variety of dialogue systems. Although the dialogue state is represented by a simple attributevalue matrix, since there is no limitation on the number of attributes, it can hold more complicated information. For example, it is possible to represent a discourse stack whose depth is limited. Recording some dialogue history is also possible. Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. For example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar (Nakano et al., 1994; Nakano and Shimazu, 1999) . The language generation module features Common Lisp functions, so there is no limitation on the description. Some of the systems we have developed feature a generation method based on hierarchical planning (Dohsaka and Shirnazu, 1997) . It is also possible to build a simple finite-state-model-based dialogue system using WIT. States can be represented by dialogue phases in WIT.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the language understanding module utilizes unification, a wide variety of linguistic phenomena can be covered. ", "mid_sen": "For example, speech repairs, particle omission, and fillers can be dealt with in the framework of unification grammar (Nakano et al., 1994; Nakano and Shimazu, 1999) . ", "after_sen": "The language generation module features Common Lisp functions, so there is no limitation on the description. "}
{"citeStart": 32, "citeEnd": 59, "citeStartToken": 32, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "Since many previous studies on Japanese NER used 5-fold cross validation for the IREX dataset, we also performed it for some our models that had the best σ 2 found in the previous experiments. The results are listed in Table 7 with references to the results of recent studies. These results not only reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. It uses many structural features that are not used in our model. Incorporating such features might improve our model further.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These results not only reconfirmed the effects of the gazetteer features shown in the previous experiments, but they also showed that our best model is comparable to the state-of-theart models. ", "mid_sen": "The system recently proposed by Sasano and Kurohashi (2008) is currently the best system for the IREX dataset. ", "after_sen": "It uses many structural features that are not used in our model. "}
{"citeStart": 75, "citeEnd": 96, "citeStartToken": 75, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been much work in other fields, including linguistics, literary theory, psychology, philosophy, and content analysis, involving subjective language. As mentioned in Section 2, the conceptualization underlying our manual annotations is based on work in literary theory and linguistics, most directly Dolezel (1973), Uspensky (1973) , Kuroda (1973 Kuroda ( , 1976 , Chatman (1978) , Cohn (1978) , Fodor (1979) , and Banfield (1982) . We also mentioned existing knowledge resources such as affective lexicons (General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998] ). Such knowledge may be used in future work to complement the work presented in this article, for example, to seed the distributional-similarity process described in Section 3.4.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As mentioned in Section 2, the conceptualization underlying our manual annotations is based on work in literary theory and linguistics, most directly Dolezel (1973), Uspensky (1973) , Kuroda (1973 Kuroda ( , 1976 , Chatman (1978) , Cohn (1978) , Fodor (1979) , and Banfield (1982) . ", "mid_sen": "We also mentioned existing knowledge resources such as affective lexicons (General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998] ). ", "after_sen": "Such knowledge may be used in future work to complement the work presented in this article, for example, to seed the distributional-similarity process described in Section 3.4."}
{"citeStart": 39, "citeEnd": 54, "citeStartToken": 39, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "Naive Bayes and perceptron are similar in that they both employ a weighted combination of all features. The decision-tree and logic-based approaches all attempt to find a combination of a relatively small set of features that accurately predict classification. After training on 1,200 examples, the symbolic structures learned for the line corpus are relatively large. Average sizes are 369 leaves for C4.5 decision trees, 742 literals for PFoIL-DLIST decision lists, 841 literals for PFoIL-DNF formulae, and 1197 literals for PFoIL-CNF formulae. However, many nodes or literals can test the same feature and the last two results include the total literal count for six separate DNF or CNF formulae (one for each sense). Therefore, each discrimination is clearly only testing a relatively small fraction of the 2,859 available features. Nearest neighbor bases its classifications on all features; however, it weights them all equally. Therefore, differential weighting is apparently necessary for high-performance on this problem. Alternative instance-based methods that weight features based on their predictive ability have also been developed (Aha et al., 1991) . Therefore, our results indicate that lexical disambiguation is perhaps best performed using methods that combine weighted evidence from all of the features rather tures actually present in the examples. Without this optimization, testing would have been several orders of magnitude slower. than making a decision by testing only a small subset of highly predictive features. Among the other methods tested, decision lists seem to perform the best. The ordering of rules employed in a decision list in order to simplify the representation and perform conflict resolution apparently gives it an advantage over other symbolic methods on this task. In addition to the results reported by Yarowsky (1994) and Mooney and Califf (1995) , it provides evidence for the utility of this representation for natural-language problems.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ordering of rules employed in a decision list in order to simplify the representation and perform conflict resolution apparently gives it an advantage over other symbolic methods on this task. ", "mid_sen": "In addition to the results reported by Yarowsky (1994) and Mooney and Califf (1995) , it provides evidence for the utility of this representation for natural-language problems.", "after_sen": "With respect to training time, the symbolic methods are significantly slower since they are searching for a simple declarative representation of the concept. "}
{"citeStart": 106, "citeEnd": 116, "citeStartToken": 106, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Our working prototype indicates that the methods described here are worth developing, and that connectionist methods can be used to generalise from the training corpus to unseen text. Since data can be represented as higher-order tuples, single layer networks can be used. The traditional problems of training times do not arise. We have also used multi-layer nets on this data: they have no advantages, and perform slightly less well (Lyon, 1994) . The supporting role of the grammatic framework and the prohibition filters should not be underestimated. Whenever the scope of the system is extended it has been found necessary to enhance these elements.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The traditional problems of training times do not arise. ", "mid_sen": "We have also used multi-layer nets on this data: they have no advantages, and perform slightly less well (Lyon, 1994) . ", "after_sen": "The supporting role of the grammatic framework and the prohibition filters should not be underestimated. "}
{"citeStart": 105, "citeEnd": 117, "citeStartToken": 105, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "• even a highest ranked pattern for i is only a probabilistic cue for membership of i, so membership should only be inferred if there are enough occurrences of patterns for i in the data to outweigh the error probability for i. This simple automated, hybrid linguistic/statistical approach contrasts with the manual linguistic analysis of the COMLEX Syntax lexicographers (Meyers et al., 1994) , who propose five criteria and five heuristics for argument-hood and six criteria and two heuristics for adjunct-hood, culled mostly from the linguistics literature. Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993) ) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. We hope to exploit this information where possible at a later stage in the development of our approach. However, recognizing same/similar arguments requires considerable quantities of lexical data or the ability to back-off to lexical semantic classes. At the moment, we exploit linguistic information about the syntactic type, obligatoriness and position of arguments, as well as the set of possible subcategorization classes, and combine this with statistical inference based on the probability of class membership and the frequency and reliability of patterns for classes.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of these are not exploitable automatically because they rest on semantic judgements which cannot (yet) be made automatically: for example, optional arguments are often 'understood' or implied if missing. ", "mid_sen": "Others are syntactic tests involving diathesis alternation possibilities (e.g. passive, dative movement, Levin (1993) ) which require recognition that the 'same' argument, defined usually by semantic class / thematic role, is occurring across argument positions. ", "after_sen": "We hope to exploit this information where possible at a later stage in the development of our approach. "}
{"citeStart": 130, "citeEnd": 146, "citeStartToken": 130, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "We first report the overall results by comparing CRFs with HMMs, and with the previously best benchmark results obtained by SVMs (Han et al., 2003) . We then break down the results to analyze various factors individually. Table 1 shows the results on dataset H with the best results in bold; (intro and page fields are not shown, following past practice (Seymore et al., 1999; Han et al., 2003) ). The results we obtained with CRFs use secondorder state transition features, layout features, as well as supported and unsupported features. Feature induction is used in experiments on dataset R; (it didn't improve accuracy on H). The results we obtained with the HMM model use a second order model for transitions, and a first order for observations. The results on SVM is obtained from (Han et al., 2003) by computing F1 measures from the precision and recall numbers they report. Table 2 shows the results on dataset R. SVM results are not available for these datasets.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We first report the overall results by comparing CRFs with HMMs, and with the previously best benchmark results obtained by SVMs (Han et al., 2003) . ", "after_sen": "We then break down the results to analyze various factors individually. "}
{"citeStart": 144, "citeEnd": 164, "citeStartToken": 144, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Cases of intracategorial synonymy are relatively straigthtforward as several electronic synonym dictionnaries for french are available (Ploux, 1997) . Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993; Lin, 1998) . techniques.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Multi word expressions however remain a problem as they are often not or only partially included in such dictionnaries. ", "mid_sen": "For these or for a specific domain, basic synonymic dictionaries can be complemented using learning methods based on distributional similarity (Pereira et al., 1993; Lin, 1998) . techniques.", "after_sen": "For intercategorial synonymy involving a derivational morphology link, some resources are available which however are only partial in that they only store morphological families that is, sets of items that are morphologically related. "}
{"citeStart": 82, "citeEnd": 100, "citeStartToken": 82, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "For any local tree, we consider only the head span of the head, and the subtree spans of any modifiers. Typically, cohesion would be determined by checking these projected spans for intersection. However, at this level of resolution, avoiding intersection becomes highly restrictive. The monotone translation in Figure 1 would become non-cohesive: nobody intersects with both its sibling pay and with its head likes at phrase index 1. This complication stems from the use of multi-word phrases that do not correspond to syntactic constituents. Restricting phrases to syntactic constituents has been shown to harm performance (Koehn et al., 2003) , so we tighten our definition of a violation to disregard cases where the only point of overlap is obscured by our phrasal resolution. To do so, we replace span intersection with a new notion of span innersection. ", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This complication stems from the use of multi-word phrases that do not correspond to syntactic constituents. ", "mid_sen": "Restricting phrases to syntactic constituents has been shown to harm performance (Koehn et al., 2003) , so we tighten our definition of a violation to disregard cases where the only point of overlap is obscured by our phrasal resolution. ", "after_sen": "To do so, we replace span intersection with a new notion of span innersection. "}
{"citeStart": 75, "citeEnd": 91, "citeStartToken": 75, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work in information extraction from research papers has been based on two major machine learning techniques. The first is hidden Markov models (HMM) (Seymore et al., 1999; Takasu, 2003) . An HMM learns a generative model over input sequence and labeled sequence pairs. While enjoying wide historical success, standard HMM models have difficulty modeling multiple non-independent features of the observation sequence. The second technique is based on discriminatively-trained SVM classifiers (Han et al., 2003) . These SVM classifiers can handle many nonindependent features. However, for this sequence labeling problem, Han et al. (2003) work in a two stages process: first classifying each line independently to assign it label, then adjusting these labels based on an additional classifier that examines larger windows of labels. Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While enjoying wide historical success, standard HMM models have difficulty modeling multiple non-independent features of the observation sequence. ", "mid_sen": "The second technique is based on discriminatively-trained SVM classifiers (Han et al., 2003) . ", "after_sen": "These SVM classifiers can handle many nonindependent features. "}
{"citeStart": 39, "citeEnd": 46, "citeStartToken": 39, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Van Halteren et al. 1998 and Brill and Wu 1998 show that part-of-speech tagger performance can be improved by combining di erent taggers. By using techniques such as majority voting, errors made by the minority of the taggers can be removed. Van Halteren et al. 1998 report that the results of such a combined approach can improve upon the accuracy error of the best individual system with as much as 19.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Van Halteren et al. 1998 and Brill and Wu 1998 show that part-of-speech tagger performance can be improved by combining di erent taggers. ", "after_sen": "By using techniques such as majority voting, errors made by the minority of the taggers can be removed. "}
{"citeStart": 310, "citeEnd": 315, "citeStartToken": 310, "citeEndToken": 315, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, It would be interesting to investigate the solution to point fl One possible way would be to disambiguate the senses of the verbs appering in the corpus, using the SRs already acquired and gathering evidence of the patterns corresponding to each sense by means of a technique similar to that used by [Yar92] . Therefore, once disambiguated the verb senses it would be possible to split the set of SRs acquired.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To use the PPs that in the corpus are attached to other complements and not to the main verb as a source of \"implicit negative examples\", in such a way that they would constrain the overgeneralization.", "mid_sen": "Finally, It would be interesting to investigate the solution to point fl One possible way would be to disambiguate the senses of the verbs appering in the corpus, using the SRs already acquired and gathering evidence of the patterns corresponding to each sense by means of a technique similar to that used by [Yar92] . ", "after_sen": "Therefore, once disambiguated the verb senses it would be possible to split the set of SRs acquired."}
{"citeStart": 56, "citeEnd": 79, "citeStartToken": 56, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set. This can be done by smoothing the observed frequencies (Church and Mercer, 1993) , or by class-based methods (Brown et al., 1991; Pereira and Tishby, 1992; Pereira et ah, 1993; Hirschman, 1986; Resnik, 1992; Brill et ah, 1990; Dagan et al., 1993) . In comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics. This allows the system to learn successfully from very sparse data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set. ", "mid_sen": "This can be done by smoothing the observed frequencies (Church and Mercer, 1993) , or by class-based methods (Brown et al., 1991; Pereira and Tishby, 1992; Pereira et ah, 1993; Hirschman, 1986; Resnik, 1992; Brill et ah, 1990; Dagan et al., 1993) . ", "after_sen": "In comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics. "}
{"citeStart": 186, "citeEnd": 207, "citeStartToken": 186, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the topic clusters from the 105th Senate as training data to adjust the parameter s min and observe trends in the data. We did not run experiments to test the effect of different values of s min on MavenRank scores, but our chosen value of 0.25 has shown to give acceptable results in similar experiments (Erkan and Radev, 2004) . We used the topic clusters from the 106th Senate as test data. For the speech document networks, there was an average of 351 nodes (speech documents) and 2142 edges per topic. For the speaker document networks, there was an average of 63 nodes (speakers) and 545 edges per topic.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used the topic clusters from the 105th Senate as training data to adjust the parameter s min and observe trends in the data. ", "mid_sen": "We did not run experiments to test the effect of different values of s min on MavenRank scores, but our chosen value of 0.25 has shown to give acceptable results in similar experiments (Erkan and Radev, 2004) . ", "after_sen": "We used the topic clusters from the 106th Senate as test data. "}
{"citeStart": 10, "citeEnd": 35, "citeStartToken": 10, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Exact inference in the MAS model is intractable. Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in . Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984) . It is used to produce a sample from a joint distribution when only conditional distributions of each variable can be efficiently computed. In Gibbs sampling, variables are sequentially sampled from their distributions conditioned on all other variables in the model. Such a chain of model states converges to a sample from the joint distribution. A naive application of this technique to LDA would imply that both assignments of topics to words z and distributions θ and ϕ should be sampled. However, demonstrated that an efficient collapsed Gibbs sampler can be constructed, where only assignments z need to be sampled, whereas the dependency on distributions θ and ϕ can be integrated out analytically.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Exact inference in the MAS model is intractable. ", "mid_sen": "Following Titov and McDonald (2008) we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in . ", "after_sen": "Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm (Geman and Geman, 1984) . "}
{"citeStart": 94, "citeEnd": 115, "citeStartToken": 94, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "We previously classified noun phrases by referential property into the following three types (Murata and Nagao 1993 ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The referential property plays an important role in clarifying the anaphoric relation.", "mid_sen": "We previously classified noun phrases by referential property into the following three types (Murata and Nagao 1993 ", "after_sen": ""}
{"citeStart": 90, "citeEnd": 112, "citeStartToken": 90, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "The generic phrase training algorithm follows an information retrieval perspective as in (Venugopal et al., 2003) but aims to improve both precision and recall with the trainable log-linear model. A clear advantage of the proposed approach over the widely used ViterbiExtract method is trainability. Under the general framework, one can put as many features as possible together under the log-linear model to evaluate the quality of a phrase and a phase pair. The phrase table extracting procedure is trainable and can be optimized jointly with the translation engine.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The generic phrase training algorithm follows an information retrieval perspective as in (Venugopal et al., 2003) but aims to improve both precision and recall with the trainable log-linear model. ", "after_sen": "A clear advantage of the proposed approach over the widely used ViterbiExtract method is trainability. "}
{"citeStart": 350, "citeEnd": 363, "citeStartToken": 350, "citeEndToken": 363, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural language word patterns. Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the past two decades computational morphology has been quite successful in dealing with the challenges posed by natural language word patterns. ", "mid_sen": "Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "after_sen": "However, Sproat (1992) observes that, despite the existence of \"working systems that are capable of doing a great deal of morphological analysis\", \"there are still outstanding problems and areas which have not received much serious attention\" (ibid., 123) . "}
{"citeStart": 85, "citeEnd": 108, "citeStartToken": 85, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the new word is a correct word according to the hunspell lexicon using the new spelling rules, we map the words.", "mid_sen": "When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus.", "after_sen": "As a last preprocessing step we remove sentences that are too long and empty lines to obtain the final training corpus."}
{"citeStart": 70, "citeEnd": 94, "citeStartToken": 70, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005) . However, our model has several advantages over this technique. First, our model has no tunable parameters: the random-walk method has many (graph connectivity, various thresholds, choice of similarity metrics, etc.). Moreover, since our model is properly Bayesian, it is straightforward to extend it to model other aspects of the problem, or to related problems. Doing so in a non ad-hoc manner in the random-walk model would be nearly impossible.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Alternatively, one could include user models to account for novelty or user preferences along the lines of Zhang et al. (2002) .", "mid_sen": "Our model is similar in spirit to the randomwalk summarization model (Otterbacher et al., 2005) . ", "after_sen": "However, our model has several advantages over this technique. "}
{"citeStart": 31, "citeEnd": 41, "citeStartToken": 31, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "A minimal grammar, set out in (Lyon, 1994) in EBNF form, is composed of 9 rules. For instance, the subject must contain a noun-type word. Applying this particular rule to sentence (3) above would eliminate candidate strings (3.1) and (3.2). We also have the 2 arbitrary limits on length of pre-subject and subject. There is a small set of 4 extensions to the grammar, or semi-local constraints. For instance, if a relative pronoun occurs, then a verb must follow in that constituent. On the technical manuals the constraints of the grammatic framework put up to 6% of declarative sentences outside our system, most commonly because the pre-subject is too long. A small number are excluded because the system cannot handle a co-ordinated head. With the length of pre-subject extended to 15 words, and subject to 12 words, an average of 2% are excluded (7 out of 351).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A minimal grammar, set out in (Lyon, 1994) in EBNF form, is composed of 9 rules. ", "after_sen": "For instance, the subject must contain a noun-type word. "}
{"citeStart": 43, "citeEnd": 54, "citeStartToken": 43, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "A drawback of the pure Lambek Calculus !_ is that it only allows for so-called 'peripheral extraction', i.e., in our example the trace should better be initial or final in the relative clause. This inflexibility of Lambek Calculus is one of the reasons why many researchers study richer systems today. For instance, the recent work by Moortgat (Moortgat 94) gives a systematic in-depth study of mixed Lambek systems, which integrate the systems L, NL, NLP, and LP. These ingredient systems are obtained by varying the Lambek calculus along two dimensions: adding the permutation rule (P) and/or dropping the assumption that the type combinator (which forms the sequences the systems talk about) is associative (N for non-associative).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This inflexibility of Lambek Calculus is one of the reasons why many researchers study richer systems today. ", "mid_sen": "For instance, the recent work by Moortgat (Moortgat 94) gives a systematic in-depth study of mixed Lambek systems, which integrate the systems L, NL, NLP, and LP. ", "after_sen": "These ingredient systems are obtained by varying the Lambek calculus along two dimensions: adding the permutation rule (P) and/or dropping the assumption that the type combinator (which forms the sequences the systems talk about) is associative (N for non-associative)."}
{"citeStart": 165, "citeEnd": 188, "citeStartToken": 165, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "mid_sen": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . ", "after_sen": "Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . "}
{"citeStart": 87, "citeEnd": 111, "citeStartToken": 87, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "Without the correct bracketing in the Treebank, strictly right-branching trees were created for all base-NPs. This has an unwelcome effect when conjunctions occur within an NP (Figure 1 ). An additional grammar rule is needed just to get a parse, but it is still not correct (Hockenmaier, 2003, p. 64) . The awkward conversion results in bracketing (a) which should be (b): (a) (consumer ((electronics) and (appliances (retailing chain)))) (b) ((((consumer electronics) and appliances) retailing) chain)", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This has an unwelcome effect when conjunctions occur within an NP (Figure 1 ). ", "mid_sen": "An additional grammar rule is needed just to get a parse, but it is still not correct (Hockenmaier, 2003, p. 64) . ", "after_sen": "The awkward conversion results in bracketing (a) which should be (b): (a) (consumer ((electronics) and (appliances (retailing chain)))) (b) ((((consumer electronics) and appliances) retailing) chain)"}
{"citeStart": 28, "citeEnd": 53, "citeStartToken": 28, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "This kind of inconsistency between sentence-level polarity and word-level polarity often occurs and causes errors in the majority voting. The reason is that the majority voting cannot take into account negation expressions or adversative conjunctions, e.g., \"I have not had any ...\" in the example above. Therefore, taking such polarity-shifting into account is important for classification of sentences using a polarity dictionary. To circumvent this problem, Kennedy and Inkpen (2006) and Hu and Liu (2004) proposed to use a manually-constructed list of polarity-shifters. However, it has limitations due to the diversity of expressions.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Therefore, taking such polarity-shifting into account is important for classification of sentences using a polarity dictionary. ", "mid_sen": "To circumvent this problem, Kennedy and Inkpen (2006) and Hu and Liu (2004) proposed to use a manually-constructed list of polarity-shifters. ", "after_sen": "However, it has limitations due to the diversity of expressions."}
{"citeStart": 166, "citeEnd": 178, "citeStartToken": 166, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "The recognition task involving arbitrary noun phrases attempts to find both baseNPs and noun phrases that contain other noun phrases. A standard data set for this task was put forward at the CoNLL-99 workshop. It consist on the same parts of the Penn Treebank as the main baseNP data set: WSJ sections 15-18 as training data and section 20 as test data 2. The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the (Ramshaw and Marcus, 1995) data sets.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It consist on the same parts of the Penn Treebank as the main baseNP data set: WSJ sections 15-18 as training data and section 20 as test data 2. ", "mid_sen": "The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the (Ramshaw and Marcus, 1995) data sets.", "after_sen": "In both tasks, performance is measured with three scores. "}
{"citeStart": 144, "citeEnd": 158, "citeStartToken": 144, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "There are many different criteria for quantifying the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview. Unfortunately, whatever the criterion selected, it is in general impractical to find the optimal clustering of the data; instead, one of a variety of algorithms must be used to find a locally optimal solution. Let us for the moment consider the case where the language model consists only of a unigram probability distribution for the words in the vocabulary, with no N-gram (for N > 1) or fuller linguistic constraints considered. Perhaps the most obvious measure of the similarity between two sentences or clusters is then Jaccard's coefficient (Everitt, 1993, p41) , the ratio of the number of words occurring in both sentences to the number occurring in either or both. Another possibility would be Euclidean distance, with each word in the vocabulary defining a dimension in a vector space. However, it makes sense to choose as a similarity measure the quantity we would like the final clustering arrangement to minimize: the expected entropy (or, equivalently, perplexity) of sentences from the domain. This goal is analogous to that used in the work described earlier on finding word classes by clustering.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clustering Algorithms", "mid_sen": "There are many different criteria for quantifying the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview. ", "after_sen": "Unfortunately, whatever the criterion selected, it is in general impractical to find the optimal clustering of the data; instead, one of a variety of algorithms must be used to find a locally optimal solution. "}
{"citeStart": 110, "citeEnd": 130, "citeStartToken": 110, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004) . Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary medical sources and demonstrates that respectable performance is possible with a feature-combination approach.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision.", "mid_sen": "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004) . ", "after_sen": "Their study also illustrates the importance of semantic classes and relations. "}
{"citeStart": 295, "citeEnd": 322, "citeStartToken": 295, "citeEndToken": 322, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to express both established and new data categories and properties, from linguistics as well as from nonlinguistic communication, we developed a new data category registry, which contains links to other resources in the LLOD cloud, in particular to the ISOcat data category repository (Windhouwer and Wright, 2012) , but also serves as a place where categories from novel research fields (mainly multimodal communication) can be collected, discussed, until they have settled down and are stable enough for an integration into more authoritative category registries, such as ISOcat. By means of this we aim to make the re- Figure 1 : A screen configuration as seen by the slider, who can see the last chat message (bottom part) and move objects with a mouse. Unused objects are stored in an area on the left. source more widely available and to enable a long and successful lifecycle for the resource.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Due to the challenging nature of the data, in particular that it contains annotations on multiple timelines, we developed a new model for the representation of this data, which we call FiESTA.", "mid_sen": "In order to express both established and new data categories and properties, from linguistics as well as from nonlinguistic communication, we developed a new data category registry, which contains links to other resources in the LLOD cloud, in particular to the ISOcat data category repository (Windhouwer and Wright, 2012) , but also serves as a place where categories from novel research fields (mainly multimodal communication) can be collected, discussed, until they have settled down and are stable enough for an integration into more authoritative category registries, such as ISOcat. ", "after_sen": "By means of this we aim to make the re- Figure 1 : A screen configuration as seen by the slider, who can see the last chat message (bottom part) and move objects with a mouse. "}
{"citeStart": 98, "citeEnd": 120, "citeStartToken": 98, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008) , although training would be much slower compared to using generative models, as in our case.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training. ", "mid_sen": "Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008) , although training would be much slower compared to using generative models, as in our case.", "after_sen": "In future work, we plan to scale up the training process with more unlabeled training data (e.g., gigaword) and investigate automatic selection of materials that are most suitable for self-training. "}
{"citeStart": 276, "citeEnd": 295, "citeStartToken": 276, "citeEndToken": 295, "sectionName": "UNKNOWN SECTION NAME", "string": "ing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot & Lang, 1989) , or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma & van Noord, 1993; Maxwell & Kaplan, 1993) . It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard & Sag, 1987) . However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. ", "mid_sen": "Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard & Sag, 1987) . ", "after_sen": "However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed."}
{"citeStart": 66, "citeEnd": 90, "citeStartToken": 66, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "To perform this experiment we take one-by-one each rule from the rule-sets produced at the rule extraction phase, take each word token from the corpus and guess its POS-set using the rule if the rule is applicable to the word. For example, if a guessing rule strips a particular suffix and a current word from the corpus does not have this suffix, we classify these word and rule as incompatible and the rule as not applicable to that word. If the rule is applicable to the word we perform look-up in the lexicon for this word and then compare the result of the guess with the information listed in the lexicon. If the guessed POS-set is the same as the POS-set stated in the lexicon, we count it as success, otherwise it is failure. The value of a guessing rule, thus, closely correlates with its estimated proportion of success /p~l which is the proportion of all positive outcomes of the rule application to the total number of the trials (n), which are, in fact, attempts to apply the rule to all the compatible words in the corpus. We also smooth/3 so as not to have zeros in.positive or negative outcome probabilities: 15 = ~.~i~ /3 estimate is a good indicator of rule accuracy. However, it frequently suffers from large estimation error due to insufficient training data. For example, ifa rule was detected to work just twice and the total number of observations was also two, its estimate/3 is very high (1, or 0.83 for the smoothed version) but clearly this is not a very reliable estimate because of the tiny size of the sample. Several smoothing methods have been proposed to reduce the estimation error. For different reasons all these smoothing methods are not very suitable in our case. In our approach we tackle this problem by calculating the lower confidence limit 7r L for the rule estimate. This can be seen as the minimal expected value of/3 for the rule if we were to draw a large number of samples. Thus with certain confidence ~ we can assume that if we used more training data, the rule estimate /3 would be no worse than the ~L limit. The lower confidence limit 7r L is calculated as: ~rL =/3 --Z(l-~)/2 * sp =/3 --z(t-~)/2 * This function favours the rules with higher estimates obtained over larger samples. Even if one rule has a high estimate but that estimate was obtained over a small sample, another rule with a lower estimate but over a large sample might be valued higher. Note also that since/3 itself is smoothed we will not have zeros in positive (/3) or negative (1 -/3) outcome probabilities. This estimation of the rule value in fact resembles that used by (Tzoukermann et al., 1995) for scoring pos-disambiguation rules for the French tagger. The main difference between the two functions is that there the z value was implicitly assumed to be 1 which corresponds to the confidence of 68%. A more standard approach is to adopt a rather high confidence value in the range of 90-95%. We adopted 90% confidence for which z(1-0.90)/2 = z0.05 = 1.65. Thus we can calculate the score for the ith rule as: /3i -1.65 * ~/P~(QP') Another important consideration for scoring a word-guessing rule is that the longer the affix or ending of the rule the more confident we are that it is not a coincidental one, even on small samples. For example, if the estimate for the word-ending \"o\" was obtained over a sample of 5 words and the estimate for the word-ending \"fulness\" was also obtained over a sample of 5 words, the later case is more representative even though the sample size is the same. Thus we need to adjust the estimation error in accordance with the length of the affix or ending. A good way to do that is to divide it by a value which increases along with the increase of the length. After several experiments we obtained:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note also that since/3 itself is smoothed we will not have zeros in positive (/3) or negative (1 -/3) outcome probabilities. ", "mid_sen": "This estimation of the rule value in fact resembles that used by (Tzoukermann et al., 1995) for scoring pos-disambiguation rules for the French tagger. ", "after_sen": "The main difference between the two functions is that there the z value was implicitly assumed to be 1 which corresponds to the confidence of 68%. "}
{"citeStart": 164, "citeEnd": 182, "citeStartToken": 164, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "Chinese Word Segmentation (CWS) has witnessed a prominent progress in the first four SIGHAN Bakeoffs. Since Xue (2003) used character-based tagging, this method has attracted more and more attention. Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units. Because the word-based models can capture the word-level contextual information and IV knowledge. Besides, many strategies are proposed to balance the IV and OOV performance (Wang et al., 2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since Xue (2003) used character-based tagging, this method has attracted more and more attention. ", "mid_sen": "Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units. ", "after_sen": "Because the word-based models can capture the word-level contextual information and IV knowledge. "}
{"citeStart": 235, "citeEnd": 253, "citeStartToken": 235, "citeEndToken": 253, "sectionName": "UNKNOWN SECTION NAME", "string": "A featnre-based tag grammar was written for this investigation (based loosely on one written by Briscoe and Waegner [1992] ), and used in conjunction with tile parser inchlded in the Alvey Tools' Grammar Development Environment (ODE) [Carroll etal, 1991] , which allows for rapid prototyping aud e,~sy analysis of parses. It should be stressed that this grammar is solely one of tags, aild so is not very detailed syntactically.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "S 4 lip (eonllna) s.", "mid_sen": "A featnre-based tag grammar was written for this investigation (based loosely on one written by Briscoe and Waegner [1992] ), and used in conjunction with tile parser inchlded in the Alvey Tools' Grammar Development Environment (ODE) [Carroll etal, 1991] , which allows for rapid prototyping aud e,~sy analysis of parses. ", "after_sen": "It should be stressed that this grammar is solely one of tags, aild so is not very detailed syntactically."}
{"citeStart": 86, "citeEnd": 114, "citeStartToken": 86, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "The first step is to take the seed, which might consist of as few as two concept words, and generate many (ideally, all, when the concept is a closed set of words) members of the class to which they belong. We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006) . For any pair of seed words S i and S j , search the corpus for word patterns of the form S i HS j , where H is a high-frequency word in the corpus (we used the 100 most frequent words in the corpus). Of these, we keep all those patterns, which we call symmetric patterns, for which S j HS i is also found in the corpus. Repeat this process to find symmetric patterns with any of the structures HSHS, SHSH or SHHS. It was shown in (Davidov and Rappoport, 2006 ) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). Other words in the class can thus be generated by searching a sub-corpus of documents including at least two concept words for those words X that appear in a sufficient number of instances of both the patterns S i HX and XHS i , where S i is a word in the class. The same can be done for the other three pattern structures. The process can be bootstrapped as more words are added to the class.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first step is to take the seed, which might consist of as few as two concept words, and generate many (ideally, all, when the concept is a closed set of words) members of the class to which they belong. ", "mid_sen": "We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006) . ", "after_sen": "For any pair of seed words S i and S j , search the corpus for word patterns of the form S i HS j , where H is a high-frequency word in the corpus (we used the 100 most frequent words in the corpus). "}
{"citeStart": 97, "citeEnd": 111, "citeStartToken": 97, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989) . The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984) . For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984) . ", "mid_sen": "For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994) .", "after_sen": "An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees. "}
{"citeStart": 83, "citeEnd": 110, "citeStartToken": 83, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "proposed in (Tsarfaty, 2006) . In our third model GT ppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007) . In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an (2007) . Finally, model GT v = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima'an, 2007; Cohen and Smith, 2007) . For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our third model GT ppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007) . ", "mid_sen": "In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an (2007) . ", "after_sen": "Finally, model GT v = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima'an, 2007; Cohen and Smith, 2007) . "}
{"citeStart": 79, "citeEnd": 106, "citeStartToken": 79, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "In the grammar and tree transforms, P is the set of productions in G i.e., the standard left-corner transform, N is the set of all productions in P which do not begin with a POS tag, and L0 is the set of left-recursive Turning now to the PCFGs estimated after applying tree transforms, we notice that grammar size does not increase nearly so dramatically. These PCFGs encode a maximum-likelihood estimate of the state transition probabilities for various stochastic generalized left-corner parsers, since a top-down parser using these grammars simulates a generalized left-corner parser. The fact that LC P G is 17 times larger than the PCFG inferred after applying T P to the tree-bank means that most of the possible transitions of a standard stochastic left-corner parser are not observed in the tree-bank training data. The state of a left-corner parser does capture some linguistic generalizations Manning and Carpenter, 1997; Roark and Johnson, 1999 , but one might still expect sparse-data problems. Note that LC td;lc L0 is only 1.4 times larger than T td;lc", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The fact that LC P G is 17 times larger than the PCFG inferred after applying T P to the tree-bank means that most of the possible transitions of a standard stochastic left-corner parser are not observed in the tree-bank training data. ", "mid_sen": "The state of a left-corner parser does capture some linguistic generalizations Manning and Carpenter, 1997; Roark and Johnson, 1999 , but one might still expect sparse-data problems. ", "after_sen": "Note that LC td;lc L0 is only 1.4 times larger than T td;lc"}
{"citeStart": 88, "citeEnd": 102, "citeStartToken": 88, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we have described how generation resources for restricted applications can be developed drawing on large-scale general generation grammars. This enables both re-use of those resources and progressive growth as new applications are met. The grammar extraction tool then makes it a simple task to extract from the large-scale resources specially tuned subgrammars for particular applications. Our approach shows some similarities to that proposed by (Rayner and Carter, 1996) for improving parsing performance by grammar pruning and specialization with respect to a training corpus. Rule components are 'chunked' and pruned when they are unlikely to contribute to a successful parse. Here we have shown how improvements in generation performance can be achieved for generation grammars by removing parts of the grammar specification that are not used in some particular sublanguage. The extracted grammar is generally known to cover the target sublanguage and so there is no loss of required coverage. Another motivation for this work is the need for smaller, but not toy-sized, systemic grammars for their experimental compilation into state-of-the-art feature logics. The ready access to consistent subgrammars of arbitrary size given with the automatic subgrammar extraction reported here allows us to investigate further the size to which feature logic representations of systemic grammar can grow while remaining practically usable. The compilation of the full grammar NIGEL has so far only proved possible for CUF (see (Henschel, 1995) ), and the resulting type deduction runs too slowly for practical applications.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ready access to consistent subgrammars of arbitrary size given with the automatic subgrammar extraction reported here allows us to investigate further the size to which feature logic representations of systemic grammar can grow while remaining practically usable. ", "mid_sen": "The compilation of the full grammar NIGEL has so far only proved possible for CUF (see (Henschel, 1995) ), and the resulting type deduction runs too slowly for practical applications.", "after_sen": "It is likely that further improvements in generation performance will be achieved when both the grammatical structures and the extracted choosers are pruned. "}
{"citeStart": 62, "citeEnd": 83, "citeStartToken": 62, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . ", "mid_sen": "We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. ", "after_sen": "The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. "}
{"citeStart": 0, "citeEnd": 12, "citeStartToken": 0, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993) ; however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents. Sidner (1992; formulated an artificial language for modeling collaborative discourse using propo~acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, ff so, selects appropriate evidence from the system's private beliefs to support the claim.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993) ; however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents. ", "mid_sen": "Sidner (1992; formulated an artificial language for modeling collaborative discourse using propo~acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. ", "after_sen": "Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. "}
{"citeStart": 165, "citeEnd": 179, "citeStartToken": 165, "citeEndToken": 179, "sectionName": "UNKNOWN SECTION NAME", "string": "The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001) . The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. The best current model (Collins, 2000) has only 6% less precision error and only 11% less recall error than the lexicalized model. The SSN parser achieves this result using much less lexical knowledge than other approaches, which all minimally use the words which occur at least 5 times, plus morphological features of the remaining words. It is also achieved without any explicit notion of lexical head.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Tags model also does much better than the only other broad coverage neural network parser (Costa et al., 2001 ).", "mid_sen": "The bottom panel of table 1 lists the results for the chosen lexicalized model (SSN-Freq>200) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001) . ", "after_sen": "The performance of the lexicalized model falls in the middle of this range, only being beaten by the three best current parsers, which all achieve equivalent performance. "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993) . They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992) . Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Still, a lexicon is needed that specifies the possible parts of speech for every word. ", "mid_sen": "Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant.", "after_sen": "The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a) ."}
{"citeStart": 124, "citeEnd": 135, "citeStartToken": 124, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "To experiment with our models, we built two different corpora, WEB-QA and TREC-QA by using the description questions from TREC 2001 (Voorhees, 2001 ) and annotating the answers retrieved from Web resp. TREC data (available at disi.unitn.it/˜silviaq). Comparative experiments with re-ranking models of increasing complexity show that: (a) PAS-PTK is far more efficient and effective than SSTK, (b) POSSK provides a remarkable further improvement on previous models. Finally, our experiments on the TREC-QA dataset, un-biased by the presence of typical Web phrasings, show that BOW is inadequate to learn relations between questions and answers. This is the reason why our kernels on linguistic structures improve it by 63%, which is a remarkable result for an IR task (Allan, 2000) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally, our experiments on the TREC-QA dataset, un-biased by the presence of typical Web phrasings, show that BOW is inadequate to learn relations between questions and answers. ", "mid_sen": "This is the reason why our kernels on linguistic structures improve it by 63%, which is a remarkable result for an IR task (Allan, 2000) .", "after_sen": ""}
{"citeStart": 117, "citeEnd": 138, "citeStartToken": 117, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998) , trMned on the Penn Wall Street Journal Treebank (Marcus et al., 1993) . We defined cooccurrence in these constructions using the standard definitions of dominance and precedence. The relation is stipulated to be transitive, so that all head nouns in a list co-occur with each other (e.g. in the phrase planes, trains, and automobiles all three nouns are counted as co-occuring with each other). Two head nouns co-occur in this algorithm if they meet the following four conditions: 3 Statistics for selecting and ranking R&S used the same figure of merit both for selecting new seed words and for ranking words in the final output. Their figure of merit was simply the ratio of the times the noun coocurs with a noun in the seed list to the total frequency of the noun in the corpus. This statistic favors low frequency nouns, and thus necessitates the inclusion of a minimum occurrence cutoff. They stipulated that no word occuring fewer than six times in the corpus would be considered by the algorithm. This cutoff has two effects: it reduces the noise associated with the multitude of low frequency words, and it removes from consideration a fairly large number of certainly valid category members. Ideally, one would like to reduce the noise without reducing the number of valid nouns. Our statistics allow for the inclusion of rare occcurances. Note that this is particularly important given our algorithm, since we have restricted the relevant occurrences to a specific type of structure; even relatively common nouns m~v not occur in the corpus more than a handful of times in such a context.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We made the simplifying assumptions that a compound noun is a string of consecutive nouns (or, in certain cases, adjectives -see discussion below), and that the head of the compound is the rightmost noun.", "mid_sen": "To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998) , trMned on the Penn Wall Street Journal Treebank (Marcus et al., 1993) . ", "after_sen": "We defined cooccurrence in these constructions using the standard definitions of dominance and precedence. "}
{"citeStart": 139, "citeEnd": 158, "citeStartToken": 139, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section, we describe each of the components of our SFST system shown in Figure 1 . The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section, we describe each of the components of our SFST system shown in Figure 1 . ", "mid_sen": "The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005 ).", "after_sen": ""}
{"citeStart": 306, "citeEnd": 324, "citeStartToken": 306, "citeEndToken": 324, "sectionName": "UNKNOWN SECTION NAME", "string": "• use of low level knowledge from the speech recognition phase, • use of high level knowledge about the domain in particular and the dialogue task in general, • a \"continue\" facility and an \"auto-loop\" facility as described by Biermann and Krishnaswamy (1976) , • a \"conditioning\" facility as described by Fink et al. (1985) , • implementation of new types of paraphrasing, • checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and • examining inter-speaker dialogue patterns.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These include the following:", "mid_sen": "• use of low level knowledge from the speech recognition phase, • use of high level knowledge about the domain in particular and the dialogue task in general, • a \"continue\" facility and an \"auto-loop\" facility as described by Biermann and Krishnaswamy (1976) , • a \"conditioning\" facility as described by Fink et al. (1985) , • implementation of new types of paraphrasing, • checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen, and • examining inter-speaker dialogue patterns.", "after_sen": "All but two of these areas for expansion are aimed at moving the expectation system from one that finds patterns in a user's dialogues and acquires historical knowledge about them to one that can acquire true procedures. "}
{"citeStart": 193, "citeEnd": 214, "citeStartToken": 193, "citeEndToken": 214, "sectionName": "UNKNOWN SECTION NAME", "string": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . EventMine-MK is available as a component of the U-Compare interoperable text mining system 4 (Kano et al., 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Of these five dimensions, only KT, CL and Source were considered during the comparison with the other two schemes, since they are directly related to discourse analysis.", "mid_sen": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . ", "after_sen": "Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . "}
{"citeStart": 110, "citeEnd": 137, "citeStartToken": 110, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "To associate semantic representations with natural language expressions, the FTAG is modified as proposed in (Gardent and Kallmeyer, 2003) . Importantly, the arguments of a semantic functor are represented by unification variables which occur both in the semantic representation of this functor and on some nodes of the associated syntactic tree. For instance in Figure 1 , the semantic index s occurring in the semantic representation of runs also occurs on the subject substitution node of the associated elementary tree.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At the end of a derivation, the top and bottom of all nodes in the derived tree are unified.", "mid_sen": "To associate semantic representations with natural language expressions, the FTAG is modified as proposed in (Gardent and Kallmeyer, 2003) . ", "after_sen": "Importantly, the arguments of a semantic functor are represented by unification variables which occur both in the semantic representation of this functor and on some nodes of the associated syntactic tree. "}
{"citeStart": 210, "citeEnd": 230, "citeStartToken": 210, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "Our project's long-range goal (see http://www.isp. pitt.edu/'intgen/) is to create a unified architecture for collaborative discourse, accommodating both interpretation and generation. Our computational approach (Thomason and Hobbs, 1997) uses a form of weighted abduction as the reasoning mechanism (Hobbs et al., 1993) and modal operators to model context. In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. From our first annotation trials, we found that the recognition of \"classical\" speech acts (Austin, 1962; Searle, 1975) by coders is fairly reliable, while recognizing contextual relationships (e.g., whether an utterance accepts a proposal) is not as reliable. Thus, we explore other features that can help us recognize how participants coordinate agreement. Our corpus study also provides a preliminary assessment of the Discourse Resource Initiative (DR/) tagging scheme. The DRI is an international \"grassroots\" effort that seeks to share corpora that have been tagged with the core features of interest to the discourse community. In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes. A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html). Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997; Poesio and Traum, 1997) , we have attempted to adapt it to our corpus and particular research questions.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes. ", "mid_sen": "A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html). Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997; Poesio and Traum, 1997) , we have attempted to adapt it to our corpus and particular research questions.", "after_sen": "First we describe our corpus, and the issue of tracking agreement. "}
{"citeStart": 40, "citeEnd": 61, "citeStartToken": 40, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Tur-ney, 2002; Pang et al., 2002) where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal. Other work (Subasic and Huettner, 2001; Morinaga et al., 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of favorability. Automatic acquisition of sentiment expressions have also been studied (Hatzivassiloglou and McKeown, 1997) , but limited to adjectives, and only one sentiment could be assigned to each word. pointed out that the multiple sentiment aspects in a document should be extracted. This paper follows that approach, but exploits deeper analysis in order to avoid the analytic failures reported by , which occurred when they used a shallow parser and only addressed a limited number of syntactic phenomena. In our in-depth approach described in the next section, two types of errors out of the four reported by were easily removed .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Tur-ney, 2002; Pang et al., 2002) where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal. ", "mid_sen": "Other work (Subasic and Huettner, 2001; Morinaga et al., 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of favorability. ", "after_sen": "Automatic acquisition of sentiment expressions have also been studied (Hatzivassiloglou and McKeown, 1997) , but limited to adjectives, and only one sentiment could be assigned to each word. "}
{"citeStart": 36, "citeEnd": 49, "citeStartToken": 36, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "This type of learners constructs a representation for document vectors belonging to a certain class during the learning phase, e.g. decision trees, decision rules or probability weightings. During the categorization phase, the representation is used to assign the appropriate class to a new document vector. Several pruning or specialization heuristics can be used to control the amount of generalization. We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. Support Vector Machines (SVMs): SVMs are described in (Vapnik, 1995) . SVMs are binary learners in that they distinguish positive and negative examples for each class. Like eager learners, they construct a representation during the learning phase, namely a hyper plane supported by vectors of positive and negative examples. For each class, a categorizer is built by computing such a hyper plane. During the categorization phase, each categorizer is applied to the new document vector, yielding the probabilities of the document belonging to a class. The probability increases with the distance of thevector from the hyper plane. A document is said to belong to the class with the highest probability. We chose SVM_Light (Joachims, 1998) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several pruning or specialization heuristics can be used to control the amount of generalization. ", "mid_sen": "We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ", "after_sen": "ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. "}
{"citeStart": 140, "citeEnd": 155, "citeStartToken": 140, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) ) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002) ), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) ) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. ", "mid_sen": "Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002) ), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003) .", "after_sen": "Our research focuses on semantic lexicon induction, which aims to generate lists of words that be-long to a given semantic class (e.g., lists of FISH or VEHICLE words). "}
{"citeStart": 82, "citeEnd": 96, "citeStartToken": 82, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "As can be reconstructed from the numbering of the facts in figure 3 the resulting processing behavior is identical to the behavior that would result from Earley generation as in Gerdemann (1991) except that the different filtering steps are performed in a bottom-up fashion. In order to obtain a generator similar to the bottom-up generator as described in Shieber (1988) the compilation process can be modified such that only lexical entries are extended with magic literals. Just like in case of Shieber's bottom-up generator, bottom-up evaluation of magic-compiled grammars produced with this Magic variant is only guaranteed to be complete in case the original grammar obeys the semantic monotonicity constraint.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As can be reconstructed from the numbering of the facts in figure 3 the resulting processing behavior is identical to the behavior that would result from Earley generation as in Gerdemann (1991) except that the different filtering steps are performed in a bottom-up fashion. ", "mid_sen": "In order to obtain a generator similar to the bottom-up generator as described in Shieber (1988) the compilation process can be modified such that only lexical entries are extended with magic literals. ", "after_sen": "Just like in case of Shieber's bottom-up generator, bottom-up evaluation of magic-compiled grammars produced with this Magic variant is only guaranteed to be complete in case the original grammar obeys the semantic monotonicity constraint."}
{"citeStart": 91, "citeEnd": 104, "citeStartToken": 91, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Even when cycles are removed from the magic part of a compiled grammar and indexing is used to avoid spurious ambiguities as discussed in the previous section, subsumption checking can not always be eliminated. The grammar must be finitely ambiguous, i.e., fulfill the off-line parsability constraint (Shieber, 1989 ). Furthermore, the grammar is required to obey what I refer to as the dependency constraint:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Even when cycles are removed from the magic part of a compiled grammar and indexing is used to avoid spurious ambiguities as discussed in the previous section, subsumption checking can not always be eliminated. ", "mid_sen": "The grammar must be finitely ambiguous, i.e., fulfill the off-line parsability constraint (Shieber, 1989 ). ", "after_sen": "Furthermore, the grammar is required to obey what I refer to as the dependency constraint:"}
{"citeStart": 3, "citeEnd": 47, "citeStartToken": 3, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "In Samuel, Carberry, and Vijay-Shanker's (1998) work on identifying collocations for dialog-act recognition, a filter similar to ours was used to eliminate redundant n-gram features: n-grams were eliminated if they contained substrings with the same entropy score as or a better entropy score than the n-gram.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A number of researchers have worked on mining collocations from text to extend lexicographic resources for machine translation and word sense disambiguation (e.g., Smajda 1993; Lin 1999; Biber 1993) .", "mid_sen": "In Samuel, Carberry, and Vijay-Shanker's (1998) work on identifying collocations for dialog-act recognition, a filter similar to ours was used to eliminate redundant n-gram features: n-grams were eliminated if they contained substrings with the same entropy score as or a better entropy score than the n-gram.", "after_sen": "While it is common in studies of collocations to omit low-frequency words and expressions from analysis, because they give rise to invalid or unrealistic statistical measures (Church and Hanks, 1990), we are able to identify higher-precision collocations by including placeholders for unique words (i.e., the ugen-n-grams). "}
{"citeStart": 103, "citeEnd": 117, "citeStartToken": 103, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992) , (Church, 1988) , (Hajji, Hladk~, 1997) ) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a \"direct\" approach to modeling, for which we have chosen an exponential probabilistic model. Such model (when predicting an event 5 y E Y in a context x) has the general form", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992) , (Church, 1988) , (Hajji, Hladk~, 1997) ) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a \"direct\" approach to modeling, for which we have chosen an exponential probabilistic model. ", "after_sen": "Such model (when predicting an event 5 y E Y in a context x) has the general form"}
{"citeStart": 264, "citeEnd": 286, "citeStartToken": 264, "citeEndToken": 286, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper describes a representation and associated compiler intended for two-level morphological descriptions of the written forms of inflecting languages. The system described is a component of the Core Language Engine (CLE; AIshawi, 1992), a general-purpose language analyser and generator implemented in Prolog which supports both a built-in lexicon and access to large external lexical databases. In this context, highly efficient word analysis and generation at run-time are less important than ensuring that the morphology mechanism is expressive, is easy to debug, and allows relatively quick compilation. Morphology also needs to be well integrated with other processing levels. In particular, it should be possible to specify relations among morphosyntactic and morphophonological rules and lexical entries; for the convenience of developers, this is done by means of feature equations. Further, it cannot be assumed that the lexicon has been fully specified when the morphology rules are compiled. Developers may wish to add and test further lexical entries without frequently recompiling the rules, and it may also be necessary to deal with unknown words at run time, for example by querying a large external lexical database or attempting spelling correction (Alshawi, 1992, pp124-7) . Also, both analysis and generation of word forms are required. Run-time speed need only be enough to make the time spent on morphology small compared to sententia] and contextual processing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Further, it cannot be assumed that the lexicon has been fully specified when the morphology rules are compiled. ", "mid_sen": "Developers may wish to add and test further lexical entries without frequently recompiling the rules, and it may also be necessary to deal with unknown words at run time, for example by querying a large external lexical database or attempting spelling correction (Alshawi, 1992, pp124-7) . ", "after_sen": "Also, both analysis and generation of word forms are required. "}
{"citeStart": 73, "citeEnd": 94, "citeStartToken": 73, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "To handle this problem, we point out some statistical tests, like the matched-pair t, sign and Wilcoxon tests (Harnett, 1982, Sec. 8.7 and 15.5) , which do not make this assumption. One can use these tests on the recall metric, but the precision and balanced F-score metric have too complex a form for these tests. For such complex metrics, we use a compute-intensive randomization test (Cohen, 1995, Sec. 5.3) , which also avoids this independence assumption.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One can use these tests on the recall metric, but the precision and balanced F-score metric have too complex a form for these tests. ", "mid_sen": "For such complex metrics, we use a compute-intensive randomization test (Cohen, 1995, Sec. 5.3) , which also avoids this independence assumption.", "after_sen": "The next section describes many of the standard tests used and their problem of assuming certain forms of independence. "}
{"citeStart": 117, "citeEnd": 138, "citeStartToken": 117, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (Lafferty et al., 2001) , and explore several practical issues in applying CRFs to information extraction in general. The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations.", "mid_sen": "In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (Lafferty et al., 2001) , and explore several practical issues in applying CRFs to information extraction in general. ", "after_sen": "The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences."}
{"citeStart": 73, "citeEnd": 87, "citeStartToken": 73, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "To select new seed words, we take the ratio of count 1 to count 2 for the noun in question. This is similar to the figure of merit used in R&:S, and also tends to promote low frequency nouns. For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993) , which is based upon the co-occurrence counts of all nouns (see Dunning for details). This statistic essentially measures how surprising the given pattern of co-occurrence would be if the distributions were completely random. For instance, suppose that two words occur forty times each, and they co-occur twenty times in a millionword corpus. This would be more surprising for two completely random distributions than if they had each occurred twice and had always co-occurred. A simple probability does not capture this fact.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is similar to the figure of merit used in R&:S, and also tends to promote low frequency nouns. ", "mid_sen": "For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993) , which is based upon the co-occurrence counts of all nouns (see Dunning for details). ", "after_sen": "This statistic essentially measures how surprising the given pattern of co-occurrence would be if the distributions were completely random. "}
{"citeStart": 14, "citeEnd": 26, "citeStartToken": 14, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Slot Grammar (McCord, 1990 ) employs a number of rule types, some of which are exclusively concerned with precedence. So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers. Extractions (i.e., discontinuities) are merely handled by a mechanism built into the parser.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The approach requires that the parser interprets several features in a special way, and it cannot restrict the scope of discontinuities.", "mid_sen": "Slot Grammar (McCord, 1990 ) employs a number of rule types, some of which are exclusively concerned with precedence. ", "after_sen": "So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers. "}
{"citeStart": 64, "citeEnd": 78, "citeStartToken": 64, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "While head-lexicalization and other tree transformations allow the construction of parsing models with more data-sensitivity and richer representations, crafting rules for these transformations has been largely an art, with heuristics handed down from researcher to researcher. What's more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further effort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to a new treebank. For example, in the rule sets used by the parsers described in (Magerman, 1995; Ratnaparkhi, 1997; Collins, 1999) , the sets of rules for finding the heads of ADJP, ADVP, NAC, PP and WHPP include rules for picking either the rightmost or leftmost FW (foreign word). The apparently haphazard placement of these rules that pick out FW and the rarity of FW nodes in the data strongly suggest these rules are the result of engineering effort. Furthermore, it is not at all apparent that tree-transforming heuristics that are useful for one parsing model will be useful for another. Finally, as is often the case with heuristics, those used in statistical parsers tend not to be data-sensitive, and ironically do not rely on the words themselves.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "What's more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further effort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to a new treebank. ", "mid_sen": "For example, in the rule sets used by the parsers described in (Magerman, 1995; Ratnaparkhi, 1997; Collins, 1999) , the sets of rules for finding the heads of ADJP, ADVP, NAC, PP and WHPP include rules for picking either the rightmost or leftmost FW (foreign word). ", "after_sen": "The apparently haphazard placement of these rules that pick out FW and the rarity of FW nodes in the data strongly suggest these rules are the result of engineering effort. "}
{"citeStart": 50, "citeEnd": 74, "citeStartToken": 50, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004) . We study the cases where a 9 Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally, we evaluate DSP on a common application of selectional preferences: choosing the correct antecedent for pronouns in text (Dagan and Itai, 1990; Kehler et al., 2004) . ", "mid_sen": "We study the cases where a 9 Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5).", "after_sen": "pronoun is the direct object of a verb predicate, v. A pronoun's antecedent must obey v's selectional preferences. "}
{"citeStart": 13, "citeEnd": 32, "citeStartToken": 13, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. In a first stage, only MEDLINE abstracts are used (Yang et al., 2004) , later other-anaphora, a very specific sub-task, are investigated using full paper content (Chen et al., 2008) . Gasperin (2009) presents a full annotation of anaphora and coreference in biomedical text, but only noun phrases referring to biomedical entities are considered. On the basis of this annotation, she implements a probabilistic anaphora resolution system. In contrast, Cohen et al. (2010) build a corpus of 97 full-text journal articles in the biomedical domain where every co-referring noun phrase is annotated (CRAFT -Colorado Richly Annotated Full Text). Their annotation guidelines follow those of the OntoNotes project (Hovy et al., 2006) , adapted to the biomedical domain. OntoNotes itself is a text corpus of approx. one million words from mainly news texts (newswire, magazines, broadcast conversations, web pages). It also contains general anaphoric coreference annotations (Pradhan et al., 2007) : events and (like in our annotation) unlimited noun phrase entity types. Kim and Webber (2006) investigate a special aspect, citation sentences where a pronoun such as \"they\" refers to a previous citation. The study is performed on astronomy journal articles and a maximum-entropy classifier is trained. Kaplan et al. (2009) investigate coreferences and citations as well, but only at a very small scale (4 articles from the Computational Linguistics journal). They focus on so-called c-sites which are the sentences following a citation that also refer to the same paper (typically by anaphora). The authors train a specific coreference model for this phenomenon. They show that exploitation of coreference chains improves the extraction of citation contexts which they then use for research paper summarization.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the basis of this annotation, she implements a probabilistic anaphora resolution system. ", "mid_sen": "In contrast, Cohen et al. (2010) build a corpus of 97 full-text journal articles in the biomedical domain where every co-referring noun phrase is annotated (CRAFT -Colorado Richly Annotated Full Text). ", "after_sen": "Their annotation guidelines follow those of the OntoNotes project (Hovy et al., 2006) , adapted to the biomedical domain. "}
{"citeStart": 136, "citeEnd": 156, "citeStartToken": 136, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carlson et al., 1999) , 1 which learns a linear classifier in feature space, and has been successful in several NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005) . In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N, E}. In the second case, E has been incremented with emotional distinctions according to the valence, i.e. E = {N, P E, N E}. Experiments used 10-fold cross-validation, with 90% train and 10% test data. 2", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The goal here is to get a good understanding of the nature of the TEP problem and explore features which may be useful.", "mid_sen": "We explore two cases of flat classification, using a variation of the Winnow update rule implemented in the SNoW learning architecture (Carlson et al., 1999) , 1 which learns a linear classifier in feature space, and has been successful in several NLP applications, e.g. semantic role labeling (Koomen, Punyakanok, Roth and Yih, 2005) . ", "after_sen": "In the first case, the set of emotion classes E consists of EMOTIONAL versus non-emotional or NEUTRAL, i.e. E = {N, E}. In the second case, E has been incremented with emotional distinctions according to the valence, i.e. E = {N, P E, N E}. Experiments used 10-fold cross-validation, with 90% train and 10% test data. "}
{"citeStart": 98, "citeEnd": 124, "citeStartToken": 98, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "We use two types of resources in our experiments. The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007) , which we use for inference and for evaluation. As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees. In case a punctuation node was not a leaf, its children are attached to the parent of the removed node. For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majliš andŽabokrtský, 2012) , which provide sufficient amount of data for our purposes. Statistics across languages are shown in Table 1 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In case a punctuation node was not a leaf, its children are attached to the parent of the removed node. ", "mid_sen": "For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C corpus (Majliš andŽabokrtský, 2012) , which provide sufficient amount of data for our purposes. ", "after_sen": "Statistics across languages are shown in Table 1 ."}
{"citeStart": 95, "citeEnd": 134, "citeStartToken": 95, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993) . More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992) . We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like \"say\".", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some form of text analysis is required to collect such a collection of pairs. ", "mid_sen": "The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993) . ", "after_sen": "More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992) . "}
{"citeStart": 82, "citeEnd": 93, "citeStartToken": 82, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "For reasons that will be elaborated in Section 2, our problem is most acute in Hebrew and some other languages (e.g., Arabic), though ambiguity problems of a similar nature occur in other languages. One such problem is sense disambiguation. In the context of machine translation, Dagan and Itai (Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) used corpora in the target language to resolve ambiguities in the source language. Yarowsky (1992) proposed a method for sense disambiguation using wide contexts. Part-of-speech tagging--deciding the correct part of speech in the current context of the sentence--has received major attention. Most successful methods have followed speech recognition systems (Jelinek, Mercer, and Roukos 1992) and used large corpora to deduce the probability of each part of speech in the current context (usually the two previous words--trigrams). These methods have reported performance in the range of 95-99% \"correct\" by word (DeRose 1988; Cutting et al. 1992; Jelinek, Mercer, and Roukos 1992; Kupiec 1992 ). (The difference in performance is due to different evaluation methods, different tag sets, and different corpora). See Church (1992) for a survey.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most successful methods have followed speech recognition systems (Jelinek, Mercer, and Roukos 1992) and used large corpora to deduce the probability of each part of speech in the current context (usually the two previous words--trigrams). ", "mid_sen": "These methods have reported performance in the range of 95-99% \"correct\" by word (DeRose 1988; Cutting et al. 1992; Jelinek, Mercer, and Roukos 1992; Kupiec 1992 ). ", "after_sen": "(The difference in performance is due to different evaluation methods, different tag sets, and different corpora). "}
{"citeStart": 57, "citeEnd": 76, "citeStartToken": 57, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "The analysis accounts for the range of data given in Kehler (1993b) , although one point of departure exists between that account and the current one with respect to clauses conjoined with but. In the previous account these cases are all classified as non-parallel, resulting in the prediction that they only require semantic source representations. In our analysis, we expect cases of pure contrast to pattern with the parallel class since these are Common Topic constructions; this is opposed to the violated expectation use of but which indicates a Coherent Situation relation. The current account makes the correct predictions; examples (20) and (21), where but has the contrast meaning, appear to be markedly less acceptable than examples (22) and 23 To summarize thus far, the data presented in the earlier account as well as examples that conflict with that analysis are all predicted by the account given here. As a final note, we consider the interaction between VP-ellipsis and gapping. The following pair of examples are adapted from those of Sag (1976, pg. 291): lZThese examples have been adapted from several in Kehler (1993b) . 24:Iohn supports Clinton, and Mary $ Bush, although she doesn't know why she does.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a final note, we consider the interaction between VP-ellipsis and gapping. ", "mid_sen": "The following pair of examples are adapted from those of Sag (1976, pg. 291): lZThese examples have been adapted from several in Kehler (1993b) . ", "after_sen": "24:Iohn supports Clinton, and Mary $ Bush, although she doesn't know why she does."}
{"citeStart": 24, "citeEnd": 42, "citeStartToken": 24, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task. The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT. The source and target sentences were treated as the observations, but the alignments were treated as hidden information learned from parallel texts using the EM algorithm. This sourcechannel model treated the task of finding the probability", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task. ", "after_sen": "The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT. "}
{"citeStart": 85, "citeEnd": 95, "citeStartToken": 85, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005) , describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence. For an example of a completely annotated abstract, see Figure 2 . Each individual PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the relevant elements: Outcomes are complete sentences, while population, problems, and interventions are short noun phrases.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The automatic extraction of PICO elements from MEDLINE citations represents a key capability integral to clinical question answering. ", "mid_sen": "This section, which elaborates on preliminary results reported in Demner-Fushman and Lin (2005) , describes extraction algorithms for population, problems, interventions, outcomes, and the strength of evidence. ", "after_sen": "For an example of a completely annotated abstract, see Figure 2 . "}
{"citeStart": 0, "citeEnd": 12, "citeStartToken": 0, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (\"certain\" and \"right\") with their immediate neighbors.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. ", "mid_sen": "Biber (1993) applies factor analysis to collocations of two target words (\"certain\" and \"right\") with their immediate neighbors.", "after_sen": "What these approaches have in common is that they classify words instead of individual occurrences. "}
{"citeStart": 202, "citeEnd": 216, "citeStartToken": 202, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand, other work has been carried out in order to acquire collocations. Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition (by the use of Part-of-Speech lters hand-crafted by a linguist) (Oueslati, 1999) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri and Tugwell, 2001 , for example). It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. The original acquisition methodology we present in the next section will allow us to overcome this limitation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the other hand, other work has been carried out in order to acquire collocations. ", "mid_sen": "Most of these endeavours have focused on purely statistical acquisition techniques (Church and Hanks, 1990 ), on linguisitic acquisition (by the use of Part-of-Speech lters hand-crafted by a linguist) (Oueslati, 1999) or, more frequently, on a combination of the two (Smadja, 1993; Kilgarri and Tugwell, 2001 , for example). ", "after_sen": "It is worth noting that although these techniques are able to identify N-V pairs, they do not specify the relationship between N and V, nor are they capable of focusing on a subset of N-V pairs. "}
{"citeStart": 109, "citeEnd": 126, "citeStartToken": 109, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "We now have two probability distributions that we need to estimate, which we do using decision trees (Breiman et al., 1984; Bahl et al., 1989) . The decision tree algorithm has the advantage that it uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node. For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where tile impurity measures how well each leaf predicts the events in the node. After the tree is grown, a heldout dataset is used to smooth the probabilities of each node with its parent (Bahl et al., 1989) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where tile impurity measures how well each leaf predicts the events in the node. ", "mid_sen": "After the tree is grown, a heldout dataset is used to smooth the probabilities of each node with its parent (Bahl et al., 1989) .", "after_sen": "To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. "}
{"citeStart": 16, "citeEnd": 42, "citeStartToken": 16, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "The results in Table 1 show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection. The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results in Table 1 show that this system performs comparably to the state of the art in overall parsing accuracy and reasonably well in edit detection. ", "mid_sen": "The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words", "after_sen": "The Switchboard corpus has special terminal symbols indicating e.g. the start and end of the reparandum. "}
{"citeStart": 128, "citeEnd": 151, "citeStartToken": 128, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "There are a number of factors that make our formulation of hedge classification both interesting and challenging from a weakly supervised learning perspective. Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features. ", "mid_sen": "This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal). ", "after_sen": "In the same vein, for the case of entity/relation extraction and classification (Collins and Singer, 1999; Zhang, 2004; Chen et al., 2006) the context of the entity or entities in consideration provides a highly relevant feature space."}
{"citeStart": 0, "citeEnd": 19, "citeStartToken": 0, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "We present a new method for generating linguistic variation projecting multiple personality traits continuously, by combining and extending previous research in statistical natural language generation (Paiva and Evans, 2005; Rambow et al., 2001; Isard et al., 2006; . While handcrafted rule-based approaches are limited to variation along a small number of discrete points (Hovy, 1988; Walker et al., 1997; Lester et al., 1997; Power et al., 2003; Cassell and Bickmore, 2003; Piwek, 2003; Rehm and André, in press), we learn models that predict parameter values for any arbitrary value on the variation dimension scales. Additionally, our data-driven approach can be applied to any dimension that is meaningful to human judges, and it provides an elegant way to project multiple dimensions simultaneously, by including the relevant dimensions as features of the parameter models' training data. Isard et al. (2006) and also propose a personality generation method, in which a data-driven personality model selects the best utterance from a large candidate set. Isard et al.'s technique has not been evaluated, while Mairesse and Walker's overgenerate and score approach is inefficient. Paiva and Evans' technique does not overgenerate (2005), but it requires a search for the optimal generation decisions according to the learned models. Our approach does not require any search or overgeneration, as parameter estimation models predict the generation decisions directly from the target variation dimensions. This technique is therefore beneficial for real-time generation. Moreover the variation dimensions of Paiva and Evans' data-driven technique are extracted from a corpus: there is thus no guarantee that they can be easily interpreted by humans, and that they generalise to other corpora. Previous work has shown that modeling the relation between personality and language is far from trivial (Pennebaker and King, 1999; Argamon et al., 2005; Oberlander and Nowson, 2006; , suggesting that the control of personality is a harder problem than the control of data-driven variation dimensions.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Additionally, our data-driven approach can be applied to any dimension that is meaningful to human judges, and it provides an elegant way to project multiple dimensions simultaneously, by including the relevant dimensions as features of the parameter models' training data. ", "mid_sen": "Isard et al. (2006) and also propose a personality generation method, in which a data-driven personality model selects the best utterance from a large candidate set. ", "after_sen": "Isard et al.'s technique has not been evaluated, while Mairesse and Walker's overgenerate and score approach is inefficient. "}
{"citeStart": 85, "citeEnd": 103, "citeStartToken": 85, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "The unsupervised approaches are not comparable with supervised ones in general, because the conventional criteria are the manual segmentations known as gold standards. Gold standards, however, cannot be unified into a single standard (Fung and Wu 1994; Sproat et al. 1996) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The unsupervised approaches are not comparable with supervised ones in general, because the conventional criteria are the manual segmentations known as gold standards. ", "mid_sen": "Gold standards, however, cannot be unified into a single standard (Fung and Wu 1994; Sproat et al. 1996) .", "after_sen": ""}
{"citeStart": 1, "citeEnd": 18, "citeStartToken": 1, "citeEndToken": 18, "sectionName": "UNKNOWN SECTION NAME", "string": "In this formula, n is the number of synsets of the translation e~t~. (Gale et al., 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. (Miller et al., 1994) found that automatic I We use English WordNet version 1.6 assignment of polysemous words in Brown Corpus to senses in WordNet was 58% correct with a heuristic of most frequently occurring sense. We adopt these previous results to develop sense ordering heuristic.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this formula, n is the number of synsets of the translation e~t~. ", "mid_sen": "(Gale et al., 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. ", "after_sen": "(Miller et al., 1994) found that automatic I We use English WordNet version 1.6 assignment of polysemous words in Brown Corpus to senses in WordNet was 58% correct with a heuristic of most frequently occurring sense. "}
{"citeStart": 120, "citeEnd": 140, "citeStartToken": 120, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003) . We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. \"yes\" and \"right\", as listed in (Cohen, 2002) , general cue phrases, e.g. \"but\" and \"alright\" (Hirschberg and Litman, 1994) , and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997) . We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002) . Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated with hesitation (Pomerantz, 1984) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003) . ", "after_sen": "We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. "}
{"citeStart": 147, "citeEnd": 166, "citeStartToken": 147, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent work in video surveillance has demonstrated the benefit of representing complex events as temporal relations between lower level subevents (Hongen et al., 2004) . Thus, to represent events in the sports domain, we would ideally first represent the basic sub events that occur in sports video (e.g., hitting, throwing, catching, running, etc.) and then build up complex events (such as home run) as a set of temporal relations between these basic events. Unfortunately, due to the limitations of computer vision techniques, reliably identifying such basic events in video is not feasible. However, sports video does have characteristics that can be exploited to effectively represent complex events.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Recent work in video surveillance has demonstrated the benefit of representing complex events as temporal relations between lower level subevents (Hongen et al., 2004) . ", "after_sen": "Thus, to represent events in the sports domain, we would ideally first represent the basic sub events that occur in sports video (e.g., hitting, throwing, catching, running, etc.) and then build up complex events (such as home run) as a set of temporal relations between these basic events. "}
{"citeStart": 128, "citeEnd": 140, "citeStartToken": 128, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. ", "mid_sen": "Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . ", "after_sen": "All systems were trained on the same data and the outputs used the same tokenization. "}
{"citeStart": 238, "citeEnd": 251, "citeStartToken": 238, "citeEndToken": 251, "sectionName": "UNKNOWN SECTION NAME", "string": "This, we believe, gives further support to the utility of treebank grammars and to the compaction method. For example, compaction methods can be applied within the DOP framework to reduce the number of trees. Also, by partially lexicalising the rule extraction process (i.e., by using some more frequent words as well as the part-of-speech tags), we may be able to achieve parsing performance similar to the best results in the field obtained in (Collins, 1996) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, compaction methods can be applied within the DOP framework to reduce the number of trees. ", "mid_sen": "Also, by partially lexicalising the rule extraction process (i.e., by using some more frequent words as well as the part-of-speech tags), we may be able to achieve parsing performance similar to the best results in the field obtained in (Collins, 1996) .", "after_sen": ""}
{"citeStart": 112, "citeEnd": 134, "citeStartToken": 112, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "The motivation for this annotation scheme is to allow the training of more sophisticated event-based information extraction systems. In contrast to the sentence-based scheme described in section 2, this scheme is applied at the level of events (Ananiadou et al., 2010) , of which there may be several within a single sentence.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The motivation for this annotation scheme is to allow the training of more sophisticated event-based information extraction systems. ", "mid_sen": "In contrast to the sentence-based scheme described in section 2, this scheme is applied at the level of events (Ananiadou et al., 2010) , of which there may be several within a single sentence.", "after_sen": ""}
{"citeStart": 207, "citeEnd": 248, "citeStartToken": 207, "citeEndToken": 248, "sectionName": "UNKNOWN SECTION NAME", "string": "We train a statistical model to select the best order of the unordered target dependency tree. An important advantage of our model is that it is global, and does not decompose the task of ordering a target sentence into a series of local decisions, as in the recently proposed order models for Machine Transition (Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Kuhn et al., 2006 \"restriction\"\"condition\" TOPIC \"all\" \"satisfy\" PASSIVE-PRES", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We train a statistical model to select the best order of the unordered target dependency tree. ", "mid_sen": "An important advantage of our model is that it is global, and does not decompose the task of ordering a target sentence into a series of local decisions, as in the recently proposed order models for Machine Transition (Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Kuhn et al., 2006 \"restriction\"\"condition\" TOPIC \"all\" \"satisfy\" PASSIVE-PRES", "after_sen": "c d e f g h (a) f e c d g h f e c d g h f e c d g h (b)"}
{"citeStart": 69, "citeEnd": 90, "citeStartToken": 69, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "In DPM, a compression of a given sentence can be obtained by finding arg max y h(y), where y ranges over possible candidate compressions of a particular length one may derive from that sentence. In the experiment described later, we set α = 0.1 for DPM, following Morooka et al. (2004) , who found the best performance with that setting for α.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In DPM, a compression of a given sentence can be obtained by finding arg max y h(y), where y ranges over possible candidate compressions of a particular length one may derive from that sentence. ", "mid_sen": "In the experiment described later, we set α = 0.1 for DPM, following Morooka et al. (2004) , who found the best performance with that setting for α.", "after_sen": ""}
{"citeStart": 4, "citeEnd": 18, "citeStartToken": 4, "citeEndToken": 18, "sectionName": "UNKNOWN SECTION NAME", "string": "Although we have reached a situation in computational linguistics where large coverage grammars are well developed and available in several formal traditions, the use of these research results in actual applications and for application to specific domains is still unsatisfactory. One reason for this is that large-scale grammar specifications incur a seemingly unnecessarily large burden of space and processing time that often does not stand in relation to the simplicity of the particular task. The usual alternatives for natural language generation to date have been the handcrafted development of application or 1This work was partially supported by the DAAD through grant D/96/17139. sublanguage specific grammars or the use of template based generation grammars. In (Busemann, 1996) both approaches are combined resulting in a practical small generation grammar tool. But still the grammars are handwritten or, if extracted from large grammars, must be adapted by hand. In general, both -the template and the handwritten application grammar approach -compromise the idea of a general NLP system architecture with reusable bodies of general linguistic resources.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "sublanguage specific grammars or the use of template based generation grammars. ", "mid_sen": "In (Busemann, 1996) both approaches are combined resulting in a practical small generation grammar tool. ", "after_sen": "But still the grammars are handwritten or, if extracted from large grammars, must be adapted by hand. "}
{"citeStart": 69, "citeEnd": 71, "citeStartToken": 69, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "3 There are several other published implementation of chart parsers [23, 20, 33] , hut they often do not give much detail on the output of the parsing process, or even side-step the problem ~1.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we shall call shared forests such data struc-2 We do not consider CF reco~zers that have asymptotically the lowest complexity, but are only of theoretical interest here [~S,5] .", "mid_sen": "3 There are several other published implementation of chart parsers [23, 20, 33] , hut they often do not give much detail on the output of the parsing process, or even side-step the problem ~1.", "after_sen": "together [33] . "}
{"citeStart": 155, "citeEnd": 178, "citeStartToken": 155, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "The data for the experiment is a corpus of German subordinate clauses extracted by regular expression matching from a 200 million token newspaper corpus. The clause length ranges between four and 12 words. Apart from infinitival VPs as verbal arguments, there are no further clausal embeddings, and the clauses do not contain any punctuation except for a terminal period. The corpus contains 4128873 tokens and 450526 clauses which yields an average of 9.16456 tokens per clause. Tokens are automatically annotated with a list of part-of-speech (PoS) tags using a computational morphological analyser based on finite-state technology (Karttunen et al. (1994) , Schiller and StSckert (1995) ).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The corpus contains 4128873 tokens and 450526 clauses which yields an average of 9.16456 tokens per clause. ", "mid_sen": "Tokens are automatically annotated with a list of part-of-speech (PoS) tags using a computational morphological analyser based on finite-state technology (Karttunen et al. (1994) , Schiller and StSckert (1995) ).", "after_sen": "A problem for practical inside-outside estimation of an inflectional language like German arises with the large number of terminal and low-level non-terminal categories in the grammar resulting from the morpho-syntactic features of words. "}
{"citeStart": 46, "citeEnd": 74, "citeStartToken": 46, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "The PCC176 (Stede, 2004 ) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12-15 sentences), with 33.000 tokens in total. The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate 3 tool. In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007) ) and rhetorical structure according to RST (Mann and Thompson, 1988) . Our annotation software architecture consists of a variety of standard, external tools that can be used effectively for the different annotation types. Their XML output is then automatically converted to a generic format (PAULA, (Dipper, 2005) ), which is read into the linguistic database ANNIS (Dipper et al., 2004) , where the annotations are aligned, so that the data can be viewed and queried across annotation levels.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate 3 tool. ", "mid_sen": "In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007) ) and rhetorical structure according to RST (Mann and Thompson, 1988) . ", "after_sen": "Our annotation software architecture consists of a variety of standard, external tools that can be used effectively for the different annotation types. "}
{"citeStart": 85, "citeEnd": 97, "citeStartToken": 85, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "2.1 St unified analysis of temporal anaphora Hinrichs' and Partee's use of a notion of reference time, provides for a unified treatment of temporal anaphoric relations in discourse, which include narrative progression especially in sequences of simple past tense sentences, temporal adverbs and temporal adverbial clauses, introduced by a temporal connective. This concept of reference time is no longer an instant of time, but rather, an interval. This approach can be summarized as follows: in the processing of a discourse, the discourse-initial sentence is argued to require some contextually determined reference time. Further event clauses in the discourse introduce a new event, which is included within the then-current reference time. Each such event also causes the reference time to be updated to a time 'just after' (Partee, 1984) this event. State clauses introduce new states, which include the current reference time, and do not update it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Further event clauses in the discourse introduce a new event, which is included within the then-current reference time. ", "mid_sen": "Each such event also causes the reference time to be updated to a time 'just after' (Partee, 1984) this event. ", "after_sen": "State clauses introduce new states, which include the current reference time, and do not update it."}
{"citeStart": 10, "citeEnd": 35, "citeStartToken": 10, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, Cabezas and Resnik (2005) experimented with incorporating WSD translations into Pharaoh, a state-of-the-art phrase-based MT system . Their WSD system provided additional translations to the phrase table of Pharaoh, which fired a new model feature, so that the decoder could weigh the additional alternative translations against its own. However, they could not automatically tune the weight of this feature in the same way as the others. They obtained a relatively small improvement, and no statistical significance test was reported to determine if the improvement was statistically significant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The authors showed that they were able to improve their model's accuracy on two simplified translation tasks: word translation and blank-filling.", "mid_sen": "Recently, Cabezas and Resnik (2005) experimented with incorporating WSD translations into Pharaoh, a state-of-the-art phrase-based MT system . ", "after_sen": "Their WSD system provided additional translations to the phrase table of Pharaoh, which fired a new model feature, so that the decoder could weigh the additional alternative translations against its own. "}
{"citeStart": 121, "citeEnd": 140, "citeStartToken": 121, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "• If the input structure for generation is provided by another AI-system, .qloba.l problems in producing svJ.Jicicnl inp 't,t iuJ'orm.a.tio~, for the generator may occur, e.g., because o[' translation mismatches in inachine translation (Kameyama, 19!)1). In this case, the generator eitl,er has to use a default or formulat.e a request for clarification in order to be abh~ to continue its processing, i.e., to produce an utterance. During simultaneous interpret a.tion requests are rather unusual. Ilere defaults allow for a sta.ndalone handling of t:he problem.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, input information can be insufficient in two respects:", "mid_sen": "• If the input structure for generation is provided by another AI-system, .qloba.l problems in producing svJ.Jicicnl inp 't,t iuJ'orm.a.tio~, for the generator may occur, e.g., because o[' translation mismatches in inachine translation (Kameyama, 19!)1). ", "after_sen": "In this case, the generator eitl,er has to use a default or formulat.e a request for clarification in order to be abh~ to continue its processing, i.e., to produce an utterance. "}
{"citeStart": 58, "citeEnd": 80, "citeStartToken": 58, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set. This can be done by smoothing the observed frequencies 7 (Church and Mercer 1993) or by class-based methods (Brown et al. 1991; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993; Hirschman 1986; Resnik 1992; Brill et al. 1990; Dagan, Marcus, and Markovitch 1993) . In comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics. This allows the system to learn successfully from very sparse data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved co-occurrences using the actual co-occurrences in the training set. ", "mid_sen": "This can be done by smoothing the observed frequencies 7 (Church and Mercer 1993) or by class-based methods (Brown et al. 1991; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993; Hirschman 1986; Resnik 1992; Brill et al. 1990; Dagan, Marcus, and Markovitch 1993) . ", "after_sen": "In comparison to these approaches, we use similarity information throughout training, and not merely for estimating co-occurrence statistics. "}
{"citeStart": 41, "citeEnd": 50, "citeStartToken": 41, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "Solution The steps of our solution are outlined in Figure 1 above. OPINE parses the reviews using MINI-PAR (Lin, 1998) and applies a simple pronoun-resolution module to parsed review data. OPINE then uses the data to find explicit product features (E). OPINE's Feature Assessor and its use of Web PMI statistics are vital for the extraction of high-quality features (see 3.2). OPINE then identifies opinion phrases associated with features in E and finds their polarity. OPINE's novel use of relaxationlabeling techniques for determining the semantic orientation of potential opinion words in the context of given features and sentences leads to high precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity extraction (see 3.3).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Solution The steps of our solution are outlined in Figure 1 above. ", "mid_sen": "OPINE parses the reviews using MINI-PAR (Lin, 1998) and applies a simple pronoun-resolution module to parsed review data. ", "after_sen": "OPINE then uses the data to find explicit product features (E). "}
{"citeStart": 82, "citeEnd": 94, "citeStartToken": 82, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "1Spivey-Knowlton et al. reported 3 experiments. One showed effects before the end of a word when there was no other appropriate word with the same initial phonology. Another showed on-line effects from adjectives and determiners during noun phrase processing. constituency, so that an initial fragment of a sentence, such as John likes, can be treated as a constituent, and hence be assigned a type and a semantics. This approach is exemplified by Combinatory Categorial Grammar, CCG (Steedman 1991) , which takes a basic CG with just application, and adds various new ways of combining elements together 2. Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence. The alternative approach, exemplified by the work of Stabler on top-down parsing (Stabler 1991) , and Pulman on left-corner parsing (Pulman 1986 ) is to associate a semantics directly with the partial structures formed during a topdown or left-corner parse. For example, a syntax tree missing a noun phrase, such as the following", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence. ", "mid_sen": "The alternative approach, exemplified by the work of Stabler on top-down parsing (Stabler 1991) , and Pulman on left-corner parsing (Pulman 1986 ) is to associate a semantics directly with the partial structures formed during a topdown or left-corner parse. ", "after_sen": "For example, a syntax tree missing a noun phrase, such as the following"}
{"citeStart": 118, "citeEnd": 134, "citeStartToken": 118, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been a number of proposals to incorporate syntactic information into phrasal decoding. Early experiments with syntactically-informed phrases (Koehn et al., 2003) , and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. (2004) modified a phrasal decoder with ITG constraints, while a number of researchers have employed syntax-driven source reordering before decoding begins (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007) . 2 We attempt something between these two approaches: our constraint is derived from a linguistic parse tree, but it is used inside the decoder, not as a preprocessing step.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There have been a number of proposals to incorporate syntactic information into phrasal decoding. ", "mid_sen": "Early experiments with syntactically-informed phrases (Koehn et al., 2003) , and syntactic reranking of K-best lists (Och et al., 2004) produced mostly negative results. ", "after_sen": "The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: "}
{"citeStart": 34, "citeEnd": 69, "citeStartToken": 34, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "All of the word-level predictors were statistically significant at the α level 0.05. Beyond this baseline, we fitted ten other linear mixed-effects models. To the inventory of wordlevel predictors, each of the ten regressions uniquely added the surprisal predictions calculated from a parser that retains at most k=1...9,100 analyses at each prefix. We evaluated the change in relative quality of fit due to surprisal with the Deviance Information Criterion (DIC) discussed in Spiegelhalter et al. (2002) . Whereas the more commonly applied Akaike Information Criterion (1973) requires the number of estimated parameters to be determined exactly, the DIC facilitates the evaluation of mixed-effects models by relaxing this requirement. When comparing two models, if one of the models has a lower DIC value, this means that the model fit has improved. Table 3 shows that the linear mixed-effects model of German reading difficulty improves when surprisal values from the dependency parser are used as predictors in addition to the word-level predictors. The coefficients on the baseline predictors remained unchanged (Equation 3) when any of the parser-based predictors was added. Table 3 also suggests the returns to be had in accounting for reading time are greatest when the beam is limited to a handful of parses. Indeed, a parser that handles a few analyses at a time (k=1,2,3) is just as valuable as one that spends far greater memory resources (k=100). This observation is consistent with observation that accuracy can be maintained even when restricted to 1% of the memory required for exhaustive parsing. The role of small k dependency parsers in determining the quality of statistical fit challenges the assumption that cognitive functions are global optima. Perhaps human parsing is boundedly rational in the sense of the bound imposed by Stack3 (Simon, 1955) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We evaluated the change in relative quality of fit due to surprisal with the Deviance Information Criterion (DIC) discussed in Spiegelhalter et al. (2002) . ", "mid_sen": "Whereas the more commonly applied Akaike Information Criterion (1973) requires the number of estimated parameters to be determined exactly, the DIC facilitates the evaluation of mixed-effects models by relaxing this requirement. ", "after_sen": "When comparing two models, if one of the models has a lower DIC value, this means that the model fit has improved. "}
{"citeStart": 161, "citeEnd": 181, "citeStartToken": 161, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "The Framework for Estimating the Referent Prior to the pronoun resolution process, sentences are transformed into a case structure by a case structure analyzer (Kurohashi & Nagao 94) . The antecedents of pronouns are determined by heuristic rules from left to right. Using these rules, our system assigns points to possible antecedents, and judges that the one having the maximum total score is the desired antecedent. Heuristic rules are classified into two kinds: Candidate enumerating rules and Candidate judging rules. Candidate enumerating rules are used in enumerating candidate antecedents and giving them points (which represent the plausibility of being the correct antecedent). Candidate judging rules are used in giving points to the candidate antecedents selected by Candidate enumerating rules. These rules are shown in Figures 1 and 2 . Surface expressions, semantic constraints, referential properties, etc. are written as conditions in the Condition part. Possible antecedents are written in the Possible-Antecedent part. Points means the plausibility of the possible antecedent.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Framework for Estimating the Referent Prior to the pronoun resolution process, sentences are transformed into a case structure by a case structure analyzer (Kurohashi & Nagao 94) . ", "after_sen": "The antecedents of pronouns are determined by heuristic rules from left to right. "}
{"citeStart": 261, "citeEnd": 281, "citeStartToken": 261, "citeEndToken": 281, "sectionName": "UNKNOWN SECTION NAME", "string": "Unfolding can be used to eliminate superfluous filtering steps. Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed (Minnen et al., 1996) the resulting processing behavior can be considered a generalization of the head corner generation approach (Shieber et al., 1990) : Without the need to rely on notions such as semantic head and chain rule, a head corner behavior can be mimicked in a strict bottom-up fashion.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfolding can be used to eliminate superfluous filtering steps. ", "mid_sen": "Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed (Minnen et al., 1996) the resulting processing behavior can be considered a generalization of the head corner generation approach (Shieber et al., 1990) : ", "after_sen": "Without the need to rely on notions such as semantic head and chain rule, a head corner behavior can be mimicked in a strict bottom-up fashion."}
{"citeStart": 120, "citeEnd": 138, "citeStartToken": 120, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "), but at high ones its precision decreases almost dramatically. Unless very high rates of misspellings are to be expected (this explains the favorable results for trigram indexing in (Franz et al., 2000) ) one cannot really recommend this method.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "), but at high ones its precision decreases almost dramatically. ", "mid_sen": "Unless very high rates of misspellings are to be expected (this explains the favorable results for trigram indexing in (Franz et al., 2000) ) one cannot really recommend this method.", "after_sen": "The subword approach (SU) clearly outperforms the previously discussed approaches. "}
{"citeStart": 70, "citeEnd": 91, "citeStartToken": 70, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) that has been developed based on data tagged with TreeTagger (Schmid, 1994) using a model from Sharoff et al. (2008) . The data processed by Tree-Tagger contained errors such as wrong definition of PoS for adverbs, wrong selection of gender for adjectives in plural and missing features for pronouns and adverbs. In order to train RFTagger, the output of TreeTagger was corrected with a set of empirical rules. In particular, the morphological features of nominal phrases were made consistent to train RFTagger: in contrast to TreeTagger, where morphological features are regarded as part of the PoS-tag, RFTagger allows for a separate handling of morphological features and POS tags.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The preparation of the Russian data includes the following stages: (1) tokenization and tagging and (2) morphological reduction.", "mid_sen": "Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008) that has been developed based on data tagged with TreeTagger (Schmid, 1994) using a model from Sharoff et al. (2008) . ", "after_sen": "The data processed by Tree-Tagger contained errors such as wrong definition of PoS for adverbs, wrong selection of gender for adjectives in plural and missing features for pronouns and adverbs. "}
{"citeStart": 42, "citeEnd": 65, "citeStartToken": 42, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "Our MST parser training procedure involves enumerating the n 2 potential tree edges (parent/child pairs). Unlike the training procedure employed by McDonald et al. (2005b) and , we provide positive and negative examples in the training data. A node can have at most one parent, providing a natural split of the n 2 training examples. For each node n i , we wish to estimate a distribution over n nodes 9 as potential parents, p(v i , e ji |e i ), the probability of the correct parent of v i being v j given the set of edges associated with its candidate parents e i . We call this the parentprediction model.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our MST parser training procedure involves enumerating the n 2 potential tree edges (parent/child pairs). ", "mid_sen": "Unlike the training procedure employed by McDonald et al. (2005b) and , we provide positive and negative examples in the training data. ", "after_sen": "A node can have at most one parent, providing a natural split of the n 2 training examples. "}
{"citeStart": 48, "citeEnd": 69, "citeStartToken": 48, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "We investigate lexical access strategies in the context of computing multiword expression associations within automatic semantic MT evaluation metrics-a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002) , NIST (Doddington, 2002) , METEOR (Banerjee and Lavie, 2005) , CDER (Leusch et al., 2006) , WER (Nießen et al., 2000) , and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. ", "mid_sen": "Common surface-form oriented metrics like BLEU (Papineni et al., 2002) , NIST (Doddington, 2002) , METEOR (Banerjee and Lavie, 2005) , CDER (Leusch et al., 2006) , WER (Nießen et al., 2000) , and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. ", "after_sen": "Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy."}
{"citeStart": 112, "citeEnd": 126, "citeStartToken": 112, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . A number of other systems have addressed part of the task. Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. ", "mid_sen": "As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. ", "after_sen": "More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . "}
{"citeStart": 193, "citeEnd": 213, "citeStartToken": 193, "citeEndToken": 213, "sectionName": "UNKNOWN SECTION NAME", "string": "The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in (Collins, 1996) and the SPATTER parser described in (Jelinek et al., 1994; Magerman, 1995) . The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions. The bigram parser is a statistical CKY-style chart parser, which uses cooccurrence statistics of headmodifier pairs to find the best parse. The maximum entropy parser is a statistical shift-reduce style parser that cannot always access head-modifier pairs. For example, the checkcons(m,n) predicate of the maximum entropy parser may use two words such that neither is the intended head of the proposed consituent that the CHECK procedure must judge. And unlike the bigram parser, the maximum entropy parser cannot use head word information besides \"flat\" chunks in the right context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The two parsers which have previously reported the best accuracies on the Penn Treebank Wall St. Journal are the bigram parser described in (Collins, 1996) and the SPATTER parser described in (Jelinek et al., 1994; Magerman, 1995) . ", "after_sen": "The parser presented here outperforms both the bigram parser and the SPATTER parser, and uses different modelling technology and different information to drive its decisions. "}
{"citeStart": 140, "citeEnd": 162, "citeStartToken": 140, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "Knowledge maps consist of nodes containing rich concept descriptions interconnected using a limited set of relationship types (Holley and Dansereau, 1984) . Learning research indicates that knowledge maps may be useful for learners to understand the macro-level structure of an information space (O'Donnell et al., 2002) . Knowledge maps have also emerged as an effective computational infrastructure to support the automated generation of conceptual browsers. Such conceptual browsers appear to allow students to focus on the science content of large educational digital libraries (Sumner et al., 2003) , such as the Digital Library for Earth System Education (DLESE.org). Knowledge maps have also shown promise as domain and student knowledge representations to support personalized learning interactions (de la Chica et al., 2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Knowledge maps consist of nodes containing rich concept descriptions interconnected using a limited set of relationship types (Holley and Dansereau, 1984) . ", "mid_sen": "Learning research indicates that knowledge maps may be useful for learners to understand the macro-level structure of an information space (O'Donnell et al., 2002) . ", "after_sen": "Knowledge maps have also emerged as an effective computational infrastructure to support the automated generation of conceptual browsers. "}
{"citeStart": 105, "citeEnd": 117, "citeStartToken": 105, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "In the [970s and early 1980s several compntational implementations motivated the use of' incremental in-. terpretation as a way of dealing with structural and lexical ambiguity (a survey is given in Haddock 1989). A sentence snch as the following has 4862 different syntactic parses due solely to attachment ambiguity (Stabler 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the [970s and early 1980s several compntational implementations motivated the use of' incremental in-. terpretation as a way of dealing with structural and lexical ambiguity (a survey is given in Haddock 1989). ", "mid_sen": "A sentence snch as the following has 4862 different syntactic parses due solely to attachment ambiguity (Stabler 1991) .", "after_sen": ""}
{"citeStart": 1, "citeEnd": 14, "citeStartToken": 1, "citeEndToken": 14, "sectionName": "UNKNOWN SECTION NAME", "string": "• Furthermore, for incremental generation, the input information is produced and hande.d own' step by step, so that it can be temporaril 9 ineomplele -although as a whole it may become sufficient. This behaviour of a generator is motiw, ted by psycholinguistic observa.tions which show that people~ start speaklug 1)e[bre all necessary linguistic material has been chosen (e.g., articulating a noun phrase be['ore, the dominating w'.rb is selected). As a consequence of undersl)ecification , incremental generation is essentially based on working with defaults, l'~lements are uttered before the processing or input consuml%ion has been finished. (Kita.no, 1990) gives an example for defaults in i:he context of simultaneous interpretation: In Japanese, negation is specified at the end of the sentence while in English, it has to be specified ill ['rent of the finite verb. Tllere[bre, during .lapanese-English translation, where analysis, l;ransfer, and general;ion are l)erformed in a para.llel and incremental way, the system has to commit, e.g., positive wdm', l~el'ore knowing the actual polarlty I.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a consequence of undersl)ecification , incremental generation is essentially based on working with defaults, l'~lements are uttered before the processing or input consuml%ion has been finished. ", "mid_sen": "(Kita.no, 1990) gives an example for defaults in i:he context of simultaneous interpretation: ", "after_sen": "In Japanese, negation is specified at the end of the sentence while in English, it has to be specified ill ['rent of the finite verb. "}
{"citeStart": 141, "citeEnd": 153, "citeStartToken": 141, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems. More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993) . We would now propose that HMM's have successfully been applied to the problem of name-finding.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems. ", "mid_sen": "More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993) . ", "after_sen": "We would now propose that HMM's have successfully been applied to the problem of name-finding."}
{"citeStart": 113, "citeEnd": 132, "citeStartToken": 113, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "We now briefly describe the translation model for the probability P(FIE); a more thorough account is provided in Brown et al. (1991) . We imagine that an English sentence E generates a French sentence F in two steps. First, each word in E independently generates zero or more French words. These words are then ordered to give a French sentence F. We denote the ith word of E by ei and the jth word of F by yj. (We employ yj rather than the more intuitive }~ to avoid confusion with the feature function notation.) We denote the number of words in the sentence E by IEI and the number of words in the sentence F by IFI. The generative process yields not only the French sentence F but also an association of the words of F with the words of E. We call this association an alignment, and denote it by A. An alignment A is parametrized by a sequence of IFI numbers aj, with 1 < ai < IE[. For every word position j in F, aj is the word position in E of the English word that generates yj. The probability p(FIE ) that F is the translation of E is expressed as the sum over all possible alignments A between E and F of the probability of F and A given E:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These two models, plus a search strategy for finding the/~ that maximizes (30) for some F, comprise the engine of the translation system.", "mid_sen": "We now briefly describe the translation model for the probability P(FIE); a more thorough account is provided in Brown et al. (1991) . ", "after_sen": "We imagine that an English sentence E generates a French sentence F in two steps. "}
{"citeStart": 144, "citeEnd": 165, "citeStartToken": 144, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "As for the membership probabilities, they must be determined solely by the relevant measure of object-to-cluster similarity, which in the present work is the relative entropy between object and cluster centroid distributions. Since no other information is available, the membership is determined by maximizing the configuration entropy for a fixed average distortion. With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data. The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977) . The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities. In the second stage of each iteration, the entropy of the membership distribution is maximized for a fixed average distortion. This joint optimization searches for a saddle point in the distortion-entropy parameters, which is equivalent to minimizing a linear combination of the two known as free energy in statistical mechanics.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data. ", "mid_sen": "The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the EM method (Dempster et al., 1977) . ", "after_sen": "The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities. "}
{"citeStart": 111, "citeEnd": 137, "citeStartToken": 111, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "For lesser-resourced languages, there is generally little data and few NLP resources available. For Hebrew, for example, we must create our own pool of learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008) , they are not adapted for dealing with potentially ill-formed learner productions. For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses. Then, the system aligns the learner answer to the gold standard answer and determines the types of deviations.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For lesser-resourced languages, there is generally little data and few NLP resources available. ", "mid_sen": "For Hebrew, for example, we must create our own pool of learner data, and while NLP tools and resources exist (Goldberg and Elhadad, 2011; Yona and Wintner, 2008; Itai and Wintner, 2008) , they are not adapted for dealing with potentially ill-formed learner productions. ", "after_sen": "For this reason, we are performing linguistic analysis on the gold standard answers to obtain optimal linguistic analyses. "}
{"citeStart": 89, "citeEnd": 104, "citeStartToken": 89, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Our PCFG models are comparable to branching process models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997) . Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998) . Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. ", "mid_sen": "Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998) . ", "after_sen": "Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997) ."}
{"citeStart": 93, "citeEnd": 117, "citeStartToken": 93, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006) . In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006) . However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are composed from elementary units of lexicalized information. Consequently, little is known about the generative capacity and computational complexity of languages over restricted non-projective dependency structures.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. ", "mid_sen": "Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006) . ", "after_sen": "However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are composed from elementary units of lexicalized information. "}
{"citeStart": 20, "citeEnd": 33, "citeStartToken": 20, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988) , who identifies and quantifies the linguistic features associated with different spoken and written text types. Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000) . Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation. A practical discussion of a central technical concern is Vossen (2001) , which tailors a general-language resource for a domain. Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures. His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. ", "mid_sen": "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000) . ", "after_sen": "Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation. "}
{"citeStart": 111, "citeEnd": 125, "citeStartToken": 111, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "There is a 1-to-1 correspondence between the parse trees generated by G and LC L G. A tree t is generated by G i there is a corresponding t 0 generated by LC L G, where each occurrence of a top-down production in the derivation of t corresponds to exactly one local tree generated by occurrence of the corresponding instance of schema 1b in the derivation of t 0 , and each occurrence of a left-corner production in t corresponds to exactly one occurrence of the corresponding instance of schema 1c in t 0 . It is straightforward to de ne a 1-to-1 tree transform T L mapping parse trees of G into parse trees of LC L G Johnson, 1998a; Roark and Johnson, 1999. In the empirical evaluation below, we estimate a PCFG from the trees obtained by applying T L to the trees in the Penn WSJ tree-bank, and compare it to the PCFG estimated from the original tree-bank trees. A stochastic top-down parser using the PCFG estimated from the trees produced by T L simulates a stochastic generalized left-corner parser, which i s a generalization of a standard stochastic left-corner parser that permits productions to be recognized top-down as well as left-corner Manning and Carpenter, 1997. Thus investigating the properties of PCFG estimated from trees transformed with T L is an easy way of studying stochastic push-down automata performing generalized left-corner parses.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There is a 1-to-1 correspondence between the parse trees generated by G and LC L G. A tree t is generated by G i there is a corresponding t 0 generated by LC L G, where each occurrence of a top-down production in the derivation of t corresponds to exactly one local tree generated by occurrence of the corresponding instance of schema 1b in the derivation of t 0 , and each occurrence of a left-corner production in t corresponds to exactly one occurrence of the corresponding instance of schema 1c in t 0 . ", "mid_sen": "It is straightforward to de ne a 1-to-1 tree transform T L mapping parse trees of G into parse trees of LC L G Johnson, 1998a; Roark and Johnson, 1999. ", "after_sen": "In the empirical evaluation below, we estimate a PCFG from the trees obtained by applying T L to the trees in the Penn WSJ tree-bank, and compare it to the PCFG estimated from the original tree-bank trees. "}
{"citeStart": 159, "citeEnd": 170, "citeStartToken": 159, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992 ], Brill's [Brill 1995a] , and MaxEnt [Ratnaparkhi 1996 ]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history. A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This requires resolving sentence boundaries before tagging. ", "mid_sen": "We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992 ], Brill's [Brill 1995a] , and MaxEnt [Ratnaparkhi 1996 ]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. ", "after_sen": "The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. "}
{"citeStart": 231, "citeEnd": 241, "citeStartToken": 231, "citeEndToken": 241, "sectionName": "UNKNOWN SECTION NAME", "string": "Bilingnal (or parallel) texts are useful as resources of linguistic knowledge as well as in applications such as machine translation. One of the major approaches to analyzing bilingual texts is the statistical approach. The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques (e.g. Brown, Lai and Mercer (1991) , Gale and Church (1993) , Chen (1993) , and Kay and RSscheisen (1993) ), statistical machine translation models (e.g. Brown, Cooke, Pietra, Pietra et al. (1990) ), finding character-level / word-level / phrase-level correspondences from bilingual texts (e.g. Gale and Church (1991) , Church (1993) , and Kupiec (1993)), and word sense disambiguation for MT (e.g. Dagan, Itai and Schwall (1991) ). In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics. For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters (Brown et al., 1991; Gale and Church, 1993) , or by statistically estimating word level correspondences (Chen, 1993; Kay and RSscheisen, 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics. ", "mid_sen": "For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters (Brown et al., 1991; Gale and Church, 1993) , or by statistically estimating word level correspondences (Chen, 1993; Kay and RSscheisen, 1993) .", "after_sen": "The statistical approach analyzes unstructured sentences in bilingual texts, and it is claimed that the results are useful enough in real applications such as machine translation and word sense disambiguation. "}
{"citeStart": 193, "citeEnd": 215, "citeStartToken": 193, "citeEndToken": 215, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been no method proposed to date, however, that learns dependencies between case frame slots in the natural language processing literature. In the past research, the distributional pattern of each case slot is learned independently, 1 One may argue that 'fly' has different word senses in these sentences and for each of these word senses there is no dependency between the case frames. Word senses are in general difficult to define precisely, however, and in language processing, they would have to be disambiguated Dora the context ~nyway, which is essentially equivalent to assuming that the dependencies between case slots exist. Thus, our proposed method can in effect 'discover' implicit word senses fi'om corpus data. and methods of resolving ambiguity are also based on the assuml:ition th.at case slots are independent (llindle and Rooth, 1991), or dependencies lmtween at most two case slots are considered (Brill and Resnik, 1994) . Thus, provision of an efl'ective method of learning de, pendencies between (;as(; slots, as well as investigation of the usefulness of the acquired dependencies in disambiguation and other natural language processing tasks would be an inll)ortant contributiota to the fie.ld.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, our proposed method can in effect 'discover' implicit word senses fi'om corpus data. ", "mid_sen": "and methods of resolving ambiguity are also based on the assuml:ition th.at case slots are independent (llindle and Rooth, 1991), or dependencies lmtween at most two case slots are considered (Brill and Resnik, 1994) . ", "after_sen": "Thus, provision of an efl'ective method of learning de, pendencies between (;as(; slots, as well as investigation of the usefulness of the acquired dependencies in disambiguation and other natural language processing tasks would be an inll)ortant contributiota to the fie.ld."}
{"citeStart": 117, "citeEnd": 146, "citeStartToken": 117, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "Speech-based Topic Segmentation A variety of supervised and unsupervised methods have been employed to segment speech input. Some of these algorithms have been originally developed for processing written text (Beeferman et al., 1999) . Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005) . In parallel, researchers ex-tensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000) . However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. In contrast, the method presented in this paper does not assume the availability of transcripts, which prevents us from using segmentation algorithms developed for written text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Others are specifically adapted for processing speech input by adding relevant acoustic features such as pause length and speaker change (Galley et al., 2003; Dielmann and Renals, 2005) . ", "mid_sen": "In parallel, researchers ex-tensively study the relationship between discourse structure and intonational variation (Hirschberg and Nakatani, 1996; Shriberg et al., 2000) . ", "after_sen": "However, all of the existing segmentation methods require as input a speech transcript of reasonable quality. "}
{"citeStart": 86, "citeEnd": 88, "citeStartToken": 86, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "l%r generation, an analogous condition on logical forms has been proposed by Shieber [13] as the \"semantic monotonicity condition,\" which requires that the :logical form of every base case must subsume some portion of the goal's logical form.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "in general, we assume that the constituent order principle makes use of a linear and non-erasing oI)eratkm tor combining stringsJ If this is the case, then M1 the words contained in the PnON value of the goal can have their lexical items selected as unit clauses to start bottom-up processing.", "mid_sen": "l%r generation, an analogous condition on logical forms has been proposed by Shieber [13] as the \"semantic monotonicity condition,\" which requires that the :logical form of every base case must subsume some portion of the goal's logical form.", "after_sen": "Base case lookup must be defined specifically tbr different grammatical theories and directions of processing by the predicate lookup/2, whose first argument is the goal and whose second argument is the selected base case. "}
{"citeStart": 106, "citeEnd": 124, "citeStartToken": 106, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "In determining whether to accept a proposed befief or evidential relationship, the evaluator first constructs an evidence set containing the system's evidence thin supports or attacks _bcl and the evidence accepted by the system that was proposed by the user as support for -bel. Each piece of evidence contains a belief _beli, and an evidential relationship supports (.beli,-bel) . Following Walker's weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. The evaluator then employs a simplified version of Galliers' belief revision mechanism 2 (Galliers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of evidence strongly outweighs that of the other, the decision to accept or reject.bel is easily made. However, if the difference in their strengths does not exceed a pre-determined 2For details on how our model determines the acceptance of a belief using the ranking of endorsements proposed by GaUiers, see (Chu-Carroll, 1995) . ..... : ............................................................... \". \"d \"\" \"[ lnf~J,S,~Teache~ 2and 3threshold, the evaluator has insufficient information to determine whether to adopt _bel and therefore will initiate an information-sharing subdialogue (Cho-Carmll and Carberry, 1995) to share information with the user so that each of them can knowiedgably re-evaluate the user's original proposal. If, during infommtion-sharing, the user provides convincing support for a belief whose negation is held by the system, the system may adopt the belief after the re-evaluation process, thus resolving the conflict without negotiation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following Walker's weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. ", "mid_sen": "The evaluator then employs a simplified version of Galliers' belief revision mechanism 2 (Galliers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. ", "after_sen": "If the strength of one set of evidence strongly outweighs that of the other, the decision to accept or reject."}
{"citeStart": 0, "citeEnd": 20, "citeStartToken": 0, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "Given a stem such as brother, Toutanova et. al's system might generate the \"stem and inflection\" corresponding to and his brother. Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required. The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008) , which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. ", "mid_sen": "Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). ", "after_sen": "Both efforts were ineffective on large data sets. "}
{"citeStart": 43, "citeEnd": 52, "citeStartToken": 43, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "Wide-coverage logic-based semantics Boxer (Bos, 2008 ) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993) . It builds on the C&C CCG parser (Clark and Curran, 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011) .", "mid_sen": "Wide-coverage logic-based semantics Boxer (Bos, 2008 ) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993) . ", "after_sen": "It builds on the C&C CCG parser (Clark and Curran, 2004) ."}
{"citeStart": 157, "citeEnd": 170, "citeStartToken": 157, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we identify a set of constraints that can be placed on unification-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of constraints is motivated by showing how they generalize constraints inherent in Linear Indexed Grammar (l_lG). We begin by describing how constraints inherent in I.IG admit tractable processing algorithms and then consider how these constraints can be generalized to a formalism that manipulates trees rather than stacks. The constraints that we identify for the tree-based system can be regarded equally well as constraints on unification-based grammar formalisms such as PArR (Shieber, 1984) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We begin by describing how constraints inherent in I.IG admit tractable processing algorithms and then consider how these constraints can be generalized to a formalism that manipulates trees rather than stacks. ", "mid_sen": "The constraints that we identify for the tree-based system can be regarded equally well as constraints on unification-based grammar formalisms such as PArR (Shieber, 1984) .", "after_sen": ""}
{"citeStart": 114, "citeEnd": 128, "citeStartToken": 114, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "ALTERF is another Open Source toolkit, whose purpose is to allow a clean combination of rule-based and corpus-driven processing in the semantic interpretation phase. There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven. ALTERF characterises semantic analysis as a task slightly extending the \"decision-list\" classification algorithm (Yarowsky, 1994; Carter, 2000) . We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms. For example, in the procedure assistant domain we represent the utterances please speak up show me the sample syringe set an alarm for five minutes from now no i said go to the next step respectively as {increase volume} {show, sample syringe} {set alarm, 5, minutes} {correction, next step} where increase volume, show, sample syringe, set alarm, 5, minutes, correction and next step are semantic atoms. As well as specifying the permitted semantic atoms themselves, we also define a target model which for each atom specifies the other atoms with which it may legitimately combine. Thus here, for example, correction may legitimately combine with any atom, but minutes may only combine with correction, set alarm or a number. 1 .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven. ", "mid_sen": "ALTERF characterises semantic analysis as a task slightly extending the \"decision-list\" classification algorithm (Yarowsky, 1994; Carter, 2000) . ", "after_sen": "We start with a set of semantic atoms, each representing a primitive domain concept, and define a semantic representation to be a non-empty set of semantic atoms. "}
{"citeStart": 40, "citeEnd": 57, "citeStartToken": 40, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "We used Zhou et al.'s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010) . Additional work has extended these features (Jiang and Zhai, 2007) or incorporated other data sources (e.g. WordNet), but in this paper we focus solely on the initial step of applying these same lexical features to the JDPA Corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used Zhou et al.'s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010) . ", "after_sen": "Additional work has extended these features (Jiang and Zhai, 2007) or incorporated other data sources (e.g. WordNet), but in this paper we focus solely on the initial step of applying these same lexical features to the JDPA Corpus."}
{"citeStart": 155, "citeEnd": 181, "citeStartToken": 155, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "As a result of characterizing filtering by a definite clause representation Magic brings filtering inside of the logic underlying the grammar. This allows it to be optimized in a processor independent and logically clean fashion. I discuss two possible filter optimizations based on a program transformation technique called unfolding ) also referred to as partial execution, e.g., in Pereira and Shieber (1987) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This allows it to be optimized in a processor independent and logically clean fashion. ", "mid_sen": "I discuss two possible filter optimizations based on a program transformation technique called unfolding ) also referred to as partial execution, e.g., in Pereira and Shieber (1987) .", "after_sen": ""}
{"citeStart": 58, "citeEnd": 76, "citeStartToken": 58, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "Textual inference problems from the PASCAL RTE Challenge (Dagan et al., 2005) differ from FraCaS problems in several important ways. (See table 5 for examples.) Instead of textbook examples of semantic phenomena, RTE problems are more naturalseeming, with premises collected \"in the wild\" from newswire text. The premises are much longer, averaging 35 words (vs. 11 words for FraCaS). Also, the RTE task aims at a binary classification: the RTE no answer combines the no and unk answers in FraCaS.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Textual inference problems from the PASCAL RTE Challenge (Dagan et al., 2005) differ from FraCaS problems in several important ways. ", "after_sen": "(See table 5 for examples.) Instead of textbook examples of semantic phenomena, RTE problems are more naturalseeming, with premises collected \"in the wild\" from newswire text. "}
{"citeStart": 90, "citeEnd": 112, "citeStartToken": 90, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "We chose the verb prediction task which is similar to other word prediction tasks (e.g., (Golding and Roth, 1999) ) and, in particular, follows the paradigm in Dagan et al., 1999; Lee, 1999) . There, a list of the confusion sets is constructed first, each consists of two different verbs. The verb vl is coupled with v2 provided that they occur equally likely in the corpus. In the test set, every occurrence of vl or v2 was replaced by a set {vl, v2} and the classification task was to predict the correct verb. For example, if a confusion set is created for the verbs \"make\" and \"sell\", then the data is altered as follows:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To evaluate word prediction as a simple language model.", "mid_sen": "We chose the verb prediction task which is similar to other word prediction tasks (e.g., (Golding and Roth, 1999) ) and, in particular, follows the paradigm in Dagan et al., 1999; Lee, 1999) . ", "after_sen": "There, a list of the confusion sets is constructed first, each consists of two different verbs. "}
{"citeStart": 123, "citeEnd": 146, "citeStartToken": 123, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "Top-down parsing techniques are attractive because of their simplicity, and can often achieve good performance in practice Roark and Johnson, 1999 . However, with a left-recursive grammar such parsers typically fail to terminate. The left-corner grammar transform converts a left-recursive grammar into a non-left-recursive one: a top-down parser using a left-corner transformed grammar simulates a left-corner parser using the original grammar Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972 . However, the left-corner transformed grammar can be signi cantly larger than the original grammar, causing numerous problems. For example, we show below that a probabilistic context-free grammar PCFG estimated from left-corner transformed Penn WSJ tree-bank trees exhibits considerably greater sparse data problems than a PCFG estimated in the usual manner, simply because the left-corner transformed grammar contains approximately 20 times more productions. The transform described in this paper produces a grammar approximately the same size as the input grammar, which is not as adversely a ected by sparse data.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Top-down parsing techniques are attractive because of their simplicity, and can often achieve good performance in practice Roark and Johnson, 1999 . ", "after_sen": "However, with a left-recursive grammar such parsers typically fail to terminate. "}
{"citeStart": 67, "citeEnd": 87, "citeStartToken": 67, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "After filtering by the generator, the remaining fstructures were weighted by the stochastic disambiguation component. Similar to stochastic disambiguation for constraint-based parsing (Johnson et al., 1999; Riezler et al., 2002) , an exponential (a.k.a. log-linear or maximumentropy) probability model on transferred structures is estimated from a set of training data. The data for estimation consists of pairs of original sentences y and goldstandard summarized f -structures s which were manually selected from the transfer output for each sentence. For training data {(s j , y j )} m j=1 and a set of possible summarized structures S(y) for each sentence y, the objective was to maximize a discriminative criterion, namely the conditional likelihood L(λ) of a summarized f -structure given the sentence. Optimization of the function shown below was performed using a conjugate gradient opti-mization routine:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After filtering by the generator, the remaining fstructures were weighted by the stochastic disambiguation component. ", "mid_sen": "Similar to stochastic disambiguation for constraint-based parsing (Johnson et al., 1999; Riezler et al., 2002) , an exponential (a.k.a. log-linear or maximumentropy) probability model on transferred structures is estimated from a set of training data. ", "after_sen": "The data for estimation consists of pairs of original sentences y and goldstandard summarized f -structures s which were manually selected from the transfer output for each sentence. "}
{"citeStart": 25, "citeEnd": 43, "citeStartToken": 25, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "Unlike other work (e.g. (Black et al., 1992 ; Materman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag. This has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information. As well, it constrains the task of building the word classification trees since the major distinctions are captured by the POS classification tree.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag.", "mid_sen": "Unlike other work (e.g. (Black et al., 1992 ; Materman, 1995)), we treat the word identities as a further refinement of the POS tags; thus we build a word classification tree for each POS tag. ", "after_sen": "This has the advantage of avoiding unnecessary data fragmentation, since the POS tags and word identities are no longer separate sources of information. "}
{"citeStart": 45, "citeEnd": 59, "citeStartToken": 45, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The first condition, originally described by Appclt (1985b) , is important because the success of the refen'ing action depends on the hearer formulating a uselifl identilication plan. We take the referring expression plan itself to be the identification plan. The mental actions in the plan will encode only useful descriptions. For the second condition to hold, the hearer must believe that the identification plan is good enough to tmiquely identify the referent when it becomes visible. This inw)lves giving enough information by using the most salient atributes of the referent.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The basis of our model is that the hearer can accept a referring expression plan if (1) the plan contains a description that is use./M for making an identification plan that tile hearer can execute to identify the referent, and (2) the hearer is confident that the identification plan is adequate.", "mid_sen": "The first condition, originally described by Appclt (1985b) , is important because the success of the refen'ing action depends on the hearer formulating a uselifl identilication plan. ", "after_sen": "We take the referring expression plan itself to be the identification plan. "}
{"citeStart": 147, "citeEnd": 166, "citeStartToken": 147, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "Dependency-based features. To capture the inter-word relationships that n-grams may not, we employ the dependency-based features previously used for stance classification in Anand et al. (2011) . These features have three variants. In the first variant, the pair of arguments involved in each dependency relation extracted by a dependency parser is used as a feature. The second variant is the same as the first except that the head (i.e., the first argument in a relation) is replaced by its part-of-speech tag. The features in the third variant, the topic-opinion features, are created by replacing each sentiment-bearing word in features of the first two types with its corresponding polarity label (i.e., + or −).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Dependency-based features. ", "mid_sen": "To capture the inter-word relationships that n-grams may not, we employ the dependency-based features previously used for stance classification in Anand et al. (2011) . ", "after_sen": "These features have three variants. "}
{"citeStart": 130, "citeEnd": 138, "citeStartToken": 130, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "[behnert 801, [Fum 86]), owe their success to the li,nitation of the domain, and cannot be applied to document with varied subjects, such ,as popular scientific magazine. To realize a domain-independent abstract generation system, a computational theory for analyzing linguistic discourse structure and its practical procedure must be established. ltobbs developed a theory in which lie arranged three kinds of relationships between sentences from the text coherency viewpoint [Hobbs 79] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To realize a domain-independent abstract generation system, a computational theory for analyzing linguistic discourse structure and its practical procedure must be established. ", "mid_sen": "ltobbs developed a theory in which lie arranged three kinds of relationships between sentences from the text coherency viewpoint [Hobbs 79] .", "after_sen": "Grosz and Sidner proposed a theory which accounted for interactions between three notions on discourse: linguistic structure, intention, and attention [C, rosz et al. 86] ."}
{"citeStart": 57, "citeEnd": 72, "citeStartToken": 57, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "We implemented MPD for first-order PBDGs using dependency weights from our existing discriminatively-trained PBDG parser (not cited to preserve anonymity). These weights are estimated by an online procedure as in McDonald (2006) , and are not intended to define a probability distribution. In an attempt to heuristically correct for this, in this experiment we used exp(αw u,v ) as the weight of the dependency between head u and dependent v, where w u,v is the weight provided by the discriminativelytrained model and α is an adjustable scaling parameter tuned to optimize MPD accuracy on development data.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We implemented MPD for first-order PBDGs using dependency weights from our existing discriminatively-trained PBDG parser (not cited to preserve anonymity). ", "mid_sen": "These weights are estimated by an online procedure as in McDonald (2006) , and are not intended to define a probability distribution. ", "after_sen": "In an attempt to heuristically correct for this, in this experiment we used exp(αw u,v ) as the weight of the dependency between head u and dependent v, where w u,v is the weight provided by the discriminativelytrained model and α is an adjustable scaling parameter tuned to optimize MPD accuracy on development data."}
{"citeStart": 193, "citeEnd": 209, "citeStartToken": 193, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "This example is characterized by its multiple ambiguous pronouns and by the fact that the final utterance achieves a shift (see figure 4) . A shift is inevitable because of constraint 3, which states that the Cb(U,~) must equal the Cp(U,-I) (since the Cp(Un-x) is directly realized by the subject of Un, \"Friedman\"). However the constraints and rules from [GJW86] would fail to make a choice here between the co-specification possibilities for the pronouns in U,. Given that the transition is a shift, there seem to be more and less coherent ways to shi~. Note that the three items being examined in order to characterize the transition between each pair of anchors 4 are the = See [BP80] and [Cho80] for conditions on coreference 3 See [Sid83] for definition and discussion of co-specification. Note that this use of co-specification is not the saxne as that used in [Se185] 4An anchor is a < Cb, Of > pair for an utterance Therefore, we propose the following extension which handles some additional cases containing multiple ambiguous pronouns: we have extended rule 2 so that there are two kinds of shifts. A transition for Un is ranked more highly if Cb(Un) = Cp(U,); this state we call shifting-1 and it represents a more coherent way to shift. The preferred ranking is continuing >-retaining >-shifting-1 ~ shifting (see figure 3 ). This extension enables us to successfully bind the \"she\" in the final utterance of the example in figure 4 to \"Friedman.\" The appendix illustrates the application of the algorithm to figure 4. Kameyama [Kam86] has proposed another extension to the [G:JW86] theory -a property-sharing constraint which attempts to enforce a parallellism between entities in successive utterances. She considers two properties: SUBJ and IDENT. With her extension, subject pronouns prefer subject antecedents and non-subject pronouns prefer non-subject antecedents. However, structural parallelism is a consequence of our ordering the Cf list by grammatical function and the preference for continuing over retaining. Furthermore, the constraints suggested in [GJW86] succeed in many cases without invoking an independent structural parallelism constraint, due to the distinction between continuing and retaining, which Kameyama fails to consider. Her example which we reproduce in figure 5 can also be accounted for using the contin- He -Max, him = Fred ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The preferred ranking is continuing >-retaining >-shifting-1 ~ shifting (see figure 3 ). ", "mid_sen": "This extension enables us to successfully bind the \"she\" in the final utterance of the example in figure 4 to \"Friedman.\" The appendix illustrates the application of the algorithm to figure 4. Kameyama [Kam86] has proposed another extension to the [G:JW86] theory -a property-sharing constraint which attempts to enforce a parallellism between entities in successive utterances. ", "after_sen": "She considers two properties: SUBJ and IDENT. "}
{"citeStart": 76, "citeEnd": 100, "citeStartToken": 76, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the direct output of the 42-topic model of the 105th-108th Senates from (Quinn et al., 2006) to further divide the speech documents into topic clusters. In their paper, they use a model where the probabilities of a document belonging to a certain topic varies smoothly over time and the words within a given document have exactly the same probability of being drawn from a particular topic. These two properties make the model different than standard mixture models (McLachlan and Peel, 2000) and the latent Dirichlet allocation model of (Blei et al., 2003) . The model of (Quinn et al., 2006) is most closely related to the model of (Blei and Lafferty, 2006) , who present a generalization of the model used by (Quinn et al., 2006) . Table 1 lists the 42 topics and their related committees.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In their paper, they use a model where the probabilities of a document belonging to a certain topic varies smoothly over time and the words within a given document have exactly the same probability of being drawn from a particular topic. ", "mid_sen": "These two properties make the model different than standard mixture models (McLachlan and Peel, 2000) and the latent Dirichlet allocation model of (Blei et al., 2003) . ", "after_sen": "The model of (Quinn et al., 2006) is most closely related to the model of (Blei and Lafferty, 2006) , who present a generalization of the model used by (Quinn et al., 2006) . "}
{"citeStart": 78, "citeEnd": 95, "citeStartToken": 78, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "To closely reproduce the experiment with the best performance carried out in (Pang et al., 2002) using SVM, we use unigram with the presence feature. We test various combinations of our features applicable to the task. For evaluation, we use ten-fold cross-validation accuracy.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Movie Review polarity dataset 7 was .", "mid_sen": "To closely reproduce the experiment with the best performance carried out in (Pang et al., 2002) using SVM, we use unigram with the presence feature. ", "after_sen": "We test various combinations of our features applicable to the task. "}
{"citeStart": 92, "citeEnd": 109, "citeStartToken": 92, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated our resolution methods on a pseudo-disambiguation task similar to that used in Rooth et al. 1999 for evaluating clustering models. We used a test set of 298 v; n; n 0 triples where v;n is chosen randomly from a test corpus of pairs, and n 0 is chosen randomly according to the marginal noun distribution for the test corpus. Precision was calculated as the number of times the disambiguation method decided for the non-random target noun n = n.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We evaluated our resolution methods on a pseudo-disambiguation task similar to that used in Rooth et al. 1999 for evaluating clustering models. ", "after_sen": "We used a test set of 298 v; n; n 0 triples where v;n is chosen randomly from a test corpus of pairs, and n 0 is chosen randomly according to the marginal noun distribution for the test corpus. "}
{"citeStart": 0, "citeEnd": 18, "citeStartToken": 0, "citeEndToken": 18, "sectionName": "UNKNOWN SECTION NAME", "string": "The presence of features performs better than frequency of features in general. Using a more general category instead of specific diseases has a positive effect on the presence-based classification. We speculate that the effect of this generalization will be bigger if a larger test set were used. Pang et al. (2002) did not compare the result of using and not using the negation context effect, so it is not clear how much it improved their result. In our task, it is clear that the MORE/ LESS feature has a significant effect on the performance, especially for the frequency features. ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We speculate that the effect of this generalization will be bigger if a larger test set were used. ", "mid_sen": "Pang et al. (2002) did not compare the result of using and not using the negation context effect, so it is not clear how much it improved their result. ", "after_sen": "In our task, it is clear that the MORE/ LESS feature has a significant effect on the performance, especially for the frequency features. "}
{"citeStart": 172, "citeEnd": 186, "citeStartToken": 172, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "Other potential applications apply the hypothesised relationship (Harris, 1968 ) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other potential applications apply the hypothesised relationship (Harris, 1968 ) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. ", "mid_sen": "One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain.", "after_sen": "However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). "}
{"citeStart": 135, "citeEnd": 163, "citeStartToken": 135, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "Given the weights in (33), the four translations in (34) can be produced, each with an associated probability: These mistranslations are all caused by boundary friction. Each of the translations in (37) and (38) would be output with an associated weight and ranked by the system. We would like to incorporate into our model a procedure whereby translation chunks extracted from the phrasal and marker lexicons are more highly regarded than those constructed by inserting words from the word-level lexicon into generalized marker chunks. That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. We have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm (Littlestone and Warmuth 1992) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "That is, we want to allocate a larger portion of the probability space to the phrasal and marker lexicons than to the generalized or wordlevel lexicons. ", "mid_sen": "We have yet to import such a constraint into our model, but we plan to do so in the near future using the weighted majority algorithm (Littlestone and Warmuth 1992) .", "after_sen": ""}
{"citeStart": 108, "citeEnd": 123, "citeStartToken": 108, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text. The effectiveness of such models is well known (Church 1988) and they are currently in use in parsers (e.g. de Marcken 1990) . Our work is an incremental improvement on these models in two ways: (1) We have run experiments regarding the amount of training data needed in moving to a new domain; (2) we have added probabilistic models of word features to handle unknown words effectively. We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text. ", "mid_sen": "The effectiveness of such models is well known (Church 1988) and they are currently in use in parsers (e.g. de Marcken 1990) . ", "after_sen": "Our work is an incremental improvement on these models in two ways: (1) We have run experiments regarding the amount of training data needed in moving to a new domain; (2) we have added probabilistic models of word features to handle unknown words effectively. "}
{"citeStart": 347, "citeEnd": 362, "citeStartToken": 347, "citeEndToken": 362, "sectionName": "UNKNOWN SECTION NAME", "string": "The concept of a domain, however, is not precisely defined across existing domain adaptation methods. Different domains typically correspond to different subcorpora, in which documents exhibit a particular combination of genre and topic, and optionally other textual characteristics such as dialect and register. This definition, however, has two major shortcomings. First, subcorpusbased domains depend on provenance information, which might not be available, or on manual grouping of documents into subcorpora, which is labor intensive and often carried out according to arbitrary criteria. Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006) . While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002) , most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009) ), making it unclear whether the proposed solutions address topic or genre differences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, the commonly used notion of a domain neglects the fact that topic and genre are two distinct properties of text (Stein and Meyer Zu Eissen, 2006) . ", "mid_sen": "While this distinction has long been acknowledged in text classification literature (Lee, 2001; Dewdney et al., 2001; Lee and Myaeng, 2002) , most work on domain adaptation in SMT uses in-domain and out-of-domain data that differs on both the topic and the genre level (e.g., Europarl political proceedings (Koehn, 2005) versus EMEA medical text (Tiedemann, 2009) ), making it unclear whether the proposed solutions address topic or genre differences.", "after_sen": "In this work, we follow text classification literature for definitions of the concepts topic and genre. "}
{"citeStart": 46, "citeEnd": 64, "citeStartToken": 46, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the ACL Anthology Network corpus (AAN) Radev et al., 2013) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. From this set, we selected 30 papers that have different numbers of incoming citations and that were consistently cited since they were published. These 30 papers received a total of about 3,500 citations from within AAN (average = 115 citation/paper, Min = 30, and Max = 338). These citations come from 1,493 unique papers. For each of these citations, we extracted a window of 4 sentences around the reference position. This brings the number of sentences in our dataset to a total of roughly 14,000 sentences. We refer to this dataset as training/testing dataset. In addition to this dataset, we created another dataset that contains 300 citations that cite 5 papers from AAN. We refer to this dataset as the development dataset. This dataset was used to determine the size of the citation context window, and to develop the feature sets used in the three tasks described in Section 3 above.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use the ACL Anthology Network corpus (AAN) Radev et al., 2013) in our evaluation. ", "after_sen": "AAN is a publicly available collection of more than 19,000 NLP papers. "}
{"citeStart": 93, "citeEnd": 118, "citeStartToken": 93, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Design-World is an experimentld enviro|unent for testiug the relationship hetween discourse strategies, task p~u'ameters ,and agents' cognitive capabilities, similar to the single ,agent TileWorld simnlalion environment [Pollack and Ringuette, 1990; Hanks et al., 199311. Design-World agents can be parametrized as to discourse strategy, and tilt elfecls of this strategy can he me~Lsured against a |'imge of cognitive and task p~u'ameters. This paper compares [l~e Explicit-Winrant strategy to the All-lmplicil strategy as strategies lot supporting deliberation. Other strategies tested in Design-World me presented elsewhere [W~dker, 1993; Walker, 1994a; Riunbow ~md Walker, 19941. 3.1 Design World l)omain and task Both agents know whal tile I)ESI(;N-IIOUSE pl~ requires and stm'l out with a set of fnmiture pieces that can he used to design each room.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper compares [l~e Explicit-Winrant strategy to the All-lmplicil strategy as strategies lot supporting deliberation. ", "mid_sen": "Other strategies tested in Design-World me presented elsewhere [W~dker, 1993; Walker, 1994a; Riunbow ~md Walker, 19941. ", "after_sen": "3.1 Design World l)omain and task Both agents know whal tile I)ESI(;N-IIOUSE pl~ requires and stm'l out with a set of fnmiture pieces that can he used to design each room."}
{"citeStart": 0, "citeEnd": 27, "citeStartToken": 0, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "We corrected the first factor setting fixed criteria for determining context length. Hernández and Gómez, (2014) highlighted the need for defining context in view of argument detection, so the context include all sentences around citation that are talking about it. However, due to the complexity of this task, we decided to replace argument detection by fixing a context delimited by a complete paragraph. The rationale for this decision was that, by definition, a paragraph is a group of related sentences about the same idea. We assumed that author's purpose when making a citation could be found using cue words and ontological concepts that are within the same paragraph.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We corrected the first factor setting fixed criteria for determining context length. ", "mid_sen": "Hernández and Gómez, (2014) highlighted the need for defining context in view of argument detection, so the context include all sentences around citation that are talking about it. ", "after_sen": "However, due to the complexity of this task, we decided to replace argument detection by fixing a context delimited by a complete paragraph. "}
{"citeStart": 123, "citeEnd": 151, "citeStartToken": 123, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "The most important characteristic of answers, as recommended by Ely et al. (2005) in their study of real-world physicians, is that they focus on bottom-line clinical advice-information that physicians can directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to \"drill down\"; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b) , but these features are also beyond the capabilities of current summarization systems.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion-it need not be repeated unless the physician wishes to \"drill down\"; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. ", "mid_sen": "We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b) , but these features are also beyond the capabilities of current summarization systems.", "after_sen": "It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. "}
{"citeStart": 153, "citeEnd": 166, "citeStartToken": 153, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996) . Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. ", "mid_sen": "At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "after_sen": "We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. "}
{"citeStart": 142, "citeEnd": 162, "citeStartToken": 142, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005) . However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998) . Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. An example of this is from CCGbank (Hockenmaier, 2003) , where all modifiers in noun-noun compound constructions modify the final noun (because the Penn Treebank, from which CCGbank is derived, does not contain the necessary information to obtain the correct bracketing). Thus there are nonnegligible, systematic errors in both the training and testing material, and the CCG parsers are being rewarded for following particular mistakes.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. ", "mid_sen": "It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998) . ", "after_sen": "Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. "}
{"citeStart": 52, "citeEnd": 70, "citeStartToken": 52, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "We performed 5-fold cross-validation and used the classification accuracy as the evaluation measure. We extracted sentiment words from General Inquirer (Stone et al., 1996) and constructed a polarity dictionary. After some preprocessing, the dictionary contains 2,084 positive words and 2,685 negative words.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We performed 5-fold cross-validation and used the classification accuracy as the evaluation measure. ", "mid_sen": "We extracted sentiment words from General Inquirer (Stone et al., 1996) and constructed a polarity dictionary. ", "after_sen": "After some preprocessing, the dictionary contains 2,084 positive words and 2,685 negative words."}
{"citeStart": 163, "citeEnd": 186, "citeStartToken": 163, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "Several problems still remain with MERT, three of which are addressed by this work. First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum 1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface. Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011) . MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors. Third, MERT is only used to tune linear model parameters, yet SMT systems have many free decoder parameters-such as distortion limit and beam size-that are not handled by MERT. MERT does not provide a principled way to set these parameters.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, the Nbest error surface explored by MERT is generally not the same as the true error surface, which means that the error rate at an optimum 1 of the N -best error surface is not guaranteed to be any close to an optimum of the true error surface. ", "mid_sen": "Second, most SMT decoders make search errors, yet MERT ignores the fact that the error surface of an error-prone decoder differs from the one of an exact decoder (Chang and Collins, 2011) . MERT calculates an envelope from candidate translations and assumes all translations on the envelope are reachable by the decoder, but these translations may become unreachable due to search errors. ", "after_sen": "Third, MERT is only used to tune linear model parameters, yet SMT systems have many free decoder parameters-such as distortion limit and beam size-that are not handled by MERT. "}
{"citeStart": 138, "citeEnd": 159, "citeStartToken": 138, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "In the first experiment, the ANLT grammar was loaded and a set of sentences was input to each of the three parsers. In order to provide an independent basis for comparison, the same sentences were also input to the SRI Core Language Engine (CLE) parser (Moore & Alshawi, 1992 ) with the CLARE2.5 grammar (Alshawi et al., 1992) , a state-of-the-art system accessible to the author. The sentences were taken from an initial sample of 175 representative sentences extracted from a corpus of approximately 1500 that form part of the ANLT package. This corpus, implicitly defining the types of construction the grammar is intended to cover, was written by the linguist who developed the ANLT grammar and is used to check for any adverse effects on coverage when the grammar is modified during grammar development. Of the initial 175 sentences, the CLARE2.5 grammar failed to parse 42 (in several cases because punctuation is strictly required but is missing from the corpus). The ANLT grammar also failed to parse three of these, plus an additional four. These sentences were removed from the sample, leaving 129 (mean length 6.7 words) of which 47 were declarative sentences, 38 wh-questions and other sentences with gaps, 20 passives, and 24 sentences containing co-ordination. Table 1 shows the total parse times and storage allocated for the BU-LC parser, the LR parser, and the CE parser, all with ANLT grammar and lexicon. All three parsers have been implemented by the author to a similar high standard: similar implementation techniques are used in all the parsers, the parsers share the same unification module, run in the same Lisp environment, have been compiled with the same optimisation settings, and have all been profiled with the same tools and hand-optimised to a similar extent. (Thus any difference in performance of more than around 15% is likely to stem from algorithmic rather than implementational reasons). Both of the predictive parsers employ one symbol of lookahead, incorporated into the parsing tables by the LALR technique. Table 1 also shows the results for the CLE parser with the CLARE2.5 grammar and lexicon. The figures include garbage collection time, and phrasal (where appropriate) processing, but not parse forest unpacking. Both grammars give a total of around 280 analyses at a similar level of detail.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the first experiment, the ANLT grammar was loaded and a set of sentences was input to each of the three parsers. ", "mid_sen": "In order to provide an independent basis for comparison, the same sentences were also input to the SRI Core Language Engine (CLE) parser (Moore & Alshawi, 1992 ) with the CLARE2.5 grammar (Alshawi et al., 1992) , a state-of-the-art system accessible to the author. ", "after_sen": "The sentences were taken from an initial sample of 175 representative sentences extracted from a corpus of approximately 1500 that form part of the ANLT package. "}
{"citeStart": 213, "citeEnd": 238, "citeStartToken": 213, "citeEndToken": 238, "sectionName": "UNKNOWN SECTION NAME", "string": "The HMM part-of-speech tagging model and corresponding Viterbi algorithm were implemented based on their description in the updated version, http://www.cs.colorado.edu/˜martin/ SLP/updated.html , of chapter 8 of (Jurafsky and Martin, 2000) . A model was trained using Maximum Likelihood from the UPenn Treebank (Marcus et al., 1993 ). The input model file is encoded using XML and thus models built by other systems can be read in and displayed.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The HMM part-of-speech tagging model and corresponding Viterbi algorithm were implemented based on their description in the updated version, http://www.cs.colorado.edu/˜martin/ SLP/updated.html , of chapter 8 of (Jurafsky and Martin, 2000) . ", "after_sen": "A model was trained using Maximum Likelihood from the UPenn Treebank (Marcus et al., 1993 ). "}
{"citeStart": 529, "citeEnd": 542, "citeStartToken": 529, "citeEndToken": 542, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances (Almuallim and Dietterich, 1991, Langley and Sage, in press ). Unfortunately, the task of designing an appropriate instance representation --also known as feature set selection --can be extraordinarily difficult, time-consuming, and knowledge-intensive (Quinlan, 1983) . This poses a problem for current statistical and machine learning approaches to natural language understanding where a new instance representation is typically required for each linguistic task tackled.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). ", "mid_sen": "Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . ", "after_sen": "In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. "}
{"citeStart": 13, "citeEnd": 33, "citeStartToken": 13, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "The four verbs which are misclassified as Object Equi and which do not have T5 codes anywhere in their entries are elect, love, represent and require. None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. In addition, Moulin et al. (1985) note that our Object Raising rule would assign mean to this category incorrectly. Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. \"intend\"), however, when it is used in this sense it must be treated as an Object Equi verb.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "None of these verbs take sentential complements and therefore they appear to be counterexamples to our Object Raising rule. ", "mid_sen": "In addition, Moulin et al. (1985) note that our Object Raising rule would assign mean to this category incorrectly. ", "after_sen": "Mean is assigned both a V3 and a T5 category in the code field associated with sense 2 (i.e. \"intend\"), however, when it is used in this sense it must be treated as an Object Equi verb."}
{"citeStart": 50, "citeEnd": 72, "citeStartToken": 50, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger reported in Mikheev (2000) , which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. ", "mid_sen": "This is where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger reported in Mikheev (2000) , which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage.", "after_sen": "Our DCA uses information derived from the entire document and thus can be used as a complement to approaches based on the local context. "}
{"citeStart": 104, "citeEnd": 114, "citeStartToken": 104, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we observe that the skew metric seems quite promising. We conjecture that appropriate values for a may inversely correspond to the degree of sparseness in the data, and intend in the future to test this conjecture on larger-scale prediction tasks. We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We conjecture that appropriate values for a may inversely correspond to the degree of sparseness in the data, and intend in the future to test this conjecture on larger-scale prediction tasks. ", "mid_sen": "We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991) .", "after_sen": "1The term \"similarity-based\", which we have used previously, has been applied to describe other models as well (L.Lee, 1997;Karov and Edelman, 1998)."}
{"citeStart": 88, "citeEnd": 100, "citeStartToken": 88, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "3. Another way of allowing pragmatic inferences to be cancelled is to assign them the status of defeasible information. Mercer (1987) formalizes pre-suppositions in a logical framework that handles defaults (Reiter, 1980) , but this approach is not tractable and it treats natural disjunction as an exclusiveor and implication as logical equivalence.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another way of allowing pragmatic inferences to be cancelled is to assign them the status of defeasible information. ", "mid_sen": "Mercer (1987) formalizes pre-suppositions in a logical framework that handles defaults (Reiter, 1980) , but this approach is not tractable and it treats natural disjunction as an exclusiveor and implication as logical equivalence.", "after_sen": "Computational approaches fail to account for the cancellation of pragmatic inferences: once presuppositions (Weischedel, 1979) or implicatures (Hirschberg, 1985; Green, 1992) are generated, they can never be cancelled. "}
{"citeStart": 78, "citeEnd": 104, "citeStartToken": 78, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997) . Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997) . ", "mid_sen": "Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework .", "after_sen": "As Figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ASCII notation supported in the framework. "}
{"citeStart": 339, "citeEnd": 359, "citeStartToken": 339, "citeEndToken": 359, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition, a χ 2 dependency analysis showed that the NM presence interacts significantly with both AsrMis (p<0.02) and SemMis (p<0.001), with fewer than expected AsrMis and SemMis in the 3 Due to random assignment to conditions, before the first problem the F and S populations are similar (e.g. no difference in pretest); thus any differences in metrics can be attributed to the NM presence/absence. However, in the second problem, the two populations are not similar anymore as they have received different forms of instruction; thus any difference has to be attributed to the NM presence/absence in this problem as well as to the NM absence/presence in the previous problem. 4 Due to logging issues, 2 S users are excluded from this analysis (13 F and 13 S users remaining). We run the subjective metric analysis from Section 5.1 on this subset and the results are similar. NM condition. The fact that in the second problem the differences are much smaller (e.g. 2% for AsrMis) and that the NM-AsrMis and NM-SemMis interactions are not significant anymore, suggests that our observations can not be attributed to a difference in population with respect to system's ability to recognize their speech. We hypothesize that these differences are due to the NM text influencing users' lexical choice. Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993) , essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001) , generation/interpretation of anaphoric expressions (Allen et al., 2001) , performance modeling (Rotaru and Litman, 2006) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We hypothesize that these differences are due to the NM text influencing users' lexical choice. ", "mid_sen": "Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993) , essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001) , generation/interpretation of anaphoric expressions (Allen et al., 2001) , performance modeling (Rotaru and Litman, 2006) ).", "after_sen": "In this paper, we study the utility of the discourse structure on the user side of a dialogue system. "}
{"citeStart": 107, "citeEnd": 125, "citeStartToken": 107, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees. A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values. For example, a question about a word is represented as 30 binary questions. These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992) . The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions. For more discussion of the use of binary decision-tree questions, see (Magerman, 1994) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, a question about a word is represented as 30 binary questions. ", "mid_sen": "These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992) . ", "after_sen": "The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions. "}
{"citeStart": 184, "citeEnd": 223, "citeStartToken": 184, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "Carroll & Weir (1997)--without actually building a parsing system--address the issue of how frequency information can be associated with lexicalised grammar formalisms, using Lexicalized Tree Adjoining Grammar (Joshi & Schabes, 1991) as a unifying framework. They consider systematically a number of alternative probao bilistic formulations, including those of Resnik (1992) and and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks. Magerman (1995) , Collins (1996) , Ratnaparkhi (1997) , Charniak (1997) and others describe implemented systems with impressive accuracy on parsing unseen data from the Penn Treebank (Marcus, Santorini & Marcinkiewicz, 1993 ). These parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur. The accuracies reported for these systems are substantially better than their (non-lexicalised) probabilistic context-free grammar analogues, demonstrating clearly the value of lexico-statistical information. However, since the grammatical descriptions are induced from atomic-labeled constituent structures in the training treebank, rather than coming from an explicit generative grammar, these systems do not make contact with traditional notions of argument structure (i.e. subcategorisation, selectional preferences of predicates for complements) in any direct sense. So although it is now possible to extract at least subcategorisation data from large corpora 2 with some degree of reliability, it would be difficult to integrate the data into this type of parsing system. present a small-scale experiment in which subcategorisation class frequency information for individual verbs w~us integrated into a robust statistical (non-lexicalised) parser. The experiment used a test corpus of 250 sentences, and used the standard GEIG bracket precision, recall and crossing measures (Grishman, Macleod & Sterling, 1992) for evaluation. While bracket precision and recall were virtually unchanged, the crossing bracket score for the lexicalised parser showed a 7% improvement. However, this difference turned out not to be statistically significant at the 95% level: some analyses got better while others got worse.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They consider systematically a number of alternative probao bilistic formulations, including those of Resnik (1992) and and implemented systems based on other underlying grammatical frameworks, evaluating their adequacy from both a theoretical and empirical perspective in terms of their ability to model particular distributions of data that occur in existing treebanks. ", "mid_sen": "Magerman (1995) , Collins (1996) , Ratnaparkhi (1997) , Charniak (1997) and others describe implemented systems with impressive accuracy on parsing unseen data from the Penn Treebank (Marcus, Santorini & Marcinkiewicz, 1993 ). ", "after_sen": "These parsers model probabilistically the strengths of association between heads of phrases, and the configurations in which these lexical associations occur. "}
{"citeStart": 161, "citeEnd": 175, "citeStartToken": 161, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "Two new heavyweight algorithms were developed in the last year. One is a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet reported on parsing English text (Magerman, 1995) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Two new heavyweight algorithms were developed in the last year. ", "mid_sen": "One is a full parser of English, using a statistically learned decision procedure; SPATTER has achieved the highest scores yet reported on parsing English text (Magerman, 1995) .", "after_sen": "Because the measurable improvement in parsing is so great (compared to manually constructed parsers), it appears to offer a qualitatively better parser. "}
{"citeStart": 21, "citeEnd": 37, "citeStartToken": 21, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "(2) s = P S summary P S topic where P S summary represents the percentage of the sentences containing at least one keyword among all the sentences in the summary, and similarly P S topic is measured using the entire topic segment. We found that the average k and s are around 3.42 and 6.33 respectively. This means that keywords are more likely to occur in the summary compared to the rest of the topic, and the chance for a summary sentence to contain at least one keyword is much higher than for the other sentences in the topic. For the graph-based methods, we notice that adding POS filtering also improves performance, similar to the TFIDF framework. However, the graph method does not perform as well as the TFIDF approach. Comparing with using TFIDF alone, the graph method (without using POS) yielded worse results. In addition to using the TFIDF for the word nodes, information from the sentences is used in the graph method since a word is linked to sentences containing this word. The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes. Unlike the study in (Wan et al., 2007) , this information does not yield any gain. We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The global information in the S-S graph (connecting a sentence to other sentences in the document) is propagated to the word nodes. ", "mid_sen": "Unlike the study in (Wan et al., 2007) , this information does not yield any gain. ", "after_sen": "We did find that the graph approach performed better in the development set, but it seems that it does not generalize to this test set."}
{"citeStart": 20, "citeEnd": 41, "citeStartToken": 20, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001 ). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing. One advantage of this approach is that, although less information is derived from the treebank, it gen-eralizes better to parsers which make different representational assumptions, and it is easier, as Pereira and Schabes did, to map unlabeled bracketings to a format more consistent with the target grammar. Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. ", "mid_sen": "More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. ", "after_sen": "In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing."}
{"citeStart": 81, "citeEnd": 101, "citeStartToken": 81, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991) , parsing (Lytinen 1986; Nagao 1994 ) and text retrieval (Krovets and Croft 1992; Voorhees 1993) . Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995) . The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987] ), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. Our verb sense disambiguation system is based on such an approach, that is, an example-based approach. A preliminary experiment showed that our system performs well when compared with systems based on other approaches, and motivated us to further explore the example-based approach (we elaborate on this experiment in Section 2.3). At the same time, we concede that other approaches for word sense disambiguation are worth further exploration, and while we focus on example-based approach in this paper, we do not wish to draw any premature conclusions regarding tlhe relative merits of different generalized approaches.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word sense disambiguation is a potentially crucial task in many NLP applications, such as machine translation (Brown, Della Pietra, and Della Pietra 1991) , parsing (Lytinen 1986; Nagao 1994 ) and text retrieval (Krovets and Croft 1992; Voorhees 1993) . ", "mid_sen": "Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995) . ", "after_sen": "The use of corpus-based approaches has grown with the use of machine-readable text, because unlike conventional rule-based approaches relying on hand-crafted selectional rules (some of which are reviewed, for example, by Hirst [1987] ), corpus-based approaches release us from the task of generalizing observed phenomena through a set of rules. "}
{"citeStart": 96, "citeEnd": 118, "citeStartToken": 96, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003) . Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002) . The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002) . ", "mid_sen": "The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. ", "after_sen": "We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. "}
{"citeStart": 200, "citeEnd": 224, "citeStartToken": 200, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . ", "mid_sen": "Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . ", "after_sen": "We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training."}
{"citeStart": 222, "citeEnd": 246, "citeStartToken": 222, "citeEndToken": 246, "sectionName": "UNKNOWN SECTION NAME", "string": "Our classifier uses the maximum entropy implementation described in Curran and Clark (2003) . Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. This will be an important property when we use sparse features like bigrams in the models below.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is achieved by selecting the model with the maximum entropy, i.e. the most uniform distribution, given the constraints.", "mid_sen": "Our classifier uses the maximum entropy implementation described in Curran and Clark (2003) . Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. ", "after_sen": "This will be an important property when we use sparse features like bigrams in the models below."}
{"citeStart": 43, "citeEnd": 55, "citeStartToken": 43, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "tences. It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996) . Unlike Erbach (1990) , however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure. Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unlike Erbach (1990) , however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure. ", "mid_sen": "Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.", "after_sen": "The following three goals serve to structure our model. "}
{"citeStart": 196, "citeEnd": 209, "citeStartToken": 196, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al., 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988) . Even though these approaches often accomplish considerable improvements with respect to efficiency or termination behavior, it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering. By bringing filtering into the logic underlying the grammar it is possible to show in a perspicuous and logically clean way how and why filtering can be optimized in a particular fashion and how various approaches relate to each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many approaches focus on exploiting specific knowledge about grammars and/or the computational task(s) that one is using them for by making filtering explicit and extending the processing strategy such that this information can be made effective.", "mid_sen": "In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al., 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988) . ", "after_sen": "Even though these approaches often accomplish considerable improvements with respect to efficiency or termination behavior, it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering. "}
{"citeStart": 403, "citeEnd": 416, "citeStartToken": 403, "citeEndToken": 416, "sectionName": "UNKNOWN SECTION NAME", "string": "Word compositions have long been a concern in lexicography (Benson et al. 1986; Miller et al. 1995) , and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, e.g., parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etc.(e.g., Abney 1989 Abney , 1990 Benson et al. 1986; Yarowsky 1995; Church, Gale, Hans, and Hindle 1989) . But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries. So an urgent problem occurs: how to automatically acquire word compositions? In general, word compositions fall into two categories: free compositions and bound compositions, i.e., collocations. Free compositions refer to those in which words can be replaced by other similar ones, while in bound compositions, words cannot be replaced freely (Benson 1990) . Free compositions are predictable, i.e., their reasonableness can be determined according to the syntactic and semantic properties of the words in them. While bound compositions are not predictable, i.e., their reasonableness cannot be derived from the syntactic and semantic properties of the words in them (Smadja 1993) . Now with the availability of large-scale corpus, automatic acquisition of word compositions, especially word collocations from them have been extensively studied(e.g., Choueka et al. 1988; Smadja 1993) . The key of their methods is to make use of some statistical means, e.g., frequencies or mutual information, to quantify the compositional strength between words. These methods are more appropriate for retrieving bound compositions, while less appropriate for retrieving free ones. This is because in free compositions, words are related with each other in a more loose way, which may result in the invalidity of mutual information and other statistical means in distinguishing reasonable compositions from unreasonable ones. In this paper, we start from a different point to explore the problem of automatic acquisition of free compositions. Although we cannot list all free compositions, we can select some typical ones as those specified in some dictionaries(e.g., Benson 1986; Zhang et al. 1994) . According to the properties held by free compositions, we can reasonably suppose that selected compositions can provide strong clues for others. Furthermore we suppose that words can be classified into clusters, with the members in each cluster similar in their compositional ability, which can be characterized as the set of the words able to combined with them to form meaningful phrases. Thus any given composition, although specifying the relation between two words literally, suggests the relation between two clusters. So for each word(or clus-ter), there exist some word clusters, the word (or the words in the cluster) can and only can combine with the words in the clusters to form meaningful phrases. We call the set of these clusters compositional frame of the word (or the cluster). A seemingly plausible method to determine compositional frames is to make use of pre-defined semantic classes in some thesauri(e.g., Miller et al. 1993; Mei et al. 1996) . The rationale behind the method is to take such an assumption that if one word can be combined with another one to form a meaningful phrase, the words similar to them in meaning can also be combined with each other. But it has been shown that the similarity between words in meaning doesn't correspond to the similarity in compositional ability (Zhu 1982) . So adopting semantic classes to construct compositional frames will result in considerable redundancy. An alternative to semantic class is word cluster based on distributional environment (Brown et al., 1992) , which in general refers to the surrounding words distributed around certain word (e.g., Hatzivassiloglou et al., 1993; Pereira et al., 1993) , or the classes of them (Bensch et al., 1995) , or more complex statistical means (Dagan et al., 1993) . According to the properties of the clusters in compositional frames, the clusters should be based on the environment, which, however, is narrowed in the given compositions. Because the given compositions are listed by hand, it is impossible to make use of statistical means to form the environment, the remaining choices are surrounding words or classes of them. Pereira et a1.(1993) put forward a method to cluster nouns in V-N compositions, taking the verbs which can combine with a noun as its environment. Although its goal is to deal with the problem of data sparseness, it suffers from the problem itself.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Word compositions have long been a concern in lexicography (Benson et al. 1986; Miller et al. 1995) , and now as a specific kind of lexical knowledge, it has been shown that they have an important role in many areas in natural language processing, e.g., parsing, generation, lexicon building, word sense disambiguation, and information retrieving, etc.(e.g., Abney 1989 Abney , 1990 Benson et al. 1986; Yarowsky 1995; Church, Gale, Hans, and Hindle 1989) . ", "after_sen": "But due to the huge number of words, it is impossible to list all compositions between words by hand in dictionaries. "}
{"citeStart": 57, "citeEnd": 68, "citeStartToken": 57, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Direct saturation of a quale role. The complement is identified as a subtype of the experiencing event or the intellectual act. In (16b) and (17b), for example, the complement directly saturates the event e2 or e3 (as the qualia structures in (18) and (19) make explicit). Partir is indeed a subtype of the experiencing event sort (parfir < ezperiencing_event) and les dchecs (chess) of the intellectual act one (les dchecs < creative-intellectual_act). By contrast, in order for (20) to be an acceptable sentence, ~tre malade 'be ill' must be reconstructed, non-standardly, as an intellectual act. Saturation of the object of the experiencing or intellectual act event. In (16c) and (17c), the complement is the object (y) of an implicit event and the saturation of the quale is only possible because the complement can be coerced to the type expected for the complement (the experiencing event or intellectual act): (16c) means that I'm sad~furious because I ezperience your leaving (as (21) makes explicit) and (17c) that I'm ingenious at performing ~he intellectual ac~ whose object is the departure (as in (22) ). There is no further specification available for the ezp_ev or intellectual-acLev variable. These two ways of saturating a quale explain what Croft (Croft, 1984) and Ernst (1985) have called the verbal/factive ambiguity of two arguments agent-oriented adjectives (see also Kiparsky and Kiparsky, 1979) . When the event is saturated, we get the eventual sense: in (17b), cleverness is predicated of the manner of playing chess (structure 19); when the object of the event is saturated, we get the faetive sense so that in (17c) cleverness is predicated of the fact of leaving (structure 22).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There is no further specification available for the ezp_ev or intellectual-acLev variable. ", "mid_sen": "These two ways of saturating a quale explain what Croft (Croft, 1984) and Ernst (1985) have called the verbal/factive ambiguity of two arguments agent-oriented adjectives (see also Kiparsky and Kiparsky, 1979) . ", "after_sen": "When the event is saturated, we get the eventual sense: in (17b), cleverness is predicated of the manner of playing chess (structure 19); when the object of the event is saturated, we get the faetive sense so that in (17c) cleverness is predicated of the fact of leaving (structure 22)."}
{"citeStart": 55, "citeEnd": 85, "citeStartToken": 55, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "When evaluating on test set sentences, if the model is unable to find a parse given the constraints of the tag dictionary, then we would have to take a score of zero for that sentence: every dependency would be \"wrong\". Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either s dcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the \"deletion\" strategy employed by Zettlemoyer and Collins (2007) , but we do it directly in the grammar. We add unary rules of the form D →u for every potential supertag u in the tree. Then, at each node spanning exactly two tokens (but no higher in the tree), we allow rules t→ D , v and t→ v, D . Recall that in §3.1, we stated that D is given extremely low probability, meaning that the parser will avoid its use unless it is absolutely necessary. Additionally, since u will still remain as the preterminal, it will be the category examined as the context by adjacent constituents.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). ", "mid_sen": "This is similar to the \"deletion\" strategy employed by Zettlemoyer and Collins (2007) , but we do it directly in the grammar. ", "after_sen": "We add unary rules of the form D →u for every potential supertag u in the tree. "}
{"citeStart": 81, "citeEnd": 92, "citeStartToken": 81, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "Explanations of the knowledge contained within each grammar principle is given in the following sections. Theory X-bar Theory uses a set of schemata to license local subtmes. We use a parametrised version of the X-bar schemata, similar to that of Muysken [Muysken1983] , but employing features which relate to the state of the head word's theta grid to give five schemata (figure 2) . A .ode includes the following features (among others):", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Theory X-bar Theory uses a set of schemata to license local subtmes. ", "mid_sen": "We use a parametrised version of the X-bar schemata, similar to that of Muysken [Muysken1983] , but employing features which relate to the state of the head word's theta grid to give five schemata (figure 2) . ", "after_sen": "A .ode includes the following features (among others):"}
{"citeStart": 181, "citeEnd": 192, "citeStartToken": 181, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner's algorithm (Sidner, 1981) proposed in (Azzam, 1996) , with further refinements from development on real-world texts. The approach is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system (Gaizauskas et al., 1995) and , Sheffield University's entry in the MUC-6 and 7 evaluations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature.", "mid_sen": "This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner's algorithm (Sidner, 1981) proposed in (Azzam, 1996) , with further refinements from development on real-world texts. ", "after_sen": "The approach is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system (Gaizauskas et al., 1995) and , Sheffield University's entry in the MUC-6 and 7 evaluations."}
{"citeStart": 72, "citeEnd": 101, "citeStartToken": 72, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "(e) to explain specific semantic selection by the notion of headedness (Pustejovsky 1995, chapter 5.3 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(b) to represent the semantic ambiguity of mental adjectives by use of dotted types (Pustejovsky 1995, chapter 6.2);", "mid_sen": "(e) to explain specific semantic selection by the notion of headedness (Pustejovsky 1995, chapter 5.3 ).", "after_sen": "The two first points will be the object of section 3.2 and the third one of 3.3. "}
{"citeStart": 146, "citeEnd": 173, "citeStartToken": 146, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "The current work has focussed on high-level mapping rules which can be used both for generation from databases and knowledge representations and also for generation from text. In future work, we will focus on mapping text (in monologue form) to dialogue. For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form. For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in Barzilay and McKeown (2001) . An important component of our future effort will be to evaluate whether automatically generating dialogues from naturally-occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For this we need to combine the highlevel rules with rules for paraphrasing the text in the monologue with text for the dialogue acts that express the same information in dialogue form. ", "mid_sen": "For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in Barzilay and McKeown (2001) . ", "after_sen": "An important component of our future effort will be to evaluate whether automatically generating dialogues from naturally-occurring monologues, following the approach described here, results in dialogues that are fluent and coherent and preserve the information from the input monologue."}
{"citeStart": 33, "citeEnd": 48, "citeStartToken": 33, "citeEndToken": 48, "sectionName": "UNKNOWN SECTION NAME", "string": "Similar results are presented by Merialdo (1994) , who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text. In addition, although Merialdo does not highlight the point, BW re-estimation starting from less than 5000 words of hand-tagged text shows early maximum behaviour. Merialdo's conclusion is that taggers should be trained using as much hand-tagged text as possible to begin with, and only then applying BW re-estimation with untagged text. The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Without a lexicon, some initial biasing of the transitions is needed if good results are to be obtained.", "mid_sen": "Similar results are presented by Merialdo (1994) , who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. ", "after_sen": "As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text. "}
{"citeStart": 94, "citeEnd": 110, "citeStartToken": 94, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "Although this methodology could be valid for certain NLP problems, such as English Part-of-Speech tagging, we think that there exists reasonable evidence to say that, in WSD, accuracy results cannot be simply extrapolated to other domains (contrary to the opinion of other authors (Ng, 1997b) ): On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a) , in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover \"all\" potential types of examples.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the aSupervised approaches, also known as data-driven or corpus-dmven, are those that learn from a previously semantically annotated corpus. ", "mid_sen": "one hand, WSD is very dependant to the domain of application (Gale et al., 1992b) --see also (Ng and Lee, 1996; Ng, 1997a) , in which quite different accuracy figures are obtained when testing an exemplar-based WSD classifier on two different corpora. ", "after_sen": "Oi1 the other hand, it does not seem reasonable to think that the training material is large and representative enough to cover \"all\" potential types of examples."}
{"citeStart": 33, "citeEnd": 51, "citeStartToken": 33, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "To verify whether our proposed hybrid kernel achieves state-of-the-art results without taking benefits of the output of Stage 1, we did some experiments without discarding any sentence. These experiments are done using Zhou et al. (2005) , TPWF kernel, SL kernel, different versions of proposed K HF kernel and K Hybrid kernel. Table 1 shows the results of 5-fold cross-validation experiments (hyper-parameters are tuned for obtaining maximum F-score). As the results show, there is a gain +0.9 points in F-score (mainly due to the boost in recall) after the addition of features related to negation scope. There is also some minor improvement due to the proposed non-target entity specific features.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To verify whether our proposed hybrid kernel achieves state-of-the-art results without taking benefits of the output of Stage 1, we did some experiments without discarding any sentence. ", "mid_sen": "These experiments are done using Zhou et al. (2005) , TPWF kernel, SL kernel, different versions of proposed K HF kernel and K Hybrid kernel. ", "after_sen": "Table 1 shows the results of 5-fold cross-validation experiments (hyper-parameters are tuned for obtaining maximum F-score). "}
{"citeStart": 54, "citeEnd": 78, "citeStartToken": 54, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The latest scorer is version v8.01, but MUC, B , CEAF e and CoNLL average scores are not changed. For evaluation on ACE-2004, we convert the system output and gold annotations into CoNLL format. 12 We do not provide results from Berkeley and HOTCoref on ACE-2004 dataset as they do not directly support ACE input. Results for HOTCoref are slightly different from the results reported in Björkelund and Kuhn (2014) . For Berkeley system, we use the reported results from Durrett and Klein (2014) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Results for HOTCoref are slightly different from the results reported in Björkelund and Kuhn (2014) . ", "mid_sen": "For Berkeley system, we use the reported results from Durrett and Klein (2014) .", "after_sen": "shows significant improvement on all metrics for both datasets. "}
{"citeStart": 0, "citeEnd": 12, "citeStartToken": 0, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (\"certain\" and \"right\") with their immediate neighbors.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several researchers have worked on learning grammatical properties of words. ", "mid_sen": "Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. ", "after_sen": "Brill et al. (1990) try to infer grammatical category from bigram statistics. "}
{"citeStart": 48, "citeEnd": 69, "citeStartToken": 48, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "Our results also confirm the insights gained by Blitzer et al. (2007) , who observed that in crossdomain polarity analysis adding more training data is not always beneficial. Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the results of the cars + cameras training data combination indicate that the cameras data does not contribute any additional information during the learning, since the results on both the movies and the web-services datasets are lower than when training only on the cameras data.", "mid_sen": "Our results also confirm the insights gained by Blitzer et al. (2007) , who observed that in crossdomain polarity analysis adding more training data is not always beneficial. ", "after_sen": "Apparently even the smallest training dataset (cameras) contain enough feature instances to learn a model which performs well on the testing data."}
{"citeStart": 31, "citeEnd": 59, "citeStartToken": 31, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions. In our approach, instead of providing the complete scoring function ourselves, we compute the parameters needed by a phrase based decoder, which in turn uses these parameters appropriately. In comparison with the DTM2, we also use minimal nonoverlapping blocks as the entries in the phrase table that we generate. Xiong et al. (2006) present a phrase reordering model under the ITG constraint using a maximum entropy framework. They model the reordering problem as a two-class classification problem, the classes being straight and inverted. The model is used to merge the phrases obtained from translating the segments in a source sentence. The decoder used is a hierarchical decoder motivated from the CYK parsing algorithm employing a beam search algorithm. The maximum entropy model is presented with features extracted from the blocks being merged and probabilities are estimated using the log-linear equation shown in (4). The work in addition to lexical features and collocational features, uses an additional metric called the information gain ratio (IGR) as a feature. The authors report an improvement of 4% BLEU score over the traditional distance based distortion model upon using the lexical features alone.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The DTM2 model differs from other phrasebased SMT models in that it avoids the redundancy present in other systems by extracting from a word aligned parallel corpora a set of minimal phrases such that no two phrases overlap with each other (Hassan et al., 2009) .", "mid_sen": "The decoding strategy in DTM2 (Ittycheriah and Roukos, 2007) is similar to a phrase-based decoder except that the score of a particular translation block is obtained from the maximum entropy model using the set of feature functions. ", "after_sen": "In our approach, instead of providing the complete scoring function ourselves, we compute the parameters needed by a phrase based decoder, which in turn uses these parameters appropriately. "}
{"citeStart": 54, "citeEnd": 73, "citeStartToken": 54, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Frame-semantic features. While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SE-MAFOR (Das et al., 2010) . Frame-word interaction features encode whether two words appear in different elements of the same frame. Hence, each frame-word interaction feature consists of (1) the name of the frame f from which it is created, and (2) an unordered word pair in which the words are taken from two frame elements of f . A frame-pair feature is represented as a word pair corresponding to the names of two frames and encodes whether the target word of the first frame appears within an element of the second frame. Finally, frame ngram features are a variant of word n-grams. For each word n-gram in the sentence, a frame n-gram feature is created by replacing one or more words in the word n-gram with the name of the frame or the frame element in which the word appears. A detailed description of these features can be found in Hasan and Ng (2013c).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While dependencybased features capture the syntactic dependencies, frame-semantic features encode the semantic representation of the concepts in a sentence. ", "mid_sen": "Following our previous work on stance classification (Hasan and Ng, 2013c), we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SE-MAFOR (Das et al., 2010) . ", "after_sen": "Frame-word interaction features encode whether two words appear in different elements of the same frame. "}
{"citeStart": 37, "citeEnd": 57, "citeStartToken": 37, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "The only Chinese unknown words which are not correctly translated in the above list are 9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979) . This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Yarowsky, 1995; 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. 1994) for sense disambiguation between multiple usages of the same word. Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and RSscheisen, 1993; Fung and Church, 1994; Fung, 1995b) . These algorithms all require parallel, translated texts as input. Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997) . Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language, and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language. In this paper,", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The only Chinese unknown words which are not correctly translated in the above list are 9 Related work Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979) . ", "mid_sen": "This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shiitze, 1992; Yarowsky, 1995; 1Lunar is not an unknown word in English, Yeltsin finds its translation in the 4-th candidate. ", "after_sen": "1994) for sense disambiguation between multiple usages of the same word. "}
{"citeStart": 116, "citeEnd": 128, "citeStartToken": 116, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005) , which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005) , which are obtained using a parser trained to determine linguistic constituency. ", "mid_sen": "Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) .", "after_sen": "The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints (Fox, 2002) , and (ii) we can model broad syntactic reordering phenomena, such as subject-verb-object constructions translating into subject-object-verb ones, as is generally the case for English and Japanese."}
{"citeStart": 294, "citeEnd": 307, "citeStartToken": 294, "citeEndToken": 307, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of the NLU systems developed in the 70's indu(le(l a kind of error recovery Inechanisln ranging flom the treatment only of spelling e.rrors, PARRY (1)arkinson c 't al., 1977) , to tile inclusion also of incomplete int)ut containing some kind of ellipsis, LAD-DEll,/LIFEll (Hendrix et al., 1977) .", "mid_sen": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . ", "after_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. "}
{"citeStart": 70, "citeEnd": 83, "citeStartToken": 70, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "This excerpt from Clark and Wilkes-Gibbs's data illustrates rejection (line 2), replacement (line 2), and acceptance (lines 3 and 4): An agent can infer a plan even if it is invalid in that agent's view (Pollack, 1990) . The evahmtion process attempts to find an instantiation of the variables such that all of the constraints are satisfied and the mental actions executable with respect to the hearer's beliefs about the speaker's beliefs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This excerpt from Clark and Wilkes-Gibbs's data illustrates rejection (line 2), replacement (line 2), and acceptance (lines 3 and 4): ", "mid_sen": "An agent can infer a plan even if it is invalid in that agent's view (Pollack, 1990) . ", "after_sen": "The evahmtion process attempts to find an instantiation of the variables such that all of the constraints are satisfied and the mental actions executable with respect to the hearer's beliefs about the speaker's beliefs."}
{"citeStart": 78, "citeEnd": 104, "citeStartToken": 78, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Now I shall show how this general equation relates to the equations for [gbo (Liberman et al., 1993, 151 ) , reproduced below:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "zi -/i /'i R $- ti-ltl Xi--1 --ti--1 ~i--1 Zl : ~ti_,ti(Xi--1) -- --Rti_,tl.xi-1 ti-1 + ti(1 -Rt,_,t,)", "mid_sen": "Now I shall show how this general equation relates to the equations for [gbo (Liberman et al., 1993, 151 ) , reproduced below:", "after_sen": "(3) HH"}
{"citeStart": 126, "citeEnd": 147, "citeStartToken": 126, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007) , continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009) , for German↔English systems.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. ", "after_sen": "As described in Rottmann and Vogel (2007) , continuous reordering rules are extracted. "}
{"citeStart": 20, "citeEnd": 42, "citeStartToken": 20, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "However, inference with these more complex models will probably itself become more complex. The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes methods (Teh et al., 2008) for DPs to adaptor grammars and other linguistic applications of HDPs.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, inference with these more complex models will probably itself become more complex. ", "mid_sen": "The MCMC sampler of Johnson et al. (2007a) used here is satifactory for small and medium-sized problems, but it would be very useful to have more efficient inference procedures. ", "after_sen": "It may be possible to adapt efficient split-merge samplers (Jain and Neal, 2007) and Variational Bayes methods (Teh et al., 2008) for DPs to adaptor grammars and other linguistic applications of HDPs."}
{"citeStart": 10, "citeEnd": 24, "citeStartToken": 10, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Functional Generative Description (Sgall et al., 1986 ) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary categorial operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. ", "mid_sen": "Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. ", "after_sen": "He assumes associative categorial operators, permuting the arguments to yield the surface ordering. "}
{"citeStart": 153, "citeEnd": 176, "citeStartToken": 153, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized. We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized. ", "mid_sen": "We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003) .", "after_sen": ""}
{"citeStart": 101, "citeEnd": 125, "citeStartToken": 101, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "r Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These systems addressed the evaluation issue as follows.", "mid_sen": "r Only qualitative observations of the responses were reported (no formal evaluation was performed) (Lapalme and Kosseim 2003; Roy and Subramaniam 2006) .", "after_sen": "r Only an automatic evaluation was performed, which relied on having model responses ."}
{"citeStart": 86, "citeEnd": 107, "citeStartToken": 86, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "Examples of the information about countability and number stored in the Japanese to English noun transfer dictionary are given in table 1. The information about noun countability preferences cannot Joe found in standard dictionaries and must Ix: entered by an English native speaker. Some tests to help determine a given noun's countability preferences are described in Bond and Ogura (1993) , which discusses the use of noun countability preferences in Japanese to English machine translation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The information about noun countability preferences cannot Joe found in standard dictionaries and must Ix: entered by an English native speaker. ", "mid_sen": "Some tests to help determine a given noun's countability preferences are described in Bond and Ogura (1993) , which discusses the use of noun countability preferences in Japanese to English machine translation.", "after_sen": ""}
{"citeStart": 66, "citeEnd": 83, "citeStartToken": 66, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , Church (1993) , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , Warwick-Armstrong and Russell (1990) , Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and English-Chinese.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "There have been quite a number of recent papers on parallel text: Brown et al (1990 Brown et al ( , 1991 Brown et al ( , 1993 , Chen (1993) , Church (1993) , , Dagan et al (1993) , Church (1991, 1993) , Isabelle (1992) , Kay and Rgsenschein (1993) , Klavans and Tzoukermann (1990) , Kupiec (1993) , Matsumoto (1991) , Ogden and Gonzales (1993) , Shemtov (1993) , Simard et al (1992) , Warwick-Armstrong and Russell (1990) , Wu (to appear). ", "after_sen": "Most of this work has been focused on European language pairs, especially English-French. "}
{"citeStart": 124, "citeEnd": 148, "citeStartToken": 124, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . ", "mid_sen": "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . ", "after_sen": "That is, the current system learns procedures rather than data structures. "}
{"citeStart": 42, "citeEnd": 51, "citeStartToken": 42, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "In the initial annotation phase, 13 % of our corpus was annotated twice by different annotators in order to measure inter-annotator agreement. We did this measurement as it was done for MUC (Hirschman et al., 1997) and in the same way as a coreference resolution system is evaluated against some gold standard: one annotation was set to be the gold standard (\"key\") and the second annotation was set to be the \"response\". Herewith we reached an inter-annotator agreement of 49.5 MUC points (for MUC score calculation see Vilain et al. (1995) ). Although the MUC measure is questionable (Luo, 2005) and the task is difficult, this number is too low and asked for improvements.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Herewith we reached an inter-annotator agreement of 49.5 MUC points (for MUC score calculation see Vilain et al. (1995) ). ", "mid_sen": "Although the MUC measure is questionable (Luo, 2005) and the task is difficult, this number is too low and asked for improvements.", "after_sen": "Therefore in a second phase, the annotation guidelines have been improved in order to cover more corner cases and to resolve possible ambiguities. "}
{"citeStart": 13, "citeEnd": 47, "citeStartToken": 13, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. ", "mid_sen": "Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. ", "after_sen": "Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. "}
{"citeStart": 158, "citeEnd": 171, "citeStartToken": 158, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "Our simulation attempts to find the hypothesis that minimizes the combined sizes of the lexicon and encoded sample. This approach is called the Minimum Description Length (MDL) paradigm and has been used recently in other domains to analyze distributional information (Li & Vitgnyi, 1993; Rissanen, 1978; Ellison, 1992 Ellison, , 1994 Brent, 1993) . For reasons explained in the next section, the system converts these character-based representations to compact binary representations, using the number of bits in the binary string as a Ine~u re of size. I)imnotac(.ic rules can I)e used to restrict tim s(wnenl,al,ion hyl)ol, hesis Sl)ace I)y preventing word I)ountlari(,s a.t certain places; for instance, /ka,l,sp:)z/ (\",:at's paws\") has six i,,ternal s(~gmental.ion I)oints (k ;~l,Sl):)z, ka: t.sl):)z, el.c), only two of which are I)honotactically allowed (ka:t Sl):)z and kmts 1)3z). '17o evaluate the usefuhmss of phonotactic knowledge, we compared results between phonotactically constrained and unconstrained simulations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our simulation attempts to find the hypothesis that minimizes the combined sizes of the lexicon and encoded sample. ", "mid_sen": "This approach is called the Minimum Description Length (MDL) paradigm and has been used recently in other domains to analyze distributional information (Li & Vitgnyi, 1993; Rissanen, 1978; Ellison, 1992 Ellison, , 1994 Brent, 1993) . ", "after_sen": "For reasons explained in the next section, the system converts these character-based representations to compact binary representations, using the number of bits in the binary string as a Ine~u re of size. "}
{"citeStart": 48, "citeEnd": 59, "citeStartToken": 48, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "We report in Section 2 on our experiments on the assignment of part of speech to words in text. The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985) , and they are currently in use in parsers (e.g. de Marcken 1990). Our work is an incremental improvement on these models in three ways:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We report in Section 2 on our experiments on the assignment of part of speech to words in text. ", "mid_sen": "The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985) , and they are currently in use in parsers (e.g. de Marcken 1990). ", "after_sen": "Our work is an incremental improvement on these models in three ways:"}
{"citeStart": 158, "citeEnd": 172, "citeStartToken": 158, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper has discussed an iuslaiice of a general probloin in tile design of convorsalional agents: when lo inchido optional infornlation. We presented and lested a nunlber of hypotheses aboul the factors lhat conlribnle Figure6: Explicit-Warranl is STILL benelicial: Strategy 1 is two Explicit-W~u'ranl agents ~md slrategy 2 is two All-Implicit agents: qlisk = Zero-Nonmatching-Bcliel~s, commcost = 10, inl~;ost = (/, retcost = to tile decision of when Io include a w:urant in a proposed. We showed thai w~ur~mts ~u'e useful when the task requires agreement on the warrant, when the win'rant is not currently s~dient, when retrieval of tile w~u-r~mt is indeterminate, or when retriewd has some associated cost, and that warranls hinder perfornlance if communication is costly ~uld if tim w~urant c~ul displace inli.)rnlation that is needed to complete the task, e.g. when AWM is very limited and wm'r[mts ~ue not required to be shared. Tile method used here is a new experimental methodelegy for computational linguistics that supports testing hypotheses about benelici~d disconrse strategies [Carletta, 1992; Pollack and Ringuelle, 1990] . The Design-World environment is b~tsed on a cognitive model of limited attention ~md supports experimenls on the interaction of discourse strategies with agents' cognitive limitations. The use of the method and the focns of lhis wtnk are novel: previous work has l~)CliSed Oil determining nnderlying mechanisnls £~r cooperative strategies rather than on investigating when a slrategy is elIective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We showed thai w~ur~mts ~u'e useful when the task requires agreement on the warrant, when the win'rant is not currently s~dient, when retrieval of tile w~u-r~mt is indeterminate, or when retriewd has some associated cost, and that warranls hinder perfornlance if communication is costly ~uld if tim w~urant c~ul displace inli.)rnlation that is needed to complete the task, e.g. when AWM is very limited and wm'r[mts ~ue not required to be shared. ", "mid_sen": "Tile method used here is a new experimental methodelegy for computational linguistics that supports testing hypotheses about benelici~d disconrse strategies [Carletta, 1992; Pollack and Ringuelle, 1990] . ", "after_sen": "The Design-World environment is b~tsed on a cognitive model of limited attention ~md supports experimenls on the interaction of discourse strategies with agents' cognitive limitations. "}
{"citeStart": 3, "citeEnd": 16, "citeStartToken": 3, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "Errors in the extraction of terms and phrases from parallel texts eventually lead to a failure in acquiring the correct term/phrase correspondences. In Kupiec (1993) and Yamamoto (1993) , term and phrase extraction is applied to both of parallel texts.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Errors in the extraction of terms and phrases from parallel texts eventually lead to a failure in acquiring the correct term/phrase correspondences. ", "mid_sen": "In Kupiec (1993) and Yamamoto (1993) , term and phrase extraction is applied to both of parallel texts.", "after_sen": "In contrast, we extract from units only Japanese terms, thereby reducing the errors caused by term/phrase recognizer. "}
{"citeStart": 177, "citeEnd": 189, "citeStartToken": 177, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "x Table 1 \" Pronoun resolution modules used in our experiments can run in isolation or with the addition of metamodules that combine the output of multiple techniques. We implemented meta-modules to interface to the genetic algorithm driver and to combine different salience factors into an overall score (similar to (Carbonell and Brown, 1988; Mitkov, 1998) ). Table 1 describes the pronoun resolution techniques implemented at this point, and shows whether they are activated for the Treebank and the TRAINS93 experiments. Although each module could run for both experiments without error, if the features a particular module uses in the DE were not available, we simply de-activated the module. When we migrate the TRIPS system to a new domain this year, all these pronoun resolution methods will be available for comparison.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "x Table 1 \" Pronoun resolution modules used in our experiments can run in isolation or with the addition of metamodules that combine the output of multiple techniques. ", "mid_sen": "We implemented meta-modules to interface to the genetic algorithm driver and to combine different salience factors into an overall score (similar to (Carbonell and Brown, 1988; Mitkov, 1998) ). ", "after_sen": "Table 1 describes the pronoun resolution techniques implemented at this point, and shows whether they are activated for the Treebank and the TRAINS93 experiments. "}
{"citeStart": 42, "citeEnd": 67, "citeStartToken": 42, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "Unlike most linguistic theories, LFG (see Kaplan and Bresnan (1982) ) treats grammatical relations as first class citizens. Accordingly, it casts its linguistic analyses in terms of a composite ontology: two independent domains --a domain of constituency information (c-structure), and a domain of grammatical function information (f-structure) -linked together in a mutually constraining manner. As has been amply demonstrated over the last fifteen years, this view permits perspicuous analyses of a wide variety of linguistic data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Unlike most linguistic theories, LFG (see Kaplan and Bresnan (1982) ) treats grammatical relations as first class citizens. ", "after_sen": "Accordingly, it casts its linguistic analyses in terms of a composite ontology: two independent domains --a domain of constituency information (c-structure), and a domain of grammatical function information (f-structure) -linked together in a mutually constraining manner. "}
{"citeStart": 123, "citeEnd": 142, "citeStartToken": 123, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to assess the extent to which existing gender inference machinery works for users who use languages other than English, we assembled Twitter datasets for languages that are both prevalent on Twitter and representative of diverse language families: Japanese, Indonesian, Turkish, and French. Each dataset consisted of approximately 1000 users who tweeted primarily in a given language. We used Amazon Mechanical Turk to manually label each user with their gender, using a language-agnostic labeling strategy (Liu and Ruths, 2013) . For classification, we employed a performant support vector machine-based (SVM) technique that has been used in a range of studies, e.g. Burger et al., 2011; .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each dataset consisted of approximately 1000 users who tweeted primarily in a given language. ", "mid_sen": "We used Amazon Mechanical Turk to manually label each user with their gender, using a language-agnostic labeling strategy (Liu and Ruths, 2013) . ", "after_sen": "For classification, we employed a performant support vector machine-based (SVM) technique that has been used in a range of studies, e.g. Burger et al., 2011; ."}
{"citeStart": 145, "citeEnd": 159, "citeStartToken": 145, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "Feature selection techniques have been widely used in information retrieval as a means for coping with the large number of words in a document; a selection is made to keep only the more relevant words. Various feature selection techniques have been used in automatic text categorization; they include document frequency (DF), information gain (IG) (Tzeras and Hartman, 1993) , minimum description length principal (Lang, 1995) , and the χ 2 statistic. (Yang and Pedersen, 1997) has found strong correlations between DF, IG and the χ 2 statistic for a term. On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance. In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001 ).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the other hand, (Rogati and Yang, 2002) reports the χ 2 to produce best performance. ", "mid_sen": "In this paper, we use TF-IDF (a kind of augmented DF) as a feature selection criterion, in order to ensure results are comparable with those in (Yahyaoui, 2001 ).", "after_sen": "TF-IDF (term frequency-inverse document frequency) is one of the widely used feature selection techniques in information retrieval (Yates and Neto, 1999) . "}
{"citeStart": 32, "citeEnd": 41, "citeStartToken": 32, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popović and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. More recently, (Goldwater and McClosky, 2005) In general, this line of work focused on translating from morphologically rich languages into English; there has been limited research in MT in the opposite direction. Koehn (2005) includes a survey of statistical MT systems in both directions for the Europarl corpus, and points out the challenges of this task. A recent work (El-Kahlout and Oflazer, 2006) experimented with English-to-Turkish translation with limited success, suggesting that inflection generation given morphological features may give positive results. In the current work, we suggest a probabilistic framework for morphology generation performed as post-processing. It can therefore be considered as complementary to the techniques described above. Our approach is general in that it is not specific to a particular language pair, and is novel in that it allows modelling of agreement on the target side. The framework suggested here is most closely related to (Suzuki and Toutanova, 2006) , which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MT. This work can be viewed as a generalization of (Suzuki and Toutanova, 2006) in that our model generates inflected forms of words, and is not limited to generating a small, closed set of case markers. In addition, the morphology generation problem is more challenging in that it requires handling of complex agreement phenomena along multiple morphological dimensions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. ", "mid_sen": "For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. ", "after_sen": "Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. "}
{"citeStart": 103, "citeEnd": 116, "citeStartToken": 103, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "The term dependent on tile grammar in the time complexity of the BU-LC unification-based parser described above is O (IC[2[RI3) , where ICI is the number of categories implicit in the grammar, and ]RI, the number of rules. The space complexity is dominated by the size of the parse forest, O(]C[) (these results are proved by Carroll, 1993) . For the ANLT grammar, in which features are nested to a maximum depth of two, ICI is finite but nevertheless extremely large (Briscoe et al., 1987b) 4.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The term dependent on tile grammar in the time complexity of the BU-LC unification-based parser described above is O (IC[2[RI3) , where ICI is the number of categories implicit in the grammar, and ]RI, the number of rules. ", "mid_sen": "The space complexity is dominated by the size of the parse forest, O(]C[) (these results are proved by Carroll, 1993) . ", "after_sen": "For the ANLT grammar, in which features are nested to a maximum depth of two, ICI is finite but nevertheless extremely large (Briscoe et al., 1987b) 4."}
{"citeStart": 4, "citeEnd": 13, "citeStartToken": 4, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "In (Lin, 2004) , I present evidence from Mandarin Chinese that this analysis is on the right track. The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "v δ P DP tire v δ -en v BE P v BE √ flat ARG δ (tire, e) ∧ BECOME(BE([ state flat]), e)", "mid_sen": "In (Lin, 2004) , I present evidence from Mandarin Chinese that this analysis is on the right track. ", "after_sen": "The rest of this paper, however, will be concerned with the computational implementation of my theoretical framework."}
{"citeStart": 98, "citeEnd": 116, "citeStartToken": 98, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to avoid sparse data problems we smoothed this distribution over tuples as described in (Rooth et al., 1999) . We assume that governor-relation pairs (g, r) and arguments a are independently generated from 25 hidden classes C, i.e.:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We based our auxiliary distribution on 3.7 million (g, r, a) tuples (belonging to 600,000 types) we extracted these parses, where g is a lexical governor (for the shallow parses, g is either a verb or a preposition), a is the head of one of its NP arguments and r is the the grammatical relationship between the governor and argument (in the shallow parses r is always OBJ for prepositional governors, and r is either SUBJ or OBJ for verbal governors).", "mid_sen": "In order to avoid sparse data problems we smoothed this distribution over tuples as described in (Rooth et al., 1999) . ", "after_sen": "We assume that governor-relation pairs (g, r) and arguments a are independently generated from 25 hidden classes C, i.e.:"}
{"citeStart": 114, "citeEnd": 138, "citeStartToken": 114, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest.", "mid_sen": "The following sections discuss related work, describe the learning procedure and evaluate it on the Brown Corpus (Francis and Ku~era, 1982) .", "after_sen": ""}
{"citeStart": 213, "citeEnd": 223, "citeStartToken": 213, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJ-RST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004) . Texts in our corpus were downloaded from the official website of People's Daily 1 , where important Caijingpinlun 2 (CJPL) articles by major media entities were republished. With over 400 authors and editors involved, our texts can be regarded as a good indicator of the general use of Chinese by Mainland native speakers. At the moment our CJPL corpus has a total of 395 texts, 785,045 characters, and 84,182 punctuation marks (including pruned spaces). Although on average there are 9.3 characters between every two marks, sentences in CJPL are long, with 51.8 characters per common sentence delimiters (Full Stop, Question Mark and Exclamation Mark).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJ-RST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004) . ", "after_sen": "Texts in our corpus were downloaded from the official website of People's Daily 1 , where important Caijingpinlun 2 (CJPL) articles by major media entities were republished. "}
{"citeStart": 247, "citeEnd": 265, "citeStartToken": 247, "citeEndToken": 265, "sectionName": "UNKNOWN SECTION NAME", "string": "To allow for this preference, we present a novel conditional alignment model of a foreign (source) sentence f = {f 1 , ..., f J } given an English (target) sentence e = {e 1 , ..., e I } and a target tree structure t. Like the classic IBM models (Brown et al., 1994) , our model will introduce a latent alignment vector a = {a 1 , ..., a J } that specifies the position of an aligned target word for each source word. Formally, our model describes p(a, f|e, t), but otherwise borrows heavily from the HMM alignment model of Ney and Vogel (1996) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To allow for this preference, we present a novel conditional alignment model of a foreign (source) sentence f = {f 1 , ..., f J } given an English (target) sentence e = {e 1 , ..., e I } and a target tree structure t. Like the classic IBM models (Brown et al., 1994) , our model will introduce a latent alignment vector a = {a 1 , ..., a J } that specifies the position of an aligned target word for each source word. ", "after_sen": "Formally, our model describes p(a, f|e, t), but otherwise borrows heavily from the HMM alignment model of Ney and Vogel (1996) ."}
{"citeStart": 127, "citeEnd": 142, "citeStartToken": 127, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexical ambiguity resolution is an important research problem for the fields of information retrieval and machine translation (Sanderson, 2000; Chan et al., 2007) . However, making fine-grained sense distinctions for words with multiple closelyrelated meanings is a subjective task (Jorgenson, 1990; Palmer et al., 2005) , which makes it difficult and error-prone. Fine-grained sense distinctions aren't necessary for many tasks, thus a possiblysimpler alternative is lexical disambiguation at the level of homographs (Ide and Wilks, 2006) . Homographs are a special case of semantically ambiguous words: Words that can convey multiple distinct meanings. For example, the word bark can imply two very different concepts -'outer layer of a tree trunk', or, 'the sound made by a dog' and thus is a homograph. Ironically, the definition of the word 'homograph' is itself ambiguous and much debated; however, in this paper we consistently use the above definition.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Lexical ambiguity resolution is an important research problem for the fields of information retrieval and machine translation (Sanderson, 2000; Chan et al., 2007) . ", "after_sen": "However, making fine-grained sense distinctions for words with multiple closelyrelated meanings is a subjective task (Jorgenson, 1990; Palmer et al., 2005) , which makes it difficult and error-prone. "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981) , which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. ", "mid_sen": "Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. ", "after_sen": "It will also be a model for our simplified logical notation (cf. Section 5). "}
{"citeStart": 81, "citeEnd": 86, "citeStartToken": 81, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "Consider the goal of adding a discourse component to a system, or evaluating and improving one that is already in place. A discourse module might combine theories on, e.g., centering or local focusing [GJW83, Sid79] [(3o578, AP86, SIS1] , control [WSSS] , or complex syntactic structures [Pri85] . How might one evaluate the relative contributions of each of these factors or compare two approaches to the same problem?", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Consider the goal of adding a discourse component to a system, or evaluating and improving one that is already in place. ", "mid_sen": "A discourse module might combine theories on, e.g., centering or local focusing [GJW83, Sid79] [(3o578, AP86, SIS1] , control [WSSS] , or complex syntactic structures [Pri85] . ", "after_sen": "How might one evaluate the relative contributions of each of these factors or compare two approaches to the same problem?"}
{"citeStart": 138, "citeEnd": 151, "citeStartToken": 138, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "The most influential problem in motivating statistical learning application in NLP tasks is that of word selection in speech recognition (Jelinek, 1998) . There, word classifiers are derived from a probabilistic language model which estimates the probability of a sentence s using Bayes rule as the product of conditional probabilities,", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The most influential problem in motivating statistical learning application in NLP tasks is that of word selection in speech recognition (Jelinek, 1998) . ", "after_sen": "There, word classifiers are derived from a probabilistic language model which estimates the probability of a sentence s using Bayes rule as the product of conditional probabilities,"}
{"citeStart": 155, "citeEnd": 167, "citeStartToken": 155, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991) :", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "JS(q,r)=-~l [D(q aVgq,r)+D(r aVgq,r) ]", "mid_sen": "The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991) :", "after_sen": "(v) D(pl(V) IIp2(V)) = EPl(V)log Pl p2(v) \" V"}
{"citeStart": 30, "citeEnd": 46, "citeStartToken": 30, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Word Sense Disambiguation (WSD) is wellknown as one of the more difficult problems in the field of natural language processing, as noted in (Gale et al, 1992; Kilgarriff, 1997; Ide and Véronis, 1998) , and others. The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms. To address the last problem, (Gale et al, 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. The lower bound should not drop below the baseline usage of the algorithm (in which every word that is disambiguated is assigned the most frequent sense) whereas the upper bound should not be too restrictive\" when the word in question is hard to disambiguate even for human judges (a measure of this difficulty is the computation of the agreement rates between human annotators). Identification and formalization of the determining contextual parameters for a word used in a given sense is the focus of WSD work that treats texts in a monolingual setting-that is, a setting where translations of the texts in other languages either do not exist or are not considered. This focus is based on the assumption that for a given word w and two of its contexts C 1 and C 2 , if C 1 ≡ C 2 (are perfectly equivalent), then w is used with the same sense in C 1 and C . A formalized definition of context for a given sense would then enable a WSD system to accurately assign sense labels to occurrences of w in unseen texts. Attempts to characterize context for a given sense of a word have addressed a variety of factors:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in a given sense, lack of a standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms. ", "mid_sen": "To address the last problem, (Gale et al, 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. ", "after_sen": "The lower bound should not drop below the baseline usage of the algorithm (in which every word that is disambiguated is assigned the most frequent sense) whereas the upper bound should not be too restrictive\" when the word in question is hard to disambiguate even for human judges (a measure of this difficulty is the computation of the agreement rates between human annotators). "}
{"citeStart": 254, "citeEnd": 276, "citeStartToken": 254, "citeEndToken": 276, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. ", "mid_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "after_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . "}
{"citeStart": 190, "citeEnd": 205, "citeStartToken": 190, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "Unfortunately, because of space limitations, it is not possible to give a complete review of previous work in this article. In the next two sections we give a detailed comparison of the models in this article to the lexicalized PCFG model of Charniak (1997) and the history-based models of Jelinek et al. (1994) , Magerman (1995) , and Ratnaparkhi (1997) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, because of space limitations, it is not possible to give a complete review of previous work in this article. ", "mid_sen": "In the next two sections we give a detailed comparison of the models in this article to the lexicalized PCFG model of Charniak (1997) and the history-based models of Jelinek et al. (1994) , Magerman (1995) , and Ratnaparkhi (1997) .", "after_sen": "For discussion of additional related work, chapter 4 of Collins (1999) attempts to give a comprehensive review of work on statistical parsing up to around 1998. "}
{"citeStart": 206, "citeEnd": 217, "citeStartToken": 206, "citeEndToken": 217, "sectionName": "UNKNOWN SECTION NAME", "string": "Applicative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994) , it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994) , and by Lambek Categorial Grammars (Lambek 1958) . It is therefore worth giving some brief indications of how it fits in with these developments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. ", "mid_sen": "Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994) , it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994) , and by Lambek Categorial Grammars (Lambek 1958) . ", "after_sen": "It is therefore worth giving some brief indications of how it fits in with these developments."}
{"citeStart": 78, "citeEnd": 92, "citeStartToken": 78, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "To describe this type of centrality, we borrow a term from The Tipping Point (Gladwell, 2002) , in which Gladwell describes a certain type of personality in a social network called a maven. A maven is a trusted expert in a specific field who influences other people by passing information and advice. In this paper, our goal is to identify authoritative speakers who control the spread of ideas within a topic. To do this, we introduce MavenRank, which measures the centrality of speeches as nodes in the type of network described in the previous paragraph.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If we associate each speech in the network with its speaker, we can try to identify the most important people in the debate based on how central their speeches are in the network.", "mid_sen": "To describe this type of centrality, we borrow a term from The Tipping Point (Gladwell, 2002) , in which Gladwell describes a certain type of personality in a social network called a maven. ", "after_sen": "A maven is a trusted expert in a specific field who influences other people by passing information and advice. "}
{"citeStart": 80, "citeEnd": 91, "citeStartToken": 80, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar (Schabes et al., 1988, LTAG) , Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven Phrase-Structure Grammar (Pollard and Sag, 1994, HPSG) , it is possible to separate lexical category assignment -the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates -from the combinatory processes that make use of such categories -such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005) . A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999) , a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed \"almost parsing\" by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006) , leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Supertagging was dubbed \"almost parsing\" by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. ", "mid_sen": "Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006) , leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007) .", "after_sen": "We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG. "}
{"citeStart": 231, "citeEnd": 245, "citeStartToken": 231, "citeEndToken": 245, "sectionName": "UNKNOWN SECTION NAME", "string": "Top-down parsing techniques are attractive because of their simplicity, and can often achieve good performance in practice Roark and Johnson, 1999 . However, with a left-recursive grammar such parsers typically fail to terminate. The left-corner grammar transform converts a left-recursive grammar into a non-left-recursive one: a top-down parser using a left-corner transformed grammar simulates a left-corner parser using the original grammar Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972 . However, the left-corner transformed grammar can be signi cantly larger than the original grammar, causing numerous problems. For example, we show below that a probabilistic context-free grammar PCFG estimated from left-corner transformed Penn WSJ tree-bank trees exhibits considerably greater sparse data problems than a PCFG estimated in the usual manner, simply because the left-corner transformed grammar contains approximately 20 times more productions. The transform described in this paper produces a grammar approximately the same size as the input grammar, which is not as adversely a ected by sparse data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, with a left-recursive grammar such parsers typically fail to terminate. ", "mid_sen": "The left-corner grammar transform converts a left-recursive grammar into a non-left-recursive one: a top-down parser using a left-corner transformed grammar simulates a left-corner parser using the original grammar Rosenkrantz and Lewis II, 1970; Aho and Ullman, 1972 . ", "after_sen": "However, the left-corner transformed grammar can be signi cantly larger than the original grammar, causing numerous problems. "}
{"citeStart": 9, "citeEnd": 37, "citeStartToken": 9, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker's stance in a corpus of congressional floor debates. This work combines graph-based and text-classification approaches to achieve 75% accuracy on Congressional debate siding over all topics. Other work applies MaxCut to the reply structure of company discussion forums (Malouf and Mullen, 2008; Murakami and Raymond, 2010; Agrawal et al., 2003) . Murakami & Raymond (2010) show that rules for identifying agreement, defined on the textual content of the post improve performance.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We predict that this difference makes it more difficult to achieve accuracies as high for 4FO-RUMS discussions as can be achieved for the congressional debates corpus.", "mid_sen": "Work by (Somasundaran and Wiebe, 2009) on idealogical debates very similar to our own show that identifying argumentation structure improves performance; their best performance is approximately 64% accuracy over all topics. ", "after_sen": "Research by (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010; Balahur et al., 2009) classifies the speaker's stance in a corpus of congressional floor debates. "}
{"citeStart": 54, "citeEnd": 89, "citeStartToken": 54, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus. Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus. ", "mid_sen": "Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. ", "after_sen": "These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy."}
{"citeStart": 50, "citeEnd": 72, "citeStartToken": 50, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "The similarity between words provides a new method for analysing the structure of text. It can be applied to computing the similarity between texts, and measuring the cohesiveness of a text which suggests coherence of the text, as we have seen in Section 5. And, we are now applying it to text segmentation [Grosz and Sidner, 1986; Youmans, 1991] , i.e. to capture the shifts of coherent scenes in a story.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It can be applied to computing the similarity between texts, and measuring the cohesiveness of a text which suggests coherence of the text, as we have seen in Section 5. ", "mid_sen": "And, we are now applying it to text segmentation [Grosz and Sidner, 1986; Youmans, 1991] , i.e. to capture the shifts of coherent scenes in a story.", "after_sen": "In future research, we intend to deal with syntagmatic relations between words. "}
{"citeStart": 66, "citeEnd": 79, "citeStartToken": 66, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "If a conjunction is unified with some other feature term, every conjunct has to be unified. Controlling the order in which operands are processed in conjunctions may save time if conjuncts can be processed first that are most likely to fail. This observation is the basis for a reordering method proposed by Kogure [1990] . If, e.g., in syntactic rule applications, the value of the attribute agreement in the representation of nominal elements leads to clashes more often than the value of the attribute definiteneness, it would in general be more efficient to unify agreement before definiteness.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Controlling the order in which operands are processed in conjunctions may save time if conjuncts can be processed first that are most likely to fail. ", "mid_sen": "This observation is the basis for a reordering method proposed by Kogure [1990] . ", "after_sen": "If, e.g., in syntactic rule applications, the value of the attribute agreement in the representation of nominal elements leads to clashes more often than the value of the attribute definiteneness, it would in general be more efficient to unify agreement before definiteness."}
{"citeStart": 90, "citeEnd": 101, "citeStartToken": 90, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "Probably the best known algorithm for tracking narrative progression is that developed by Kamp (1979) , Hinrichs (1981) , and Partee (1984) , which formalises the observation that an event will occur just after a preceding event, while a state will overlap with a preceding event. This algorithm gives the correct results in examples such as the following:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Probably the best known algorithm for tracking narrative progression is that developed by Kamp (1979) , Hinrichs (1981) , and Partee (1984) , which formalises the observation that an event will occur just after a preceding event, while a state will overlap with a preceding event. ", "after_sen": "This algorithm gives the correct results in examples such as the following:"}
{"citeStart": 120, "citeEnd": 139, "citeStartToken": 120, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Our annotation guidelines 1 are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al., 2004) . The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our annotation guidelines 1 are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al., 2004) . ", "after_sen": "The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure."}
{"citeStart": 117, "citeEnd": 130, "citeStartToken": 117, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "# correct constituents in P Recall = # constituents in T # correct constituents in P Precision = # constituents in P A constituent in P is \"correct\" if there exists a constituent in T of the same label that spans the same words. Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of (Collins, 1996) and (Magerman, 1995) . Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995) ~, which have the best previously published parsing accuracies on the Wall St. Journal domain. It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse. Suppose there exists a \"perfect\" reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse. The performance of this \"perfect\" scheme is then an upper bound on the performance of any reranking scheme that might be used to reorder the top N parses. Figure 9 shows that the \"perfect\" scheme would achieve roughly 93% precision and recall, which is a dramatic increase over the top 1 accuracy of 87% precision and 86% recall. Figure 10 shows that the \"Exact Match\", which counts the percentage of times 2Results for SPATTER on section 23 are reported in (Collins, 1996) Parser Precision Maximum Entropy ° 86.8% Maximum Entropy* 87.5% (Collins, 1996) * 85.7% (Magerman, 1995) Table 5 : Results on 2416 sentences of section 23 (0 to 100 words in length) of the WSJ Treebank. Evaluations marked with ~ ignore quotation marks. Evaluations marked with * collapse the distinction between ADVP and PRT, and ignore all punctuation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "# correct constituents in P Recall = # constituents in T # correct constituents in P Precision = # constituents in P A constituent in P is \"correct\" if there exists a constituent in T of the same label that spans the same words. ", "mid_sen": "Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of (Collins, 1996) and (Magerman, 1995) . ", "after_sen": "Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995) ~, which have the best previously published parsing accuracies on the Wall St. Journal domain. "}
{"citeStart": 159, "citeEnd": 177, "citeStartToken": 159, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "Similarly, other work has shown the utility in the IR domain of ranking the relevance of cited papers by using supplementary index terms extracted from the content of citations in citing papers, including methods that search through a fixed character-length window (O'Connor, 1982; Bradshaw, 2003) , or that focus solely on the sentence containing the citation (Ritchie et al., 2008) for acquiring these terms. A prior case study (Ritchie et al., 2006) pointed out the challenges in proper identification of the full span of a citation in running text and acknowledged that fixed-width windows have their limits. In contrast to this, endeavors have been made to extract the entire span of a citation by using cue-phrases collected and deemed salient by statistical merit (Nanba et al., 2000; Nanba et al., 2004) . This has met in evaluations with some success.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A prior case study (Ritchie et al., 2006) pointed out the challenges in proper identification of the full span of a citation in running text and acknowledged that fixed-width windows have their limits. ", "mid_sen": "In contrast to this, endeavors have been made to extract the entire span of a citation by using cue-phrases collected and deemed salient by statistical merit (Nanba et al., 2000; Nanba et al., 2004) . ", "after_sen": "This has met in evaluations with some success."}
{"citeStart": 56, "citeEnd": 86, "citeStartToken": 56, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998) . In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In both cases the investigators were able to achieve significant improvements over the previous best tagging results. ", "mid_sen": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998) .", "after_sen": "The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997) , Charniak (1997) and Ratnaparkhi (1997) . "}
{"citeStart": 163, "citeEnd": 175, "citeStartToken": 163, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "The results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of nondeterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley & Sharman, 1991) , and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore & Dowding, 1991) . However, on the assumption that incorrect prediction of gaps is the main avoidable source of performance degradation (c.f. Moore & Dowding) , further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results show that the LR parser is approximately 35% faster than the BU-LC parser, and allocates about 30% less storage. ", "mid_sen": "The magnitude of the speed-up is less than might be expected, given the enthusiastic advocation of nondeterministic CF LR parsing for NL by some researchers (e.g. Tomita, 1987; Wright, Wrigley & Sharman, 1991) , and in the light of improvements observed for predictive over pure bottom-up parsing (e.g. Moore & Dowding, 1991) . ", "after_sen": "However, on the assumption that incorrect prediction of gaps is the main avoidable source of performance degradation (c.f. Moore & Dowding) , further investigation shows that the speed-up is near the maximum that is possible with the ANLT grammar (around 50%)."}
{"citeStart": 211, "citeEnd": 222, "citeStartToken": 211, "citeEndToken": 222, "sectionName": "UNKNOWN SECTION NAME", "string": "Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world. For Frege (1892) , Russell (1905) , and Quine (1949) \"everything exists\"; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994) . We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world. ", "mid_sen": "For Frege (1892) , Russell (1905) , and Quine (1949) \"everything exists\"; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994) . ", "after_sen": "We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts)."}
{"citeStart": 130, "citeEnd": 141, "citeStartToken": 130, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983 ). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . ", "mid_sen": "While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. ", "after_sen": "This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) ."}
{"citeStart": 0, "citeEnd": 14, "citeStartToken": 0, "citeEndToken": 14, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, the field of dialectometry, as introduced by S~guy (1971, 1973) , has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. There is however no firm agreement on just how to compute the distance matrices. Sfiguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make. The need to figure out such systems as the comparative phonology of various linguistic sites can be very time-consuming and fraught with arbitrary choices.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. ", "mid_sen": "Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. ", "after_sen": "Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. "}
{"citeStart": 75, "citeEnd": 99, "citeStartToken": 75, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long. We report these filtered results in Table 14 . Filtered results are consistently higher (as expected). Results are about 0.9% absolute higher on the development set, and about 0.6% higher on the test set. The contribution of the RAT feature across sets is negligible (or small and unstable), resulting in less than 0.1% absolute loss on the dev set, but about 0.15% gain on the test set. For clarity and conciseness, we only show the best model (with RAT) in Table 14 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long. ", "after_sen": "We report these filtered results in Table 14 . "}
{"citeStart": 195, "citeEnd": 207, "citeStartToken": 195, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "Engineering usually is not based on concrete specifications, research from linguistics provides an informal specification. Software Engineering developed many methods to assess the quality of a program, ranging from static analysis of the program code to dynamic testing of the program's behavior. Here, we adapt dynamic testing, which means running the implemented program against a set of test cases. The test cases are designed to maximize the probability of detecting errors in the program, i.e., incorrect conditions, incompatible assumptions on subsequent branches, etc. (for overviews, cf. (Hetzel, 1988; Liggesmeyer, 1990) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here, we adapt dynamic testing, which means running the implemented program against a set of test cases. ", "mid_sen": "The test cases are designed to maximize the probability of detecting errors in the program, i.e., incorrect conditions, incompatible assumptions on subsequent branches, etc. (for overviews, cf. (Hetzel, 1988; Liggesmeyer, 1990) ).", "after_sen": ""}
{"citeStart": 36, "citeEnd": 60, "citeStartToken": 36, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "Dependency parsing creates a syntax tree where words are directly linked according to their relation. These links refine cooccurrence based contexts by utilizing syntactic indications of how words are related. Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Padó and Lapata, 2007; Baroni et al., 2010) . We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature. We note that recently Kern et al. (2010) achieved good WSI performance with only a small, manually-tuned subset of all relations as context.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Padó and Lapata, 2007; Baroni et al., 2010) . ", "mid_sen": "We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature. ", "after_sen": "We note that recently Kern et al. (2010) achieved good WSI performance with only a small, manually-tuned subset of all relations as context."}
{"citeStart": 68, "citeEnd": 81, "citeStartToken": 68, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995) , for the case where the words have the same part of speech. We confirm experimentally that Bayes and Trigrams have complementary performance, Trigrams being better when the words in the confusion set have different parts of speech, and Bayes being better when they have the same part of speech. We introduce a hybrid method, Tribayes, that exploits this complementarity by invoking each method when it is strongest. Tribayes achieves the best accuracy of the methods under consideration in all situations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the latter case, it is reduced to simply guessing whichever word in the confusion set is the most common representative of its part-of-speech class.", "mid_sen": "We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995) , for the case where the words have the same part of speech. ", "after_sen": "We confirm experimentally that Bayes and Trigrams have complementary performance, Trigrams being better when the words in the confusion set have different parts of speech, and Bayes being better when they have the same part of speech. "}
{"citeStart": 77, "citeEnd": 111, "citeStartToken": 77, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, Kendall's % which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996) , is a nonparametric measure of the association between random variables (Gibbons, 1993) . In our context, it looks for correlation between the behavior of q and r on pairs of verbs. Three versions exist; we use the simplest, Ta, here:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that it incorporates unigram probabilities as well as the two distributions q and r.", "mid_sen": "Finally, Kendall's % which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996) , is a nonparametric measure of the association between random variables (Gibbons, 1993) . ", "after_sen": "In our context, it looks for correlation between the behavior of q and r on pairs of verbs. "}
{"citeStart": 229, "citeEnd": 260, "citeStartToken": 229, "citeEndToken": 260, "sectionName": "UNKNOWN SECTION NAME", "string": "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996) , are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003 ] and rules [Galley et al. 2004; Chiang et al. 2005] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006) . But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009) ; discovery of paraphrases (Bannard and Callison-Burch 2005) ; and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003 ] and rules [Galley et al. 2004; Chiang et al. 2005] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006) . ", "mid_sen": "But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009) ; discovery of paraphrases (Bannard and Callison-Burch 2005) ; and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008) .", "after_sen": "IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. "}
{"citeStart": 71, "citeEnd": 85, "citeStartToken": 71, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees. A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values. For example, a question about a word is represented as 30 binary questions. These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992) . The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions. For more discussion of the use of binary decision-tree questions, see (Magerman, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions. ", "mid_sen": "For more discussion of the use of binary decision-tree questions, see (Magerman, 1994) .", "after_sen": ""}
{"citeStart": 29, "citeEnd": 49, "citeStartToken": 29, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "MorphAna: Morphological Analysis provided by sines yields the word stems of nouns, verbs and adjectives, as well as the full forms of unknown words. We are using a lexicon of approx. 100000 word stems of German (Neumann et al., 1997) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We are using a lexicon of approx. ", "mid_sen": "100000 word stems of German (Neumann et al., 1997) .", "after_sen": "STP-Heuristics: Shallow parsing techniques are used to heuristically identify sentences containing relevant information. "}
{"citeStart": 188, "citeEnd": 207, "citeStartToken": 188, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009) . However, there is little work that studies a collective system in which members individually write summaries.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This can eventually be extended to events and issues that are evolving either in time or scope such as elections, wars, or the economy.", "mid_sen": "In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009) . ", "after_sen": "However, there is little work that studies a collective system in which members individually write summaries."}
{"citeStart": 188, "citeEnd": 200, "citeStartToken": 188, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "As for the other antecedents of Commit in Table 3 , it is not surprising that only 4 Open-Options occur given the circumstances in which this tag is used (see Figure 1 ). These Open-Options appear to function as tentative proposals like indeterminate AD+ Os, as the dialogue between the Open-Option and the Commit develops according to hypothesis 1.2. We were instead surprised that AD+Cs are a very common category among the antecedents of Commit (20%); the second commit appears to simply reconfirm the commitment expressed by the first (Walker, 1993; Walker, 1996) , and does not appear to count as a proposal. Finally, the Other column is a collection of miscellaneous antecedents, such as Info-Requests and cases where the antecedent is unclear, that need further analysis. For further details, see (Di Eugenio et al., 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These Open-Options appear to function as tentative proposals like indeterminate AD+ Os, as the dialogue between the Open-Option and the Commit develops according to hypothesis 1.2. ", "mid_sen": "We were instead surprised that AD+Cs are a very common category among the antecedents of Commit (20%); the second commit appears to simply reconfirm the commitment expressed by the first (Walker, 1993; Walker, 1996) , and does not appear to count as a proposal. ", "after_sen": "Finally, the Other column is a collection of miscellaneous antecedents, such as Info-Requests and cases where the antecedent is unclear, that need further analysis. "}
{"citeStart": 44, "citeEnd": 63, "citeStartToken": 44, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "In their t'nndamental experiment, Clark and Wilkes-Gibbs (1986) demonstrated that conversants use a set of inherently collaborative procedures to establish the mutual belief that the hearer has understood a reference. In the experiment, two subjects were each given a set of hard-to-describe tangram figures that were kept hidden from the other. One subject was required to get the other subject to rearrange his set to match the ordering of her set, and to do so through conversation alone. Thus, the two subjects were obliged to collaborate on constructing descriptions of the figm'es that would allow them to be unambiguously identified; for example, the one that looks like an angel with a stick.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In their t'nndamental experiment, Clark and Wilkes-Gibbs (1986) demonstrated that conversants use a set of inherently collaborative procedures to establish the mutual belief that the hearer has understood a reference. ", "after_sen": "In the experiment, two subjects were each given a set of hard-to-describe tangram figures that were kept hidden from the other. "}
{"citeStart": 104, "citeEnd": 116, "citeStartToken": 104, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "To evaluate iterative training, we extracted maximum probability (Viterbi) trees for the 600 clause test set in each iteration of parsing. For extraction of a maximal probability parse in unlexicalized training, we used Schmid's lopar parser (Schmid, 1999) . Trees were mapped to a database of parser generated markup guesses, and we measured precision and recall against the manually annotated category names and spans. Precision gives the ratio of correct guesses over all guesses, and recall the ratio of correct guesses over the number of phrases identified by human annotators. Here, we render only the precision/recall results on pairs of category names and spans, neglecting less interesting measures on spans alone. For the figures of adjusted recall, the number of unparsed misses has been subtracted from the number of possibilities. Figure 9 : Precision measures on all verb frames", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To evaluate iterative training, we extracted maximum probability (Viterbi) trees for the 600 clause test set in each iteration of parsing. ", "mid_sen": "For extraction of a maximal probability parse in unlexicalized training, we used Schmid's lopar parser (Schmid, 1999) . ", "after_sen": "Trees were mapped to a database of parser generated markup guesses, and we measured precision and recall against the manually annotated category names and spans. "}
{"citeStart": 0, "citeEnd": 32, "citeStartToken": 0, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. Rubenstein and Goodenough (1965) reported an intra-subject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. The values may again not be compared directly. Furthermore, we cannot generalize from these results, because the number of participants which repeated our experiment was too low.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. ", "mid_sen": "Rubenstein and Goodenough (1965) reported an intra-subject correlation of r=.85 for 15 subjects judging the similarity of a subset (36) of the original 65 word pairs. ", "after_sen": "The values may again not be compared directly. "}
{"citeStart": 329, "citeEnd": 343, "citeStartToken": 329, "citeEndToken": 343, "sectionName": "UNKNOWN SECTION NAME", "string": "In the rest of this paper I will lay out a proposal for handling reduplication with finite-state methods. As a starting point, I adopt Bird & Ellison (1994) 's One-Level Phonology, a monostratal constraintbased framework where phonological representations, morphemes and generalizations are all finitestate automata (FSAs) and constraint combination is accomplished via automata intersection. While it is possible to transfer much of the present proposal to the transducer-based setting that is often preferred nowadays, the monostratal approach still offers an attractive alternative due to its easy blend with monostratal grammars such as HPSG and the good prospects for machine learning of its surfacetrue constraints (Ellison (1992) , Belz (1998)) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a starting point, I adopt Bird & Ellison (1994) 's One-Level Phonology, a monostratal constraintbased framework where phonological representations, morphemes and generalizations are all finitestate automata (FSAs) and constraint combination is accomplished via automata intersection. ", "mid_sen": "While it is possible to transfer much of the present proposal to the transducer-based setting that is often preferred nowadays, the monostratal approach still offers an attractive alternative due to its easy blend with monostratal grammars such as HPSG and the good prospects for machine learning of its surfacetrue constraints (Ellison (1992) , Belz (1998)) .", "after_sen": "After a brief survey of important kinds of reduplication in §2, section §3 explains the necessary extensions of One-Level Phonology to deal with the challenges presented by reduplication, within the larger domain of prosodic morphology in general. "}
{"citeStart": 251, "citeEnd": 253, "citeStartToken": 251, "citeEndToken": 253, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, in an order-monotonic grammar formalism that uses sequence union as the operation for combining strings, a combination of items would be useless which results in a sign in which the words are not in the same order as in the input string [14] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Without indexing, there are too many combinations of items which are useless for a proof of the goal, in fact there may be infinitely many items so that termination problems can arise.", "mid_sen": "For example, in an order-monotonic grammar formalism that uses sequence union as the operation for combining strings, a combination of items would be useless which results in a sign in which the words are not in the same order as in the input string [14] .", "after_sen": "We generalize the indexing scheme from chart parsing in order to allow different operations for the combination of strings. "}
{"citeStart": 140, "citeEnd": 153, "citeStartToken": 140, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994) . In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991) . Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time. However, in the context of unification-based parsing, left-recursive grammars have the formal power of a Turing machine, therefore detection of all infinite loops due to left-recursion is impossible (Shieber, 1992) . So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994) . ", "after_sen": "In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991) . "}
{"citeStart": 142, "citeEnd": 161, "citeStartToken": 142, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "llowever, using the co-occurrence statistics requires a huge corpus that covers even most rare words. We recently developed word vectors that are derived from an ordinary dictionary by measuring the interword distances in the word definitions (Niwa and Nitta 1993) . 'this method, by its nature, h~s no prol)lom handling rare words. In this paper we examine the nsefldness of these distance vectors as semantic re W resentations by comparing them with co-occur,'ence vectors.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "llowever, using the co-occurrence statistics requires a huge corpus that covers even most rare words. ", "mid_sen": "We recently developed word vectors that are derived from an ordinary dictionary by measuring the interword distances in the word definitions (Niwa and Nitta 1993) . 'this method, by its nature, h~s no prol)lom handling rare words. ", "after_sen": "In this paper we examine the nsefldness of these distance vectors as semantic re W resentations by comparing them with co-occur,'ence vectors."}
{"citeStart": 41, "citeEnd": 64, "citeStartToken": 41, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. While there, edges between words are drawn iff words co-occur in enumerations, we use the cooccurrence graph. Dorow and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w (without w) and clustering it with MCL. We replace MCL by CW. The clusters are interpreted as representations of word senses.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The number of senses is not known in advance, therefore has to be determined by the method.", "mid_sen": "Similar to the approach as presented in (Dorow and Widdows, 2003) we construct a word graph. ", "after_sen": "While there, edges between words are drawn iff words co-occur in enumerations, we use the cooccurrence graph. "}
{"citeStart": 45, "citeEnd": 65, "citeStartToken": 45, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "We see that the image modalities are much more useful than they are in compositionality prediction. The SURF modality does extremely well in particular, but the GIST features also provide statistically significant improvements over the text-only model. Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. This seems to provide additional evidence of Bruni et al. (2012b) 's suggestion that something like a distributional hypothesis of images is plausible. Once again, the clusters of images using SURF causes a dramatic drop in performance. Combined with the evidence from the compositionality assessment, this shows that the SURF clusters are actively confusing the models and not providing semantic information. GIST clusters, on the other hand, are providing a marginal improvement over the text-only model, but the result is not significant. We take a qualitative look into the GIST clusters in the next section.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the SURF and GIST image features tend to capture object-likeness and scene-likeness respectively, it is possible that words which share associates are likely related through common settings and objects that appear with them. ", "mid_sen": "This seems to provide additional evidence of Bruni et al. (2012b) 's suggestion that something like a distributional hypothesis of images is plausible. ", "after_sen": "Once again, the clusters of images using SURF causes a dramatic drop in performance. "}
{"citeStart": 202, "citeEnd": 221, "citeStartToken": 202, "citeEndToken": 221, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexicalist approaches to MT, particularly those incorporating the technique of Shake-and-Bake generation (Beaven, 1992a; Beaven, 1992b; Whitelock, 1994) , combine the linguistic advantages of transfer (Arnold et al., 1988; Allegranza et al., 1991) and interlingual (Nirenburg et al., 1992; Dorr, 1993) approaches. Unfortunately, the generation algorithms described to date have been intractable. In this paper, we describe an alternative generation component which has polynomial time complexity.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Lexicalist approaches to MT, particularly those incorporating the technique of Shake-and-Bake generation (Beaven, 1992a; Beaven, 1992b; Whitelock, 1994) , combine the linguistic advantages of transfer (Arnold et al., 1988; Allegranza et al., 1991) and interlingual (Nirenburg et al., 1992; Dorr, 1993) approaches. ", "after_sen": "Unfortunately, the generation algorithms described to date have been intractable. "}
{"citeStart": 19, "citeEnd": 42, "citeStartToken": 19, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper assumes some familiarity with the TAG formalism. ", "mid_sen": "(Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. ", "after_sen": "TAGs have been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies."}
{"citeStart": 18, "citeEnd": 31, "citeStartToken": 18, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "The reader will be surprised how simple the underlying model is. The result of the tagger comparison seems to support the maxime \"the simplest is the best\". However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application. We argue that it is not only the choice of the general model that determines the result of the tagger but also the various \"small\" decisions on alternatives.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, in this paper we clarify a number of details that are omitted in major previous publications concerning tagging with Markov models. ", "mid_sen": "As two examples, (Rabiner, 1989) and (Charniak et al., 1993) give good overviews of the techniques and equations used for Markov models and part-ofspeech tagging, but they are not very explicit in the details that are needed for their application. ", "after_sen": "We argue that it is not only the choice of the general model that determines the result of the tagger but also the various \"small\" decisions on alternatives."}
{"citeStart": 100, "citeEnd": 126, "citeStartToken": 100, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data. This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features that have been shown to be optimal for word sense disambiguation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. ", "mid_sen": "A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . ", "after_sen": "A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). "}
{"citeStart": 82, "citeEnd": 114, "citeStartToken": 82, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach follows Langkilde-Geary (2002) and Callaway (2003) in aiming to leverage the Penn Treebank to develop a broad-coverage surface realizer for English. However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. In this regard, our approach is more similar to the ones pursued more recently by Carroll, Oepen and Velldal (2005; 2005; 2006) , Nakanishi et al. (2005) and Cahill and van Genabith (2006) with HPSG and LFG grammars.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, while these earlier, generation-only approaches made use of converters for transforming the outputs of Treebank parsers to inputs for realization, our approach instead employs a shared bidirectional grammar, so that the input to realization is guaranteed to be the same logical form constructed by the parser. ", "mid_sen": "In this regard, our approach is more similar to the ones pursued more recently by Carroll, Oepen and Velldal (2005; 2005; 2006) , Nakanishi et al. (2005) and Cahill and van Genabith (2006) with HPSG and LFG grammars.", "after_sen": "While we consider our approach to be the first to employ a supertagger for realization, or hypertagger, the approach is clearly reminiscent of the LTAG tree models of Srinivas and Rambow (2000) . "}
{"citeStart": 80, "citeEnd": 99, "citeStartToken": 80, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now 7 . Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As full features are used, the former and the latter provide the similar performance.", "mid_sen": "Due to using a global model like CRFs, our previous work in (Zhao et al., 2006; Zhao and Kit, 2008c) reported the best results over the evaluated corpora of Bakeoff-2 until now 7 . ", "after_sen": "Though those results are slightly better than the results here, we still see that the results of character-level dependency parsing approach (Scheme E) are comparable to those state-of-the-art ones on each evaluated corpus."}
{"citeStart": 50, "citeEnd": 61, "citeStartToken": 50, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "The paradigm of two-level morphology (Koskenniemi, 1983) has become popular for handling word formation phenomena in a variety of languages. The original formulation has been extended to allow morphotactic constraints to be expressed by feature specification (Trost, 1990; A1shawi et al, 1991) rather than Koskenniemi's less perspicuous device of continuation classes. Methods for the automatic compilation of rules from a notation convenient for the rule-writer into finitestate automata have also been developed, allowing the efficient analysis and synthesis of word forms. The automata may be derived from the rules alone (Trost, 1990) , or involve composition with the lexicon (Karttunen, Kaplan and Zaenen, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Methods for the automatic compilation of rules from a notation convenient for the rule-writer into finitestate automata have also been developed, allowing the efficient analysis and synthesis of word forms. ", "mid_sen": "The automata may be derived from the rules alone (Trost, 1990) , or involve composition with the lexicon (Karttunen, Kaplan and Zaenen, 1992) .", "after_sen": "However, there is often a trade-off between runtime efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compilation and the size of its output, and the independence of the morphological and lexical components. "}
{"citeStart": 232, "citeEnd": 251, "citeStartToken": 232, "citeEndToken": 251, "sectionName": "UNKNOWN SECTION NAME", "string": "As a seinant{c representation of words, distltllCe w~ctors are expected to depend very weakly on the particular source dictionary. We eolilpared two sets of distance vectors, one from I,I)OCE (Procter 1978) and the other from COBUILD (Sinclair 1987) , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves (Niwa and Nitta 1993 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a seinant{c representation of words, distltllCe w~ctors are expected to depend very weakly on the particular source dictionary. ", "mid_sen": "We eolilpared two sets of distance vectors, one from I,I)OCE (Procter 1978) and the other from COBUILD (Sinclair 1987) , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves (Niwa and Nitta 1993 ).", "after_sen": "We will now describe some technical details al)Ollt the derivation of distance vectors."}
{"citeStart": 74, "citeEnd": 96, "citeStartToken": 74, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "We may also compare our approach with the projection architecture of LFG (Kaplan & Bresnan, 1982; Kaplan, 1995) . There is a close similarity of the LFG projections (c-structure and f-structure) to the dimensions used here (order domain structure and dependency tree, respectively). C-structure and order domains represent surface ordering, whereas f-structure and dependency tree show the subcategorization or valence requirements. What is more, these projections or dimensions are linked in both accounts by'an element-wise mapping. The dif-ference between the two architectures lies in the linkage of the projections or dimensions: LFG maps f-structure off c-structure. In contrast, the dependency relation is taken to be primitive here, and ordering restrictions are taken to be indicators or consequences of dependency relations (see also BrSker (1998b BrSker ( , 1998a ).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This dependence of hierarchical structure on ordering is absent from our proposal.", "mid_sen": "We may also compare our approach with the projection architecture of LFG (Kaplan & Bresnan, 1982; Kaplan, 1995) . ", "after_sen": "There is a close similarity of the LFG projections (c-structure and f-structure) to the dimensions used here (order domain structure and dependency tree, respectively). "}
{"citeStart": 73, "citeEnd": 85, "citeStartToken": 73, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "The table makes clear that enriching our grammar improves the syntactic performance as well as morphological disambiguation (segmentation and POS tagging) accuracy. This supports our main thesis that decisions taken by single, improved, grammar are beneficial for both tasks. When using the segmentation pruning (using HSPELL) for unseen tokens, performance improves for all tasks as well. Yet we note that the better grammars without pruning outperform the poorer grammars using this technique, indicating that the syntactic context aids, to some extent, the disambiguation of unknown tokens. Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks. We first note that the accuracy results of our system are overall higher on their setup, on all measures, indicating that theirs may be an easier dataset. Secondly, for all our models we provide better fine-and coarse-grained POS-tagging accuracy, and all pruned models outperform the Oracle results reported by them. 12 In terms of syntactic disambiguation, even the simplest grammar pruned with HSPELL outperforms their non-Oracle results. Without HSPELL-pruning, our simpler grammars are somewhat lagging behind, but as the grammars improve the gap is bridged. The addition of vertical markovization enables non-pruned models to outperform all previously reported re- sults. Furthermore, the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith. This essentially means that a better grammar tunes the joint model for optimized syntactic disambiguation at least in as much as their hyper parameters do. An interesting observation is that while vertical markovization benefits all our models, its effect is less evident in Cohen and Smith.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Yet we note that the better grammars without pruning outperform the poorer grammars using this technique, indicating that the syntactic context aids, to some extent, the disambiguation of unknown tokens. ", "mid_sen": "Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks. ", "after_sen": "We first note that the accuracy results of our system are overall higher on their setup, on all measures, indicating that theirs may be an easier dataset. "}
{"citeStart": 223, "citeEnd": 234, "citeStartToken": 223, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "We have investigated 35,206 tuples, consisting of an entity, a description, an article ID, and the position (sentence number) in the article in which the entity-description pair occurs. Since there are 11,504 distinct entities, we had on average 3.06 distinct descriptions per entity (DDPE). Table 2 shows the distribution of DDPE values across the corpus. Notice that a large number of entities (9,053 out of the 11,504) have a single description. These are not as interesting for our analysis as the remaining 2,451 entities that have DDPE values between 2 and 24. Text generation usually involves lexical choicethat is, choosing one way of referring to an entity over another. Lexical choice refers to a variety of decisions that have to made in text generation. For example, picking one among several equivalent (or nearly equivalent) constructions is a form of lexical choice (e.g., \"The Utah Jazz handed the Boston Celtics a de fear' vs. \"The Utah Jazz defeated the Boston Celtics\" (Robin, 1994) ). We are interested in a different aspect of the problem: namely learning the rules that can be used for automatically selecting an appropriate description of an entity in a specific context. To be feasible and scaleable, a technique for solving a particular case of the problem of lexicai choice must involve automated learning. It is also useful if the technique can specify enough constraints on the text to be generated so that the number of possible surface realizations that match the semantic constraints is reduced significantly. The easiest case in which lexical choice can be made is when the full surface structure can be used, and when it has been automatically extracted from a corpus. Of course, the constraints on the use of the structure in the generated text have to be reasonably similar to the ones in the source text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical choice refers to a variety of decisions that have to made in text generation. ", "mid_sen": "For example, picking one among several equivalent (or nearly equivalent) constructions is a form of lexical choice (e.g., \"The Utah Jazz handed the Boston Celtics a de fear' vs. \"The Utah Jazz defeated the Boston Celtics\" (Robin, 1994) ). ", "after_sen": "We are interested in a different aspect of the problem: namely learning the rules that can be used for automatically selecting an appropriate description of an entity in a specific context. "}
{"citeStart": 94, "citeEnd": 118, "citeStartToken": 94, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991) , multidocument summarization (McKeown et al., 2002) , automatic evaluation of MT (Denkowski and Lavie, 2010) , and TE (Dinu and Wang, 2009) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. ", "mid_sen": "They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991) , multidocument summarization (McKeown et al., 2002) , automatic evaluation of MT (Denkowski and Lavie, 2010) , and TE (Dinu and Wang, 2009) .", "after_sen": "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005) . "}
{"citeStart": 79, "citeEnd": 104, "citeStartToken": 79, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems unlikely that these continuously variable aspcct:s of fluent natural language can be captured by a purely combinatoric model. This naturally leads to the qtwstion of how best to introduce quantitative modeli,g into language processing. It is not, of course, nec-,,ssary for the quantities of a quantitative model to be probabilities. For example, we may wish to define realvalued functions on parse trees that reflect the extent to which the trees conform to, .say, minimal attachment and parallelism between conjuncts. Such functions have been used in tandem with statistical functions in experiments on disambiguation (for instance Alshawi and (',a.rter 1994) . Another example is connection strengths i, m~ural network approaches to language processing, th,mgh it. has been shown that certain networks are ~,tfectively computing probabilities (Richard and Lippmann 1991) . Nevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing. The case f.r probability theory is strengthened by a well devel-,,p-d empirical methodology in the form of statistical I,:~ramet.ccr estimation. There is also the strong connecl i,,n between probability theory and the formal theory .1\" i.formation and communication, a connection that has been exploited in speech recognition, for example I~qing tim concept of entropy to provide a motivated way ,.f measuring the complexity of a recognition problem (.h'lim'k et ai. 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another example is connection strengths i, m~ural network approaches to language processing, th,mgh it. ", "mid_sen": "has been shown that certain networks are ~,tfectively computing probabilities (Richard and Lippmann 1991) . ", "after_sen": "Nevertheless, probability theory does offer a coherent and relatively well understood framework for selecting between uncertain alternatives, making it a natural choice for quantitative language processing. "}
{"citeStart": 22, "citeEnd": 48, "citeStartToken": 22, "citeEndToken": 48, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluate the extraction quality of our system (FL) against 7 different baselines. In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. The second baseline is the MEAD summarizer with all its settings set to default. The third baseline is LexRank (Erkan and Radev, 2004 ) run on the entire set of citation sentences of the target paper. The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. In one variation (FL-1), we remove the sentence filtering component. In another variation (FL-2), we remove the sentence classification component; so, all the sen-tences are assumed to come from one category in the subsequent components. In a third variation (FL-3), the clustering component is removed. To make the comparison of the extraction quality to those baselines fair, we remove the author name replacement component from our system and all its variations. Table 6 shows the average ROUGE-L scores (with 95% confidence interval) for the summaries of the 30 papers in dataset2 generated using our system and the different baselines. The two human summaries were used as models for comparison. The Human score reported in the table is the result of comparing the two human summaries to each others. Statistical significance was tested using a 2-tailed paired t-test. The results are statistically significant at the 0.05 level.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The third baseline is LexRank (Erkan and Radev, 2004 ) run on the entire set of citation sentences of the target paper. ", "mid_sen": "The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. ", "after_sen": "The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. "}
{"citeStart": 99, "citeEnd": 110, "citeStartToken": 99, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of using citations as a source of information has been explored extensively in the field of bibliometrics, and more recently in the field of computational linguistics. State-of-the-art citations identification mechanisms focus either on detecting explicit citations i.e. those that consist of either the author names and the year of publication or bracketed numbers only, or include a small sentence window around the explicit citation as input text (Councill et al., 2008; Radev et al., 2009; Ritchie et al., 2008) . The assumption behind this approach is that all related mentions of the paper would be concentrated in the immediate vicinity of the anchor text. However, this assumption does not generally hold true (Teufel, 2010; Sugiyama et al., 2010) . The phenomenon of trying to determine a citations's citation context has a long tradition in library sciences (O'Connor, 1982) , and its connection with coreference has been duely noted (Kim et al., 2006; Kaplan et al., 2009) . Consider Figure 1 , which illustrates a typical case. While the first sentence cites the target paper explicitly using the name of the primary author along with the year of publication of the paper, the remaining sentences mentioning the same paper appear after a gap and contain an indirect and implicit reference to that paper. These mentions occur two sentences after the formal citation in the form of anaphoric it and the lexical hook METEOR. Most current techniques, with the exception of Qazvinian and Radev (2010) , are not able to detect linguistic mentions of citations in such forms. Ignoring such mentions and examining only the sentences contain-ing an explicit citation results in loss of information about the cited paper. While this phenomenon is problematic for applications like scientific summarisation (Abu-Jbara and Radev, 2011) , it has a particular relevance for citation sentiment detection (Athar, 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Ignoring such mentions and examining only the sentences contain-ing an explicit citation results in loss of information about the cited paper. ", "mid_sen": "While this phenomenon is problematic for applications like scientific summarisation (Abu-Jbara and Radev, 2011) , it has a particular relevance for citation sentiment detection (Athar, 2011) .", "after_sen": "Citation sentiment detection is an attractive task. "}
{"citeStart": 52, "citeEnd": 75, "citeStartToken": 52, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 162, "citeEnd": 188, "citeStartToken": 162, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "The core of the disambiguation algorithm is a computation of semantic similarity using the WordNet taxonomy, a topic recently investigated by a number of people (Leacock and Chodorow, 1994; Resnik, 1995; Sussna, 1993) . In this paper, I restrict my attention to WordNet's IS-A taxonomy for nouns, and take an approach in which semantic similarity is evaluated on the basis of the information content shared by the items being compared. The intuition behind the approach is simple: the more similar two words are, the more informative will be the most specific concept that subsumes them both. (That is, their least upper bound in the taxonomy; here a concept corresponds to a WordNet synset.) The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes (Lee et al., 1993; Rada et al., 1989) also captures this, albeit indirectly, when the semantic network is just an IS-A hierarchy: if the minimal path of IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find their least upper bound. However, there are problems with the simple path-length definition of semantic similarity, and experiments using WordNet show that other measures of semantic similarity, such as the one employed here, provide a better match to human similarity judgments than simple path length does (Resnik, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The core of the disambiguation algorithm is a computation of semantic similarity using the WordNet taxonomy, a topic recently investigated by a number of people (Leacock and Chodorow, 1994; Resnik, 1995; Sussna, 1993) . ", "after_sen": "In this paper, I restrict my attention to WordNet's IS-A taxonomy for nouns, and take an approach in which semantic similarity is evaluated on the basis of the information content shared by the items being compared. "}
{"citeStart": 162, "citeEnd": 166, "citeStartToken": 162, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "That is, the current system learns procedures rather than data structures. ", "mid_sen": "There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982) .", "after_sen": "However, the latter methodologies have not been applied to dialogue acquisition."}
{"citeStart": 94, "citeEnd": 116, "citeStartToken": 94, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, the role of child-directed speech needs to be examined more carefully. Child-directed speech displays helpful features such as shorter phrases and fewer reductions (Bernstein Ratner, 1996; van de Weijer, 1999) . These features may make segmentation easier to learn, but the strong results presented here for adult-directed speech make it trickier to argue that this help is necessary for learning.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally, the role of child-directed speech needs to be examined more carefully. ", "mid_sen": "Child-directed speech displays helpful features such as shorter phrases and fewer reductions (Bernstein Ratner, 1996; van de Weijer, 1999) . ", "after_sen": "These features may make segmentation easier to learn, but the strong results presented here for adult-directed speech make it trickier to argue that this help is necessary for learning."}
{"citeStart": 52, "citeEnd": 63, "citeStartToken": 52, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "The concept of chunking was introduced by Abney in (Abney, 1991) . He suggested to develop a chunking parser which uses a two-part syntactic analysis: creating word chunks (partial trees) and attaching the chunks to create complete syntactic trees. Abney obtained support for such a chunking stage from psycholinguistic literature. Ramshaw and Marcus used transformationbased learning (TBL) for developing two chunkers (Ramshaw and Marcus, 1995) . One was trained to recognize baseNPs and the other was trained to recognize both NP chunks and VP chunks. Ramshaw and Marcus approached the chunking task as a tagging problem. Their baseNP training and test data from the Wall Street Journal corpus are still being used as benchmark data for current chunking experiments. (Ramshaw and Marcus, 1995) shows that baseNP recognition (Fz=I =92.0) is easier than finding both NP and VP chunks (Fz=1=88.1) and that increasing the size of the training data increases the performance on the test set.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Related work", "mid_sen": "The concept of chunking was introduced by Abney in (Abney, 1991) . ", "after_sen": "He suggested to develop a chunking parser which uses a two-part syntactic analysis: creating word chunks (partial trees) and attaching the chunks to create complete syntactic trees. "}
{"citeStart": 9, "citeEnd": 39, "citeStartToken": 9, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. Instead, all NPs were made rightbranching, as shown in this example:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(NP (NN lung) (NN cancer) (NNS deaths))", "mid_sen": "CCGbank (Hockenmaier and Steedman, 2007) is the primary English corpus for Combinatory Categorial Grammar (CCG) (Steedman, 2000) and was created by a semi-automatic conversion from the Penn Treebank. ", "after_sen": "However, CCG is a binary branching grammar, and as such, cannot leave NP structure underspecified. "}
{"citeStart": 72, "citeEnd": 84, "citeStartToken": 72, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of \"similar\" events that have been seen. For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs. This requires a reasonable definition of verb similarity and a similarity estimation method. In Hindle's proposal, words are similar if we have strong statistical evidence that they tend to participate in the same events. His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. While it may be worth basing such a model on preexisting sense classes (Resnik, 1992) , in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w. Most other class-based modeling techniques for natural language rely instead on \"hard\" Boolean classes (Brown et al., 1990) . Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above. Our approach avoids both problems.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. ", "mid_sen": "While it may be worth basing such a model on preexisting sense classes (Resnik, 1992) , in the work described here we look at how to derive the classes directly from distributional data. ", "after_sen": "More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w. "}
{"citeStart": 84, "citeEnd": 107, "citeStartToken": 84, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared our system (\"CWT+translit\") with the Japanese-Japanese IR system, where (unlike the evaluation in Section 4.2) transliteration was applied only to \"ma-i-ni-n-gu (mining)\" and \"ko-ro-ke-i-sho-n (collocation)\". Table 4 show the recall-precision curve and l 1-point average precision for each system, respectively, from which one can see that our CLIR system is quite comparable with the monolingual IR system in performance. In addition, from Figure 5 to 7, one can see that the monolingual system generally performs better at lower re(:all while the CLIR system pertbrms b(,It(,r at higher recall. For further investigation, let us discuss similar (~xperim(mtal results reported by Kando and Aizawa (1998) , where a bilingual dictionary produced ti'om Japanese/English keyword pairs in the NACSIS documents is used for query translation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, from Figure 5 to 7, one can see that the monolingual system generally performs better at lower re(:all while the CLIR system pertbrms b(,It(,r at higher recall. ", "mid_sen": "For further investigation, let us discuss similar (~xperim(mtal results reported by Kando and Aizawa (1998) , where a bilingual dictionary produced ti'om Japanese/English keyword pairs in the NACSIS documents is used for query translation.", "after_sen": "Their evaluation method is almost the same as pertbrmed in our experinmnts. "}
{"citeStart": 96, "citeEnd": 115, "citeStartToken": 96, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "Our astronomy corpus has been manually annotated with domain-specific named entity information (Murphy et al., 2006) . There are 12 coarsegrained categories and 43 fine-grained categories including star, galaxy, telescope, as well as a number of the usual categories including person, organisation and location. Both the coarse-grained and fine-grained categories were used as features.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our astronomy corpus has been manually annotated with domain-specific named entity information (Murphy et al., 2006) . ", "after_sen": "There are 12 coarsegrained categories and 43 fine-grained categories including star, galaxy, telescope, as well as a number of the usual categories including person, organisation and location. "}
{"citeStart": 425, "citeEnd": 438, "citeStartToken": 425, "citeEndToken": 438, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances (Almuallim and Dietterich, 1991, Langley and Sage, in press ). Unfortunately, the task of designing an appropriate instance representation --also known as feature set selection --can be extraordinarily difficult, time-consuming, and knowledge-intensive (Quinlan, 1983) . This poses a problem for current statistical and machine learning approaches to natural language understanding where a new instance representation is typically required for each linguistic task tackled.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). ", "mid_sen": "Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . ", "after_sen": "In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. "}
{"citeStart": 78, "citeEnd": 92, "citeStartToken": 78, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...) , appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish). Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category. Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques. We begin with a small set of seed words for a category. We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well. As an example, the seed word lists used in our experiments are shown below.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category. ", "mid_sen": "Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques. ", "after_sen": "We begin with a small set of seed words for a category. "}
{"citeStart": 21, "citeEnd": 35, "citeStartToken": 21, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003) . They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003) . ", "mid_sen": "They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. ", "after_sen": "That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. "}
{"citeStart": 77, "citeEnd": 93, "citeStartToken": 77, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003) . The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005) . In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003) , to learning translation lexicon from monolingual and/or comparable corpora Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996) . While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006) , and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007) . However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent. NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006) , but the need for language specific knowledge restricts its applicability across languages. We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008) . In this work, we extend the approach and provide a detailed description of the empirical studies.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent. ", "mid_sen": "NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006) , but the need for language specific knowledge restricts its applicability across languages. ", "after_sen": "We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008) . "}
{"citeStart": 77, "citeEnd": 97, "citeStartToken": 77, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "We take some core ideas from our previous work on mining script information (Regneri et al., 2010) . In this earlier work, we focused on event structures and their possible realizations in natural language. The corpus used in those experiments were short crowd-sourced descriptions of everyday tasks written in bullet point style. We aligned them with a hand-crafted similarity measure that was specifically designed for this text type. In this current work, we target the general task of extracting paraphrases for events rather than the much more specific scriptrelated task. The current approach uses a domainindependent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, they use MSA at the sentence level rather than at the discourse level.", "mid_sen": "We take some core ideas from our previous work on mining script information (Regneri et al., 2010) . ", "after_sen": "In this earlier work, we focused on event structures and their possible realizations in natural language. "}
{"citeStart": 102, "citeEnd": 124, "citeStartToken": 102, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and . We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. These tools currently train in less than 10 minutes on the standard training materials and tag faster than TNT, the fastest existing POS tagger. These tools use a highly optimised GIS implementation and provide sophisticated Gaussian smoothing (Chen and Rosenfeld, 1999) . We expect even faster training times when we move to conjugate gradient methods.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We can also load a lexicon into memory that is shared between all of the components, reducing the memory use.", "mid_sen": "The implementation has been inspired by experience in extracting information from very large corpora (Curran and Moens, 2002) and performing experiments on maximum entropy sequence tagging (Curran and . ", "after_sen": "We have already implemented a POS tagger, chunker, CCG supertagger and named entity recogniser using the infrastructure. "}
{"citeStart": 197, "citeEnd": 220, "citeStartToken": 197, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Experimental Setup Our decoder is based on the phrase-based approach to translation (Och and Ney, 2004) and contains various feature functions including phrase relative frequency, word-level alignment statistics, and lexicalized re-ordering models (Tillmann, 2004; . We tuned the feature weights on a development set using lattice-based minimum error rate training (MERT) (Macherey et al., The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010) . The corpus split is available at http://nlp.stanford.edu/projects/arabic.shtml.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Experimental Setup Our decoder is based on the phrase-based approach to translation (Och and Ney, 2004) and contains various feature functions including phrase relative frequency, word-level alignment statistics, and lexicalized re-ordering models (Tillmann, 2004; . ", "mid_sen": "We tuned the feature weights on a development set using lattice-based minimum error rate training (MERT) (Macherey et al., The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010) . ", "after_sen": "The corpus split is available at http://nlp.stanford.edu/projects/arabic.shtml."}
{"citeStart": 96, "citeEnd": 108, "citeStartToken": 96, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "word sense disambiguation, information retrieval, natural language generation and so on. Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "word sense disambiguation, information retrieval, natural language generation and so on. ", "mid_sen": "Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). ", "after_sen": "However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) )."}
{"citeStart": 157, "citeEnd": 181, "citeStartToken": 157, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "The grammar is a manually developed headed context-free phrase structure grammar for German subordinate clauses with 5508 rules and The formalism is that of Carroll and Rooth (1998) , henceforth C+R:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The grammar is a manually developed headed context-free phrase structure grammar for German subordinate clauses with 5508 rules and The formalism is that of Carroll and Rooth (1998) , henceforth C+R:", "after_sen": "mother -> non-heads head' non-heads (freq)"}
{"citeStart": 72, "citeEnd": 91, "citeStartToken": 72, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiments we use a smaller unit of analysis called a speech document by taking all of the text of a speaker within a single record. The capitalization and punctuation is then removed from the text as in (Monroe et al., 2006) and then the text stemmed using Porter's Snowball II stemmer 2 . Figure 1 shows an example speech document for speaker 15703 (Herb Kohl of Wisconsin) that has been generated from the record in Figure 1 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our experiments we use a smaller unit of analysis called a speech document by taking all of the text of a speaker within a single record. ", "mid_sen": "The capitalization and punctuation is then removed from the text as in (Monroe et al., 2006) and then the text stemmed using Porter's Snowball II stemmer 2 . Figure 1 shows an example speech document for speaker 15703 (Herb Kohl of Wisconsin) that has been generated from the record in Figure 1 .", "after_sen": "In addition to speech documents, we also use speaker documents. "}
{"citeStart": 58, "citeEnd": 76, "citeStartToken": 58, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we apply Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a) to the semantic role labeling task on NomBank. ASO is a recently proposed linear multi-task learning algorithm based on empirical risk minimization. The method requires the use of multiple auxiliary problems, and its effectiveness may vary depending on the specific auxiliary problems used. ASO has been shown to be effective on the following natural language processing tasks: text categorization, named entity recognition, part-of-speech tagging, and word sense disambiguation (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) . This paper makes two significant contributions. First, we present a novel application of ASO to the SRL task on NomBank. We explore the effect of different auxiliary problems, and show that learning predictive structures with ASO results in significantly improved SRL accuracy. Second, we achieve accuracy higher than that reported in (Jiang and Ng, 2006) and advance the state of the art in SRL research.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We explore the effect of different auxiliary problems, and show that learning predictive structures with ASO results in significantly improved SRL accuracy. ", "mid_sen": "Second, we achieve accuracy higher than that reported in (Jiang and Ng, 2006) and advance the state of the art in SRL research.", "after_sen": "The rest of this paper is organized as follows. "}
{"citeStart": 63, "citeEnd": 78, "citeStartToken": 63, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "We now describe a maximum entropy model that assigns to each location in a French sentence a score that is a measure of the safety in cutting the sentence at that location. We begin as in the word translation problem, with a training sample of English-French sentence pairs (E, F) randomly extracted from the Hansard corpus. For each sentence pair we use the basic translation model to compute the Viterbi alignment between E and F. We also use a stochastic part-of-speech tagger as described in Merialdo (1990) to label each word in F with its part of speech. For each position j in F we then construct a (x, y) training event. The value y is rift if a rift belongs at position j and is no-rift otherwise. The context information x is reminiscent of that employed in the word translation application described earlier. It includes a six-word window of French words: three to the left of yj and three to the right of yj. It also includes the part-of-speech tags for these words, and the classes of these words as derived from a mutual-information clustering scheme described in . The complete (x, y) pair is illustrated in Figure 9 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each sentence pair we use the basic translation model to compute the Viterbi alignment between E and F. ", "mid_sen": "We also use a stochastic part-of-speech tagger as described in Merialdo (1990) to label each word in F with its part of speech. ", "after_sen": "For each position j in F we then construct a (x, y) training event. "}
{"citeStart": 72, "citeEnd": 83, "citeStartToken": 72, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. We are going to make such a comparison with the theories proposed by J. Hobbs (1979 Hobbs ( , 1982 that represent a more computationally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch (1983) , who are more interested in addressing psychological and cognitive aspects of discourse coherence. The quoted works seem to be good representatives for each of the directions; they also point to related literature.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At this point it may be proper to comment on the relationship between our theory of coherence and theories advocated by others. ", "mid_sen": "We are going to make such a comparison with the theories proposed by J. Hobbs (1979 Hobbs ( , 1982 that represent a more computationally oriented approach to coherence, and those of T.A. van Dijk and W. Kintch (1983) , who are more interested in addressing psychological and cognitive aspects of discourse coherence. ", "after_sen": "The quoted works seem to be good representatives for each of the directions; they also point to related literature."}
{"citeStart": 54, "citeEnd": 79, "citeStartToken": 54, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Gerdemann and G6tz's Troll system (see [G6Tz 1993] , [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1¥oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Proof, From theorem 24 and proposition 26.", "mid_sen": "Gerdemann and G6tz's Troll system (see [G6Tz 1993] , [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. ", "after_sen": "In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. "}
{"citeStart": 92, "citeEnd": 107, "citeStartToken": 92, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "Non-Linearity A Semitic stem consists of a root and a vowel melody, arranged according to a canonical pattern. For example, Arabic/kuttib/ 'caused to write -perfect passive' is composed from the root morpheme {ktb} 'notion of writing' and the vowel melody morpheme {ul} 'perfect passive'; the two are arranged according to the pattern morpheme {CVCCVC} 'causative'. This phenomenon is analysed by (McCarthy, 1981) along the fines of autosegmental phonology (Goldsmith, 1976) . The analysis appears in (1). 1", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Arabic/kuttib/ 'caused to write -perfect passive' is composed from the root morpheme {ktb} 'notion of writing' and the vowel melody morpheme {ul} 'perfect passive'; the two are arranged according to the pattern morpheme {CVCCVC} 'causative'. ", "mid_sen": "This phenomenon is analysed by (McCarthy, 1981) along the fines of autosegmental phonology (Goldsmith, 1976) . ", "after_sen": "The analysis appears in (1). "}
{"citeStart": 225, "citeEnd": 238, "citeStartToken": 225, "citeEndToken": 238, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 127, "citeEnd": 145, "citeStartToken": 127, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "Thougll a 50\"/,, precision and recall might be reasonable for human assisted tasks, like in lexicography, supervised translation, etc., it is not \"fair enough\" if collocational analysis must serve a fully automated system. In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. ", "mid_sen": "Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "after_sen": "In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a \"large enough\" number of words. "}
{"citeStart": 23, "citeEnd": 38, "citeStartToken": 23, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "We now give a more detailed comparison of the models in this article to the parser of Charniak (1997) . The model described in Charniak (1997) has two types of parameters:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We now give a more detailed comparison of the models in this article to the parser of Charniak (1997) . ", "mid_sen": "The model described in Charniak (1997) has two types of parameters:", "after_sen": ""}
{"citeStart": 118, "citeEnd": 130, "citeStartToken": 118, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . ", "mid_sen": "This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . ", "after_sen": "We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. "}
{"citeStart": 71, "citeEnd": 97, "citeStartToken": 71, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "We test our cohesion-enhanced Moses decoder trained using 688K sentence pairs of Europarl French-English data, provided by the SMT 2006 Shared Task (Koehn and Monz, 2006) . Word alignments are provided by GIZA++ (Och and Ney, 2003) with grow-diag-final combination, with infrastructure for alignment combination and phrase extraction provided by the shared task. We decode with Moses, using a stack size of 100, a beam threshold of 0.03 and a distortion limit of 4. Weights for the log-linear model are set using MERT, as implemented by Venugopal and Vogel (2005) . Our tuning set is the first 500 sentences of the SMT06 development data. We hold out the remaining 1500 development sentences for development testing (dev-test), and the entirety of the provided 2000-sentence test set for blind testing (test). Since we require source dependency trees, all experiments test English to French translation. English dependency trees are provided by Minipar (Lin, 1994) . Our cohesion constraint directly targets sentences for which an unmodified phrasal decoder produces uncohesive output according to the definition in Section 2. Therefore, we present our results not only on each test set in its entirety, but also on the subsets defined by whether or not the baseline naturally produces a cohesive translation. The sizes of the resulting evaluation sets are given in Table 2 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We decode with Moses, using a stack size of 100, a beam threshold of 0.03 and a distortion limit of 4. ", "mid_sen": "Weights for the log-linear model are set using MERT, as implemented by Venugopal and Vogel (2005) . ", "after_sen": "Our tuning set is the first 500 sentences of the SMT06 development data. "}
{"citeStart": 1, "citeEnd": 25, "citeStartToken": 1, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "A text is not just a sequence of words, but it also has coherent structure. The meaning of each word in a text depends on the structure of the text. Recognizing the structure of text is an essential task in text understanding. [Grosz and Sidner, 1986] One of the valuable indicators of the structure of text is lexical cohesion. [Halliday and Hasan, 1976] Lexical cohesion is the relationship between words, classified as follows:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "[Grosz and Sidner, 1986] One of the valuable indicators of the structure of text is lexical cohesion. ", "mid_sen": "[Halliday and Hasan, 1976] Lexical cohesion is the relationship between words, classified as follows:", "after_sen": ""}
{"citeStart": 58, "citeEnd": 72, "citeStartToken": 58, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "In both cases, we report PARSEVAL labeled bracket scores (Magerman, 1995) , with the brackets labeled by syntactic categories but not grammatical functions. Rather than reporting precision and recall of labelled brackets, we report only the F-score, i.e. the harmonic mean of precision and recall. Table 1 shows the effect of rule type choice, and Table 2 lists the effect of the GF re-annotations. From Table 1 , we see that Markov rules achieve the best performance, ahead of both standard rules as well as our formulation of probabilistic LP/ID rules.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second battery of experiments was performed on the model with Markov rules.", "mid_sen": "In both cases, we report PARSEVAL labeled bracket scores (Magerman, 1995) , with the brackets labeled by syntactic categories but not grammatical functions. ", "after_sen": "Rather than reporting precision and recall of labelled brackets, we report only the F-score, i.e. the harmonic mean of precision and recall. "}
{"citeStart": 88, "citeEnd": 101, "citeStartToken": 88, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "2.'he semantic weight of a word or its informativeness can be related to its frequency (l~esnik, 1995) . Itere, we calculate the number of occurrence of each word within the definitions of nouns and verbs in our dictionary. The most frequent word \"a\" occurs 2600 times among a total of 38000 word occurrences. Only 1% of the words occur more than 130 times, 5% occur more than 30 times but over 60% occur less than 5 times.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We deem words in the definition to be important if they have a large semantic weight.", "mid_sen": "2.'he semantic weight of a word or its informativeness can be related to its frequency (l~esnik, 1995) . ", "after_sen": "Itere, we calculate the number of occurrence of each word within the definitions of nouns and verbs in our dictionary. "}
{"citeStart": 72, "citeEnd": 87, "citeStartToken": 72, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Notice that might and ]eared appear within a head model's two head window, but not within the trigram model's two word window. We may therefore expect that a head model would make a more accurate prediction. Srinivas (1997b) presents a two pass head trigram model. In the first pass, it tags words as either head words or non-head words. Training data for this pass is obtained using a head percolation table (Magerman (1995) ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the first pass, it tags words as either head words or non-head words. ", "mid_sen": "Training data for this pass is obtained using a head percolation table (Magerman (1995) ", "after_sen": "(i))~(H(i)lH(i-1)H(i-2)) i=1 (1)"}
{"citeStart": 87, "citeEnd": 110, "citeStartToken": 87, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The filtering algorithm behaves extremely well. Although the worst case runtime is still O(n 4 ), the best case has improved to n 3 ; empirically it seems to significantly reduce the amount of time spent exploring spans. Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005) . ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although the worst case runtime is still O(n 4 ), the best case has improved to n 3 ; empirically it seems to significantly reduce the amount of time spent exploring spans. ", "mid_sen": "Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005) . ", "after_sen": ""}
{"citeStart": 170, "citeEnd": 175, "citeStartToken": 170, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "Why should we, as computational linguists, be interested in factors that contribute to the interactivity of a discourse? There are both theoretical and practical motivations. First, we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant, multi-utterance discourses [Po186, CP86] . Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . Since conversation is a collaborative process [CWG86, SSJ74] , models of conversation can provide the basis for extending planning theories [GS90, CLNO90] . When the situation requires the negotiation of a collaborative plan, these theories must account for the interacting beliefs and intentions of multiple participants. ~,From a practical perspective, there is ample evidence that limited mixed-initiative has contributed to lack of system usability. Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems: They don't allow users to participate in the reasoning process, or to ask the questions they want answered [PHW82, Kid85, FL89] . In addition, question answering systems often fail to take account of the system's role as a conversational partner.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant, multi-utterance discourses [Po186, CP86] . ", "mid_sen": "Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . ", "after_sen": "Since conversation is a collaborative process [CWG86, SSJ74] , models of conversation can provide the basis for extending planning theories [GS90, CLNO90] . "}
{"citeStart": 135, "citeEnd": 160, "citeStartToken": 135, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000) . Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight. In our case the probabilities have been estimated using the maximum likelihood estimate, smoothed adding a small constant (0.1) when probabilities are zero. Decisions taken with negative values were discarded (Agirre & Martinez, 2001b) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At present we have chosen one algorithm which does not combine features (Decision Lists) and another which does combine features (AdaBoost).", "mid_sen": "Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000) . ", "after_sen": "Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight. "}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed a web-based interface to obtain human judgments of semantic relatedness for each automatically generated concept pair. Test subjects were invited via email to participate in the experiment. Thus, they were not supervised during the experiment. Gurevych (2006) observed that some annotators were not familiar with the exact definition of semantic relatedness. Their results differed particularly in cases of antonymy or distributionally related pairs. We created a manual with a detailed introduction to SR stressing the crucial points. The manual was presented to the subjects before the experiment and could be re-accessed at any time. During the experiment, one concept pair at a time was presented to the test subjects in random ordering. Subjects had to assign a discrete relatedness value {0,1,2,3,4} to each pair. Figure 2 shows the system's GUI.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, they were not supervised during the experiment. ", "mid_sen": "Gurevych (2006) observed that some annotators were not familiar with the exact definition of semantic relatedness. ", "after_sen": "Their results differed particularly in cases of antonymy or distributionally related pairs. "}
{"citeStart": 250, "citeEnd": 262, "citeStartToken": 250, "citeEndToken": 262, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations, such as (multi-component) tree-adjoining grammars (Joshi and Schabes, 1997; Weir, 1988) , IO macro grammars (Fisher, 1968) , and (parallel) multiple contextfree grammars (Seki et al., 1991) . For instance, the TAG in Figure 3 is represented by the Datalog program in Figure 4 . Moreover, the method of reduc- Figure 3 : A TAG with one initial tree (left) and one auxiliary tree (right) tion extends to the problem of tactical generation (surface realization) for these grammar formalisms coupled with Montague semantics (under a certain restriction). Our method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars (de Groote, 2001) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "N(i, j) :− unicorn(i, j).", "mid_sen": "In this paper, we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations, such as (multi-component) tree-adjoining grammars (Joshi and Schabes, 1997; Weir, 1988) , IO macro grammars (Fisher, 1968) , and (parallel) multiple contextfree grammars (Seki et al., 1991) . ", "after_sen": "For instance, the TAG in Figure 3 is represented by the Datalog program in Figure 4 . "}
{"citeStart": 123, "citeEnd": 135, "citeStartToken": 123, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "A leading method for utilizing context information for classification and extraction of relationships is that of patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006) . The standard classification process is to find in an auxiliary corpus a set of patterns in which a given training word pair co-appears, and use pattern-word pair co-appearance statistics as features for machine learning algorithms.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It thus makes sense to try to minimize the usage of such resources, and utilize only corpus contexts in which the relevant words appear.", "mid_sen": "A leading method for utilizing context information for classification and extraction of relationships is that of patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006) . ", "after_sen": "The standard classification process is to find in an auxiliary corpus a set of patterns in which a given training word pair co-appears, and use pattern-word pair co-appearance statistics as features for machine learning algorithms."}
{"citeStart": 57, "citeEnd": 69, "citeStartToken": 57, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis. They put an assumption that syntactic and lexical/semantic features are dependent on each other. In their models, syntactic and lexical/semantic features are combined together, and this causes each parameter to depend on both syntactic and lexical/semantic features.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.", "mid_sen": "For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. ", "after_sen": "As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words. "}
{"citeStart": 95, "citeEnd": 116, "citeStartToken": 95, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir (2005) . They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation. Our approach, on the other hand, discriminatively sets millions of individual similarity values. Like Weeds and Weir (2005) , our similarity values are asymmetric.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated.", "mid_sen": "The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir (2005) . ", "after_sen": "They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation. "}
{"citeStart": 0, "citeEnd": 16, "citeStartToken": 0, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to capture CFG-based categorial information, we add a CAT feature to the f-structures automatically generated from the Penn-II and Penn-III Treebanks. Its value is the syntactic category of the lexical item whose lemma gives rise to the PRED value at that particular level of embedding. This makes it possible to classify words and their semantic forms based on their syntactic category and reduces the risk of inaccurate assignment of subcategorization frame frequencies due to POS ambiguity, distinguishing, for example, between the nominal and verbal occurrences of the lemma fight. With this, the output for the verb impose in Figure 4 is impose(v,[subj, obj, obl:on]). For some of our experiments, we conflate the different verbal (and other) tags used in the Penn Treebanks to a single verbal marker (Table 4) . As a further extension, the extraction procedure reads off the syntactic category of the head of each of the subcategorized syntactic functions: impose (v,[subj(n) ,obj(n),obl:on]). 3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. Dalrymple (2001) argues that there are cases, albeit exceptional ones, in which constraints on syntactic category are an issue in subcategorization. In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "3 In this way, our methodology is able to produce surface syntactic as well as abstract functional subcategorization details. ", "mid_sen": "Dalrymple (2001) argues that there are cases, albeit exceptional ones, in which constraints on syntactic category are an issue in subcategorization. ", "after_sen": "In contrast to much of the work reviewed in Section 3, which limits itself to the extraction of surface syntactic subcategorization details, our system can provide this information as well as details of grammatical function."}
{"citeStart": 123, "citeEnd": 140, "citeStartToken": 123, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. Although evaluated on different data sets, this result is consistent with results from previous work (Gatt and Belz, 2008; Gatt et al., 2009) . However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, our result shows that the graphbased approaches perform quite competitively under the condition of perfect knowledge and perception. ", "mid_sen": "Although evaluated on different data sets, this result is consistent with results from previous work (Gatt and Belz, 2008; Gatt et al., 2009) . ", "after_sen": "However, what is more interesting here is that while graph-based approaches perform well when the agent has perfect knowledge of the environment, as its human partner, these approaches literally fall apart with close to 40% performance degradation when applied to the situation where the agent's representation of the shared world is problematic and full of mistakes."}
{"citeStart": 135, "citeEnd": 169, "citeStartToken": 135, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "The second column of Table 2 gives the results of the POS-based model, the third column gives the results of incorporating the detection and correction of speech repairs and detection of intonational phrase boundary tones, and the fourth column gives the results of adding in silence information. As can be seen, modeling the user's utterances improves POS tagging, identification of discourse markers, and word perplexity; with the POS error rate decreasing by 3.1% and perplexity by 5.3%. Furthermore, adding in silence information to help detect the boundary tones and speech repairs results in a further improvement, with the overall POS tagging error rate decreasing by 8.6% and reducing perplexity by 7.8%. In contrast, a word-based trigram backoff model (Katz, 1987) built with the CMU statistical language modeling toolkit (Rosenfeld, 1995) achieved a perplexity of 26.13. Thus our full language model results in 14.1% reduction in perplexity. Table 3 gives the results of detecting intonational boundaries. The second column gives the results of adding the boundary tone detection to the POS model, the third column adds silence information, Table 4 : Detecting and Correcting Speech Repairs and the fourth column adds speech repair detection and correction. We see that adding in silence information gives a noticeable improvement in detecting boundary tones. Furthermore, adding in the speech repair detection and correction further improves the results of identifying boundary tones. Hence to detect intonational phrase boundaries in spontaneous speech, one should also model speech repairs. Table-4 gives the results of detecting and correcting speech repairs. The detection results report the number of repairs that were detected, regardless of whether the type of repair (e.g. modification repair versus abridged repair) was properly determined. The second column gives the results of adding speech repair detection to the POS model. The third column adds in silence information. Unlike the case for boundary tones, adding silence does not have much of an effect. 4 The fourth column adds in speech repair correction, and shows that taking into account the correction, gives better detection rates (Heeman, Loken-Kim, and Allen, 1996) . The fifth column adds in boundary tone detection, which improves both the detection and correction of speech repairs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unlike the case for boundary tones, adding silence does not have much of an effect. ", "mid_sen": "4 The fourth column adds in speech repair correction, and shows that taking into account the correction, gives better detection rates (Heeman, Loken-Kim, and Allen, 1996) . ", "after_sen": "The fifth column adds in boundary tone detection, which improves both the detection and correction of speech repairs."}
{"citeStart": 118, "citeEnd": 133, "citeStartToken": 118, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexical ambiguity resolution is an important research problem for the fields of information retrieval and machine translation (Sanderson, 2000; Chan et al., 2007) . However, making fine-grained sense distinctions for words with multiple closelyrelated meanings is a subjective task (Jorgenson, 1990; Palmer et al., 2005) , which makes it difficult and error-prone. Fine-grained sense distinctions aren't necessary for many tasks, thus a possiblysimpler alternative is lexical disambiguation at the level of homographs (Ide and Wilks, 2006) . Homographs are a special case of semantically ambiguous words: Words that can convey multiple distinct meanings. For example, the word bark can imply two very different concepts -'outer layer of a tree trunk', or, 'the sound made by a dog' and thus is a homograph. Ironically, the definition of the word 'homograph' is itself ambiguous and much debated; however, in this paper we consistently use the above definition.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lexical ambiguity resolution is an important research problem for the fields of information retrieval and machine translation (Sanderson, 2000; Chan et al., 2007) . ", "mid_sen": "However, making fine-grained sense distinctions for words with multiple closelyrelated meanings is a subjective task (Jorgenson, 1990; Palmer et al., 2005) , which makes it difficult and error-prone. ", "after_sen": "Fine-grained sense distinctions aren't necessary for many tasks, thus a possiblysimpler alternative is lexical disambiguation at the level of homographs (Ide and Wilks, 2006) . "}
{"citeStart": 68, "citeEnd": 80, "citeStartToken": 68, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Agglutinative languages could be handled ef-flciently by the current mechanism if specifications were provided for the affix combinations that were likely to occur at all often in real texts. A backup mechanism could then be provided which attempted a slower, but more complete, direct application of the rules for the rarer cases. The interaction of morphological analysis with spelling correction (Carter, 1992; Oflazer, 1994; Bowden, 1995) is another possibly fruitful area of work. Once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affixstripping, which would be amenable to exactly the technique outlined by Carter (1992) . As in that work, a discrimination net of root forms would be required; however, this could be augmented independently of spelling pattern creation, so that the flexibility resulting from not composing the lexicon with the spelling rules would not be lost.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A backup mechanism could then be provided which attempted a slower, but more complete, direct application of the rules for the rarer cases. ", "mid_sen": "The interaction of morphological analysis with spelling correction (Carter, 1992; Oflazer, 1994; Bowden, 1995) is another possibly fruitful area of work. ", "after_sen": "Once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affixstripping, which would be amenable to exactly the technique outlined by Carter (1992) . "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "We cannot compare our syllabification accuracy with Goldwater's and others' previous work because that work used different, supervised training data and phonological representations based on British rather than American pronunciation. Goldwater et al. (2006a) showed that modeling dependencies between adjacent words dramatically improves word segmentation accuracy. It is not possible to write an adaptor grammar that directly implements Goldwater's bigram word segmentation model because an adaptor grammar has one DP per adapted nonterminal (so the number of DPs is fixed in advance) while Goldwater's bigram model has one DP per word type, and the number of word types is not known in advance. However it is pos- Figure 7 : The unigram syllable adaptor grammar, which generates each word as a sequence of up to three Syllables. Word-initial Onsets and word-final Codas are distinguished using the suffixes \"I\" and \"F\" respectively; these are propagated through the grammar to ensure that these appear in the correct positions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We cannot compare our syllabification accuracy with Goldwater's and others' previous work because that work used different, supervised training data and phonological representations based on British rather than American pronunciation. ", "mid_sen": "Goldwater et al. (2006a) showed that modeling dependencies between adjacent words dramatically improves word segmentation accuracy. ", "after_sen": "It is not possible to write an adaptor grammar that directly implements Goldwater's bigram word segmentation model because an adaptor grammar has one DP per adapted nonterminal (so the number of DPs is fixed in advance) while Goldwater's bigram model has one DP per word type, and the number of word types is not known in advance. "}
{"citeStart": 89, "citeEnd": 105, "citeStartToken": 89, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "One method we investigate is a simple bag-ofword model as in monolingual LSA. We treat each sentence pair as a document and do not distinguish source words and target words as if they are terms generated from the same vocabulary. A sparse matrix W characterizing word-document cooccurrence is constructed. Following the notation in section 2.1, the ij-th entry of the matrix W is defined as in (Bellegarda, 2000) ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A sparse matrix W characterizing word-document cooccurrence is constructed. ", "mid_sen": "Following the notation in section 2.1, the ij-th entry of the matrix W is defined as in (Bellegarda, 2000) ", "after_sen": "W ij = (1 − w i ) c ij c j ,"}
{"citeStart": 9, "citeEnd": 33, "citeStartToken": 9, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "11 From (Zollmann and Vogel, 2011) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags. Thus, to be convenient, we only conduct experiments with the SAMT system.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We only use one sample to extract the translation grammar because multiple samples would result in a grammar that would be too large.", "mid_sen": "11 From (Zollmann and Vogel, 2011) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags. ", "after_sen": "Thus, to be convenient, we only conduct experiments with the SAMT system."}
{"citeStart": 81, "citeEnd": 104, "citeStartToken": 81, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent works on knowledge representation are somewhat related to Osgood's semantic differential. Most of them describe meaning of words using special symbols like microfeatures [Waltz and Pollack, 1985; Hendler, 1989 ] that correspond to the semantic dimensions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recent works on knowledge representation are somewhat related to Osgood's semantic differential. ", "mid_sen": "Most of them describe meaning of words using special symbols like microfeatures [Waltz and Pollack, 1985; Hendler, 1989 ] that correspond to the semantic dimensions.", "after_sen": "However, the following problems arise from the semantic differential procedure as measurement of meaning. "}
{"citeStart": 118, "citeEnd": 132, "citeStartToken": 118, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "We regard this task as translation from text to sentiment units, because we noticed that the deep language analysis techniques which are required for the extraction of sentiment units are analogous to those which have been studied for the purpose of language translation. We implemented an accurate sentiment analyzer by making use of an existing transfer-based machine translation engine (Watanabe, 1992) , replacing the translation patterns and bilingual lexicons with sentiment patterns and a sentiment polarity lexicon. Although we used many techniques for deep language analysis, the system was implemented at a surprisingly low development cost because the techniques for machine translation could be reused in the architecture described in this paper.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We regard this task as translation from text to sentiment units, because we noticed that the deep language analysis techniques which are required for the extraction of sentiment units are analogous to those which have been studied for the purpose of language translation. ", "mid_sen": "We implemented an accurate sentiment analyzer by making use of an existing transfer-based machine translation engine (Watanabe, 1992) , replacing the translation patterns and bilingual lexicons with sentiment patterns and a sentiment polarity lexicon. ", "after_sen": "Although we used many techniques for deep language analysis, the system was implemented at a surprisingly low development cost because the techniques for machine translation could be reused in the architecture described in this paper."}
{"citeStart": 116, "citeEnd": 139, "citeStartToken": 116, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004) . Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since then, there has been a large body of work addressing the flaws of the EM-based approach. ", "mid_sen": "Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004) . Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. ", "after_sen": "Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006) ."}
{"citeStart": 88, "citeEnd": 101, "citeStartToken": 88, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "However, building such a model is computationally expensive. Since the space of possible joint labelings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions. To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000) . We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings. These candidate labelings are in turn input to a joint model which can use global features and re-score the candidates. Both the local and global re-ranking models are log-linear (maximum entropy) models.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the space of possible joint labelings is exponential in the number of parse tree nodes, a model cannot exhaustively consider these labelings unless it makes strong independence assumptions. ", "mid_sen": "To overcome this problem, we adopt a discriminative re-ranking approach reminiscent of (Collins, 2000) . ", "after_sen": "We use a local model, which labels arguments independently, to generate a smaller number of likely joint labelings. "}
{"citeStart": 138, "citeEnd": 168, "citeStartToken": 138, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "There are several measures of correctness that can be taken when results are evaluated. The most lenient is whether or not the subject and head markers are placed correctly -the type of measure used in the IBM/Lancaster work (Black, Garside and Leech, 1993 ). Since we are working towards a hierarchical language structure, we may want the words within constituents correctly tagged, ready for the next stage of processing. \"correct-A\" also requires that the words within the subject are correctly tagged. The results in Tables 2 and 3 give an indication of performance levels.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are several measures of correctness that can be taken when results are evaluated. ", "mid_sen": "The most lenient is whether or not the subject and head markers are placed correctly -the type of measure used in the IBM/Lancaster work (Black, Garside and Leech, 1993 ). ", "after_sen": "Since we are working towards a hierarchical language structure, we may want the words within constituents correctly tagged, ready for the next stage of processing. "}
{"citeStart": 81, "citeEnd": 99, "citeStartToken": 81, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999) , which is shown in Equation (2). This simplified version does not take word classes into account as described in (Brown et al., 1993) are the lengths of the target sentence and the source sentence respectively. j is the position index of the source word. ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999) , which is shown in Equation (2). ", "mid_sen": "This simplified version does not take word classes into account as described in (Brown et al., 1993) are the lengths of the target sentence and the source sentence respectively. ", "after_sen": "j is the position index of the source word. "}
{"citeStart": 181, "citeEnd": 198, "citeStartToken": 181, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea behind this is to create a specialized grammar that retains a high coverage but allows very much faster parsing. This has turned out to be possible -speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a] .1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This has turned out to be possible -speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a] .1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. ", "mid_sen": "In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation.", "after_sen": "The resulting specialized grammar was compiled into LR parsing tables, and a special LR parser exploited their special properties, see [Samuelsson 1994b ]."}
{"citeStart": 113, "citeEnd": 134, "citeStartToken": 113, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997) . However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997) . ", "after_sen": "However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b) ."}
{"citeStart": 178, "citeEnd": 192, "citeStartToken": 178, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "Top-down chart parsing differs from the algorithm described above only in the prediction-step, which predicts applicable rules top-down. Contrary to bottomup parsing, however, the adaptation of a top-down algorithm for UG requires some special care. For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in Shieber (1985) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Contrary to bottomup parsing, however, the adaptation of a top-down algorithm for UG requires some special care. ", "mid_sen": "For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in Shieber (1985) .", "after_sen": "Top-down prediction with a restrictor R (where R is a (finite) set of paths through a feature-structure) amounts to the following:"}
{"citeStart": 154, "citeEnd": 176, "citeStartToken": 154, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "However, there is often a trade-off between runtime efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compilation and the size of its output, and the independence of the morphological and lexical components. In compilation, one may compose any or all of (a) the two-level rule set, (b) the set of affixes and their allowed combinations, and (c) the lexicon; see Kaplan and Kay (1994 / for an exposition of the mathematical basis. The type of compilation appropriate for rapid development and acceptable run-time performance depends on, at least, the nature of the language being described and the number of base forms in the lexicon; that is, on the position in the three-dimensional space defined by (a), (b) and (c). For example, English inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological anMysis can be carried out at compile time, producing a list of analysed word forms which need only be looked up at run time, or a network which can be traversed very simply. Alternatively, there may be no need to provide as powerful a mechanism as two-level morphology at all; a simpler device such as affix stripping (A1shawi, 1992, pll9ff) or merely listing all inflected forms explicitly may be preferable.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, there is often a trade-off between runtime efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compilation and the size of its output, and the independence of the morphological and lexical components. ", "mid_sen": "In compilation, one may compose any or all of (a) the two-level rule set, (b) the set of affixes and their allowed combinations, and (c) the lexicon; see Kaplan and Kay (1994 / for an exposition of the mathematical basis. ", "after_sen": "The type of compilation appropriate for rapid development and acceptable run-time performance depends on, at least, the nature of the language being described and the number of base forms in the lexicon; that is, on the position in the three-dimensional space defined by (a), (b) and (c). "}
{"citeStart": 94, "citeEnd": 105, "citeStartToken": 94, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm (Lewin, 1990) , or an inside-out algorithm with a free w~riable constraint (IIobbs and Shieber, 1987) . The propositions fbrlncd can then be judged for plausibility. To imitate jungle path phenomena, these pla.usi o bility judgements need to feed back into the scoping procedure for the next fragment. For example, if' every man is taken to be scoped outside a book after processing the fragment l?vcry man ga~c. a book, [;hen this preference should be preserved when deterlnining the scope for the full sentence l?very uza~t gave a book lo a child. Thus instead of doing ~dl quantitier scoping at the end of the sentence, each new quantilier is scoped relative to the existing quantifiers (and operators such as negation, intensional verbs etc.). A preliminary irnplemenl, ation achieves this by annotating the semantic representations with node nantes, a.nd recording which quantifiers are 'discharged' at. which nodes, and in which order.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "12) gives(< V,X,ITIall(X)>,< 3,y,book(y)>,< -~,z,'l'>)", "mid_sen": "Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm (Lewin, 1990) , or an inside-out algorithm with a free w~riable constraint (IIobbs and Shieber, 1987) . ", "after_sen": "The propositions fbrlncd can then be judged for plausibility. "}
{"citeStart": 230, "citeEnd": 249, "citeStartToken": 230, "citeEndToken": 249, "sectionName": "UNKNOWN SECTION NAME", "string": "To avoid introducing omissions and mistakes into the linguistic information in the initial segmentations of the bilingual data, we perform a statistical character-based alignment: First, every Chinese character in the bitexts is separated by white spaces so that individual characters are recognized as unique /words0 or alignment targets. Then, they are associated with English words using a statistical word aligner. By representing the English and Chinese sentences as e I 1 = e 1 e 2 ...e I and c J 1 = c 1 c 2 ...c J , respectively, where e i and c j represent single elements of the sentences, we define their alignment as a K 1 , of which each element is a span a k =< s, t > and represents the alignment of the English word e s with the Chinese character c t . Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <e I 1 , c J 1 , a K 1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign 1 ( (Neubig et al., 2011) ; (Neubig et al., 2012) ) which uses Bayesian learning and inversion transduction grammars.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By representing the English and Chinese sentences as e I 1 = e 1 e 2 ...e I and c J 1 = c 1 c 2 ...c J , respectively, where e i and c j represent single elements of the sentences, we define their alignment as a K 1 , of which each element is a span a k =< s, t > and represents the alignment of the English word e s with the Chinese character c t . ", "mid_sen": "Then, the corpus of unlabeled bilingual data can be represented as the set of sentence tuples <e I 1 , c J 1 , a K 1 > To obtain the character-based alignment, we employ an open-source toolkit Pialign 1 ( (Neubig et al., 2011) ; (Neubig et al., 2012) ) which uses Bayesian learning and inversion transduction grammars.", "after_sen": ""}
{"citeStart": 165, "citeEnd": 177, "citeStartToken": 165, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "To predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. This may be sufficient for written discourse. For oral discourse, however, we must also consider the way intonation affects the interpretation of a sentence, especially the cases in which it alters the predictions of centering theories. I investigate this via a phenomenon that, by the strictest interpretation of either centering or intonation theories, should not occur --the case of pitch accented pronominals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To predict and track the center of attention in discourse, theories of centering (Grosz et al., 1983; Brennan et al., 1987; Grosz et al., 1989) and immediate focus (Sidner, 1986) rely on syntactic and grammatical features of the text such as pronominalization and surface sentence position. ", "after_sen": "This may be sufficient for written discourse. "}
{"citeStart": 23, "citeEnd": 37, "citeStartToken": 23, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "However, SVM classifiers are too slow. Famous SVM-Light 3.50 (Joachims, 1999) ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, SVM classifiers are too slow. ", "mid_sen": "Famous SVM-Light 3.50 (Joachims, 1999) took 1.2 days to classify 569,994 vectors derived from 2 MB documents.", "after_sen": ""}
{"citeStart": 47, "citeEnd": 70, "citeStartToken": 47, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "Now we are ready to encode the problem of generating grammatical LTAG derivation trees into PDDL. PDDL (McDermott, 2000) is the standard input language for modern planning systems. It is based on the well-known STRIPS language (Fikes and Nilsson, 1971) . In this paradigm, a planning state is defined as a finite set of ground atoms of predicate logic that are true in this state; all other atoms are assumed to be false. Actions have a number of parameters, as well as a precondition and effect, both of which are logical formulas. When a planner tries to apply an action, it will first create an action instance by binding all parameters to constants from the domain. It must then verify that the precondition of the action instance is satisfied in the current state. If so, the action can be applied, in which case the effect is processed in order to change the state. In STRIPS, the precondition and effect both had to be conjunctions of atoms or negated atoms; positive effects are interpreted as making the atom true in the new state, and negative ones as making it false. PDDL permits numerous extensions to the formulas that can be used as preconditions and effects. Each action in our planning problem encodes the effect of adding some elementary tree to the derivation tree. An initial tree with root category A translates to an action with a parameter u for the identity of the node that the current tree is substituted into. The action carries the precondition subst(A, u), and so can only be applied if u is an open substitution node in the current derivation with the correct category A. Auxiliary trees are analogous, but carry the precondition canadjoin(A, u). The effect of an initial tree is to remove the subst condition from the planning state (to record that the substitu- tion node u is now filled); an auxiliary tree has an effect ¬mustadjoin(A, u) to indicate that any obligatory adjunction constraint is satisfied but leaves the canadjoin condition in place to allow multiple adjunctions into the same node. In both cases, effects add subst, canadjoin and mustadjoin atoms representing the substitution nodes and adjunction sites that are introduced by the new elementary tree. One remaining complication is that an action must assign new identities to the nodes it introduces; thus it must have access to a tree index that was not used in the derivation tree so far. We use the number of the current plan step as the index. We add an atom step(1) to the initial state of the planning problem, and we introduce k different copies of the actions for each elementary tree, where k is some upper limit on the plan size. These actions are identical, except that the i-th copy has an extra precondition step(i) and effects ¬step(i) and step(i + 1). It is no restriction to assume an upper limit on the plan size, as most modern planners search for plans smaller than a given maximum length anyway. Fig. 2 shows some of the actions into which the grammar in Fig. 1 translates. We display only one copy of each action and have left out most of the canadjoin effects. In addition, we use an initial state containing the atoms subst(S, 1.self) and step(1) and a final state consisting of the following goal:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "PDDL (McDermott, 2000) is the standard input language for modern planning systems. ", "mid_sen": "It is based on the well-known STRIPS language (Fikes and Nilsson, 1971) . ", "after_sen": "In this paradigm, a planning state is defined as a finite set of ground atoms of predicate logic that are true in this state; all other atoms are assumed to be false. "}
{"citeStart": 74, "citeEnd": 91, "citeStartToken": 74, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003) . In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003) . ", "after_sen": "In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques."}
{"citeStart": 140, "citeEnd": 153, "citeStartToken": 140, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the translation dictionary trained from the training data to further improve the alignment results. When we train the bi-directional statistical word alignment models with the training data, we get two word alignment results for the training data. By taking the intersection of the two word alignment results, we build a new alignment set. The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000) . Based on the extended alignment links, we build a translation dictionary. In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Based on the extended alignment links, we build a translation dictionary. ", "mid_sen": "In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.", "after_sen": "Based on the alignment results on the out-of-domain corpus, we build a translation dictionary filtered with a threshold . "}
{"citeStart": 104, "citeEnd": 124, "citeStartToken": 104, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996) , KPML (Bateman, 1996) , MUMBLE (Meteer et al., 1987) , and RealPro (Lavoie and Rambow, 1997) , which produce natural language text from an abstract semantic representation. These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The approach of writing individual templates is convenient, but may not scale to complex domains in which hundreds or thousands of templates would be necessary, and may have shortcomings in maintainability and text quality (e.g., see (Reiter, 1995) for a discussion).", "mid_sen": "There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996) , KPML (Bateman, 1996) , MUMBLE (Meteer et al., 1987) , and RealPro (Lavoie and Rambow, 1997) , which produce natural language text from an abstract semantic representation. ", "after_sen": "These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text."}
{"citeStart": 0, "citeEnd": 29, "citeStartToken": 0, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "The CKY algorithm is a bottom-up recognition algorithm for CI=G. For a given grammar G and input string al ... a,~ the algorithm constructs an array P, having n 2 elements, where element P [i, j] stores all and only those nonterminals of G that derive the substring ai...aj. A naive adaptation of this algorithm for I_lG recognition would involve storing a set of nonterminals and their associated stacks. But since stack length is at least proportional to the length of the input string, the resultant algorithm would exhibit exponential space and time complexity in the worst case. Vijay-Shanker and Weir (1993) showed that the behaviour of the naive algorithm can be improved upon. In I_lG derivations the application of a rule cannot depend on more than a bounded portion of the top of the stack. Thus, rather than storing the whole of the. potentially unbounded stack in a particular array entry, it suffices to store just . It is not difficult to show that only that portion of the derivation below the terminator node is dependent on more than the top of the stack ha. It follows that for any stack a'a, if there is a derivation of the substring %...he from B[c~'c~] (see tree (b)), then there is a corresponding derivation of ai...aj from A[al~rcr '] (see tree (c)). This captures the sense in which I_IG derivations exhibit \"context-freeness\". Efficient storage of stacks can therefore be achieved by storing in Pit, j] just that bounded amount of information (nonterminal plus top of stack) relevant to rule application, together with a pointer to any entry in Pip, q] representing a subderivation from an object B [c~'a] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But since stack length is at least proportional to the length of the input string, the resultant algorithm would exhibit exponential space and time complexity in the worst case. ", "mid_sen": "Vijay-Shanker and Weir (1993) showed that the behaviour of the naive algorithm can be improved upon. ", "after_sen": "In I_lG derivations the application of a rule cannot depend on more than a bounded portion of the top of the stack. "}
{"citeStart": 106, "citeEnd": 129, "citeStartToken": 106, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. ", "mid_sen": "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. ", "after_sen": "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. "}
{"citeStart": 332, "citeEnd": 355, "citeStartToken": 332, "citeEndToken": 355, "sectionName": "UNKNOWN SECTION NAME", "string": "Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm (Pang and Lee, 2005) ; by taking a semisupervised graph-based learning approach (Goldberg and Zhu, 2006) ; and by using \"optimal stacks\" of SVMs (Koppel and Schler, 2006) . However, each of these methods have shortcomings (Section 2). Additionally, during the learning process, all approaches employ a set of word/punctuation features collected across all rating categories. Hence, the number of features may be very large compared to the number of training samples, which can lead to the model overfitting the data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Consequently, these methods generally do not perform well, while methods which incorporate sample similarity information achieve improved performance (Pang and Lee, 2005) .", "mid_sen": "Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm (Pang and Lee, 2005) ; by taking a semisupervised graph-based learning approach (Goldberg and Zhu, 2006) ; and by using \"optimal stacks\" of SVMs (Koppel and Schler, 2006) . ", "after_sen": "However, each of these methods have shortcomings (Section 2). "}
{"citeStart": 138, "citeEnd": 161, "citeStartToken": 138, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "c5.0 3 , a commercial version of c4.5 Quinlan, 1993, performs top-down induction of decision trees tdidt. On the basis of an instance base of examples, c5.0 constructs a decision tree which compresses the classi cation information in the instance base by exploiting differences in relative importance of di erent features. Instances are stored in the tree as paths A demo of the NP and VP chunker is available at http: www.sfb441.unituebingen.de ~dejean chunker.h tml 3 Available from http: www.rulequest.com of connected nodes ending in leaves which contain classi cation information. Nodes are connected via arcs denoting feature values. Feature information gain mutual information between features and class is used to determine the order in which features are employed as tests at all levels of the tree Quinlan, 1993 . With the full input representation words and POS tags, we were not able to run complete experiments. We therefore experimented only with the POS tags with a context of two left and right. We h a ve used the default parameter setting with decision trees combined with value grouping. We have used a nearest neighbor algorithm ib1-ig, here listed as MBL and a decision tree algorithm IGTree from the TiMBL learning package Daelemans et al., 1999b . Both algorithms store the training data and classify new items by choosing the most frequent classi cation among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. Each feature receives a weight which is based on the amount of information which it provides for computing the classi cation of the items in the training data. ib1-ig uses these weights for computing the distance between a pair of data items and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree Daelemans et al., 1999b . We will use their default parameters except for the ib1-ig parameter for the numberof examined nearest neighbors k which we have set to 3 Daelemans et al., 1999a . The classi ers use a left and right context of four words and partof-speech tags. For the four IO representations we have used a second processing stage which used a smaller context but which included information about the IO tags predicted by the rst processing phase Tjong Kim Sang, 2000. When building a classi er, one must gather evidence for predicting the correct class of an item from its context. The Maximum Entropy MaxEnt framework is especially suited for integrating evidence from various information sources. Frequencies of evidence class combinations called features are extracted from a sample corpus and considered to beproperties of the classi cation process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active i.e. a feature is active when the context meets the requirements given by the feature. For every class the weights of the active features are combined and the best scoring class is chosen Berger et al., 1996 . For the classier built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features consisting of one of the mentioned information sources and complex features combinations thereof were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software Dehaspe, 1997 . MBSL Argamon et al., 1999 uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP including complete phrases. These sequences may include a few context tags, up to a prespeci ed max context. During inference, MBSL tries to 'tile' each POS string with parts of noun-phrases from the memory. If the string could befully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm. Adding a context extends the possibilities for tiling, thereby giving more opportunities to better candidates. The approach of MBSL to the problem of identifying baseNPs is sequence-based rather than word-based, that is, decisions are taken per POS sequence, or per candidate, but not for a single word. In addition, the tiling process gives no preference to any direction in the sentence. The tiles may b e of any length, up to the maximal length of a phrase in the training data, which gives MBSL a generalization power that compensates for the setup of using only POS tags. The results presented here were obtained by optimizing MBSL parameters based on 5-fold CV on the training data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We h a ve used the default parameter setting with decision trees combined with value grouping. ", "mid_sen": "We have used a nearest neighbor algorithm ib1-ig, here listed as MBL and a decision tree algorithm IGTree from the TiMBL learning package Daelemans et al., 1999b . ", "after_sen": "Both algorithms store the training data and classify new items by choosing the most frequent classi cation among training items which are closest to this new item. "}
{"citeStart": 14, "citeEnd": 34, "citeStartToken": 14, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Juan le dio pufialadas a Marla (ii) Juan le dio una pufialada a Marla Both of these sentences translate literally to \"John gave stab wound(s) to Mary.\" However, the first sentence is the repetitive version of the action (i.e., there were multiple stab wounds), whereas the second sentence is the non-repetitive version of the action (i. e., there was only one stab wound). This distinction is characterized by means of the atomicity feature. In (26)(i), the event is associated with the features [+d,+t,-a], whereas, in (26)(ii) the event is associated with the features [+d,+t,+a] . According to [Bennett et al., 1990] (in the spirit of [Moens and Steedman, 1988] ), predicates are allowed to undergo an atomicity \"coercion\" in which an inherently non-atomic predicate (such as dio) may become atomic under certain conditions. These conditions are language-specific in nature, i.e., they depend on the lexical-semantic structure of the predicate in question. Given the featural scheme that is imposed on top of the lexical-semantic framework, it is easy to specify coercion functions for each language. For example, the atomicity function for the stab example would specify that a singular NP verbal object maps a [+d,-a] predicate into a [+d,+a] predicate i.e., a non-atomic event becomes atomic if it is associated with a singular NP object. Thus, the notion of feature-based coercion is cross-linguistically applicable, providing a useful foundation for a model of interlingual machine translation.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (26)(i), the event is associated with the features [+d,+t,-a], whereas, in (26)(ii) the event is associated with the features [+d,+t,+a] . ", "mid_sen": "According to [Bennett et al., 1990] (in the spirit of [Moens and Steedman, 1988] ), predicates are allowed to undergo an atomicity \"coercion\" in which an inherently non-atomic predicate (such as dio) may become atomic under certain conditions. ", "after_sen": "These conditions are language-specific in nature, i.e., they depend on the lexical-semantic structure of the predicate in question. "}
{"citeStart": 167, "citeEnd": 177, "citeStartToken": 167, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "• Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996) , for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra struc-tural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967) . One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979) , which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992) . Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Charniak (1996) , for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. ", "mid_sen": "On the other hand, if the examples consist of raw sentences with no extra struc-tural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967) . ", "after_sen": "One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979) , which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. "}
{"citeStart": 125, "citeEnd": 135, "citeStartToken": 125, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper presents a general learning approach for identifying syntactic patterns, based on the SNoW learning architecture (Roth, 1998; Roth, 1999) . The SNoW learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. SNoW is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large -of which NLP is a principal example. Preliminary versions of it have already been used successfully on several tasks in natural language processing (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998) . In particular, SNoW's sparse architecture supports well chaining and combining predictors to produce a coherent inference. This property of the architecture is the base for the learning approach studied here in the context of shallow parsing.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information -has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998) .", "mid_sen": "This paper presents a general learning approach for identifying syntactic patterns, based on the SNoW learning architecture (Roth, 1998; Roth, 1999) . ", "after_sen": "The SNoW learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. "}
{"citeStart": 215, "citeEnd": 234, "citeStartToken": 215, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "The last five years have seen a surge of interest in the problem of textual inference, that is, automatically determining whether a natural-language hypothesis can be inferred from a given premise. A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle. Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005) , pattern-based relation extraction (Romano et al., 2006) , or approximate matching of predicate-argument structure (Hickl et al., 2006) . Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years. H: No rabies cases have been confirmed. Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle. ", "mid_sen": "Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005) , pattern-based relation extraction (Romano et al., 2006) , or approximate matching of predicate-argument structure (Hickl et al., 2006) . ", "after_sen": "Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: "}
{"citeStart": 164, "citeEnd": 183, "citeStartToken": 164, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (Di-Marco, Hirst, and Stede, 1993; Hirst, 1995) . Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words (Church et al., 1994) , are inefficient in time and space. This paper presents a new statistical approach to modeling context that provides a preliminary solution to an important sub-problem, that of determining the nearsynonym that is most typical, or expected, if any, in a given context. Although weaker than full lexical choice, because it doesn't choose the 'best' word, we believe that it is a necessary first step, because it would allow one to determine the effects of choosing a non-typical word in place of the typical word. The approach relies on a generalization of lexical co-occurrence that allows for an implicit representation of the differences between two (or more) words with respect to any actual context. For example, our implemented lexical choice program selects mistake as most typical for the 'gap' in sentence (1), and error in (2).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (Di-Marco, Hirst, and Stede, 1993; Hirst, 1995) . ", "mid_sen": "Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words (Church et al., 1994) , are inefficient in time and space. ", "after_sen": "This paper presents a new statistical approach to modeling context that provides a preliminary solution to an important sub-problem, that of determining the nearsynonym that is most typical, or expected, if any, in a given context. "}
{"citeStart": 192, "citeEnd": 212, "citeStartToken": 192, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the A1vey NL Tools (ANLT; Briscoe et al., 1987a) . Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987) . In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et al., 1992; .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the A1vey NL Tools (ANLT; Briscoe et al., 1987a) . ", "after_sen": "Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987) . "}
{"citeStart": 97, "citeEnd": 116, "citeStartToken": 97, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "We experiment in the biological domain with the eight-category AZ scheme (Table 1) adapted from (Mizuta et al., 2006) and described in (Contractor et al., 2012) . The results show that our constrained model substantially outperforms a baseline unconstrained Maximum Entropy Model. While this type of constrained models have previously improved the feature-based model performance mostly in the weakly supervised and domain adaptation scenarios (e.g. (Mann and McCallum, 2007; Mann and Mc-Callum, 2008; Ganchev et al., 2010 )), we demonstrate substantial gains both when the Maximum En- ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This algorithm enables us to break the multi-class prediction into a pipeline of consecutive, simpler predictions which can be better assisted by the encoded knowledge.", "mid_sen": "We experiment in the biological domain with the eight-category AZ scheme (Table 1) adapted from (Mizuta et al., 2006) and described in (Contractor et al., 2012) . ", "after_sen": "The results show that our constrained model substantially outperforms a baseline unconstrained Maximum Entropy Model. "}
{"citeStart": 65, "citeEnd": 86, "citeStartToken": 65, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991) . All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a) , and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991) . ", "mid_sen": "All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a) , and the first two are distributed as part of the ANLT package. ", "after_sen": "The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node)."}
{"citeStart": 30, "citeEnd": 45, "citeStartToken": 30, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "The delicacy of testsuite construction is acknowledged in EAGLES, 1996, p.37 . Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. ", "mid_sen": "The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "after_sen": "The use of corpora with various levels of annotation has been studied, but the recommendations are that much manual work is required to turn corpus examples into test cases e.g., Balkan and Fouvry, 1995 . "}
{"citeStart": 209, "citeEnd": 218, "citeStartToken": 209, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "Our thesaurus brings up only alternatives that have the same part-of-speech with the target word. The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus (Roget, 1852), dictionaries of synonyms (Hayakawa, 1994) , or clusters acquired from corpora (Lin, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our thesaurus brings up only alternatives that have the same part-of-speech with the target word. ", "mid_sen": "The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus (Roget, 1852), dictionaries of synonyms (Hayakawa, 1994) , or clusters acquired from corpora (Lin, 1998) .", "after_sen": "In this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context. "}
{"citeStart": 158, "citeEnd": 173, "citeStartToken": 158, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary compari- son (Dagan et al., 1999) . That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools. 587,833 (80%) of the pairs served as a training set from which to calculate base probabilities. From the other 20%, we prepared test sets as follows: after discarding pairs occurring in the training data (after all, the point of similarity-based estimation is to deal with unseen pairs), we split the remaining pairs into five partitions, and replaced each nounverb pair (n, vl) with a noun-verb-verb triple (n, vl, v2) such that P(v2) ~ P(vl). The task for the language model under evaluation was to reconstruct which of (n, vl) and (n, v2) was the original cooccurrence. Note that by construction, (n, Vl) was always the correct answer, and furthermore, methods relying solely on unigram frequencies would perform no better than chance. Test-set performance was measured by the error rate, defined as T(# of incorrect choices + (# of ties)/2), where T is the number of test triple tokens in the set, and a tie results when both alternatives are deemed equally likely by the language model in question.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary compari- son (Dagan et al., 1999) . ", "mid_sen": "That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools. 587,833 (80%) of the pairs served as a training set from which to calculate base probabilities. ", "after_sen": "From the other 20%, we prepared test sets as follows: after discarding pairs occurring in the training data (after all, the point of similarity-based estimation is to deal with unseen pairs), we split the remaining pairs into five partitions, and replaced each nounverb pair (n, vl) with a noun-verb-verb triple (n, vl, v2) such that P(v2) ~ P(vl). "}
{"citeStart": 243, "citeEnd": 257, "citeStartToken": 243, "citeEndToken": 257, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995) . For automatic approaches, the following properties of the C-tests are beneficial: The given prefix restricts the solution space to a single solution (in almost all cases) which enables automatic scoring without providing a guessing option. In addition, the prefix hint allows for a narrower deletion pattern (every second gap) providing more empirical evidence for the students' abilities on less text.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to overcome this and other weaknesses of the cloze test, Klein-Braley and Raatz (1984) propose the Ctest as a more stable alternative. ", "mid_sen": "Thorough analyses following the principles of test theory indicate advantages of the C-test over the cloze test regarding empirical validity, reliability, and correlation with other language tests (Babaii and Ansary, 2001; Klein-Braley, 1997; Jafarpur, 1995) . ", "after_sen": "For automatic approaches, the following properties of the C-tests are beneficial: "}
{"citeStart": 52, "citeEnd": 71, "citeStartToken": 52, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "The hint selection mechanism generates hints automatically. For a low specificity hint it selects as-yet unmentioned object and hints at it, for example, \"Here's a hint: Your answer should mention a battery.\" For high-specificity, it attempts to hint at a two-place relation, for example, \"Here's a hint: the battery is connected to something.\" The tutorial policy makes a high-level decision as to which strategy to use (for example, \"acknowledge the correct part and give a high specificity hint\") based on the answer analysis and dialogue context. At present, the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers. 1 In addition to a remediation policy, the tutorial planner implements an error recovery policy ). Since the system accepts unrestricted input, interpretation errors are unavoidable. Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. I don't know the word power.\" The help message is accompanied with a hint at the appropriate level, also depending on the number of previous incorrect and non-interpretable answers.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the system accepts unrestricted input, interpretation errors are unavoidable. ", "mid_sen": "Our recovery policy is modeled on the TargetedHelp (Hockey et al., 2003) policy used in task-oriented dialogue. ", "after_sen": "If the system cannot find an interpretation for an utterance, it attempts to produce a message that describes the problem but without giving away the answer, for example, \"I'm sorry, I'm having a problem understanding. "}
{"citeStart": 140, "citeEnd": 149, "citeStartToken": 140, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . ", "mid_sen": "Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "after_sen": ") ( ) ( ) , ( log ) , ( 2 f P w P f w P f w MI ="}
{"citeStart": 94, "citeEnd": 105, "citeStartToken": 94, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Much research has been done on knowledge acquisition from large-scale annotated corpora as a rich source of linguistic knowledge. Major works done to create English POS taggers (henceforth, \"taggers\"), for example, include (Church 1988) , (Kupiec 1992) , (Brill 1992) and (Voutilainen et al. 1992) . The problem with this framework, however, is that such reliable corpora are hardly available due to a huge amount of the labor-intensive work required. In case of the acquisition of non-core knowledge, such as specific, lexically or domain dependent knowledge, preparation of annotated corpora becomes more serious problem. One viable approach then is to utilize plain text corpora instead, as in (Mikheev 1996) . But The method proposed by (Mikheev 1996) has its own weaknesses, in that it is restricted in scope. That is, it aims to acquire rules for unknown words in corpora from their ending characters without looking at the context. In the meantime, (Brill 1995a ) (Brill 1995b) proposed a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning. The weakness of his method is that the effect of unsupervised learning decreases as the training corpus size increases. The problem in using plain text corpora for knowledge acquisition is that we need a human supervisor who can evaluate and sift the obtained knowledge. An alternative to this would be to use a number of modules of a welldeveloped NLP system which stores most of the highly reliable general rules. Here, one module functions as a supervisor for other modules, since all these modules are designed to work cooperatively and the knowledges stored in each module are correlated. Keeping this idea in mind, we propose a new unsupervised learning method for obtaining linguistic rules from plain text corpora using the existing linguistic knowledge. This method has been implemented in the rule extraction system APRAS (Automatic POS Rule Acquisition System), which automatically acquires rules for refining the morphological analyzer (tagger) in our English-Japanese MT system ASTRANSAC (Hirakawa et al. 1991) through the interaction between the system's tagger and parser on the assumption that they are considerably accurate. This paper is organized as follows: Section 2 illustrates the basic idea of our method; Section 3 gives the outline of APRAS; Sections 4 and 5 describe our experiments.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much research has been done on knowledge acquisition from large-scale annotated corpora as a rich source of linguistic knowledge. ", "mid_sen": "Major works done to create English POS taggers (henceforth, \"taggers\"), for example, include (Church 1988) , (Kupiec 1992) , (Brill 1992) and (Voutilainen et al. 1992) . ", "after_sen": "The problem with this framework, however, is that such reliable corpora are hardly available due to a huge amount of the labor-intensive work required. "}
{"citeStart": 260, "citeEnd": 287, "citeStartToken": 260, "citeEndToken": 287, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 269, "citeEnd": 287, "citeStartToken": 269, "citeEndToken": 287, "sectionName": "UNKNOWN SECTION NAME", "string": "The mentioned studies use word-clusters for interpolated n-gram language models. Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger (Black et al., 1992 , Ushioda, 1996 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The mentioned studies use word-clusters for interpolated n-gram language models. ", "mid_sen": "Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger (Black et al., 1992 , Ushioda, 1996 .", "after_sen": "In this case a decision tree contains binary questions to decide the properties of a word."}
{"citeStart": 340, "citeEnd": 351, "citeStartToken": 340, "citeEndToken": 351, "sectionName": "UNKNOWN SECTION NAME", "string": "When we consider full sentence processing, as opposed to incremental processing, the use of lexicalised grammars has a major advantage over the use of more standard rule based grammars. In processing a sentence using a lexicalised formalism we do not have to look at the grammar as a whole, but only at the grammatical information indexed by each of the words. Thus increases in the size of a grammar don't necessarily effect efficiency of processing, provided the increase in size is due to the addition of new words, rather than increased lexical ambiguity. Once the full set of possible lexical entries for a sentence is collected, they can, if required, then be converted back into a set of phrase structure rules (which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar), before being parsing with a standard algorithm such as Earley's (Earley 1970) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus increases in the size of a grammar don't necessarily effect efficiency of processing, provided the increase in size is due to the addition of new words, rather than increased lexical ambiguity. ", "mid_sen": "Once the full set of possible lexical entries for a sentence is collected, they can, if required, then be converted back into a set of phrase structure rules (which should correspond to a small subset of the rule based formalism equivalent to the whole lexicalised grammar), before being parsing with a standard algorithm such as Earley's (Earley 1970) .", "after_sen": "In incremental parsing we cannot predict which words will appear in the sentence, so cannot use the same technique. "}
{"citeStart": 3, "citeEnd": 16, "citeStartToken": 3, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. As Church (1988) rightly pointed out, however, \"Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not. Estimates from the Brown Corpus can be misleading. For example, the capitalized word 'Acts' is found twice in the Brown Corpus, both times as a proper noun (in a title). It would be misleading to infer from this evidence that the word 'Acts' is always a proper noun.\"", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "is usually handled by POS taggers, which treat capitalized words in the same way as other categories, that is, by accounting for the immediate syntactic context and using estimates collected from a training corpus. ", "mid_sen": "As Church (1988) rightly pointed out, however, \"Proper nouns and capitalized words are particularly problematic: some capitalized words are proper nouns and some are not. ", "after_sen": "Estimates from the Brown Corpus can be misleading. "}
{"citeStart": 44, "citeEnd": 56, "citeStartToken": 44, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "Following Gruber (1965), Jackendoff (1976), Boons (1985) , we approach motion verbs in terms of some \"localist semantical\" role labels. The linguistic study of French intransitive motion verbs (see eg. (Asher & Sablayrolles, 1994a) ) we have realized has allowed the definition of an ontology for \"location\" in three basic concepts:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following Gruber (1965), Jackendoff (1976), Boons (1985) , we approach motion verbs in terms of some \"localist semantical\" role labels. ", "after_sen": "The linguistic study of French intransitive motion verbs (see eg. "}
{"citeStart": 174, "citeEnd": 197, "citeStartToken": 174, "citeEndToken": 197, "sectionName": "UNKNOWN SECTION NAME", "string": "One aspect of the representation that is particularly useful in the translation application is its convenience for partial and/or incremental representation of content we can refine the representation by the addition of furthor edges. A fully specified denotation of the meaning of a s,mtence is rarely required for translation, and as w,~ pointed out when discussing logic representations, a c~mq~lete specification may not have been intended by th,, slwaker. Although we have not provided a denotatio.al semantics for sets of relation edges, we anticipate that this will be possible along the lines developed in m(motonic semantics (Alshawi and Crouch 1992) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A fully specified denotation of the meaning of a s,mtence is rarely required for translation, and as w,~ pointed out when discussing logic representations, a c~mq~lete specification may not have been intended by th,, slwaker. ", "mid_sen": "Although we have not provided a denotatio.al semantics for sets of relation edges, we anticipate that this will be possible along the lines developed in m(motonic semantics (Alshawi and Crouch 1992) .", "after_sen": "Translation Parameters '1'o bc practical, a model for P(CtIC,) needs to decompose the source and target graphs C~ and Ct into subgraphs small enough that subgraph translation parameters can be estimated. "}
{"citeStart": 223, "citeEnd": 245, "citeStartToken": 223, "citeEndToken": 245, "sectionName": "UNKNOWN SECTION NAME", "string": "We remain several points below the bestperforming team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and Bohnet, 2009) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our efforts during this iteration of the NEG task were primarily focused on enhancing our methods of choosing the best RE once the reference type was selected.", "mid_sen": "We remain several points below the bestperforming team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and Bohnet, 2009) .", "after_sen": ""}
{"citeStart": 82, "citeEnd": 113, "citeStartToken": 82, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "We conducted two experiments to evaluate MIMIC's automatic adaptation capabilities. We compared MIMIC with two control systems: MIMIC-SI, a system-initiative version of MIMIC in which the system retains both initiatives throughout the dialogue, and MIMIC-MI, a nonadaptive mixed-initiative version of MIMIC that resembles the behavior of many existing dialogue systems. In this section we summarize these experiments and their results. A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000) . Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information. User satisfaction was assessed by asking the subjects to fill out a questionnaire after interacting with each version of the system. Furthermore, a number of performance features, largely based on the PARADISE dialogue evaluation scheme (Walker et al., 1997) , were automatically logged, derived, or manually annotated. In addition, we logged the cues automatically detected in each user utterance, as well as the initiative distribution for each turn and the dialogue acts selected to generate each system response.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section we summarize these experiments and their results. ", "mid_sen": "A companion paper describes the evaluation process and results in further detail (Chu-Carroll and Nickerson, 2000) . ", "after_sen": "Each experiment involved eight users interacting with MIMIC and MIMIC-SI or MIMIC-MI to perform a set of tasks, each requiring the user to obtain specific movie information. "}
{"citeStart": 244, "citeEnd": 265, "citeStartToken": 244, "citeEndToken": 265, "sectionName": "UNKNOWN SECTION NAME", "string": "General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the A1vey NL Tools (ANLT; Briscoe et al., 1987a) . Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987) . In addition to syntactic processing, the systems incorporate lexical, morphological, and semantic processing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. Alshawi et al., 1992; .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "General-purpose natural language (NL) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include SRI's CLARE system (Alshawi et al., 1992) and the A1vey NL Tools (ANLT; Briscoe et al., 1987a) . ", "after_sen": "Using a declarative formalism helps ease the task of developing and maintaining the grammar (Kaplan, 1987) . "}
{"citeStart": 21, "citeEnd": 31, "citeStartToken": 21, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Performance would probably be improved by better models of morphology and/or phonology. An ngram model of morpheme sequences (e.g. like Goldwater uses) might avoid some of the mistakes mentioned in Section 8. Feature-based or gestural phonology (Browman and Goldstein, 1992) might help model segmental variation. Finite-state models (Belz, 2000) might be more compact. Prosody, stress, and other sub-phonemic cues might disambiguate some problem situations (Hockema, 2006; Rytting, 2007; Salverda et al., 2003) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Feature-based or gestural phonology (Browman and Goldstein, 1992) might help model segmental variation. ", "mid_sen": "Finite-state models (Belz, 2000) might be more compact. ", "after_sen": "Prosody, stress, and other sub-phonemic cues might disambiguate some problem situations (Hockema, 2006; Rytting, 2007; Salverda et al., 2003) ."}
{"citeStart": 25, "citeEnd": 49, "citeStartToken": 25, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "The results suggest that neither version of Roget's is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget's 1987. Even on the largest set (Finkelstein et al., 2001) , however, the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4 . The difference between the 1911 Thesaurus and Vector would be statistically signifi- (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri, even at p < 0.1 for a two-tailed test. These data sets are too small for a meaningful comparison of systems with close correlation scores.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget's 1987. ", "mid_sen": "Even on the largest set (Finkelstein et al., 2001) , however, the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4 . ", "after_sen": "The difference between the 1911 Thesaurus and Vector would be statistically signifi- (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri, even at p < 0.1 for a two-tailed test. "}
{"citeStart": 81, "citeEnd": 102, "citeStartToken": 81, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic 190 properties employed in our models are similar to the properties of Johnson et al. (1999) which incorporate general linguistic principles into a log-linear model. They refer to boththe c(onstituent)structure and the f(eature)-structure of the LFG parses. Examples are properties for c-structure nodes, corresponding to standard production properties, c-structure subtrees, indicating argument versus adjunct attachment, f-structure attributes, corresponding to grammatical functions used in LFG, atomic attribute-value pairs in fstructures, complexity of the phrase being attached to, thus indicating both high and low a ttachment, non-right-branching behavior of nonterminal nodes, non-parallelism of coordinations.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The basic 190 properties employed in our models are similar to the properties of Johnson et al. (1999) which incorporate general linguistic principles into a log-linear model. ", "after_sen": "They refer to boththe c(onstituent)structure and the f(eature)-structure of the LFG parses. "}
{"citeStart": 113, "citeEnd": 135, "citeStartToken": 113, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012) , which deal with automatic generation of classic fill in the blank questions. Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Examples such asÖzbal et al. (2013) and Valitutti et al. (2013) use template filling techniques guided by quantified notions of humor or how catchy a phrase is.", "mid_sen": "Our motivation for generation of material for language education exists in work such as Sumita et al. (2005) and Mostow and Jang (2012) , which deal with automatic generation of classic fill in the blank questions. ", "after_sen": "Our work is naturally complementary to these efforts, as their methods require a corpus of in-vocab text to serve as seed sentences."}
{"citeStart": 82, "citeEnd": 99, "citeStartToken": 82, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "Nodes in the input layer of the network represent simple relations on the input sentence and are being used as the input features. Target nodes represent words that are of interest; in the case studied here, each of the word candidates for prediction is represented as a target node. An input sentence, along with a designated word of interest in it, is mapped into a set of features which are active in it; this representation is presented to the input layer of SNoW and propagates to the target nodes. Target nodes are linked via weighted edges to (some of) the input features. Let At = {Q,... , i,~} be the set of features that are active in an example and are linked to the target node t. Then the linear unit corresponding to t is active iff t E w i > Ot, iEAt where w~ is the weight on the edge connecting the ith feature to the target node t, and Ot is the threshold A given example is treated autonomously by each target subnetwork; an example labeled t may be treated as a positive example by the subnetwork for t and as a negative example by the rest of the target nodes. The learning policy is on-line and mistake-driven; several update rules can be used within SNOW. The most successful update rule is a variant of Littlestone's Winnow update rule (Littlestone, 1988 ), a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992) . This mechanism is implemented via the sparse architecture of SNOW. That is, (1) input features are allocated in a data driven way -an input node for the feature i is allocated only if the feature i was active in any input sentence and (2) a link (i.e., a non-zero weight) exists between a target node t and a feature i if and only if i was active in an example labeled t.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The learning policy is on-line and mistake-driven; several update rules can be used within SNOW. ", "mid_sen": "The most successful update rule is a variant of Littlestone's Winnow update rule (Littlestone, 1988 ), a multiplicative update rule that is tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992) . ", "after_sen": "This mechanism is implemented via the sparse architecture of SNOW. "}
{"citeStart": 118, "citeEnd": 131, "citeStartToken": 118, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and Johnson, 2005) , to address the problem. Reranking enables us to incorporate truly global features to the model of named entity tagging, and we aim to realize the state-of-the-art performance without depending on rule-based post-processes.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.", "mid_sen": "In this paper, we use reranking architecture, which was successfully applied to the task of natural language parsing (Collins, 2000; Charniak and Johnson, 2005) , to address the problem. ", "after_sen": "Reranking enables us to incorporate truly global features to the model of named entity tagging, and we aim to realize the state-of-the-art performance without depending on rule-based post-processes."}
{"citeStart": 73, "citeEnd": 93, "citeStartToken": 73, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "Neural Networks: Neural Networks are a special kind of \"non-symbolic\" eager learning algo-rithm. The neural network links the vector elements to the document categories The learning phase defines thresholds for the activation of neurons. In the categorization phase, a new document vector leads to the activation of a single category. For details we refer to (Wiener et al., 1995) . In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et al., 1996) . LVQ has been used in its default configuration only. No adaptation to the application domain has been made.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For details we refer to (Wiener et al., 1995) . ", "mid_sen": "In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et al., 1996) . LVQ has been used in its default configuration only. ", "after_sen": "No adaptation to the application domain has been made."}
{"citeStart": 27, "citeEnd": 40, "citeStartToken": 27, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection. It minimizes the perplexity of the induced class-based n-gram language model compared to the original word-based model. Using the clusters, we then adjust the TF weighting by integrating with the cluster term frequency (CTF):", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus we want to assign higher weights to the words in this cluster.", "mid_sen": "We used the SRILM toolkit (Stolcke, 2002) for automatic word clustering over the entire document collection. ", "after_sen": "It minimizes the perplexity of the induced class-based n-gram language model compared to the original word-based model. "}
{"citeStart": 71, "citeEnd": 94, "citeStartToken": 71, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "The calculation of the intersection of a CFG and a FSA is very simple (Bar-Hillel et al., 1961) . The (context-free) grammar defining this intersection is simply constructed by keeping track of the state names in the non-terminal category symbols. For each rule 9 [o -'-' Xl...X. there are", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The calculation of the intersection of a CFG and a FSA is very simple (Bar-Hillel et al., 1961) . ", "after_sen": "The (context-free) grammar defining this intersection is simply constructed by keeping track of the state names in the non-terminal category symbols. "}
{"citeStart": 180, "citeEnd": 200, "citeStartToken": 180, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. Gerdemann's generator follows a head-driven strategy in order to avoid inefficient evaluation orders. More specifically, the head of the right-hand side of each grammar rule is distinguished, and distinguished categories are scanned or predicted upon first. The resulting evaluation strategy is similar to that of the head-corner approach (Shieber et al., 1990 ; Gerdemann and IIinrichs, in press): prediction follows the main flow of semantic information until a lexical pivot is reached, and only then are the headdependent subparts of the construction built up in a bottom-up fashion. This mixture of top-down and bottom-up information flow is crucial since the topdown semantic information from the goal category must be integrated with the bottom-up subcategorization information from the lexicon. A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing. Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. ", "mid_sen": "By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) .", "after_sen": ""}
{"citeStart": 50, "citeEnd": 71, "citeStartToken": 50, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which i s u s e d t o d ene a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG b y Johnson et al. (1999) , pseudo-likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999) . We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our models on an exact-match task (i.e. percentage of exact match of most probable parse with correct parse) on 550 manually examined examples with on average 5.4 analyses gave 86% precision. Another evaluation on a verb frame recognition task (i.e. percentage of agreement between subcategorization frames of main verb of most probable parse and correct parse) gave 90% precision on 375 manually disambiguated examples with an average ambiguity of 25. Clearly, a direct comparison of these results to stateof-the-art statistical parsers cannot bemade because of di erent training and test data and other evaluation measures. However, we w ould like to draw the following conclusions from our experiments:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which i s u s e d t o d ene a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. ", "mid_sen": "In previous work on log-linear models for LFG b y Johnson et al. (1999) , pseudo-likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. ", "after_sen": "However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. "}
{"citeStart": 92, "citeEnd": 104, "citeStartToken": 92, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Our project's long-range goal (see http://www.isp. pitt.edu/'intgen/) is to create a unified architecture for collaborative discourse, accommodating both interpretation and generation. Our computational approach (Thomason and Hobbs, 1997) uses a form of weighted abduction as the reasoning mechanism (Hobbs et al., 1993) and modal operators to model context. In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. From our first annotation trials, we found that the recognition of \"classical\" speech acts (Austin, 1962; Searle, 1975) by coders is fairly reliable, while recognizing contextual relationships (e.g., whether an utterance accepts a proposal) is not as reliable. Thus, we explore other features that can help us recognize how participants coordinate agreement. Our corpus study also provides a preliminary assessment of the Discourse Resource Initiative (DR/) tagging scheme. The DRI is an international \"grassroots\" effort that seeks to share corpora that have been tagged with the core features of interest to the discourse community. In order to use the core scheme, it is anticipated that each group will need to refine it for their particular purposes. A usable draft core scheme is now available for experimentation (see http://www.georgetown.edu/luperfoy/Discourse-Treebank/dri-home.html). Whereas several groups are working with the unadapted core DR/ scheme (Core and Allen, 1997; Poesio and Traum, 1997) , we have attempted to adapt it to our corpus and particular research questions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we describe the corpus study portion of our project, which is an integral part of our investigation into recognizing how conversational participants coordinate agreement. ", "mid_sen": "From our first annotation trials, we found that the recognition of \"classical\" speech acts (Austin, 1962; Searle, 1975) by coders is fairly reliable, while recognizing contextual relationships (e.g., whether an utterance accepts a proposal) is not as reliable. ", "after_sen": "Thus, we explore other features that can help us recognize how participants coordinate agreement. "}
{"citeStart": 0, "citeEnd": 16, "citeStartToken": 0, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous approaches mainly focused on English relations. Most of them were evaluated on the ACE 2004 data set (or a sub set of it) which defined 7 relation types and 23 subtypes. Although Chinese processing is of the same importance as English and other Western language processing, unfortunately few work has been published on Chinese relation extraction. Che et al (2005) defined an improved edit distance kernel over the original Chinese string representation around particular entities. The only relation they studied is PERSON-AFFLIATION. The insufficient study in Chinese relation extraction drives us to investigate how to find an approach that is particularly appropriate for Chinese.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although Chinese processing is of the same importance as English and other Western language processing, unfortunately few work has been published on Chinese relation extraction. ", "mid_sen": "Che et al (2005) defined an improved edit distance kernel over the original Chinese string representation around particular entities. ", "after_sen": "The only relation they studied is PERSON-AFFLIATION. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "repeat arcs will of course facilitate copying, as we shall see in a moment. Bird & Ellison (1992) came close to discovering a useful device for reduplication when they noted that automaton intersection has at least indexedgrammar power (ibid., p.48). They demonstrated their claim by showing that odd-length strings of indefinite length like the one described by the regular expression (a bcde f g)+ can be repeated by intersecting them with an automaton accepting only strings of even length: the result is (abede f gabede f g) +.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "repeat arcs will of course facilitate copying, as we shall see in a moment. ", "mid_sen": "Bird & Ellison (1992) came close to discovering a useful device for reduplication when they noted that automaton intersection has at least indexedgrammar power (ibid., p.48). ", "after_sen": "They demonstrated their claim by showing that odd-length strings of indefinite length like the one described by the regular expression (a bcde f g)+ can be repeated by intersecting them with an automaton accepting only strings of even length: the result is (abede f gabede f g) +."}
{"citeStart": 232, "citeEnd": 246, "citeStartToken": 232, "citeEndToken": 246, "sectionName": "UNKNOWN SECTION NAME", "string": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. ", "mid_sen": "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . ", "after_sen": "But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. "}
{"citeStart": 68, "citeEnd": 91, "citeStartToken": 68, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Our classifier uses the maximum entropy implementation described in Curran and Clark (2003) . Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. This will be an important property when we use sparse features like bigrams in the models below.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is achieved by selecting the model with the maximum entropy, i.e. the most uniform distribution, given the constraints.", "mid_sen": "Our classifier uses the maximum entropy implementation described in Curran and Clark (2003) . Generalised Iterative Scaling (GIS) is used to estimate the values of the weights and we use a Gaussian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting. ", "after_sen": "This will be an important property when we use sparse features like bigrams in the models below."}
{"citeStart": 304, "citeEnd": 316, "citeStartToken": 304, "citeEndToken": 316, "sectionName": "UNKNOWN SECTION NAME", "string": "The delicacy of testsuite construction is acknowledged in EAGLES, 1996, p.37 . Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. ", "mid_sen": "Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. ", "after_sen": "The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category."}
{"citeStart": 51, "citeEnd": 63, "citeStartToken": 51, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally, to make out the noun phrases in a text means to parse the text and to resolve the attachment relations among the constituents. However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information. Do we really need to fully parse the texts in every application? Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. These tell us that it is not necessary to completely parse the texts for some applications.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Do we really need to fully parse the texts in every application? ", "mid_sen": "Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. ", "after_sen": "These tell us that it is not necessary to completely parse the texts for some applications."}
{"citeStart": 155, "citeEnd": 186, "citeStartToken": 155, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "To assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll & Briscoe, 1993) , a wide-coverage grammar of English. The grammar is defined in metagrammatical formalism which is compiled into a unification-based 'object gran~mar'--a syntactic variant of the Definite Clause Grammar formalism (Pereira & Warren, 1980 )--containing 84 features and 782 phrase structure rules. Parsing uses fixed-arity term unification. The grammar provides full coverage of the following constructions: declarative sentences, imperatives and questions (yes/no, tag and wh-questions); all unbounded dependency types (topicalisation, relativisation, wh-questions); a relatively exhaustive treatment of verb and adjective complement types; phrasal and prepositional verbs of many complement types; passivisation; verb phrase extraposition; sentence and verb phrase modification; noun phrase complements and pre-and post-modification; partitives; coordination of all major category types; and nominal and adjectival comparatives.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll & Briscoe, 1993) , a wide-coverage grammar of English. ", "after_sen": "The grammar is defined in metagrammatical formalism which is compiled into a unification-based 'object gran~mar'--a syntactic variant of the Definite Clause Grammar formalism (Pereira & Warren, 1980 )--containing 84 features and 782 phrase structure rules. "}
{"citeStart": 88, "citeEnd": 128, "citeStartToken": 88, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Word vectors reflecting word meanings are expected to enable numerical approaches to semantics. Some early attempts at vector representation in I)sycholinguistics were the semantic d (O'erential approach (Osgood et al. 1957) and the associative distribution apl)roach (Deese 1962) . llowever, they were derived manually through psychological experiments. An early attempt at automation was made I)y Wilks el aL (t990) us-.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word vectors reflecting word meanings are expected to enable numerical approaches to semantics. ", "mid_sen": "Some early attempts at vector representation in I)sycholinguistics were the semantic d (O'erential approach (Osgood et al. 1957) and the associative distribution apl)roach (Deese 1962) . llowever, they were derived manually through psychological experiments. ", "after_sen": "An early attempt at automation was made I)y Wilks el aL (t990) us-."}
{"citeStart": 51, "citeEnd": 53, "citeStartToken": 51, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems a perfectly valid rule of conversation not to tell people what they already know. Indeed, Grice's QUANTITY lllaxim has often been interpreted this way: Do not make your contribution more informative than is required [f] . Stalnaker, as well, suggests that to assert something that is already presupposed is to attempt to do something that is already done [14] . Thus, the notion of what is informative is judged against a background of what is presupposed, i.e. propositions that all conversants assume are mutually known or believed. These propositions are known as the COMMON GROUND [10, 5] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, the notion of what is informative is judged against a background of what is presupposed, i.e. propositions that all conversants assume are mutually known or believed. ", "mid_sen": "These propositions are known as the COMMON GROUND [10, 5] .", "after_sen": "The various formulations of this 'no redundancy' rule permeate many computational analyses of natural language and notions of eooperativity. "}
{"citeStart": 0, "citeEnd": 12, "citeStartToken": 0, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010) , the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB . Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007) , trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. ", "mid_sen": "Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007) , trained on the PADT. ", "after_sen": "His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser."}
{"citeStart": 74, "citeEnd": 92, "citeStartToken": 74, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "The Fidditch parser requires a lexicon including informatkm about base word fornls atld syntactic constraints (e.g,. tile complement structure of verbs). Non-trivial preliminary work is tllus necessary in tuning the lexicon for the different domains and sublanguages. A second problem with the Fidditch parser is poor performances: tilt_' recall and precision at detecting word collocations are declared to be as low as 50%, I-iowever it is unclear if this value applies only to SVO triples, and how it has been derived. The recall is low because tile Fidditch parser, as other partial parsers (Sekine et al, 1992; Resnik and Hearst, i993) , only detect links between adjacent or near-adjacent words.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A second problem with the Fidditch parser is poor performances: tilt_' recall and precision at detecting word collocations are declared to be as low as 50%, I-iowever it is unclear if this value applies only to SVO triples, and how it has been derived. ", "mid_sen": "The recall is low because tile Fidditch parser, as other partial parsers (Sekine et al, 1992; Resnik and Hearst, i993) , only detect links between adjacent or near-adjacent words.", "after_sen": "Thougll a 50\"/,, precision and recall might be reasonable for human assisted tasks, like in lexicography, supervised translation, etc., it is not \"fair enough\" if collocational analysis must serve a fully automated system. "}
{"citeStart": 198, "citeEnd": 219, "citeStartToken": 198, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "For our compact representation to be useful, we need to be able to optimize our objective without expanding all possible simple sentences. A relatively straight-forward extension of the inside-outside algorithm for chart-parses allows us to learn and perform inference in our compact representation (a similar algorithm is presented in (Geman & Johnson, 2002) ). We omit details for lack of space.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For our compact representation to be useful, we need to be able to optimize our objective without expanding all possible simple sentences. ", "mid_sen": "A relatively straight-forward extension of the inside-outside algorithm for chart-parses allows us to learn and perform inference in our compact representation (a similar algorithm is presented in (Geman & Johnson, 2002) ). ", "after_sen": "We omit details for lack of space."}
{"citeStart": 258, "citeEnd": 270, "citeStartToken": 258, "citeEndToken": 270, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 134, "citeEnd": 155, "citeStartToken": 134, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997) . Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998) . In contrast, the techniques examined in this article are corpus-based and data-driven. The process of composing a planned response for a new request is informed by probabilistic and lexical properties of the requests and responses in the corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The automation of help-desk responses has been previously tackled using mainly knowledge-intensive paradigms, such as expert systems (Barr and Tessler 1995) and case-based reasoning (Watson 1997) . ", "after_sen": "Such technologies require significant human input, and are difficult to create and maintain (Delic and Lahaix 1998) . "}
{"citeStart": 38, "citeEnd": 64, "citeStartToken": 38, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "An implementation of the extant SDRT (Asher and Lascarides, 2003) glue logic for building discourse structures is insufficient to deal with open domain text, and we cannot envision an extended version at the present time able to deal with the problem. Thus, we have opted for a machine learning based approach to discourse parsing based on superficial features, like BNL. To build an implementation to test these ideas, we have had to devise a corpus of texts annotated for discourse structure in SDRT.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here we detail some experiences we have had with the discourse annotation process.", "mid_sen": "An implementation of the extant SDRT (Asher and Lascarides, 2003) glue logic for building discourse structures is insufficient to deal with open domain text, and we cannot envision an extended version at the present time able to deal with the problem. ", "after_sen": "Thus, we have opted for a machine learning based approach to discourse parsing based on superficial features, like BNL. "}
{"citeStart": 149, "citeEnd": 163, "citeStartToken": 149, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996) . Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is by now commonplace knowledge that accurate syntactic parsing is not possible given only a context-free grammar with standard Penn Treebank (Marcus et al., 1993) labels (e.g., S, N P , etc.) (Charniak, 1996) . ", "mid_sen": "Instead researchers condition parsing decisions on many other features, such as parent phrase-marker, and, famously, the lexical-head of the phrase (Magerman, 1995; Collins, 1996; Collins, 1997; Johnson, 1998; Charniak, 2000; Henderson, 2003; Klein and Manning, 2003; Matsuzaki et al., 2005) (and others) .", "after_sen": "One particularly perspicuous way to view the use of extra conditioning information is that of tree-transformation (Johnson, 1998; Klein and Manning, 2003) . "}
{"citeStart": 99, "citeEnd": 119, "citeStartToken": 99, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "OPINE is built on top of KnowItAll, a Web-based, domain-independent information extraction system (Etzioni et al., 2005) . Given a set of relations of interest, KnowItAll instantiates relation-specific generic extraction patterns into extraction rules which find candidate facts. KnowItAll's Assessor then assigns a probability to each candidate. The Assessor uses a form of Point-wise Mutual Information (PMI) between phrases that is estimated from Web search engine hit counts (Turney, 2001) . It computes the PMI between each fact and automatically generated discriminator phrases (e.g., \"is a scanner\" for the isA() relationship in the context of the Scanner class). Given fact f and discriminator d, the computed PMI score is:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "OPINE is built on top of KnowItAll, a Web-based, domain-independent information extraction system (Etzioni et al., 2005) . ", "after_sen": "Given a set of relations of interest, KnowItAll instantiates relation-specific generic extraction patterns into extraction rules which find candidate facts. "}
{"citeStart": 102, "citeEnd": 123, "citeStartToken": 102, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993) ; it measures the degree to which word m can be substituted into the contexts in which n appears. If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999) , as is the case for relative frequencies, then we may write the confusion probability as follows:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "L2(q,r) = Ll(q,r) = cos(q, r) = Jac(q, r) = ~v (q(v) -r(v)) 2 Iq(v) -r(v)l V ~-~v q(v)r(v) X/~-~v q(v) 2 V/Y~-v r(v) 2 I{v : q(v) > 0 and r(v) > 0}l I{v I q(v) > 0 or r(v) > O}l Figure 1: Well-known functions", "mid_sen": "The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993) ; it measures the degree to which word m can be substituted into the contexts in which n appears. ", "after_sen": "If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999) , as is the case for relative frequencies, then we may write the confusion probability as follows:"}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus. Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In previous research, word meanings have been statistically modeled based on syntactic information derived from a corpus. ", "mid_sen": "Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. ", "after_sen": "These methods are useful for the organization of words deep within a hierarchy, but do not seem to provide a solution for the top levels of the hierarchy."}
{"citeStart": 225, "citeEnd": 241, "citeStartToken": 225, "citeEndToken": 241, "sectionName": "UNKNOWN SECTION NAME", "string": "The Data Mental adjectives which denote an emotional state or a competence (agent-oriented, following Ernst, 1983) present interesting syntactic and semantic polymorphic behaviour, as noted in the literature (see for example Leh~er, 1990 and Croft, 1984) . In this paper, we focus on the representative members of these classes in I and Ih", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Data Mental adjectives which denote an emotional state or a competence (agent-oriented, following Ernst, 1983) present interesting syntactic and semantic polymorphic behaviour, as noted in the literature (see for example Leh~er, 1990 and Croft, 1984) . ", "after_sen": "In this paper, we focus on the representative members of these classes in I and Ih"}
{"citeStart": 92, "citeEnd": 118, "citeStartToken": 92, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "It is beyond the scope of the present study to undertake a full comparison between Web counts and frequencies re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998) . Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998) . ", "mid_sen": "Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. ", "after_sen": "Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "However, the following problems arise from the semantic differential procedure as measurement of meaning. The procedure is not based on the denotative meaning of a word, but only on the connotative emotions attached to the word; it is difficult to choose the relevant dimensions, i.e. the dimensions required for the sufficient semantic space. Morris and Hirst [1991] used Roget's thesaurus as knowledge base for determining whether or not two words are semantically related. For example, the semantic relation of truck/car and drive/car are captured in the following way: This method can capture Mmost all types of semantic relations (except emotional and situational relation), such as paraphrasing by superordinate (ex. cat/pet), systematic relation (ex. north/east), and non-systematic relation (ex. theatre/fi]~).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The procedure is not based on the denotative meaning of a word, but only on the connotative emotions attached to the word; it is difficult to choose the relevant dimensions, i.e. the dimensions required for the sufficient semantic space. ", "mid_sen": "Morris and Hirst [1991] used Roget's thesaurus as knowledge base for determining whether or not two words are semantically related. ", "after_sen": "For example, the semantic relation of truck/car and drive/car are captured in the following way: "}
{"citeStart": 58, "citeEnd": 77, "citeStartToken": 58, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Ih.r,., we haw~ separated the context into a contingent s,,I ,ff contextual propositions S and a set R of (monol i ngual) 'meaning postulates', or selectional restrictions, that constrain the word sense predicates in all contexts. .1 is a set of assumptions sufficient to support the in-I,'rl)n'lation ¢ given S and R. In other words, this is h,~crl)rctal, ion as abduction ' (Itobbs et al. 1988) , since ~!)(i,('lion, not deduction, is needed to arrive at the :~>.'d H II I~tiOIIS ,4. 'l'h(\" ,host common types of meaning postulates in R art, t h,,s~\" for restriction, hyponymy, and disjointness, , \\l,l'<:.~sed a.'~ follows:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": ".1 is a set of assumptions sufficient to support the in-I,'rl)n'lation ¢ given S and R. ", "mid_sen": "In other words, this is h,~crl)rctal, ion as abduction ' (Itobbs et al. 1988) , since ~!)(i,('lion, not deduction, is needed to arrive at the :~>.'d H II I~tiOIIS ,4. 'l'h(\" ,host common types of meaning postulates in R art, t h,,s~\" for restriction, hyponymy, and disjointness, , \\l,l'<:.~sed a.'~ follows:", "after_sen": "HI (.l'l, X2) ~ p2(x! ) restriction; t,:¢(x) --* p3(x) hyponymy; -~(pa(x) A p4(x)) disjointness."}
{"citeStart": 103, "citeEnd": 113, "citeStartToken": 103, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper gives an overview of how natural language is converted to a representation that the neural nets can handle, and how the problem is reduced to a manageable size. It then outlines the neural net selection process. A comprehensive account is given in (Lyon, 1994) ; descriptions of the neural net process are also in (Lyon, 1993; Lyon and Frank, 1992) . This is a hybrid system. The core process is data driven, as the parameters of the neural networks are derived from training text. The neural net is trained in supervised mode on examples that have been manually marked \"correct\" and \"incorrect\". It will then be able to classify unseen examples. However, the initial processing stages, in which the problem size is constrained, operate within a skeletal grammatic framework. Computational tractability is further addressed by reducing data through the application of prohibitive rules as local constraints. The pruning process is remarkably effective.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It then outlines the neural net selection process. ", "mid_sen": "A comprehensive account is given in (Lyon, 1994) ; descriptions of the neural net process are also in (Lyon, 1993; Lyon and Frank, 1992) . ", "after_sen": "This is a hybrid system. "}
{"citeStart": 95, "citeEnd": 100, "citeStartToken": 95, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "The main factor seems to be that even though Expt is not syntactically a question, the little red piece is the focus of a question, and as such is in focus despite the fact that the syntactic construction there is supposedly focuses a hole in the green plunger ... [Sid79] . These examples suggest that a questioned entity is left focused until the point in the dialogue at which the question is resolved. The fact that well has been noted as a marker of response to questions supports this analysis [Sch87] . Thus the relevant factor here may be the switching of control among discourse participants [WS88]. These mixed-initiati.ve features make these sequences inherently different than text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These examples suggest that a questioned entity is left focused until the point in the dialogue at which the question is resolved. ", "mid_sen": "The fact that well has been noted as a marker of response to questions supports this analysis [Sch87] . ", "after_sen": "Thus the relevant factor here may be the switching of control among discourse participants [WS88]. "}
{"citeStart": 57, "citeEnd": 80, "citeStartToken": 57, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Our method is grounded in Appraisal Theory, developed by Martin and White (2005) , which analyzes the way opinion is expressed. Following Martin and White, we define: Attitude type is type of appraisal being expressed-one of affect, appreciation, or judgment ( Figure 1 ). Affect refers to an emotional state (e.g., 'happy', 'angry'), and is the most explicitly subjective type of appraisal. The other two types express evaluation of external entities, differentiating between intrinsic appreciation of object properties (e.g., 'slender', 'ugly') and social judgment (e.g., 'heroic', 'idiotic'). Orientation is whether the attitude is positive ('good') or negative ('bad'). Force describes the intensity of the appraisal. Force is largely expressed via modifiers such as 'very' (increased force), or 'slightly' (decreased force), but may also be expressed lexically, for example 'greatest' vs. 'great' vs. 'good'. Polarity of an appraisal is marked if it is scoped in a polarity marker (such as 'not'), or unmarked otherwise. Other attributes of appraisal are affected by negation; e.g., 'not good' also has the opposite orientation from 'good'. Target type is a domain-dependent semantic type for the target. This attribute takes on values from a domain-dependent taxonomy, representing important (and easily extractable) distinctions between targets in the domain.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our method is grounded in Appraisal Theory, developed by Martin and White (2005) , which analyzes the way opinion is expressed. ", "after_sen": "Following Martin and White, we define: Attitude type is type of appraisal being expressed-one of affect, appreciation, or judgment ( Figure 1 ). "}
{"citeStart": 0, "citeEnd": 16, "citeStartToken": 0, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "Clustering approaches have so far been applied only to is-a extraction. These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members' lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo (1999) proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These methods use clustering algorithms to group words according to their meanings in text, label the clusters using its members' lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. ", "mid_sen": "Caraballo (1999) proposed the first attempt, which used conjunction and apposition features to build noun clusters. ", "after_sen": "Recently, extended this approach by making use of all syntactic dependency features for each noun. "}
{"citeStart": 76, "citeEnd": 94, "citeStartToken": 76, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "Our qualitative and quantitative models have a similar overall structure and there are clear parallels between the factoring of logical constraints and statistical parameters, for example monolingual postulates and dependency parameters, bilingual postulates and translation parameters. The parallelism would have been closer if we had adopted ID/LP style rules (Gazdar et al. 1985) in the qualitative model. However, we argued in section 3 that our qualitative model suffered from lack of robustness, from having only the crudest means for choosing between competing hypotheses, and from being computationally intractable for large vocabularies.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our qualitative and quantitative models have a similar overall structure and there are clear parallels between the factoring of logical constraints and statistical parameters, for example monolingual postulates and dependency parameters, bilingual postulates and translation parameters. ", "mid_sen": "The parallelism would have been closer if we had adopted ID/LP style rules (Gazdar et al. 1985) in the qualitative model. ", "after_sen": "However, we argued in section 3 that our qualitative model suffered from lack of robustness, from having only the crudest means for choosing between competing hypotheses, and from being computationally intractable for large vocabularies."}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, many people have looked at cascaded and/or shallow parsing and GR assignment. Abney (1991) is one of the first who proposed to split up parsing into several cascades. He suggests to first find the chunks and then the dependecies between these chunks. Grefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions. Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree. (Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. However, their subject and object finders are independent of their chunker (i.e. not cascaded).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results. ", "mid_sen": "Argamon et al. (1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification. ", "after_sen": "However, their subject and object finders are independent of their chunker (i.e. not cascaded)."}
{"citeStart": 247, "citeEnd": 271, "citeStartToken": 247, "citeEndToken": 271, "sectionName": "UNKNOWN SECTION NAME", "string": "We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991) , 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs 3 (Finkelstein et al., 2001) . We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. We measure the correlation with Pearson's Correlation Coefficient. A preliminary experiment set out to determine whether there is any advantage to indexing the words in a phrase separately, for example, whether the phrase \"change of direction\" should be indexed only as a whole, or as all of \"change\", \"of\", \"direction\" and \"change of direction\". The outcome of this experiment appears in Table 4 . There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget's. In the remaining experiments, we have each word in a phrase indexed.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. ", "mid_sen": "The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991) , 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs 3 (Finkelstein et al., 2001) . ", "after_sen": "We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. "}
{"citeStart": 73, "citeEnd": 84, "citeStartToken": 73, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. ", "mid_sen": "Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004) , two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006) . ", "after_sen": "All systems were trained on the same data and the outputs used the same tokenization. "}
{"citeStart": 141, "citeEnd": 165, "citeStartToken": 141, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "As alluded to in Section 2.2, we use a reorderingbased loss function to improve word order in a machine translation system. In particular, we use a system of source-side reordering rules which, given a parse of the source sentence, will reorder the sentence into a target-side order (Collins et al., 2005) . In our experiments we work with a set of English-Japanese reordering rules 1 and gold reorderings based on human generated correct reordering of an aligned target sentences. We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use a reordering score based on the reordering penalty from the METEOR scoring metric. ", "mid_sen": "Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure.", "after_sen": "reorder-score = 1 − # chunks − 1 # unigrams matched − 1 reorder-cost = 1 − reorder-score All reordering augmented-loss experiments are run with the same treebank data as the baseline (the training portions of PTB, Brown, and QTB). "}
{"citeStart": 85, "citeEnd": 95, "citeStartToken": 85, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. ", "mid_sen": "In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . ", "after_sen": "In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. "}
{"citeStart": 65, "citeEnd": 83, "citeStartToken": 65, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we use the hierarchical learning strategy since it simplifies the problem by letting us focus on relation detection only. The relation classification stage remains unchanged and we will show that it benefits from improved detection. For experiments on both relation detection and relation classification, we use SVM 4 (Vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3. However, for the analysis in section 3.2 and the purification preprocess steps in section 4.2, we use a MaxEnt 5 model since it outputs probabilities for its predictions. For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, for the analysis in section 3.2 and the purification preprocess steps in section 4.2, we use a MaxEnt 5 model since it outputs probabilities for its predictions. ", "mid_sen": "For the choice of features, we use the full set of features from Zhou et al. (2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011) .", "after_sen": ""}
{"citeStart": 119, "citeEnd": 130, "citeStartToken": 119, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "The paradigm of two-level morphology (Koskenniemi, 1983) has become popular for handling word formation phenomena in a variety of languages. The original formulation has been extended to allow morphotactic constraints to be expressed by feature specification (Trost, 1990; A1shawi et al, 1991) rather than Koskenniemi's less perspicuous device of continuation classes. Methods for the automatic compilation of rules from a notation convenient for the rule-writer into finitestate automata have also been developed, allowing the efficient analysis and synthesis of word forms. The automata may be derived from the rules alone (Trost, 1990) , or involve composition with the lexicon (Karttunen, Kaplan and Zaenen, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The paradigm of two-level morphology (Koskenniemi, 1983) has become popular for handling word formation phenomena in a variety of languages. ", "mid_sen": "The original formulation has been extended to allow morphotactic constraints to be expressed by feature specification (Trost, 1990; A1shawi et al, 1991) rather than Koskenniemi's less perspicuous device of continuation classes. ", "after_sen": "Methods for the automatic compilation of rules from a notation convenient for the rule-writer into finitestate automata have also been developed, allowing the efficient analysis and synthesis of word forms. "}
{"citeStart": 48, "citeEnd": 60, "citeStartToken": 48, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "The experimental setup follows the paradigm of (Teufel, 2001 ). However, while (Teufel, 2001) developed a Q-A task to evaluate summaries showing the contribution of a scientific article in relation to previous work, the purpose of the Q-A task at hand Initially we had four experimental conditions but one was dropped, so is not presented in this context is to show the usefulness of the extracted summaries in answering questions on the paper, and how they compare to a discourse-agnostic baseline. In the case of (Teufel, 2001 ) the task consists of a fixed set of five questions, the same for all articles tuned particularly to the relation of current and previous work. By contrast, the current Q-A task aims to show how well the summaries represent the content of the entire paper, which means that questions are individual to each paper and required domain knowledge to create.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To ensure that each evaluator considered only one type of summary per paper, so as to avoid bias from previous stimuli, and to make sure all experts were exposed to all papers and all types of summary, the 12 experts were assigned to four groups (G1-G4) and were allocated 28 summaries each according to the Latin Square design in Table 1 . 1 .", "mid_sen": "The experimental setup follows the paradigm of (Teufel, 2001 ). ", "after_sen": "However, while (Teufel, 2001) developed a Q-A task to evaluate summaries showing the contribution of a scientific article in relation to previous work, the purpose of the Q-A task at hand Initially we had four experimental conditions but one was dropped, so is not presented in this context is to show the usefulness of the extracted summaries in answering questions on the paper, and how they compare to a discourse-agnostic baseline. "}
{"citeStart": 104, "citeEnd": 135, "citeStartToken": 104, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) . Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. For example, we may find textual 4 evidence of a high likelihood of agreement be- 4 Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information. For example, although most votes in our corpus were almost completely along party lines (and despite the fact that sameparty information is easily incorporated via the methods we propose), we did not use party-affiliation data. Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants' political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (\"I second that!\") or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf. Agrawal et al. (2003) ). Agreement evidence can be a powerful aid in our classification task: for example, we can easily categorize a complicated (or overly terse) document if we find within it indications of agreement with a clearly positive text.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most sentiment-polarity classifiers proposed in the recent literature categorize each document independently. ", "mid_sen": "A few others incorporate various measures of inter-document similarity between the texts to be labeled (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) . ", "after_sen": "Many interesting opinion-oriented documents, however, can be linked through certain relationships that occur in the context of evaluative discussions. "}
{"citeStart": 380, "citeEnd": 403, "citeStartToken": 380, "citeEndToken": 403, "sectionName": "UNKNOWN SECTION NAME", "string": "C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text-and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\" While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991) . 2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicatorsaverage sentence length and type-token ratio -obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool. Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text-and word-specific factors. ", "mid_sen": "The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\" While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991) . 2 In our model, we aim at combining features touching all levels of language. ", "after_sen": "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. "}
{"citeStart": 13, "citeEnd": 34, "citeStartToken": 13, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "We used φ 2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as χ 2 (Zhang, S. Vogel. 2005) , log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005) . The φ 2 statistics for a pair of words e i and f j is computed as ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used φ 2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as χ 2 (Zhang, S. Vogel. 2005) , log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005) . ", "after_sen": "The φ 2 statistics for a pair of words e i and f j is computed as "}
{"citeStart": 38, "citeEnd": 61, "citeStartToken": 38, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "This section describes how we transliterate Arabic words or phrases. Given a word such as JJ K A kP or a phrase such as J @ P Q K P , we want to find the English transliteration for it. This is not just a romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoff and Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007) , we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. ", "mid_sen": "Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007) , we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million).", "after_sen": "We develop a similarity metric for Arabic and English words. "}
{"citeStart": 276, "citeEnd": 298, "citeStartToken": 276, "citeEndToken": 298, "sectionName": "UNKNOWN SECTION NAME", "string": "The BEETLE II system we present was built to serve as a platform for research in computational linguistics and tutoring, and can be used for taskbased evaluation of algorithms developed for other domains. We are currently developing an annotation scheme for the data we collected to identify student paraphrases of correct answers. The annotated data will be used to evaluate the accuracy of existing paraphrasing and textual entailment approaches and to investigate how to combine such algorithms with the current deep linguistic analysis to improve system robustness. We also plan to annotate the data we collected for evidence of misunderstandings, i.e., situations where the system arrived at an incorrect interpretation of a student utterance and took action on it. Such annotation can provide useful input for statistical learning algorithms to detect and recover from misun-derstandings. In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010) . Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006) . Using a deep generator to automatically generate system feedback gives us a level of control over the output and will allow us to devise experiments to study those issues in more detail.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In dialogue management and generation, the key issue we are planning to investigate is that of linguistic alignment. ", "mid_sen": "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words (for example, using better terminology) but doesn't explicitly explain the reason why different terminology is needed (Dzikovska et al., 2010) . ", "after_sen": "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain (Ward and Litman, 2006) . "}
{"citeStart": 135, "citeEnd": 153, "citeStartToken": 135, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. In future work we plan to experiment with richer representations, e.g. including long-range n-grams (Rosenfeld, 1996) , class n-grams (Brown et al., 1992) , grammatical features (Amaya and Benedy, 2001 ), etc'.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The sentence representation we chose for this work is rather simple, and was intended primarily to demonstrate the efficacy of our approach. ", "mid_sen": "In future work we plan to experiment with richer representations, e.g. including long-range n-grams (Rosenfeld, 1996) , class n-grams (Brown et al., 1992) , grammatical features (Amaya and Benedy, 2001 ), etc'.", "after_sen": "The main computational bottleneck in our approach is the generation of negative samples from the current model. "}
{"citeStart": 27, "citeEnd": 45, "citeStartToken": 27, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word \"line\". The seven algorithms that he evaluated are: a Naive-Bayes classifier (Duda and Hart, 1973) , a perceptron (Rosenblatt, 1958) , a decisiontree learner (Quinlan, 1993) , a k nearest-neighbor classifier (exemplar-based learner) (Cover and Hart, 1967) , logic-based DNF and CNF learners (Mooney, 1995) , and a decision-list learner (Rivest, 1987) . His results indicate that the simple Naive-Bayes algorithm gives the highest accuracy on the \"line\" corpus tested. Past research in machine learning has also reported that the Naive-Bayes algorithm achieved good performance on other machine learning tasks (Clark and Niblett, 1989; Kohavi, 1996) . This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is in spite of the conditional independence assumption made by the Naive-Bayes algorithm, which may be unjustified in the domains tested. ", "mid_sen": "Gale, Church and Yarowsky (Gale et al., 1992a; Gale et al., 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation.", "after_sen": "On the other hand, our past work on WSD (Ng and Lee, 1996) used an exemplar-based (or nearest neighbor) learning approach. "}
{"citeStart": 117, "citeEnd": 129, "citeStartToken": 117, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "Reasonable starting values for minimum divergence estimation is to set i = 0 for i = 1 : : : n . This yields a distribution which minimizes the divergence to p 0 , over the set of models p to which the constraints p i ] = q i ] i = 1 : : : n have yet to beapplied. Clearly, this argument applies to both complete-data and incomplete-data estimation. Note that for a uniformly distributed reference model p 0 , the minimum divergence model is a maximum entropy model (Jaynes, 1957) . In Sec. 4, we will demonstrate that a uniform initialization of the IM algorithm shows a signi cant improvement in likelihood maximization as well as in linguistic performance when compared to standard random initialization.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clearly, this argument applies to both complete-data and incomplete-data estimation. ", "mid_sen": "Note that for a uniformly distributed reference model p 0 , the minimum divergence model is a maximum entropy model (Jaynes, 1957) . ", "after_sen": "In Sec. 4, we will demonstrate that a uniform initialization of the IM algorithm shows a signi cant improvement in likelihood maximization as well as in linguistic performance when compared to standard random initialization."}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "As noted in (Partee, 1984) , this analysis does not extend in a straightforward manner to cases in which the operator when is replaced by (an unrestricted) before or after, in such quantified contexts. Constructing a similar DRS for such sentences gives the wrong truth conditions. For example, Figure la shows a DRS for sentence 1, according to the principles above, rl -the reference time, used for the interpretation of the main clause is placed in the universe of the antecedent box. Because the temporal connective is 'before', rl is restricted to lie before el. The embedding conditions determine, that this reference time be universally quantified over, causing an erroneous reading in which for each event, el, of John's calling, for each earlier time rl, he lights up a cigarette. Paraphrasing this, we could say that John lights up cigarettes at all times preceding each phone call, not just once preceding each phone call. We did not encounter this problem in the DRS in Figure 4 , since although the reference time rl, is universally quantified over in that DRS as well, it is also restricted, to immediately follow el. It is similarly restricted if 'before' is replaced with 'just before' or 'ten minutes before'. But, (unrestricted) 'before' is analyzed as 'some time before', and thus the problem arises. We will henceforth informally refer to this problem as Partee's quantification problem. Partee (1984) suggests that in these cases we somehow have to insure that the reference time, rz, appears in the universe of the consequent DRS, causing it to be existentially quantified over, giving the desired interpretation. De Swart (1991) notes that simply moving rl to the right-hand box does not agree with Hinrichs' assumption, that temporal clauses are processed before the main clause, since they update the reference time, with respect to which the main clause will be inter-preted. In our proposed solution, the 'reference time' is indeed moved to the right box, but it is a different notion of reference time, and (as will be shown) exempt from this criticism.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Partee (1984) suggests that in these cases we somehow have to insure that the reference time, rz, appears in the universe of the consequent DRS, causing it to be existentially quantified over, giving the desired interpretation. ", "mid_sen": "De Swart (1991) notes that simply moving rl to the right-hand box does not agree with Hinrichs' assumption, that temporal clauses are processed before the main clause, since they update the reference time, with respect to which the main clause will be inter-preted. ", "after_sen": "In our proposed solution, the 'reference time' is indeed moved to the right box, but it is a different notion of reference time, and (as will be shown) exempt from this criticism."}
{"citeStart": 89, "citeEnd": 100, "citeStartToken": 89, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Machine learning methods should be interchangeable: Transformation-based learning (TBL) (Brill, 1993) and Memory-based learning (MBL) (Daelemans et al., 2002) have been applied to many different problems, so a single interchangeable component should be used to represent each method. We will base these components on the design of Weka (Witten and Frank, 1999) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is particularly important in NLP because of the high redundancy across tasks and approaches.", "mid_sen": "Machine learning methods should be interchangeable: Transformation-based learning (TBL) (Brill, 1993) and Memory-based learning (MBL) (Daelemans et al., 2002) have been applied to many different problems, so a single interchangeable component should be used to represent each method. ", "after_sen": "We will base these components on the design of Weka (Witten and Frank, 1999) ."}
{"citeStart": 60, "citeEnd": 79, "citeStartToken": 60, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "To be used in the next section, we now show that the PCFG G obtained as above is consistent. The line of our argument below follows a proof provided in (Chi and Geman, 1998) for the maximum likelihood estimator based on finite tree distributions. Without loss of generality, we assume that in G the start symbol S is never used in the right-hand side of a rule.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To be used in the next section, we now show that the PCFG G obtained as above is consistent. ", "mid_sen": "The line of our argument below follows a proof provided in (Chi and Geman, 1998) for the maximum likelihood estimator based on finite tree distributions. ", "after_sen": "Without loss of generality, we assume that in G the start symbol S is never used in the right-hand side of a rule."}
{"citeStart": 108, "citeEnd": 129, "citeStartToken": 108, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting befiefs; therefore we employ a re, cursive model for collaboration that captures extended negotiation and represents the structure of the discourse. Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. In addition, we provide a process for selecting among multiple possible pieces of evidence.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . ", "mid_sen": "However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting befiefs; therefore we employ a re, cursive model for collaboration that captures extended negotiation and represents the structure of the discourse. ", "after_sen": "Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. "}
{"citeStart": 147, "citeEnd": 166, "citeStartToken": 147, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of Light et al. (2004) . As noted by Light et al., speculative assertions are to be identified on the basis of judgements about the author's intended meaning, rather than on the presence of certain designated hedge terms.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Specifically, S is to be partitioned into two disjoint sets, one representing sentences that contain some form of hedging, and the other representing those that do not.", "mid_sen": "To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of Light et al. (2004) . ", "after_sen": "As noted by Light et al., speculative assertions are to be identified on the basis of judgements about the author's intended meaning, rather than on the presence of certain designated hedge terms."}
{"citeStart": 60, "citeEnd": 79, "citeStartToken": 60, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999 ): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. ", "mid_sen": "For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999 ): ", "after_sen": "CASE is relevant, not redundant, and can be predicted with sufficient accuracy. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Since our experiments, other related work in NLP has been performed. Some of this work addresses related but different classification tasks. Three studies classify reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, Pennock 2003) . The input is assumed to be a review, so this task does not include finding subjective documents in the first place. The first study listed above (Turney 2002) uses a variation of the semantic similarity procedure presented in Wiebe (2000) (Section 3.4). The third (Dave, Lawrence, and Pennock 2003) uses ngram features identified with a variation of the procedure presented in (Section 3.3) . Tong (2001) addresses finding sentiment timelines, that is, tracking sentiments over time in multiple documents. For clues of subjectivity, he uses manually developed lexical rules, rather than automatically learning them from corpora. Similarly, Gordon et al. (2003) use manually developed grammars to detect some types of subjective language. Agrawal et al. (2003) partition newsgroup authors into camps based on quotation links. They do not attempt to recognize subjective language.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similarly, Gordon et al. (2003) use manually developed grammars to detect some types of subjective language. ", "mid_sen": "Agrawal et al. (2003) partition newsgroup authors into camps based on quotation links. ", "after_sen": "They do not attempt to recognize subjective language."}
{"citeStart": 0, "citeEnd": 17, "citeStartToken": 0, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been some attempts to capture the behavior of semantic categories in a distributional setting, despite the unavailability of sense-annotated corpora. For example, Hearst and Schtltze (1993) take steps toward a distributional treatment of WordNet-based classes, using Schtltze's (1993) approach to constructing vector representations from a large co-occurrence matrix. Yarowsky's (1992) algorithm for sense disambiguation can be thought of as a way of determining how Roget's thesaurus categories behave with respect to contextual features. And my own treatment of selectional constraints (Resnik, 1993) provides a way to describe the plausibility of co-occuffence in terms of WordNet's semantic categories, using co-occurrence relationships mediated by syntactic structure. In each case, one begins with known semantic categories (WordNet synsets, Roget's numbered classes) and non-sense-annotated text, and proceeds to a distributional characterization of semantic category behavior using co-occurrence relationships.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Hearst and Schtltze (1993) take steps toward a distributional treatment of WordNet-based classes, using Schtltze's (1993) approach to constructing vector representations from a large co-occurrence matrix. ", "mid_sen": "Yarowsky's (1992) algorithm for sense disambiguation can be thought of as a way of determining how Roget's thesaurus categories behave with respect to contextual features. ", "after_sen": "And my own treatment of selectional constraints (Resnik, 1993) provides a way to describe the plausibility of co-occuffence in terms of WordNet's semantic categories, using co-occurrence relationships mediated by syntactic structure. "}
{"citeStart": 97, "citeEnd": 105, "citeStartToken": 97, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "The machine-readable thesaurus we used in this study was derived from GETESS , an ontology for the tourism domain. Each concept in the ontology is associated with one lexical item, which expresses this concept. From this ontology, word classes were derived in the following manner. A class was formed by words lexicalizing all child concepts of a given concept. For example, the concept CULTURAL_EVENT in the ontology has successor concepts PERFORMANCE, OPERA, FESTIVAL, associated with words performance, opera, festival correspondingly. Though these words are not synonyms in the traditional sense, they are taken to constitute one semantic class, since out of all words of the ontology's lexicon their meanings are closest. The thesaurus thus derived contained 1052 words and phrases (the corpus used in the study had data on 756 of them). Out of the 756 concepts, 182 were non-final; correspondingly, 182 word classes were formed. The average depth level of the thesaurus is 5.615, the maximum number of levels is 9. The corpus from which distributional data was obtained was extracted from a web site advertising hotels around the world . It contained around 1 million words. Collection of distributional data was carried out in the following settings. The preprocessing of corpus included a very simple stemming (most common inflections were chopped off; irregular forms of verbs, adjectives and nouns were changed to their first forms). The context of usage was delineated by a window of 3 words on either side of the target word, without transgressing sentence boundaries. In case a stop word other than a proper noun appeared inside the window, the window was accordingly expanded. The stoplist included 50 most frequent words of the British National Corpus, words listed as function words in the BNC, and proper nouns not appearing in the sentence-initial position. The obtained frequencies of cooccurrence were weighted by the 1+log weight function. The distributional similarity was measured by means of three different similarity measures: the Jaccard's coefficient, L1 distance, and the skew divergence. This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999 ) which compared several well known measures on similar tasks and found these three to be superior to many others. Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The distributional similarity was measured by means of three different similarity measures: the Jaccard's coefficient, L1 distance, and the skew divergence. ", "mid_sen": "This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999 ) which compared several well known measures on similar tasks and found these three to be superior to many others. ", "after_sen": "Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999) ."}
{"citeStart": 59, "citeEnd": 85, "citeStartToken": 59, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the files sw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak(2004) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the files sw4004.", "mid_sen": "mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak(2004) .", "after_sen": "The input to the system consists of the terminal symbols from the trees in the corpus section mentioned above. "}
{"citeStart": 45, "citeEnd": 61, "citeStartToken": 45, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "To elmbIe complete, fully word by word parsing re quires a way of encoding an intinite nmnber of partiM l, rees. There are several possibilities. 'Fhe first is to use a language describing trees where we can express the fact that ,]ohn is donfinatcd by the suode, but do not have to speciiy what it. is ilmnediately dominated by (e.g. D-Theory, Marcus et ah 198a) . Semantic representations could be tbrmed word by word by extracting 'default' syntax trees (by strengthening dominance links into immediated dominance links wherever possible).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "'Fhe first is to use a language describing trees where we can express the fact that ,]ohn is donfinatcd by the suode, but do not have to speciiy what it. ", "mid_sen": "is ilmnediately dominated by (e.g. D-Theory, Marcus et ah 198a) . ", "after_sen": "Semantic representations could be tbrmed word by word by extracting 'default' syntax trees (by strengthening dominance links into immediated dominance links wherever possible)."}
{"citeStart": 109, "citeEnd": 126, "citeStartToken": 109, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001 ) and subsequently extended by (Ng and Cardie, 2002) . Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, annotated with coreference relations, for both training and testing. They also outlined a list of features to extract for training the resolver to recognize the coreference relations. Specifically, (Soon et al., 2001 ) established a list of 12 features that compare a given anaphor with a candidate antecedent, e.g. gender agreement, number agreement, both being pronouns, both part of the same semantic class (i.e. WordNet synset hyponyms/hypernyms), etc. For training the resolver, a corpus annotated with anaphors and their antecedents is processed, and pairs of anaphor and candidate antecedents are created so as to have only one positive instance per anaphor (the annotated antecedent). Negative examples are created by taking all occurrences of noun phrases that occur between the anaphor and its antecedent in the text. The antecedent in these steps is also always considered to be to the left of, or preceding, the anaphor; cataphors are not addressed in this technique.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To create and train our coreference resolver, we used a combination of techniques as outlined originally by (Soon et al., 2001 ) and subsequently extended by (Ng and Cardie, 2002) . ", "after_sen": "Mimicking their approaches, we used the corpora provided for the MUC-7 coreference resolution task (LDC2001T02, 2001), which includes sets of newspaper articles, annotated with coreference relations, for both training and testing. "}
{"citeStart": 131, "citeEnd": 144, "citeStartToken": 131, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006) . However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006) . ", "after_sen": "However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. "}
{"citeStart": 65, "citeEnd": 66, "citeStartToken": 65, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems a perfectly valid rule of conversation not to tell people what they already know. Indeed, Grice's QUANTITY lllaxim has often been interpreted this way: Do not make your contribution more informative than is required [f] . Stalnaker, as well, suggests that to assert something that is already presupposed is to attempt to do something that is already done [14] . Thus, the notion of what is informative is judged against a background of what is presupposed, i.e. propositions that all conversants assume are mutually known or believed. These propositions are known as the COMMON GROUND [10, 5] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Indeed, Grice's QUANTITY lllaxim has often been interpreted this way: ", "mid_sen": "Do not make your contribution more informative than is required [f] . ", "after_sen": "Stalnaker, as well, suggests that to assert something that is already presupposed is to attempt to do something that is already done [14] . "}
{"citeStart": 165, "citeEnd": 184, "citeStartToken": 165, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "Here we test how well the systems perform using the same small annotated training set, the 3299 words of elementary school reading comprehension test bodies used in Ferro et al. (1999) . 2 We are mainly interested in comparing the parts of the system that takes in syntax (noun, verb, etc.) chunks (also known as groups) and finds the GRs between those chunks. So for the experiment, we used the general TiMBL system to just reconstruct the part of the MB system that takes in chunks and finds GRs. The input to both this reconstructed part and the TR system is data that has been manually annotated for syntax chunks and GRs, along with automatic lexeme and sentence segmentation and part-ofspeech tagging. In addition, the TR system has manual named-entity annotation, and automatic estimations for verb properties and preposition and subordinate conjunction attachments (Ferro et al., 1999) . Because the MB system was originally designed to handle GRs attached to verbs (and not noun to noun GRs, etc.), we ran the reconstructed part to only find GRs to verbs, and ignored other types of GRs when comparing the reconstructed part with the TR system. The test set is the 1151 word test set used in Ferro et al. (1999) . Only GRs to verbs were examined, so the effective training set GR count fell from 1963 to 1298 and test set GR count from 748 to 500.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One cannot directly compare the two systems from the descriptions given in Ferro et al. (1999) and Buchholz et al. (1999) , as the results in the descriptions were based on different data sets and on different assumptions of what is known and what needs to be found.", "mid_sen": "Here we test how well the systems perform using the same small annotated training set, the 3299 words of elementary school reading comprehension test bodies used in Ferro et al. (1999) . 2 We are mainly interested in comparing the parts of the system that takes in syntax (noun, verb, etc.) chunks (also known as groups) and finds the GRs between those chunks. ", "after_sen": "So for the experiment, we used the general TiMBL system to just reconstruct the part of the MB system that takes in chunks and finds GRs. "}
{"citeStart": 172, "citeEnd": 192, "citeStartToken": 172, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "1 Introduction \"Unification-based\" Grammars (UBGs) can capture a wide variety of linguistically important syntactic and semantic constraints. However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999) . Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, because these constraints can be non-local or context-sensitive, developing stochastic versions of UBGs and associated estimation procedures is not as straight-forward as it is for, e.g., PCFGs. ", "mid_sen": "Recent work has shown how to define probability distributions over the parses of UBGs (Abney, 1997) and efficiently estimate and use conditional probabilities for parsing (Johnson et al., 1999) . ", "after_sen": "Like most other practical stochastic grammar estimation procedures, this latter estimation procedure requires a parsed training corpus."}
{"citeStart": 42, "citeEnd": 63, "citeStartToken": 42, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lari and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. Briscoe and Waegner (1992) argued that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initialization. It attempts to seed the grammar in a favorable search space by first training it with data from an existing corpus. Section 4 discusses the induction strategies in more detail.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. ", "mid_sen": "For finding a good initial parameter set, Lari and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. ", "after_sen": "Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. "}
{"citeStart": 163, "citeEnd": 182, "citeStartToken": 163, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "1. Most linguistic approaches account for the defeasibility of pragmatic inferences by analyzing them in a context that consists of all or some of the previous utterances, including the current one. Context (Karttunen, 1974; Kay, 1992) , procedural rules (Gazdar, 1979; Karttunen and Peters, 1979) , lexical and syntactic structure (Weischedel, 1979) , intentions (Hirschberg, 1985) , or anaphoric constraints (Sandt, 1992; Zeevat, 1992) decide what presuppositions or implicatures are projected as pragmatic inferences for the utterance that is analyzed. The problem with these approaches is that they assign a dual life to pragmatic inferences: in the initial stage, as members of a simple or complex utterance, they are defeasible. However, after that utterance is analyzed, there is no possibility left of cancelling that inference. But it is natural to have implicatures and presuppositions that are inferred and cancelled as a sequence of utterances proceeds: research in conversation repairs (I-Iirst et M., 1994) abounds in such examples. We address this issue in more detail in section 3.3.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, after that utterance is analyzed, there is no possibility left of cancelling that inference. ", "mid_sen": "But it is natural to have implicatures and presuppositions that are inferred and cancelled as a sequence of utterances proceeds: research in conversation repairs (I-Iirst et M., 1994) abounds in such examples. ", "after_sen": "We address this issue in more detail in section 3.3."}
{"citeStart": 184, "citeEnd": 208, "citeStartToken": 184, "citeEndToken": 208, "sectionName": "UNKNOWN SECTION NAME", "string": "In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agn~ et al, 1994) . Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). When a sentence was analysed successfully, several semantic analyses were, in general, created, and a selection was made from among these on the basis of trained preference functions (Alshawi and Carter, 1994) . For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). ", "mid_sen": "When a sentence was analysed successfully, several semantic analyses were, in general, created, and a selection was made from among these on the basis of trained preference functions (Alshawi and Carter, 1994) . ", "after_sen": "For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis."}
{"citeStart": 72, "citeEnd": 92, "citeStartToken": 72, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. We refer to these as linguistic level representations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . ", "mid_sen": "Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. ", "after_sen": "The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. "}
{"citeStart": 126, "citeEnd": 140, "citeStartToken": 126, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995) . In Riloff and Shepherd (1997) , noun co-occurrence statistics were used to indicate nominal cate-gory membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as follows:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Utilizing existing resources, such as on-line corpora, to aid in this task could improve performance both by decreasing the time to construct the lexicon and by improving its quality.", "mid_sen": "Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995) . ", "after_sen": "In Riloff and Shepherd (1997) , noun co-occurrence statistics were used to indicate nominal cate-gory membership, for the purpose of aiding in the construction of semantic lexicons. "}
{"citeStart": 106, "citeEnd": 117, "citeStartToken": 106, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "We begin by introducing the distinction between athematic auxiliary trees and complement auxiliary trees (Kroch, 1989) , which are meant to exhaustively characterize the auxiliary trees used in any natural language TAG grammar. 2 An athematic auxiliary tree does not subcategorize for or assign a thematic role to its foot node, so the head of the foot node becomes the head of the phrase at the root. The structure of an athematic auxiliary tree may thus be described as:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We begin by introducing the distinction between athematic auxiliary trees and complement auxiliary trees (Kroch, 1989) , which are meant to exhaustively characterize the auxiliary trees used in any natural language TAG grammar. ", "after_sen": "2 An athematic auxiliary tree does not subcategorize for or assign a thematic role to its foot node, so the head of the foot node becomes the head of the phrase at the root. "}
{"citeStart": 118, "citeEnd": 140, "citeStartToken": 118, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "In the first maximum alignment based approach we will consider, the definitions of s i,pred and s i,j are inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the first maximum alignment based approach we will consider, the definitions of s i,pred and s i,j are inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length.", "after_sen": "s i,pred = 1 (prec e i,pred ,f i,pred + rece i,pred ,f i,pred ) si,j = 1 (prec e i,j ,f i,j + rece i,j ,f i,j )"}
{"citeStart": 80, "citeEnd": 102, "citeStartToken": 80, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "where f j is the source word corresponding to e i and C is its context. Using the maximum entropy model, binary classifiers are trained for every target block in the vocabulary. These classifiers predict if a particular target block should be present given the source word and its context. This model is similar to the global lexical selection (GLS) model described in (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) except that in GLS, the predicted target blocks are not associated with any particular source word unlike the case here.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These classifiers predict if a particular target block should be present given the source word and its context. ", "mid_sen": "This model is similar to the global lexical selection (GLS) model described in (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) except that in GLS, the predicted target blocks are not associated with any particular source word unlike the case here.", "after_sen": "For the set of experiments in this paper, we used a context of size 6, containing three words to the left and three words to the right. "}
{"citeStart": 26, "citeEnd": 38, "citeStartToken": 26, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "We can offer here only a brief overview of stratified logic. The reader is referred to Marcu (1994) for a comprehensive study. Stratified logic supports one type of indefeasible information and two types of defeasible information, namely, infelicitously defeasible and felicitously defeasible. The notion of infelicitously defeasible information is meant to capture inferences that are anomalous to cancel, as in:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We can offer here only a brief overview of stratified logic. ", "mid_sen": "The reader is referred to Marcu (1994) for a comprehensive study. ", "after_sen": "Stratified logic supports one type of indefeasible information and two types of defeasible information, namely, infelicitously defeasible and felicitously defeasible. "}
{"citeStart": 186, "citeEnd": 198, "citeStartToken": 186, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "Instead, it would seem that the real contrast driving the shift towards statistics in language processing is a contrast between qualitative systems dealing exclusively with combinatoric constraints, and quantitative systems that involve computing numerical functions. This bears dir~.ctly on the problems of brittleness and complexity that discrete approaches to language processing share wll,ll, for example, reasoning systems based on traditional logical inference. It relates to the inadequacy of the dominant theories in linguistics to capture 'shades' of meaning or degrees of acceptability which are often recognized by people outside the field as important inherent properties of natural language. The qualitativequantitative distinction can also be seen as underlying the difference between classification systems based on I'cature specifications, as used in unification formalisms (Shicber 1986) , and clustering based on a variable de-gr~,e of granularity (e.g. Pereira, Tishby and Lee 1993).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It relates to the inadequacy of the dominant theories in linguistics to capture 'shades' of meaning or degrees of acceptability which are often recognized by people outside the field as important inherent properties of natural language. ", "mid_sen": "The qualitativequantitative distinction can also be seen as underlying the difference between classification systems based on I'cature specifications, as used in unification formalisms (Shicber 1986) , and clustering based on a variable de-gr~,e of granularity (e.g. Pereira, Tishby and Lee 1993).", "after_sen": "It seems unlikely that these continuously variable aspcct:s of fluent natural language can be captured by a purely combinatoric model. "}
{"citeStart": 65, "citeEnd": 85, "citeStartToken": 65, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989) . The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984) . For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989) . ", "mid_sen": "The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984) . ", "after_sen": "For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994) ."}
{"citeStart": 7, "citeEnd": 34, "citeStartToken": 7, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Narration: The Narration relation is characterized by a series of events displaying forward movement of time, such as in passage (la). As did Lascarides and Asher (1993) , we capture this ordering as a constraint imposed by the Narration coherence relation itself.-4", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Narration relation is characterized by a series of events displaying forward movement of time, such as in passage (la). ", "mid_sen": "As did Lascarides and Asher (1993) , we capture this ordering as a constraint imposed by the Narration coherence relation itself.", "after_sen": "-4"}
{"citeStart": 108, "citeEnd": 116, "citeStartToken": 108, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "EBMT is based on the idea of performing translation by imitating translation examples of similar sentences [Nagao 84] . In this type of translation system, a large amount of bi/multi-lingual translation examples has been stored in a textual database and input expressions are rendered in the target language by retrieving from the database that example which is most similar to the input.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "EBMT is based on the idea of performing translation by imitating translation examples of similar sentences [Nagao 84] . ", "after_sen": "In this type of translation system, a large amount of bi/multi-lingual translation examples has been stored in a textual database and input expressions are rendered in the target language by retrieving from the database that example which is most similar to the input."}
{"citeStart": 160, "citeEnd": 178, "citeStartToken": 160, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004) , the Polarity data set 5 created by (Pang and Lee, 2004) , and the MPQA data set created by (Wiebe et al., 2005) . 6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves sentence-level classification.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004) , the Polarity data set 5 created by (Pang and Lee, 2004) , and the MPQA data set created by (Wiebe et al., 2005) . 6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves sentence-level classification.", "after_sen": "The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993) . "}
{"citeStart": 1, "citeEnd": 28, "citeStartToken": 1, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "Observe that even though SyntaLex-2 uses a larger context than SyntaLex-1 it does not do much better than the latter, in fact, its accuracies are slightly lower. We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P£ ¥ ¢ and P¢ ) are likely to be overwhelmed by idiosyncrasies of the data. (Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P£ ¥ ¢ and P¢ ) are likely to be overwhelmed by idiosyncrasies of the data. ", "mid_sen": "(Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training data per task are shown to benefit from a larger context.", "after_sen": "Duluth-ELSS (a sister system of SyntaLex) achieves an accuracy of 61.7%. "}
{"citeStart": 20, "citeEnd": 32, "citeStartToken": 20, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same way that tags are allocated to words, or to punctuation marks, they can represent the boundaries of syntactic constituents, such as noun phrases and verb phrases. Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do. (Atwell, 1987) and (Church, 1989) have used this approach. If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994) . Our approach uses a similar concept, but differs in that embedded syntactic constituents are detected one at a time in separate steps. There are only 2 hypertags -the opening and closing brackets marking the possible location(s) of the syntactic constituent in question. Using this representation a hierarchical language structure is converted to a string of tags represented by a linear vector.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Boundary markers can be considered invisible tags, or hypertags, which have probabilistic relationships with adjacent tags in the same way that words do. ", "mid_sen": "(Atwell, 1987) and (Church, 1989) have used this approach. ", "after_sen": "If embedded syntactic constituents are sought in a single pass, this can lead to computational overload (Pocock and Atwell, 1994) . "}
{"citeStart": 193, "citeEnd": 216, "citeStartToken": 193, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. ", "mid_sen": "The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). ", "after_sen": "The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. "}
{"citeStart": 205, "citeEnd": 216, "citeStartToken": 205, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "In a system like INTARC-1.3, the analysis tree is of much higher importance than the recovered string; for the goal of speech translation an adequate semantic representation for a string with word errors is more important than a good string with a wrong reading. The grammar scores have only indirect influence on the string; their main function is picking the right tree. We cannot measure something like a \"tree recognition rate\" or \"rule accuracy\", because there is no treebank for our grammar. The word accuracy results cannot be compared to word accuracy as usually applied to an acoustic decoder in isolation. We counted only those words as recognized which could be built into a valid parse from the beginning of the utterance. Words to the right which could not be integrated into a parse, were counted as deletions ---although they might have been correct in standard word accuracy terms. This evaluation method is much harder than standard word accuracy, but it appears to be a good approximation to \"rule accuracy\". Using this strict method we achieved a word accuracy of 47%, which is quite promising. Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989 ) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a) , and (Weber 1995) . Recognition rates had been improved there for read speech. In spontaneous speech we could not achieve the same effects.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using this strict method we achieved a word accuracy of 47%, which is quite promising. ", "mid_sen": "Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989 ) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a) , and (Weber 1995) . ", "after_sen": "Recognition rates had been improved there for read speech. "}
{"citeStart": 60, "citeEnd": 72, "citeStartToken": 60, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs. Combining statistical and parsing methods has been done by (Hindle, 1990; Hindle and Rooths,1991) and (Smadja and McKewon, 1990; Smadja,1991) . The novel aspect of our study is that we collect not only operational pairs, but triples, such as N_prep N, V_prep_N etc. In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words. By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation (e.g. for=purpose,beneficiary). To extract syntactic associations two methods have been adopted in the literature. Smadja attempts to apply syntactic information to a set of automatically collected collocations (statistics-first). Hindle performs syntactic parsing before collocational analysis (syntax-first). In our study, we decided to adopt the syntax-first approach, because:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs. ", "mid_sen": "Combining statistical and parsing methods has been done by (Hindle, 1990; Hindle and Rooths,1991) and (Smadja and McKewon, 1990; Smadja,1991) . ", "after_sen": "The novel aspect of our study is that we collect not only operational pairs, but triples, such as N_prep N, V_prep_N etc. "}
{"citeStart": 80, "citeEnd": 85, "citeStartToken": 80, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "To modify tim Asset in such a way that it was based in a likelihood ratio test [Dun93] . It seems that this kind of tests have a better performance than nmtual inl'ormation when the counts are sma.ll, ~m it is the case.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To measure the Assoe by means of Mutual lntbrmarion between the pair v-s and c. In this way, tim syntactic position also wouhl provide iutbrmalion (statistical evidence) for measuring the most appropriate classes.", "mid_sen": "To modify tim Asset in such a way that it was based in a likelihood ratio test [Dun93] . ", "after_sen": "It seems that this kind of tests have a better performance than nmtual inl'ormation when the counts are sma.ll, ~m it is the case."}
{"citeStart": 41, "citeEnd": 64, "citeStartToken": 41, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "aWe consider graph-based approaches to WSI, which typically construct a graph from word occurrences or collocations. The core problem is how to identify sense-specific information within the graph in order to perform sense induction. Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and to identify sense-specific subgraphs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The core problem is how to identify sense-specific information within the graph in order to perform sense induction. ", "mid_sen": "Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and to identify sense-specific subgraphs.", "after_sen": "We reinterpret the challenge of identifying sensespecific information in a co-occurrence graph as one of community detection, where a community is de-fined as a group of connected nodes that are more connected to each other than to the rest of the graph (Fortunato, 2010) . "}
{"citeStart": 85, "citeEnd": 111, "citeStartToken": 85, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "Word reordering between German and English is a complex problem. Encouraged by the success of chunk-based verb reordering lattices on Arabic-English (Bisazza and Federico, 2010) , we tried to adapt the same approach to the German-English language pair. It turned out that there is a larger variety of long reordering patterns in this case. Nevertheless, some experiments performed after the official evaluation showed promising results. We plan to pursue this work in several directions: Defining a lattice weighting scheme that distinguishes between original word order and reordering paths could help the decoder select the more promising path through the lattice. Applying similar reordering rules to the training corpus would reduce the mismatch between the training data and the reordered input sentences. Finally, it would be useful to explore the impact of different distortion limits on the decoding of reordering lattices in order to find an optimal trade-off between decoderdriven short-range and lattice-driven long-range reordering.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word reordering between German and English is a complex problem. ", "mid_sen": "Encouraged by the success of chunk-based verb reordering lattices on Arabic-English (Bisazza and Federico, 2010) , we tried to adapt the same approach to the German-English language pair. ", "after_sen": "It turned out that there is a larger variety of long reordering patterns in this case. "}
{"citeStart": 223, "citeEnd": 224, "citeStartToken": 223, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "The assumption, rather than being a hypothesi~ or a default, get upgraded to a support type of linguistic as a result of the IRU. The fact that different II~U's address different assumptions leads to the perception that some 1KU's are better evidence for understanding than others, e.g. a PARAPHRASE i8 stronger evidence of understanding than a REPEAT [3] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The assumption, rather than being a hypothesi~ or a default, get upgraded to a support type of linguistic as a result of the IRU. ", "mid_sen": "The fact that different II~U's address different assumptions leads to the perception that some 1KU's are better evidence for understanding than others, e.g. a PARAPHRASE i8 stronger evidence of understanding than a REPEAT [3] .", "after_sen": "In addition, any next utterance by the addressee can upgrade the strength of the underlying assumptions to default (See Figure 1 ). "}
{"citeStart": 49, "citeEnd": 69, "citeStartToken": 49, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2 . Using these weights, we run Hiero's decoder to perform the actual translation of the MT 2003 test sentences and obtained a BLEU score of 29.73, as shown in the row Hiero of Table 1. This is higher than the score of 28.77 reported in (Chiang, 2005) , perhaps due to differences in word segmentation, etc. Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005 ), the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is higher than the score of 28.77 reported in (Chiang, 2005) , perhaps due to differences in word segmentation, etc. ", "mid_sen": "Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005 ), the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve.", "after_sen": ""}
{"citeStart": 106, "citeEnd": 116, "citeStartToken": 106, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of using preferences among theories is new, hence it was described in more detail. \"Coherence,\" as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983) , with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The idea of using preferences among theories is new, hence it was described in more detail. ", "mid_sen": "\"Coherence,\" as outlined above, can be understood as a declarative (or static) version of marker passing (Hirst 1987; Charniak 1983) , with one difference: the activation spreads to theories that share a predicate, not through the IS-A hierarchy, and is limited to elementary facts about predicates appearing in the text.", "after_sen": "The metalevel rules we are going to discuss in Section 6, and that deal with the Gricean maxims and the meaning of \"but,\" can be easily expressed in the languages of set theory or higher order logic, but not everything expressible in those languages makes sense in natural language. "}
{"citeStart": 120, "citeEnd": 130, "citeStartToken": 120, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "A common way to combine different models consists of selecting the model that is most confident regarding its decision (Burke 2002) . However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). Hence, prior to selecting the most confident method, we need to find a way to compare the different measures of confidence. Because the performances of the different methods are comparable, we do this by establishing a link between confidence and performance. In other words, our meta-level process learns to predict the performance of the different methods from their confidence levels on the basis of previous experience. These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section, we describe a meta-level process which can automatically select a response-generation method to address a new request without using such thresholds.", "mid_sen": "A common way to combine different models consists of selecting the model that is most confident regarding its decision (Burke 2002) . ", "after_sen": "However, in our case, the individual confidence (applicability) measures employed by our response-generation methods are not comparable (e.g., the retrieval score in Doc-Ret is different in nature from the prediction probability in Doc-Pred). "}
{"citeStart": 113, "citeEnd": 121, "citeStartToken": 113, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Some previous work on distributional similarity between nouns has used only a single grammatical relation (e.g., Lee 1999) , whereas other work has considered multiple grammatical relations (e.g., Lin 1998a) . We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from different relations. In previous work (Weeds 2003) , we found that considering the subject relation as well as the direct-object relation did not improve performance on a pseudo-disambiguation task.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.", "mid_sen": "Some previous work on distributional similarity between nouns has used only a single grammatical relation (e.g., Lee 1999) , whereas other work has considered multiple grammatical relations (e.g., Lin 1998a) . ", "after_sen": "We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from different relations. "}
{"citeStart": 41, "citeEnd": 53, "citeStartToken": 41, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "The fourth algorithm, first described in Lauer (1994) , differs in one striking manner from the other three. It uses what I will call the DEPENDENCY MO-DEL. This model utilises the following procedure when given three nouns at, n2 and n3:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After some tuning, the accuracy was about 73%, as compared with a baseline of 64% achieved by always bracketing the first two nouns together.", "mid_sen": "The fourth algorithm, first described in Lauer (1994) , differs in one striking manner from the other three. ", "after_sen": "It uses what I will call the DEPENDENCY MO-DEL. "}
{"citeStart": 56, "citeEnd": 60, "citeStartToken": 56, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "Given three nouns nl, n2 and nz: Only more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model. The simplest of these is reported in Pustejovsky et al (1993) . Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents. Whichever is found is then chosen as the more closely bracketed pair. For example, when backup compiler disk is encountered, the analysis will be:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given three nouns nl, n2 and nz: Only more recently has it been suggested that corpus statistics might provide the oracle, and this idea is the basis of the three algorithms which use the adjacency model. ", "mid_sen": "The simplest of these is reported in Pustejovsky et al (1993) . ", "after_sen": "Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents. "}
{"citeStart": 14, "citeEnd": 28, "citeStartToken": 14, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section we present some applications of our analysis to related constructions. First, we consider the past perfect, as in sentence 2. De Swart (1991) gives this example to illustrate the inability to interpret temporal connectives without the use of the reference times. According to (de Swart, 1991) , the subordinate clause determines the reference time of the verb, which lies anteriorly to the event time. Trying to use the event times would give the wrong analysis. This would seem to be troublesome for our approach, which uses the location time of the event in the main clause, and not its reference time. However, this is not a problem, since our analysis of the perfect by the use of the operator perf, analyses the eventuality referred to by the main clause, as the result state of a previous event. The temporal relation in the sentence is inclusion between the event time of Anne's coming home, and the location time of the result state of Paul's already having prepared dinner.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "De Swart (1991) gives this example to illustrate the inability to interpret temporal connectives without the use of the reference times. ", "mid_sen": "According to (de Swart, 1991) , the subordinate clause determines the reference time of the verb, which lies anteriorly to the event time. ", "after_sen": "Trying to use the event times would give the wrong analysis. "}
{"citeStart": 168, "citeEnd": 191, "citeStartToken": 168, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "arg n~x P(TIS ) = arg n~x P(SIT ) • P(T) (1) P(SIT ) and P(T) are approximated as in Equation (2), which has commonly been used in the recent statistical NLP research (Church and Mercer, 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our task, i.e., to select T which maximizes P(TIS), is transformed into Equation (1) through use of the Bayesian theorem.", "mid_sen": "arg n~x P(TIS ) = arg n~x P(SIT ) • P(T) (1) P(SIT ) and P(T) are approximated as in Equation (2), which has commonly been used in the recent statistical NLP research (Church and Mercer, 1993) .", "after_sen": "EQUATION"}
{"citeStart": 65, "citeEnd": 78, "citeStartToken": 65, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "This algorithm differs from some commonly used methods. In feed forward networks trained in supervised mode to perform a classification task different penalty measures can be used to trigger a weight update. Back propagation and some single layer training methods typically minimise a metric based on the least squared error (LSE) between desired and actual activation of the output nodes. The reason why a differentiable error measure of this sort is necessary for multi-layer nets is well documented, for example see (Rumelhart and Mc-Clelland, 1986 ). However, for single layer nets we can choose to update weights directly: the error at an output node can trigger weight updates on the connections that feed it. Solutions with LSE are not necessarily the same as minimising the number of misclassifications, and for certain types of data this second method of direct training may be appropriate. Now, in the natural language domain it is desirable to get information from infrequent as well as common events. Rare events, rather than being noise, can make a useful contribution to a classification task. We need a method that captures information from infrequent events, and adopt a direct measure of misclassification. This may be better suited to data with a \"Zipfian\" distribution (Shannon, 1951) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We need a method that captures information from infrequent events, and adopt a direct measure of misclassification. ", "mid_sen": "This may be better suited to data with a \"Zipfian\" distribution (Shannon, 1951) .", "after_sen": "The update factor is chosen to meet several requirements. "}
{"citeStart": 174, "citeEnd": 199, "citeStartToken": 174, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "3 Anaphora Resolution System 3.1 Procedure Before starting the anaphora resolution process, the syntactic structure analyzer transforms sentences into dependency structures (Kurohashi and Nagao, 1994) . Antecedents are determined by heuristic rules for each noun from left to right in the sentences. Using these rules, our system gives possible antecedents points, and it determines that the possible antecedent having the maximum total score is the desired antecedent. This is because a several types of information are combined in anaphora resolution. An increase in the points of a possible antecedent corresponds to an increase of the plausibility of the possible antecedent.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, when tonari (neighbor or next) modifies ie (house), we judge that the antecedent of tonari (neighbor or next) is ie (house) in the first sentence.", "mid_sen": "3 Anaphora Resolution System 3.1 Procedure Before starting the anaphora resolution process, the syntactic structure analyzer transforms sentences into dependency structures (Kurohashi and Nagao, 1994) . ", "after_sen": "Antecedents are determined by heuristic rules for each noun from left to right in the sentences. "}
{"citeStart": 156, "citeEnd": 180, "citeStartToken": 156, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous studies have explored the relationship between ambiguity and subjectivity. They have shown that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses (Wiebe and Mihalcea, 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Previous studies have explored the relationship between ambiguity and subjectivity. ", "mid_sen": "They have shown that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses (Wiebe and Mihalcea, 2006) .", "after_sen": "Lexical and syntactical ambiguity usually can be resolved from contextual information and/or common consensus. "}
{"citeStart": 206, "citeEnd": 219, "citeStartToken": 206, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda (Ushioda, 1996) . In this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When inspecting manually, the binary word tree representation appears to be the most easy to understand.", "mid_sen": "A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda (Ushioda, 1996) . ", "after_sen": "In this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections."}
{"citeStart": 60, "citeEnd": 75, "citeStartToken": 60, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "Work similar to that described here has been carried out by Merialdo (1994) , with broadly similar conclusions. We will discuss this work below. The principal contribution of this work is to separate the effect of the lexical and transition parameters of the model, and to show how the results vary with different degree of similarity between the training and test data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The outcome of the two experiments together points to heuristics for making effective use of training and reestimation, together with some directions for further research.", "mid_sen": "Work similar to that described here has been carried out by Merialdo (1994) , with broadly similar conclusions. ", "after_sen": "We will discuss this work below. "}
{"citeStart": 67, "citeEnd": 84, "citeStartToken": 67, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of alignment -source-to-target or target-to-source. Hence in addition to the functions f as shown in Equation 1 we train another alignment function g :", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "EQUATION", "mid_sen": "For the work reported in this paper, we have used the GIZA++ tool (Och and Ney, 2003) which implements a string-alignment algorithm. ", "after_sen": "GIZA++ alignment however is asymmetric in that the word mappings are different depending on the direction of alignment -source-to-target or target-to-source. "}
{"citeStart": 73, "citeEnd": 85, "citeStartToken": 73, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "Order-independence can be shown to hold for grammars that contain no unary or epsilon ('empty') rules, i.e. rules whose righthand sides have one or zero elements. The grammar that we have extracted from PTB II, and which is used in the compaction experiments reported in the next section, is one that excludes such rules. For further discussion, and for the proof of the order independence see (Krotov, 1998) . Unary and sister rules were collapsed with the sister nodes, e.g. the structure (S (NP -NULL-) (VP VB (NP (QP ...))) .) will produce the following rules: S -> VP., VP -> VB QPand QP", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The grammar that we have extracted from PTB II, and which is used in the compaction experiments reported in the next section, is one that excludes such rules. ", "mid_sen": "For further discussion, and for the proof of the order independence see (Krotov, 1998) . ", "after_sen": "Unary and sister rules were collapsed with the sister nodes, e.g. the structure (S (NP -NULL-) (VP VB (NP (QP ...))) .) will produce the following rules: S -> VP., VP -> VB QPand QP"}
{"citeStart": 49, "citeEnd": 73, "citeStartToken": 49, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Results for SVM-HKE and 1 -LLM classifiers with sparse feature space The performances of parsers having SVM-HKE and 1 -LLM classifiers with and without the fstrie are given in Table 4 . The fstries successfully speeded up the SVM-HKE and 1 -LLM classifiers by factors of 10.7 (SVM-HKE, d = 4, σ = 0.0006) and 11.6 ( 1 -LLM, d = 3, ω = 3.0). We obtained more speedup when we used fstries for classifiers with more sparse feature space F d (Figures 5 and 6 ). The parsing speed with d = 3 models are now comparable to the parsing speed with d = 2 models. Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |x d | in the classification. This result conforms to the results reported in (Kudo and Matsumoto, 2003) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Without fstries, little speed-up of SVM-HKE classifiers versus the SVM-KE classifiers (in Table 3) was obtained due to the mild reduction in the average number of active features |x d | in the classification. ", "mid_sen": "This result conforms to the results reported in (Kudo and Matsumoto, 2003) .", "after_sen": "The parsing speed reached 14,937 sentences per second with accuracy of 90.91% (SVM-HKE, d = 3, σ = 0.002). "}
{"citeStart": 49, "citeEnd": 68, "citeStartToken": 49, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "The work that is most similar to ours is that of Chang et al. (2007) , who introduced the Constraint Driven Learning algorithm (CODL). Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets). For each unlabeled example, they use the current model along with their set of constraints to select a set of k automatically labeled examples which best meet the constraints. These induced examples are then added to their training set and, after processing each unlabeled dataset, they perform full model optimization with the concatenation of training data and newly generated training items. The augmented-loss algorithm can be viewed as an online version of this algorithm which performs model updates based on the augmented-loss functions directly (rather than adding a set of examples to the training set). Unlike the CODL approach, we do not perform complete optimization on each iteration over the unlabeled dataset; rather, we incorporate the updates in our online learning algorithm. As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and Mc-Callum (2010) and Ganchev et al. (2010) . Again, these works are typically interested in using the extrinsic metric -or, in general, extrinsic information -to optimize the intrinsic metric in the absence of any labeled intrinsic data. Our goal is to optimize both simultaneously.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.", "mid_sen": "The work that is most similar to ours is that of Chang et al. (2007) , who introduced the Constraint Driven Learning algorithm (CODL). ", "after_sen": "Their algorithm specifically optimizes a loss function with the addition of constraints based on unlabeled data (what we call extrinsic datasets). "}
{"citeStart": 10, "citeEnd": 29, "citeStartToken": 10, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "(3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1 (a). Our work creates the correct CCG derivation, shown in Figure 1(b) , and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005 ) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. ", "mid_sen": "PropBank (Palmer et al., 2005 ) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. ", "after_sen": ""}
{"citeStart": 28, "citeEnd": 46, "citeStartToken": 28, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "It can also be compared to (Zhang and Wu, 2012) , which uses a multi-level termhood method to extract terminology candidates from a bilingual corpus. Their system achieves an F-score of 79.6% with CRF. Both works use similar techniques to ours, but most of the terminology extraction tasks are performed using a Chinese corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our work can be compared to (Zhang et al., 2008) which uses CRF for automatic keyword extraction from documents; they reported promising results (F-score of 51.25 percent) on a Chinese corpus.", "mid_sen": "It can also be compared to (Zhang and Wu, 2012) , which uses a multi-level termhood method to extract terminology candidates from a bilingual corpus. ", "after_sen": "Their system achieves an F-score of 79.6% with CRF. "}
{"citeStart": 51, "citeEnd": 56, "citeStartToken": 51, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Different concepts have been used in the literature as primitives. ", "mid_sen": "These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. ", "after_sen": "An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. "}
{"citeStart": 13, "citeEnd": 26, "citeStartToken": 13, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "This type of learners constructs a representation for document vectors belonging to a certain class during the learning phase, e.g. decision trees, decision rules or probability weightings. During the categorization phase, the representation is used to assign the appropriate class to a new document vector. Several pruning or specialization heuristics can be used to control the amount of generalization. We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. Support Vector Machines (SVMs): SVMs are described in (Vapnik, 1995) . SVMs are binary learners in that they distinguish positive and negative examples for each class. Like eager learners, they construct a representation during the learning phase, namely a hyper plane supported by vectors of positive and negative examples. For each class, a categorizer is built by computing such a hyper plane. During the categorization phase, each categorizer is applied to the new document vector, yielding the probabilities of the document belonging to a class. The probability increases with the distance of thevector from the hyper plane. A document is said to belong to the class with the highest probability. We chose SVM_Light (Joachims, 1998) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several pruning or specialization heuristics can be used to control the amount of generalization. ", "mid_sen": "We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ", "after_sen": "ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. "}
{"citeStart": 371, "citeEnd": 394, "citeStartToken": 371, "citeEndToken": 394, "sectionName": "UNKNOWN SECTION NAME", "string": "The throughput of the CE parser is half that of the LR parser, and also less than that of the BU-LC parser. However, it is intermediate between the two in terms of storage allocated. Part of the difference in performance between it and the LR parser is due to the fact that it performs around 15% more unifications. This might be expected since the corresponding finite state automaton is not determinised--to avoid theoretical exponential time complexity on grammar size~ thus paying a price at run time. Additional reasons for the relatively poor performance of the CE parser are the overheads involved in maintaining a sparse representation of the chart, and the fact that with the ANLT grammar it generates less \"densely packed\" parse forests, since its parse table, with 14% more states (though fewer actions) than the LALR(1) table, encodes more contextual distinctions (Billot & Lang, 1989:146) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This might be expected since the corresponding finite state automaton is not determinised--to avoid theoretical exponential time complexity on grammar size~ thus paying a price at run time. ", "mid_sen": "Additional reasons for the relatively poor performance of the CE parser are the overheads involved in maintaining a sparse representation of the chart, and the fact that with the ANLT grammar it generates less \"densely packed\" parse forests, since its parse table, with 14% more states (though fewer actions) than the LALR(1) table, encodes more contextual distinctions (Billot & Lang, 1989:146) .", "after_sen": "Given that the ANLT and CLARE2.5 grammars have broadly similar (wide) coverage and return very similar numbers of syntactic analyses for the same inputs, the significantly better throughlint of the three parsers described in this paper ovcr the CLE parser 6 indicates that they do not contain any significant implementational deficiencies which would bias the results 7."}
{"citeStart": 116, "citeEnd": 134, "citeStartToken": 116, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "There are many different criteria for quantifying the (dis)similarity between (analyses of) two sentences or between two clusters of sentences; Everitt (1993) provides a good overview. Unfortunately, whatever the criterion selected, it is in general impractical to find the optimal clustering of the data; instead, one of a variety of algorithms must be used to find a locally optimal solution. Let us for the moment consider the case where the language model consists only of a unigram probability distribution for the words in the vocabulary, with no N-gram (for N > 1) or fuller linguistic constraints considered. Perhaps the most obvious measure of the similarity between two sentences or clusters is then Jaccard's coefficient (Everitt, 1993, p41) , the ratio of the number of words occurring in both sentences to the number occurring in either or both. Another possibility would be Euclidean distance, with each word in the vocabulary defining a dimension in a vector space. However, it makes sense to choose as a similarity measure the quantity we would like the final clustering arrangement to minimize: the expected entropy (or, equivalently, perplexity) of sentences from the domain. This goal is analogous to that used in the work described earlier on finding word classes by clustering.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Let us for the moment consider the case where the language model consists only of a unigram probability distribution for the words in the vocabulary, with no N-gram (for N > 1) or fuller linguistic constraints considered. ", "mid_sen": "Perhaps the most obvious measure of the similarity between two sentences or clusters is then Jaccard's coefficient (Everitt, 1993, p41) , the ratio of the number of words occurring in both sentences to the number occurring in either or both. ", "after_sen": "Another possibility would be Euclidean distance, with each word in the vocabulary defining a dimension in a vector space. "}
{"citeStart": 116, "citeEnd": 130, "citeStartToken": 116, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar development is facilitated by a chart browser that permits a quick and efficient discovery of grammar bugs (Carroll, 1997a) . Fig.  3 shows that the ambiguity in the chart is quite considerable even though grammar and corpus are restricted. For the entire corpus, we computed an average 9202 trees per clause. In the chart browser, the categories filling the cells indicate the most probable category for that span with their estimated frequencies. The pop-up window under IP presents the ranked list of all possible categories for the covered span. Rules (chart edges) with frequencies can be viewed with a further menu. In the chart browser, colors are used to display frequencies (between 0 and 1) estimated by the inside-outside algorithm. This allows properties shared across tree analyses to be checked at a glance; often grammar and estimation bugs can be detected without mouse operations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With very few exceptions (rules for coordination, S-rule), the rules do not have more than two daughters.", "mid_sen": "Grammar development is facilitated by a chart browser that permits a quick and efficient discovery of grammar bugs (Carroll, 1997a) . ", "after_sen": "Fig.  3 shows that the ambiguity in the chart is quite considerable even though grammar and corpus are restricted. "}
{"citeStart": 17, "citeEnd": 32, "citeStartToken": 17, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "Of course there are areas requiring further consideration. In contrast to humans, who seem to leap to conclusions based on incomplete evidence, our approach employs a conservative form of generalization, taking the disjunction of actually observed values only. While this has the advantage of not leading to overgeneralization, the requirement of having to encounter all subtypes in order to infer their common supertype is not realistic (sparse-data problem). In (2) sense_organ as the semantic type of the first argument ofperzipiert is only acquired because the simplified hierarchy in fig. 1 has nose and ear as its only subtypes. Here the work of Li & Abe (1995) who use the MDL principle to generalize over the slots of observed case frames might prove fruitful.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 has nose and ear as its only subtypes. ", "mid_sen": "Here the work of Li & Abe (1995) who use the MDL principle to generalize over the slots of observed case frames might prove fruitful.", "after_sen": "An important question is how to administrate alternative parses and their update hypotheses. "}
{"citeStart": 248, "citeEnd": 274, "citeStartToken": 248, "citeEndToken": 274, "sectionName": "UNKNOWN SECTION NAME", "string": "Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) ) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002) ), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) ) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. ", "mid_sen": "Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002) ), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003) .", "after_sen": "Our research focuses on semantic lexicon induction, which aims to generate lists of words that be-long to a given semantic class (e.g., lists of FISH or VEHICLE words). "}
{"citeStart": 295, "citeEnd": 312, "citeStartToken": 295, "citeEndToken": 312, "sectionName": "UNKNOWN SECTION NAME", "string": "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associated with each other under a common parent that captures the properties these verbs all share (Dang et al., 1998) . The highest nodes in the hierarchy are occupied by generalized PAR schemas which represent the basic predicate-argument structures for entire groups of subordinate actions. The lower nodes are occupied by progressively more speci c schemas that inherit information from the generalized PARs, and can be instantiated with arguments from natural language to represent a speci c action such as John hit the ball with his bat. The example in Figure 1 is a generalized PAR schema for contact ac-1 Objects and agents are stored in a hierarchy a n d have a n umber of properties associated with them. Properties of the objects may include their location and status. Agents have capabilities, such as the ability t o w alk or swim, and properties such as their strength and height. path duration motion force manner : MANNER 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The representation also allows for traditional statespace properties of actions, such a s applicability conditions and preparatory actions that have to be satised before the action can be executed, and termination conditions and post assertions which determine when an action is concluded and what changes it makes to the environment state.", "mid_sen": "We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associated with each other under a common parent that captures the properties these verbs all share (Dang et al., 1998) . ", "after_sen": "The highest nodes in the hierarchy are occupied by generalized PAR schemas which represent the basic predicate-argument structures for entire groups of subordinate actions. "}
{"citeStart": 78, "citeEnd": 99, "citeStartToken": 78, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010) , G-index (Egghe, 2006) , and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. ", "mid_sen": "Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010) , G-index (Egghe, 2006) , and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "after_sen": "For example, the number of papers published by a researcher only tells how productive she or he is. "}
{"citeStart": 53, "citeEnd": 77, "citeStartToken": 53, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . ", "mid_sen": "The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . ", "after_sen": "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. "}
{"citeStart": 109, "citeEnd": 122, "citeStartToken": 109, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Instead of feature based syntax trees and first-order logical forms we will adopt a simpler, monostratal representation that is more closely related to those found in dependency grammars (e.g. Hudson 1984). Dependency representations have been used in large scale qualitative machine translation systems, notably by McCord (1988) . The notion of a lexical 'head' of a phrase is central to these representations because they concentrate on relations between such lexical heads. In our case, the dependency representation is monostratal in that the relations may include ones normally classified as belonging to syntax, semantics or l)ragmatics.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Instead of feature based syntax trees and first-order logical forms we will adopt a simpler, monostratal representation that is more closely related to those found in dependency grammars (e.g. Hudson 1984). ", "mid_sen": "Dependency representations have been used in large scale qualitative machine translation systems, notably by McCord (1988) . ", "after_sen": "The notion of a lexical 'head' of a phrase is central to these representations because they concentrate on relations between such lexical heads. "}
{"citeStart": 54, "citeEnd": 74, "citeStartToken": 54, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "We consider both string-level features such as computing n-gram precision against a target-language corpus as well as several syntaxbased features. We parse each input sentence into a dependency tree and compared aspects of it against a large target-language dependency treebank. In addition to adapting the idea of Head Word Chains (Liu and Gildea, 2005) , we also compared the input sentence's argument structures against the treebank for certain syntactic categories.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We parse each input sentence into a dependency tree and compared aspects of it against a large target-language dependency treebank. ", "mid_sen": "In addition to adapting the idea of Head Word Chains (Liu and Gildea, 2005) , we also compared the input sentence's argument structures against the treebank for certain syntactic categories.", "after_sen": "Due to the large feature space to explore, we chose to work with support vector regression as the learning algorithm. "}
{"citeStart": 109, "citeEnd": 134, "citeStartToken": 109, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "A handful of problems that occurred during the conversion process were corrected manually. The first indicator of a problem was the presence of a possessive. This is unexpected, because possessives were already bracketed properly when CCGbank was originally created (Hockenmaier, 2003, §3.6.4) . Secondly, a non-flattened node should not be assigned a supertag that it did not already have. This is because, as described previously, a non-leaf node could dominate any kind of structure. Finally, we expect the lowest spanning node to cover only the NML or JJP bracket and one more constituent to the right. If it doesn't, because of unusual punctuation or an incorrect bracket, then it may be an error. In all these cases, which occur throughout the corpus, we manually analysed the derivation and fixed any errors that were observed. cases were flagged by this approach, or 1.90% of the 26,993 brackets converted to CCG. Table 1 shows the causes of these problems. The most common cause of errors was possessives, as the con-version process highlighted a number of instances where the original CCGbank analysis was incorrect. An example of this error can be seen in Figure 3(a) , where the possessive doesn't take any arguments. Instead, largest aid donor incorrectly modifies the NP one word at a time. The correct derivation after manual analysis is in (b).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first indicator of a problem was the presence of a possessive. ", "mid_sen": "This is unexpected, because possessives were already bracketed properly when CCGbank was originally created (Hockenmaier, 2003, §3.6.4) . ", "after_sen": "Secondly, a non-flattened node should not be assigned a supertag that it did not already have. "}
{"citeStart": 37, "citeEnd": 59, "citeStartToken": 37, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. ", "after_sen": "The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968) ."}
{"citeStart": 82, "citeEnd": 100, "citeStartToken": 82, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Keyword-based search engines have been one of the most highly utilized internet tools in recent years. Nevertheless, search performance remains unsatisfactory at most e-commerce sites (Hagen et al., 2000) . Librarians and search professionals have traditionally favored Boolean keyword search systems, which, when successful, return a small set of relevant hits. However, the success of these systems critically depends on the choice of the right keywords and the appropriate Boolean operators. As the population of search engine users has grown beyond a small dedicated search professional community and as these new users are less familiar with the contents they are searching, it has become harder for them to formulate successful keyword queries. To improve search performance, one can improve search engine accuracy with respect to fixed keyword queries, or provide the search engine with better queries, those more likely to retrieve good results. While there is much on-going work in the IR community on the former topic, we have taken the latter approach by providing a natural language search interface and automatically generating keyword queries that utilize advanced search features typically unused by end users. We believe that natural language questions are easier for users to construct than keyword queries, thus shifting the burden of optimal query formulation from the user to the system. Such questions also eliminate much of the ambiguity of keyword queries that often leads to poor results. Furthermore, the methodology we describe may be applied to different search engines with only minor modification.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Keyword-based search engines have been one of the most highly utilized internet tools in recent years. ", "mid_sen": "Nevertheless, search performance remains unsatisfactory at most e-commerce sites (Hagen et al., 2000) . ", "after_sen": "Librarians and search professionals have traditionally favored Boolean keyword search systems, which, when successful, return a small set of relevant hits. "}
{"citeStart": 98, "citeEnd": 118, "citeStartToken": 98, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Based on the pronunciation error data of learners of English as a second language as reported in (Swan and Smith, 2002) , we propose the use of what we will term pseudofeatures. The pseudo features in this study are same as in Tao et al. (2006) . Swan & Smith (2002) 's study covers 25 languages including Asian languages such as Thai, Korean, Chinese and Japanese, European languages such as German, Italian, French and Polish, and Middle East languages such as Arabic and Farsi. The substitution/insertion/deletion errors of phonemes were collected from this data. The following types of errors frequently occur in second language learners' speech production.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To capture these sound change patterns, additional features such as \"deletion of stop/fricative consonant in the coda position\" must be considered.", "mid_sen": "Based on the pronunciation error data of learners of English as a second language as reported in (Swan and Smith, 2002) , we propose the use of what we will term pseudofeatures. ", "after_sen": "The pseudo features in this study are same as in Tao et al. (2006) . "}
{"citeStart": 112, "citeEnd": 132, "citeStartToken": 112, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser (Collins, 1997; Collins, 1999; Charniak, 2000) . These parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser (Johnson et al., 1999; Collins and Duffy, 2005; Charniak and Johnson, 2005) . Alternatively, discriminative models can be used to search the complete space of possible parses (Taskar et al., 2004; McDonald et al., 2005) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mainstream approaches in statistical parsing are based on nondeterministic parsing techniques, usually employing some kind of dynamic programming, in combination with generative probabilistic models that provide an n-best ranking of the set of candidate analyses derived by the parser (Collins, 1997; Collins, 1999; Charniak, 2000) . ", "mid_sen": "These parsers can be enhanced by using a discriminative model, which reranks the analyses output by the parser (Johnson et al., 1999; Collins and Duffy, 2005; Charniak and Johnson, 2005) . ", "after_sen": "Alternatively, discriminative models can be used to search the complete space of possible parses (Taskar et al., 2004; McDonald et al., 2005) ."}
{"citeStart": 137, "citeEnd": 161, "citeStartToken": 137, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.", "mid_sen": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "after_sen": "We have a different perspective than these lines of inquiry. "}
{"citeStart": 61, "citeEnd": 73, "citeStartToken": 61, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "The bilingual corpus contains literal translations, to allow the extrapolation of mapping information from SL to TL, though this may affect the translation quality. The Phrase aligner module (PAM) performs offline SL -TL word and phrase alignment within this corpus. PAM serves as a language-independent method for mapping corresponding terms within a language pair, by circumventing the problem of achieving compatibility between the outputs of two different parsers, one for the SL and one for the TL. PAM relies on a single parser for the one language and generates an appropriate phrasing model for the other language in an automated manner. The phrases are assumed to be flat and linguistically valid. As a parser, any available tool may be used (the TreeTagger (Schmid, 1994) is used in the present implementation for English). PAM processes a bilingual corpus of SL -TL sentence pairs, taking into account the parsing information in one language (in the current implementation the TL side) and making use of a bilingual lexicon and information on potential phrase heads; the output being the bilingual corpus aligned at word, phrase and clause level. Thus, at a phrasal level, the PAM output indicates how an SL structure is transformed into the TL. For instance, based on a sentence pair from the parallel corpus, the SL sentence with structure A-B-C-D is transformed into A'-C'-D'-B', where X is a phrase in SL and X' is a phrase in TL. Further PAM details are reported in Tambouratzis et al. (2011).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The phrases are assumed to be flat and linguistically valid. ", "mid_sen": "As a parser, any available tool may be used (the TreeTagger (Schmid, 1994) is used in the present implementation for English). ", "after_sen": "PAM processes a bilingual corpus of SL -TL sentence pairs, taking into account the parsing information in one language (in the current implementation the TL side) and making use of a bilingual lexicon and information on potential phrase heads; the output being the bilingual corpus aligned at word, phrase and clause level. "}
{"citeStart": 32, "citeEnd": 46, "citeStartToken": 32, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Purely as a method of estimation as well, the superiority of MI)L over MLE is supported by convincing theoretical findings (c.f. ( Barton and Cover, 1991; Yamanishi, 1992) ). For instance, the speed of convergence of the models selected by MDL to the true model is known to be near optiinal. (The models selected by MDL converge to the true model approximately at the rate of 1/s where s is the nmnber of parameters in the true model, whereas for MLE the rate is l/t, where t is the size of the domain, or in our context, the total number of elements of N\" x V.) 'Consistency' is another desirable property of MDL, which is not shared by MLE. That is, the number of parame- (Rissanen, 1989) . Both of these prol>erties of MI)I, ar~ Oml>irically w'ri/ied in our present (;Ollt(?x[,, as will be show,: in t.ho t:(,xl section. In particular, we haw~ compared l,h(' p(u'forn:a.nc0 of employing an M1)L-based simula.ted annealing against that of one 1)ascd on M[,I\", ill hierarchical woM clust.c'ring.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(The models selected by MDL converge to the true model approximately at the rate of 1/s where s is the nmnber of parameters in the true model, whereas for MLE the rate is l/t, where t is the size of the domain, or in our context, the total number of elements of N\" x V.) 'Consistency' is another desirable property of MDL, which is not shared by MLE. ", "mid_sen": "That is, the number of parame- (Rissanen, 1989) . ", "after_sen": "Both of these prol>erties of MI)I, ar~ Oml>irically w'ri/ied in our present (;Ollt(?x[,, as will be show,: in t.ho t:(,xl section. "}
{"citeStart": 14, "citeEnd": 27, "citeStartToken": 14, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "Graph (a) in Figure 3 shows a precision curve for each method. Sup and Uns respectively indicate the supervised and unsupervised versions of our method. The figure indicates that Sup outperforms all the others and shows a high precision rate of about 94% at the top 1,000. Remember that this is the result of using 100,000 definition sentence pairs. Thus, we estimate that Sup can extract about 300,000 paraphrase pairs with a precision rate of about 94%, if we use all 29,661,812 definition sentence pairs that we acquired. Furthermore, we measured precision after trivial paraphrase pairs were discarded from the evaluation samples of each method. A candidate phrase pair (p 1 , p 2 ) is regarded as trivial if the pronunciation is the same between p 1 and p 2 , 10 or all of the content words contained in p 1 are the same as those of p 2 . Graph (b) gives a precision curve for each method. Again, Sup outperforms the others too, and maintains a precision rate of about 90% until the top 1,000. These results support our claim II. The upper half of Table 2 shows the number of extracted paraphrases with/without trivial pairs for each method. 11 Sup and Uns extracted many more paraphrases. It is noteworthy that Sup performed the best in terms of both precision rate and the number of extracted paraphrases. Table 3 shows examples of correct and incorrect outputs of Sup. As the examples indicate, many of the extracted paraphrases are not specific to definition sentences and seem very reusable. However, there are few paraphrases involving metaphors or idioms in the outputs due to the nature of definition sentences. In this regard, we do not claim that our method is almighty. We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this regard, we do not claim that our method is almighty. ", "mid_sen": "We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.", "after_sen": "In graphs (a) and (b), the precision of the SMT method goes up as rank goes down. "}
{"citeStart": 108, "citeEnd": 119, "citeStartToken": 108, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "In recent SENSEVAL-3 evaluations, the most successful approaches for all words word sense disambiguation relied on information drawn from annotated corpora. The system developed by (Decadt et al., 2004) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection. A separate \"word expert\" is learned for each ambiguous word, using a concatenated corpus of English sense- (Yuret, 2004) , which combines two Naive Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word. The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system developed by (Decadt et al., 2004) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection. ", "mid_sen": "A separate \"word expert\" is learned for each ambiguous word, using a concatenated corpus of English sense- (Yuret, 2004) , which combines two Naive Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word. ", "after_sen": "The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%."}
{"citeStart": 144, "citeEnd": 164, "citeStartToken": 144, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . ", "mid_sen": "In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) .", "after_sen": "Besides, an increasing concern in current projects is that of linguistic relevance of the analysis t)erformed by the grammar correction system. "}
{"citeStart": 39, "citeEnd": 50, "citeStartToken": 39, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975 Grice ( , 1978 , under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. ", "mid_sen": "Secondly, the cooperative principle of Grice (1975 Grice ( , 1978 , under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. ", "after_sen": "Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. "}
{"citeStart": 156, "citeEnd": 169, "citeStartToken": 156, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary disambiguation (Palmer et al. 1997) , parsing (Magerman 1995) and word segmentation (Meknavin et al. 1997) . We employ the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm for word extraction.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary disambiguation (Palmer et al. 1997) , parsing (Magerman 1995) and word segmentation (Meknavin et al. 1997) . ", "after_sen": "We employ the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm for word extraction."}
{"citeStart": 62, "citeEnd": 88, "citeStartToken": 62, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . ", "mid_sen": "For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) .", "after_sen": ""}
{"citeStart": 78, "citeEnd": 92, "citeStartToken": 78, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "3.1 Ontology Various authors (including Link, 1983 , Bach, 1986 , Krifka, 1989 , Eberle, 1990 have proposed modeltheoretic treatments in which a parallel ontological distinction is made between substances and things, processes and events, etc. A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount. As such, the approach developed here may be seen as building upon the work of Carlson (1977) and his successors; it also represents one way to further formalize the intuitions found in Moens and Steedman (1988) and Jackendoff (1991) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A similarly parallel distinction is employed here, but in a rather different way: unlike the above treatments, the present account models substances, processes, and other such entities as abstract kinds whose realizations vary in amount. ", "mid_sen": "As such, the approach developed here may be seen as building upon the work of Carlson (1977) and his successors; it also represents one way to further formalize the intuitions found in Moens and Steedman (1988) and Jackendoff (1991) .", "after_sen": "Following Schubert and Pelletier (1987) , the present account distinguishes individuals from kinds, but not from stages or quantities. "}
{"citeStart": 72, "citeEnd": 82, "citeStartToken": 72, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "log(230S/5487056) s(red) = --1og(1/5487056) --0.500955, -1og(106064/5487056) s(and) = --1og(1/5487056) = 0.254294. We estimated the significance of the words excluded from the word list [West, 1953] at the average significance of their word classes. This interpolation virtually enlarged West's 5,000,000-word corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "log(230S/5487056) s(red) = --1og(1/5487056) --0.500955, -1og(106064/5487056) s(and) = --1og(1/5487056) = 0.254294. ", "mid_sen": "We estimated the significance of the words excluded from the word list [West, 1953] at the average significance of their word classes. ", "after_sen": "This interpolation virtually enlarged West's 5,000,000-word corpus."}
{"citeStart": 54, "citeEnd": 75, "citeStartToken": 54, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning. In particular, we consider Conditional Random Fields (Lafferty et al., 2001 ) and a variation of AutoSlog (Riloff, 1996a) . CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003) , Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a) . While CRFs treat source identification as a sequence tagging task, Au-toSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence. We hypothesized that a combination of the two techniques would perform better than either one alone.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We investigate two very different learning-based methods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning. ", "mid_sen": "In particular, we consider Conditional Random Fields (Lafferty et al., 2001 ) and a variation of AutoSlog (Riloff, 1996a) . CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003) , Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a) . ", "after_sen": "While CRFs treat source identification as a sequence tagging task, Au-toSlog views the problem as a pattern-matching task, acquiring symbolic patterns that rely on both the syntax and lexical semantics of a sentence. "}
{"citeStart": 196, "citeEnd": 205, "citeStartToken": 196, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999) , as well as the general vector distance given by cosine (cos). These are the measures (aside from SPD) that performed best in our pilot experiments.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using $ f to determine whether to generalize an estimate to a parent node in the hierarchy.", "mid_sen": "We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999) , as well as the general vector distance given by cosine (cos). ", "after_sen": "These are the measures (aside from SPD) that performed best in our pilot experiments."}
{"citeStart": 184, "citeEnd": 199, "citeStartToken": 184, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. In a first stage, only MEDLINE abstracts are used (Yang et al., 2004) , later other-anaphora, a very specific sub-task, are investigated using full paper content (Chen et al., 2008) . Gasperin (2009) presents a full annotation of anaphora and coreference in biomedical text, but only noun phrases referring to biomedical entities are considered. On the basis of this annotation, she implements a probabilistic anaphora resolution system. In contrast, Cohen et al. (2010) build a corpus of 97 full-text journal articles in the biomedical domain where every co-referring noun phrase is annotated (CRAFT -Colorado Richly Annotated Full Text). Their annotation guidelines follow those of the OntoNotes project (Hovy et al., 2006) , adapted to the biomedical domain. OntoNotes itself is a text corpus of approx. one million words from mainly news texts (newswire, magazines, broadcast conversations, web pages). It also contains general anaphoric coreference annotations (Pradhan et al., 2007) : events and (like in our annotation) unlimited noun phrase entity types. Kim and Webber (2006) investigate a special aspect, citation sentences where a pronoun such as \"they\" refers to a previous citation. The study is performed on astronomy journal articles and a maximum-entropy classifier is trained. Kaplan et al. (2009) investigate coreferences and citations as well, but only at a very small scale (4 articles from the Computational Linguistics journal). They focus on so-called c-sites which are the sentences following a citation that also refer to the same paper (typically by anaphora). The authors train a specific coreference model for this phenomenon. They show that exploitation of coreference chains improves the extraction of citation contexts which they then use for research paper summarization.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. ", "mid_sen": "In a first stage, only MEDLINE abstracts are used (Yang et al., 2004) , later other-anaphora, a very specific sub-task, are investigated using full paper content (Chen et al., 2008) . Gasperin (2009) presents a full annotation of anaphora and coreference in biomedical text, but only noun phrases referring to biomedical entities are considered. ", "after_sen": "On the basis of this annotation, she implements a probabilistic anaphora resolution system. "}
{"citeStart": 112, "citeEnd": 131, "citeStartToken": 112, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of this paper is to give a detailed account of the techniques used in TnT. ", "mid_sen": "Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . ", "after_sen": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . "}
{"citeStart": 40, "citeEnd": 64, "citeStartToken": 40, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "The Dutch translation was then used to reproduce the English experiments by Miller & Charles and Rubenstein & Goodenough. Since the instructions concerning Similarity of meaning are un-clear in the original experiments, we reproduced each experiment with three different kinds of instructions, which are stressing similarity aspects, stressing relatedness aspects, and no instructions. These instructions were explained to the participants by an example of each value that could be assigned to a word pair and a general description. The WordSimilarity-353 Test Collection (Finkelstein et al., 2002) was used to obtain example word pairs for each value that could be assigned to a word pair. This dataset contains two sets of English word pairs with similarity scores assigned by humans. The first set of this collection contains 153 word pairs, with their scores, from 0 to 10, assigned by 13 subjects. In addition, participants were asked to rate the word pairs on similarity. From this set, examples were chosen stressing similarity aspects. The second set contains 200 word pairs, with human-assigned scores, from 0 to 10, by 16 subjects. In this case, participants were asked to rate the word pairs based on relatedness. From this set, examples were chosen stressing relatedness aspects. Each word pair that was chosen to serve as an example word pair was translated into Dutch. For stressing similarity, participants were asked to indicate to what degree two words could replace each other. For example, if two words were interchangeable, they were told to assign the highest value. They were instructed to assign a lower value to a word pair like aardappelmesje 'potato peeler' & mes 'knife', because mes 'knife' can be used instead of aardappelmesje 'potato peeler', but not the other way around. For stressing relatedness aspects, participants were asked to focus on how likely it is that words occur in the same situation. For example, it is very likely that computer 'computer' & internet 'internet' occur in the same situation together, whereas this is less likely the case for komkommer 'cucumber' & professor 'professor'. Finally for the no instructions case, the interpretation was left to the participant, except that we indicated that synonyms resulted in the highest score. Combining the two English experiments with the three different kinds of instructions thus yielded six different sets. For convenience, we will use abbreviations to refer to the six experiments. The abbreviation Mc will be used for the translation of the dataset by Miller & Charles. Rg will be used for the translation of the dataset by Rubenstein & Goodenough. In addition, the three kinds of instructions will be abbreviated in the following way: No for no instruction, Sim for similarity, and Rel for relatedness. By combining the abbreviations, we can refer to each of the six experiments. The difference between the results of the different instructions turned out to be neither significant, nor systematic. We thus assume that the instructions have not been effective to override the basic intuition of the participants.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These instructions were explained to the participants by an example of each value that could be assigned to a word pair and a general description. ", "mid_sen": "The WordSimilarity-353 Test Collection (Finkelstein et al., 2002) was used to obtain example word pairs for each value that could be assigned to a word pair. ", "after_sen": "This dataset contains two sets of English word pairs with similarity scores assigned by humans. "}
{"citeStart": 123, "citeEnd": 139, "citeStartToken": 123, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002) . For our experiments, we use the SVM implementation of (Chang and Lin, 2001) as it is able to work on multiclass problems to output the classification probability for each class. Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002) . For local collocations, we use 3 features, w −1 w +1 , w −1 , and w +1 , where w −1 (w +1 ) is the token immediately to the left (right) of the current ambiguous word occurrence w. For parts-of-speech, we use 3 features, P −1 , P 0 , and P +1 , where P 0 is the POS of w, and P −1 (P +1 ) is the POS of w −1 (w +1 ). For surrounding words, we consider all unigrams (single words) in the surrounding context of w. These unigrams can be in a different sentence from w. We perform feature selection on surrounding words by including a unigram only if it occurs 3 or more times in some sense of w in the training data.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002) . ", "after_sen": "For our experiments, we use the SVM implementation of (Chang and Lin, 2001) as it is able to work on multiclass problems to output the classification probability for each class. "}
{"citeStart": 57, "citeEnd": 80, "citeStartToken": 57, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "What distinguishes our work from work on subjectivity in other fields is that we focus on (1) automatically learning knowledge from corpora, (2) automatically performing contextual disambiguation, and (3) using knowledge of subjectivity in NLP applications. This article expands and integrates the work reported in Wiebe and Wilson (2002) , , and Wiebe (2000) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "What distinguishes our work from work on subjectivity in other fields is that we focus on (1) automatically learning knowledge from corpora, (2) automatically performing contextual disambiguation, and (3) using knowledge of subjectivity in NLP applications. ", "mid_sen": "This article expands and integrates the work reported in Wiebe and Wilson (2002) , , and Wiebe (2000) .", "after_sen": "Previous work in NLP on the same or related tasks includes sentence-level and document-level subjectivity classifications. "}
{"citeStart": 171, "citeEnd": 185, "citeStartToken": 171, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "In 2002, a detailed annotation scheme was developed for a government-sponsored project. We only mention aspects of the annotation scheme relevant to this paper. The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982) . The goal is to identify and characterize expressions of private states in a sentence. Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al., 1985) . For example, in sentence (1) the writer is expressing a negative evaluation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We only mention aspects of the annotation scheme relevant to this paper. ", "mid_sen": "The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982) . ", "after_sen": "The goal is to identify and characterize expressions of private states in a sentence. "}
{"citeStart": 103, "citeEnd": 104, "citeStartToken": 103, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Hindle and Rooth also used collocational information to solve ambiguities of pp-attachment in English [5] . Ambiguities arc resolved by comparing the strength of associativity between a preposition and a verb and the preposition and a nominal head. The strength of iLssociativity is calculated on the basis of occurrence frequencies of word collocations in a corpus. Besides the word collocations information, we also use semantic knowledge, nanlely~ a thesaurus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each possible structure of a compound noun, the preference is calculated based on this colloo cational information and the structure with the highest score wins.", "mid_sen": "Hindle and Rooth also used collocational information to solve ambiguities of pp-attachment in English [5] . ", "after_sen": "Ambiguities arc resolved by comparing the strength of associativity between a preposition and a verb and the preposition and a nominal head. "}
{"citeStart": 225, "citeEnd": 247, "citeStartToken": 225, "citeEndToken": 247, "sectionName": "UNKNOWN SECTION NAME", "string": "The right-side context of a non-terminal category -the probability of generating a category to the right of the current constituent's category -corresponds directly to the category transitions used for the HMM supertagger of Garrette et al. (2014) . Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to encourage our model to choose trees in which the constituent labels \"fit\" into their supertag contexts, we want to bias our context parameters toward context categories that are combinable with the constituent label.", "mid_sen": "The right-side context of a non-terminal category -the probability of generating a category to the right of the current constituent's category -corresponds directly to the category transitions used for the HMM supertagger of Garrette et al. (2014) . ", "after_sen": "Thus, the right-side context prior mean θ RCTX-0 t can be biased in exactly the same way as the HMM supertagger's transitions: toward context supertags that connect to the constituent label."}
{"citeStart": 113, "citeEnd": 128, "citeStartToken": 113, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "The close link between restricted forms of nonprojective dependency languages and mildly contextsensitive grammar formalisms provides a promising starting point for future work. On the practical side, it should allow us to benefit from the experience in building parsers for mildly context-sensitive formalisms when addressing the task of efficient nonprojective dependency parsing, at least in the frame-work of grammar-driven parsing. This may eventually lead to a better trade-off between structural flexibility and computational efficiency than that obtained with current systems. On a more theoretical level, our results provide a basis for comparing a variety of formally rather distinct grammar formalisms with respect to the sets of dependency structures that they can generate. Such a comparison may be empirically more adequate than one based on traditional notions of generative capacity (Kallmeyer, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On a more theoretical level, our results provide a basis for comparing a variety of formally rather distinct grammar formalisms with respect to the sets of dependency structures that they can generate. ", "mid_sen": "Such a comparison may be empirically more adequate than one based on traditional notions of generative capacity (Kallmeyer, 2006) .", "after_sen": ""}
{"citeStart": 112, "citeEnd": 127, "citeStartToken": 112, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "a. For all the training data, the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal (1999) . The substitution/insertion/deletion cost for the string alignment algorithm is based on the baseline cost from (Tao et al, 2006) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For all the training data, the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal (1999) . ", "mid_sen": "The substitution/insertion/deletion cost for the string alignment algorithm is based on the baseline cost from (Tao et al, 2006) .", "after_sen": "b. "}
{"citeStart": 1, "citeEnd": 21, "citeStartToken": 1, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "The work by Ramshaw and Marcus has inspired three other groups to build chunking algorithms. (Argamon et al., 1998) introduce Memory-Based Sequence Learning and use it for different chunking experiments. Their algorithm stores sequences of POS tags with chunk brackets and uses this information for recognizing chunks in unseen data. It performed slightly worse on baseNP recognition than the (Ramshaw and Marcus, 1995) experiments (Fz=1=91.6). (Cardie and Pierce, 1998) uses a related method but they only store POS tag sequences forming complete baseNPs. These sequences were applied to unseen tagged data aIter which post-processing repair rules were used for fixing some frequent errors. This approach performs worse than othe.r reported approaches (Fo=I =90.9).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The work by Ramshaw and Marcus has inspired three other groups to build chunking algorithms. ", "mid_sen": "(Argamon et al., 1998) introduce Memory-Based Sequence Learning and use it for different chunking experiments. ", "after_sen": "Their algorithm stores sequences of POS tags with chunk brackets and uses this information for recognizing chunks in unseen data. "}
{"citeStart": 63, "citeEnd": 83, "citeStartToken": 63, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "The results of ALLiS, c5.0, MBSL and SNoW have been converted to the O and the C repre-Detailed results of our experiments are available on http: lcg-www.uia.ac.be ~erikt npcombi 5 The retagging was necessary to assure that the performance rates obtained here would be similar to rates obtained for texts for which n o T reebank POS tags are available. sentation. Together with the bracket representations of the other three techniques, this gave us a total of seven O results and seven C results. These two data streams have been combined with the combination techniques described in section 2.4. After this, we built baseNPs from the O and C results of each combination technique, like described in section 2.2. The bracket accuracies and the F =1 scores for test data can be found in Table 2 . All combinations improve the results of the best individual classi er. The best results were obtained with a memory-based stacked classier. This is di erent from the combination results presented in Van Halteren et al. 1998 Argamon et al. 1999 -91.6 91.6 91.6 Table 3: The overall performance of the majority voting combination of our best ve systems selected on tuning data performance applied to the standard data set put forward by Ramshaw and Marcus 1995 together with an overview of earlier work. The accuracy scores indicate how often a word was classi ed correctly with the representation used O, C or IOB1. The combined system outperforms all earlier reported results for this data set.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The best results were obtained with a memory-based stacked classier. ", "mid_sen": "This is di erent from the combination results presented in Van Halteren et al. 1998 Argamon et al. 1999 -91.6 91.6 91.6 Table 3: The overall performance of the majority voting combination of our best ve systems selected on tuning data performance applied to the standard data set put forward by Ramshaw and Marcus 1995 together with an overview of earlier work. ", "after_sen": "The accuracy scores indicate how often a word was classi ed correctly with the representation used O, C or IOB1. "}
{"citeStart": 146, "citeEnd": 161, "citeStartToken": 146, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "The results presented in this paper demonstrate that the application of linguistic information from automaticallylearned mathematical models can significantly enhance both the precision and the recall of pattern-based hyponymy extraction techniques. Using a graph model of noun similarity we were able to obtain an almost fivefold improvement in recall, though the precision of this technique is clearly affected by the correctness of the \"seed-relationships\" used. Using LSA filtering we eliminated spurious relations extracted by the original pattern method, reducing errors by 30%. Such filtering also eliminated spurious relations learned using the graph model that were the result of lexical ambiguity and of seed hyponymy relations inappropriate for the technique, reducing errors by 33%. This paper suggests many possibilities for future work. First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in (Caraballo, 1999) , perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node. We are considering how to extend our techniques to such a task.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper suggests many possibilities for future work. ", "mid_sen": "First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in (Caraballo, 1999) , perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node. ", "after_sen": "We are considering how to extend our techniques to such a task."}
{"citeStart": 189, "citeEnd": 212, "citeStartToken": 189, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant.", "mid_sen": "The present paper is concerned with tagging languages and sublanguages for which no a priori knowledge about grammatical categories is available, a situation that occurs often in practice (Brill and Marcus, 1992a) .", "after_sen": "Several researchers have worked on learning grammatical properties of words. "}
{"citeStart": 58, "citeEnd": 84, "citeStartToken": 58, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Dataset The dataset is built using the same guidelines as Mitchell and Lapata (2008) , using transi-tive verbs obtained from CELEX 1 paired with subjects and objects. We first picked 10 transitive verbs from the most frequent verbs of the BNC. For each verb, two different non-overlapping meanings were retrieved, by using the JCN (Jiang Conrath) information content synonymity measure of WordNet to select maximally different synsets. For instance for 'meet' we obtained 'visit' and 'satisfy'. For each original verb, ten sentences containing that verb with the same role were retrieved from the BNC. Examples of such sentences are 'the system met the criterion' and 'the child met the house'. For each such sentence, we generated two other related sentences by substituting their verbs by each of their two synonyms. For instance, we obtained 'the system satisfied the criterion' and 'the system visited the criterion' for the first meaning and 'the child satisfied the house' and 'the child visited the house' for the second meaning . This procedure provided us with 200 pairs of sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This degree decreases for the pair 'the child met the house' and 'the child satisfied the house'.", "mid_sen": "Dataset The dataset is built using the same guidelines as Mitchell and Lapata (2008) , using transi-tive verbs obtained from CELEX 1 paired with subjects and objects. ", "after_sen": "We first picked 10 transitive verbs from the most frequent verbs of the BNC. "}
{"citeStart": 51, "citeEnd": 79, "citeStartToken": 51, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "The traditional agglomerative technique for clustering has been described above. There is some 3That is, for each site i, one considers all other pairs of sites, j and k, and asks whether the linguistic difference between i and j is greater or less than that between i and k. One counts 1 if the answer is the same for both distance matrices, -1 if it is different. //'c is the average of these numbers. variation in how the distance between two clusters is measured. For this study I used the average distance between all pairs of elements that are in different clusters. I compared agglomeration to a top-down method that Kaufman and Rousseeuw (1990) call partitioning around medoids. This model reduces the 0(2 Jr) intractability of topdown approaches discussed above by dramatically reducing the number of binary partitions that are considered. Here one seeks to divide the sites into two groups by finding the two representative sites (the medoids) around which all the other sites cluster in such a way as to give the least average distance between the sites and their representatives. This is therefore a O(N 3) algorithm, comparable in efficiency to agglomeration. The process was repeated recursively on each dialect.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For this study I used the average distance between all pairs of elements that are in different clusters. ", "mid_sen": "I compared agglomeration to a top-down method that Kaufman and Rousseeuw (1990) call partitioning around medoids. ", "after_sen": "This model reduces the 0(2 Jr) intractability of topdown approaches discussed above by dramatically reducing the number of binary partitions that are considered. "}
{"citeStart": 39, "citeEnd": 61, "citeStartToken": 39, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "The context vectors used so far only capture information about distributional interactions with the 250 most frequent words. Intuitively, it should be possible to gain accuracy in tag induction by using information from more words. One way to do this is to let the right context vector record which classes of left conte~t vectors occur to the right of a word. The rationale is that words with similar left context characterize words to their right in a similar way. For example, \"seemed\" and \"would\" have similar left contexts, and they characterize the right contexts of \"he\" and \"the firefighter\" as potentially containing an inflected verb form. Rather than having separate entries in its right context vector for \"seemed\", \"would\", and \"likes\", a word like \"he\" can now be characterized by a generalized entry for \"inflected verb form occurs frequently to my right\". This proposal was implemented by applying a singular value decomposition to the 47025-by-250 matrix of left context vectors and clustering the resulting context vectors into 250 classes. A generalized right context vector v for word w was then formed by counting how often words from these 250 classes occurred to the right of w. Entry vi counts the number of times that a word from class i occurs to the right of w in the corpus (as opposed to the number of times that the word with frequency rank i occurs to the right of w). Generalized left context vectors were derived by an analogous procedure using word-based right context vectors. Note that the information about left and right is kept separate in this computation. This differs from previous approaches (Finch and Chater, 1992; Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. There are arguably fewer different types of right syntactic contexts than types of syntactic categories. For example, transitive verbs and prepositions belong to different syntactic categories, but their right contexts are virtually identical in that they require a noun phrase. This generalization could not be exploited if left and right context were not treated separately.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that the information about left and right is kept separate in this computation. ", "mid_sen": "This differs from previous approaches (Finch and Chater, 1992; Schfitze, 1993) in which left and right context vectors of a word are always used in one concatenated vector. ", "after_sen": "There are arguably fewer different types of right syntactic contexts than types of syntactic categories. "}
{"citeStart": 78, "citeEnd": 93, "citeStartToken": 78, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "The three main factors in this algorithm are (a) accessible text regions, (b) semantic consistency, and (c) dynamic syntactic preference. The algorithm is invoked for each sentence after the earlier finite-state transduction phases have determined the best sequence(s) of nominal and verbal expressions. Crucially, each nominal expression is associated with a set of template data objects that record various linguistic and textual attributes of the referring expressions contained in it. These data objects are similar to discourse referents in discourse semantics (Karttunen, 1976; Kamp, 1981; Heim, 1982; Kamp and Reyle, 1993) 2Other referential relationships such as subset and partwhole did not reach sufficiently reliable interannotator agreements. Only identity of reference had a sufficiently high agreement rate (about 85%) between two human annotators.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Crucially, each nominal expression is associated with a set of template data objects that record various linguistic and textual attributes of the referring expressions contained in it. ", "mid_sen": "These data objects are similar to discourse referents in discourse semantics (Karttunen, 1976; Kamp, 1981; Heim, 1982; Kamp and Reyle, 1993) 2Other referential relationships such as subset and partwhole did not reach sufficiently reliable interannotator agreements. ", "after_sen": "Only identity of reference had a sufficiently high agreement rate (about 85%) between two human annotators."}
{"citeStart": 318, "citeEnd": 331, "citeStartToken": 318, "citeEndToken": 331, "sectionName": "UNKNOWN SECTION NAME", "string": "All rules and lexieal entries in the CLE are compiled to a form that allows normal Prolog unification to be used for category matching at run time. The same compiled forms are used for analysis and generation, but are indexed differently. Each feature for a major category is assigned a unique position in the compiled Prolog term, and features for which finite value sets have been specified are compiled into vectors in a form that allows boolean expressions, involving negation as well as conjunction and disjunction, to be conjoined by unification (see Mellish, 1988; Alshawi, 1992, pp46-48) . The compilation of morphological information is motivated by the nature of the task and of the languages to be handled. As discussed in Section 1, we expect the number of affix combinations to be limited, but the lexicon is not necessarily known in advance. Morphophonological interactions may be quite complex, and the purpose of morphological processing is to derive syntactic and semantic analyses from words and vice versa for the purpose of full NLP. Reasonably quick compilation is required, and run-time speed need only be moderate.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The same compiled forms are used for analysis and generation, but are indexed differently. ", "mid_sen": "Each feature for a major category is assigned a unique position in the compiled Prolog term, and features for which finite value sets have been specified are compiled into vectors in a form that allows boolean expressions, involving negation as well as conjunction and disjunction, to be conjoined by unification (see Mellish, 1988; Alshawi, 1992, pp46-48) . ", "after_sen": "The compilation of morphological information is motivated by the nature of the task and of the languages to be handled. "}
{"citeStart": 68, "citeEnd": 85, "citeStartToken": 68, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "Given a large sample of a genre, instrumentation allows you to determine the likely constructions of that genre. Eliminating unused disjuncts allows faster Table 2 : Corpora used for adaptation parsing due to a smaller grammar. An experiment was conducted with several corpora as detailed in Table 2 . There was some e ort to cover the corpus HC-DE, but no grammar development based on the other corpora. The NEWS-SC corpus is part the corpus of verb-nal sentences used by Beil et al., 1999 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There was some e ort to cover the corpus HC-DE, but no grammar development based on the other corpora. ", "mid_sen": "The NEWS-SC corpus is part the corpus of verb-nal sentences used by Beil et al., 1999 .", "after_sen": "A training set of 1000 sentences from each corpus was parsed with an instrumented base grammar. "}
{"citeStart": 74, "citeEnd": 86, "citeStartToken": 74, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "hl otrr model, each agerlt associates a numeric confidence value with each of tile attributes in the relSrring expression, and by composing these, computes a level of confidence in the adequacy of the complete referring expression plan that can be interpreted as ranging from low confidence to high confidence. The present composition function is simple addition, but one could envision more complex systems to compute confidence, such as an algebra of confidence or a non-numeric system. If the overall confidence value exceeds some set value, the agent's confidence threshold, then the agent believes the plan is adequate. That is, if the agent is the initiator, she believes that the other will be able to understand the reference; if the agent is the other, he believes that he has understood the reference. Now, the confidence value of each attribute is equivalent to its salience within the context of the referring expression. Salience, for our purposes in directiongiving, is primarily visual prominence, but can also involve identifiability, familiarity, and functional importance (Devlin, 1976; Lynch, 1960) . One approach is to encode the salient properties in a static hierarchy as Davis (1989) , and Reiter and Dale (1992) have done. I But, ideally, salience should depend on the context sun-ounding the referent. For example, the height of a tall building would normally be salient, but not if it were surrounded by other tall buildings. This computation Would be quite complex, so we have adopted a middle ground between the simple contextindependent approaches, and a full-blown contextual analysis. The middle ground involves taking the type of object into account when choosing attributes and landmarks that relate to it. For example, height and architectural style can be very salient features for describing a building, but not for describing an intersection, for which having a sign or traffic lights is important. This approach still allows us to encode salience in a hierarchy, but it is dependent on the referent. Table 1 shows an example of a simple salience hierarchy that an agent might have. The hierarchy is actually a set of partial orderings of attributes, represented by lambda expressions, indexed by object type. In the table, the confidence value of using architectural style to describe a building is 4. The confidence value of a tall building is 3, and so this attribute is less salient than architectural style. The other rows (for describing intersections) follow similarly. 2 Each agent has his own beliefs about salience. It is the difference in their beliefs that leads to the necessity for collaboration on reference. Ideally, the initiator should construct referring expressions with the recipients' (believed) beliefs about salience in mind, but we have chosen to avoid this complexity by making the simplifying assumption that the initiator is an expert IThese models assmne that all agents have identical beliefs, which is clearly insufficient for modeling collaborative dialogue.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Salience, for our purposes in directiongiving, is primarily visual prominence, but can also involve identifiability, familiarity, and functional importance (Devlin, 1976; Lynch, 1960) . ", "mid_sen": "One approach is to encode the salient properties in a static hierarchy as Davis (1989) , and Reiter and Dale (1992) have done. ", "after_sen": "I But, ideally, salience should depend on the context sun-ounding the referent. "}
{"citeStart": 69, "citeEnd": 82, "citeStartToken": 69, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "1Spivey-Knowlton et al. reported 3 experiments. One showed effects before the end of a word when there was no other appropriate word with the same initial phonology. Another showed on-line effects from adjectives and determiners during noun phrase processing. constituency, so that an initial fragment of a sentence, such as John likes, can be treated as a constituent, and hence be assigned a type and a semantics. This approach is exemplified by Combinatory Categorial Grammar, CCG (Steedman 1991) , which takes a basic CG with just application, and adds various new ways of combining elements together 2. Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence. The alternative approach, exemplified by the work of Stabler on top-down parsing (Stabler 1991) , and Pulman on left-corner parsing (Pulman 1986 ) is to associate a semantics directly with the partial structures formed during a topdown or left-corner parse. For example, a syntax tree missing a noun phrase, such as the following", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "constituency, so that an initial fragment of a sentence, such as John likes, can be treated as a constituent, and hence be assigned a type and a semantics. ", "mid_sen": "This approach is exemplified by Combinatory Categorial Grammar, CCG (Steedman 1991) , which takes a basic CG with just application, and adds various new ways of combining elements together 2. ", "after_sen": "Incremental interpretation can then be achieved using a standard bottom-up shift reduce parser, working from left to right along the sentence. "}
{"citeStart": 109, "citeEnd": 122, "citeStartToken": 109, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "It is interesting to compare this analysis with that described in Dalrymple, Shieber, and Pereira (1991) and Pereira (1990 Pereira ( , 1991 . Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise. At a later stage the quantifier assumption is \"discharged,\" capturing all occurrences of the free variable. Thus their analysis of something like every manager disappeared would proceed as follows:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "It is interesting to compare this analysis with that described in Dalrymple, Shieber, and Pereira (1991) and Pereira (1990 Pereira ( , 1991 . ", "after_sen": "Recall that in their treatment, quantified noun phrases are treated in two stages: firstly, what they call a \"free variable\" of type e is introduced in the NP position, with an associated \"quantifier assumption,\" which is added as a kind of premise. "}
{"citeStart": 93, "citeEnd": 109, "citeStartToken": 93, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "Of as the method of using and not using the knowledge of dependencies. Again, we found that for the part of the test data in which dependency is present, the use of the dei)endency knowledge can be used to improve the accuracy of a disambiguation method, Mthough our experimental results are inconclusive at this stage. We also used the 357 verbs and their case frames used in Experiment 1 to acquire class-based case frame patterns using the proposed method. We randomly selected 100 verbs among these 35r verbs and attempted to acquire their case frame patterns. We generalized the case slots within each of these case frames using the method proposed by (Li and Abe, 1995) to obtain class-based case slots, and then replaced the word-based case slots in the data with the obtained class-based case slots. What resulted are class-based case frame examples. We used these data as input to the learning algorithm and acquired case frame patterns for each of' the 100 verbs. We found iJmt no two case slots are determined as dependent in any of the case frame patterns. This is because the number of parameters in a class based model is very large compared to the size of the data we had available. Our experimental result verifies the validity in practice of the assumption widely made in statistical natural language processing that class-based case slots (and also word-based case slots) are mutually independent, at least when the data size available is that provided by the current version of the Penn Tree Bank. This is an empirical finding that is worth noting, since up to now the independence assumption was based soMy on hu- man intuit, ion, to the best of our knowledge. To test how large a data size is required to eslimate a class-based model, we conducted the following experiment. We defined an artifMal class-based model and genera.ted some data. according to its distribution. We then used the data to estimate a class-based model (dendroid distribution), and evaluated the estimated model by measuring the mlmber of dependencies (dependency arcs) it has and the KL distance between the estimated model and the true model. We repeatedly generated data and obserwed the learning 'curve', nan,ely the relationship between the number of dependencies in the estimated model and the data. size used in estimation, and the relationship betweett the KI, distance between the estimated and true modols and the data size. We defined two other models and conducted the same experiments. Figure 2 shows the results of these experiments for these three artificial models averaged ower tO trials. (The number of parameters in Modell, Model2, and Model3 are 18, 30, and 44 respectiv(_'ly, while the number of dependencies are 1, 3, aud 5 respectively.) We see that to accurately estimate a model the data size required is as large as 100 times the nmnber of parameters. Since a class-based mode[ tends to have more than 100 parameters usually, the current data size available in the Penn Tree Bank is not enough for accurate estimation of the dependencies wilhin case fi'antes of most verbs.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We randomly selected 100 verbs among these 35r verbs and attempted to acquire their case frame patterns. ", "mid_sen": "We generalized the case slots within each of these case frames using the method proposed by (Li and Abe, 1995) to obtain class-based case slots, and then replaced the word-based case slots in the data with the obtained class-based case slots. ", "after_sen": "What resulted are class-based case frame examples. "}
{"citeStart": 83, "citeEnd": 102, "citeStartToken": 83, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010) . Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques (Soon et al., 2001; Ng and Cardie, 2002) . However, the corpus that they use consists of only 94 citations to 4 papers and is likely to be too small to be representative.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This task differs from citation sentiment detection, which is in a sense a \"lower level\" of analysis.", "mid_sen": "Some other recent work has focused on the problem of implicit citation extraction (Kaplan et al., 2009; Qazvinian and Radev, 2010) . Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques (Soon et al., 2001; Ng and Cardie, 2002) . ", "after_sen": "However, the corpus that they use consists of only 94 citations to 4 papers and is likely to be too small to be representative."}
{"citeStart": 278, "citeEnd": 298, "citeStartToken": 278, "citeEndToken": 298, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar checking stelnmed as a logical application from forlner attelni)ts to natural language. ulMerstanding by comtmters. Many of the NLU systems developed in the 70's indu(le(l a kind of error recovery Inechanisln ranging flom the treatment only of spelling e.rrors, PARRY (1)arkinson c 't al., 1977) , to tile inclusion also of incomplete int)ut containing some kind of ellipsis, LAD-DEll,/LIFEll (Hendrix et al., 1977) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ulMerstanding by comtmters. ", "mid_sen": "Many of the NLU systems developed in the 70's indu(le(l a kind of error recovery Inechanisln ranging flom the treatment only of spelling e.rrors, PARRY (1)arkinson c 't al., 1977) , to tile inclusion also of incomplete int)ut containing some kind of ellipsis, LAD-DEll,/LIFEll (Hendrix et al., 1977) .", "after_sen": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . "}
{"citeStart": 34, "citeEnd": 47, "citeStartToken": 34, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "6We certainly would have benefited from lexical semantic information, e.g. The correct date is missing would not be captured by our approach. honen et al., 1996) ). Neural networks are rather sensitive to misconfigurations. The boosting for RIP-PER seems to run into problems of overfitting. We noted that in six trials the accuracy could be improved in Combined compared to MorphAna, but in four trials, boosting led to deterioration. This effect is also mentioned in (Quinlan, 1996) . These figures are slightly lower than the ones reported by (Neumann and Schmeier, 1999 ) that were obtained from a different data set. Moreover, these data did not contain multiple queries in one e-mall.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We noted that in six trials the accuracy could be improved in Combined compared to MorphAna, but in four trials, boosting led to deterioration. ", "mid_sen": "This effect is also mentioned in (Quinlan, 1996) . ", "after_sen": "These figures are slightly lower than the ones reported by (Neumann and Schmeier, 1999 ) that were obtained from a different data set. "}
{"citeStart": 171, "citeEnd": 193, "citeStartToken": 171, "citeEndToken": 193, "sectionName": "UNKNOWN SECTION NAME", "string": "The decoding algorithm employed for this chunkbased statistical translation is based on the beam search algorithm for word alignment statistical translation presented in (Tillmann and Ney, 2000) , which generates outputs in left-to-right order by consuming input in an arbitrary order.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The decoding algorithm employed for this chunkbased statistical translation is based on the beam search algorithm for word alignment statistical translation presented in (Tillmann and Ney, 2000) , which generates outputs in left-to-right order by consuming input in an arbitrary order.", "after_sen": "The decoder consists of two stages:"}
{"citeStart": 156, "citeEnd": 177, "citeStartToken": 156, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "Moreover, PDFBox cannot reliably recover reading order from text typeset in multiple columns (again, depending on the PDF generator used). OCR introduces sporadic character and layout recognition errors, but overall works robustly, cf. discussion and a recent and even more accurate approach in Schäfer et al. (2012) . The main part of the corpus creation endeavor consisted in manual annotation assisted by a customized version of the MMAX2 annotation tool (Müller and Strube, 2006) , operating on the extracted raw texts (the annotators had the possibility to open and view the original PDF files). In a second step, the corpus was then augmented with automatically created annotations.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, PDFBox cannot reliably recover reading order from text typeset in multiple columns (again, depending on the PDF generator used). ", "mid_sen": "OCR introduces sporadic character and layout recognition errors, but overall works robustly, cf. discussion and a recent and even more accurate approach in Schäfer et al. (2012) . ", "after_sen": "The main part of the corpus creation endeavor consisted in manual annotation assisted by a customized version of the MMAX2 annotation tool (Müller and Strube, 2006) , operating on the extracted raw texts (the annotators had the possibility to open and view the original PDF files). "}
{"citeStart": 150, "citeEnd": 162, "citeStartToken": 150, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "Perplexity is related to entropy as follows. The observed perplexity Po of a language model with respect to an (imaginary) infinite test sequence wl, w2, ... is defined through the formula (see [Jelinek 1990]) In Po = lim --lln p (wi, ..., wn) rl--* OO n", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Perplexity is related to entropy as follows. ", "mid_sen": "The observed perplexity Po of a language model with respect to an (imaginary) infinite test sequence wl, w2, ... is defined through the formula (see [Jelinek 1990]) In Po = lim --lln p (wi, ..., wn) rl--* OO n", "after_sen": "Here p (wl,..., Wn) denotes the probability of the word string Wl, ..., W n."}
{"citeStart": 155, "citeEnd": 167, "citeStartToken": 155, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009) . In total, the system consists of 166 separate replacement rules that together perform the verb chunk translation. In practice, the input is given to the first transducer, after which its output is passed to the second, and so forth, in a cascade. Each rule in the system is unambiguous in its output; that is, for each input in a particular step along the verb chain transfer, the transducers never produce multiple outputs (i.e. the transducers in question are functional). Some of the rules are joined together with composition, yielding a total of 55 separate transducers. In principle, all the rules could be composed together into one monolithic transducer, but in practice the size of the composed transducer is too large to be feasible. The choice to combine some transducers while leaving others separate is largely a memory/translation speed tradeoff.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The verbal chunk transfer is a very complex module because of the nature of Spanish and Basque auxiliary verb constructions, and is the main subject of this paper.", "mid_sen": "This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009) . ", "after_sen": "In total, the system consists of 166 separate replacement rules that together perform the verb chunk translation. "}
{"citeStart": 185, "citeEnd": 189, "citeStartToken": 185, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "Other general purpose methods that can he used to maximize ~()~) include gradient ascent and conjugate gradient. An optimization method specifically tailored to the maximum entropy problem is the iterative scaling algorithm of Darroch and Ratcliff (1972) . We present here a version of this algorithm specifically designed for the problem at hand; a proof of the monotonicity and convergence of the algorithm is given in Della Pietra et al. (1995) . The algorithm is applicable whenever the feature functions fi (x, y) are nonnegative:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An optimization method specifically tailored to the maximum entropy problem is the iterative scaling algorithm of Darroch and Ratcliff (1972) . ", "mid_sen": "We present here a version of this algorithm specifically designed for the problem at hand; a proof of the monotonicity and convergence of the algorithm is given in Della Pietra et al. (1995) . ", "after_sen": "The algorithm is applicable whenever the feature functions fi (x, y) are nonnegative:"}
{"citeStart": 84, "citeEnd": 106, "citeStartToken": 84, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Tone Transcription In this section I present the problem of relating sequences of F0 values to ton~ transcriptions. I argue that Hidden Markov Models are unsuited to the task and I demonstrate the importance of having a complltational tool which allows phonologists to experiment with F0 scaling parameters. Fo Scaling This section gives a mathematical basis for a general approach to F0 scaling which, it is hoped, will be applicable to any tone language. I derive an F0 prediction function from first, principles and show how the model of Liberman et al. (1993) for tile Nigerian iangu:~ge Igbo is a special case. Tone and Fo in Bamileke Dschang Here I present some data from my own fieldwork and give a statistical analysis, using the same technique used by Liberman et al. I then show how the general model of the previous section is instantiated for this language. This demonstrates the versatility of the general model, since it can be applied to two very different tone languages. Imphunentatlons This section provides two non-deterministic techniques for transcribing an F0 string. The first method uses a genetic algorithm while the second method uses simulated annealing. The performance of both implementations is evaluated and compared on a range of artificial and real data. Finally, I give some examples of multiple, automatically-generated transcriptions of the same F0 data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Fo Scaling This section gives a mathematical basis for a general approach to F0 scaling which, it is hoped, will be applicable to any tone language. ", "mid_sen": "I derive an F0 prediction function from first, principles and show how the model of Liberman et al. (1993) for tile Nigerian iangu:~ge Igbo is a special case. ", "after_sen": "Tone and Fo in Bamileke Dschang Here I present some data from my own fieldwork and give a statistical analysis, using the same technique used by Liberman et al. I then show how the general model of the previous section is instantiated for this language. "}
{"citeStart": 115, "citeEnd": 140, "citeStartToken": 115, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "The corpora. Three corpora were used in this study. At the initial stage two development corpora were used: a digitalized early draft of the Jurafsky-Martin textbook (Jurafsky and Martin, 2000) and the WeScience Corpus, a set of Wikipedia articles in the domain of Natural Language Processing (Ytrestøl et al., 2009) . 1 The former served as a source of seed domain terms with definitions, while the latter was used for seed pattern creation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Three corpora were used in this study. ", "mid_sen": "At the initial stage two development corpora were used: a digitalized early draft of the Jurafsky-Martin textbook (Jurafsky and Martin, 2000) and the WeScience Corpus, a set of Wikipedia articles in the domain of Natural Language Processing (Ytrestøl et al., 2009) . 1 The former served as a source of seed domain terms with definitions, while the latter was used for seed pattern creation.", "after_sen": "For acquisition of definitional patterns and pattern refinement we used the ACL Anthology, a digital archive of scientific papers from conferences, workshops, and journals on Computational Linguistics and Language Technology (Bird et al., 2008) . 2 The corpus consisted of 18,653 papers published between 1965 and 2011, with a total of 66,789,624 tokens and 3,288,073 sentences. "}
{"citeStart": 0, "citeEnd": 12, "citeStartToken": 0, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds. Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. While such patterns produce false training examples, the resulting noise often only introduces minor distortions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds. ", "mid_sen": "Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. ", "after_sen": "An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. "}
{"citeStart": 86, "citeEnd": 98, "citeStartToken": 86, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "These three phenomena of spoken dialog, however, cannot be resolved without recourse to syntactic information. Speech repairs, for example, are often signaled by syntactic anomalies. Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994) . However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983) . Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, in order to determine the extent of the reparanduin, one needs to take into account the parallel structure that typically exists between the reparandum and alteration, which relies on at identifying the s:?ntactic roles, or part-of-speech (POS) tags, of the words involved (Bear, Dowding, and Shriberg, 1992; Heeman and Allen, 1994) . ", "mid_sen": "However, speech repairs disrupt the context that is needed to determine the POS tags (Hindle, 1983) . ", "after_sen": "Hence, speech repairs, as well as boundary tones and discourse markers, must be resolved during syntactic disambiguation."}
{"citeStart": 10, "citeEnd": 41, "citeStartToken": 10, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "Following Strzalkowski and Brandow (1997) and Peters and Drexel (2004) we have implemented a transformation-based learning (TBL) algorithm (Brill, 1995) . This method iteratively improves the match (as measured by token error rate) of a collection of corresponding source and target token sequences by positing and applying a sequence of substitution rules. In each iteration the source and target tokens are aligned using a minimum edit distance criterion. We refer to maximal contiguous subsequences of non-matching tokens as error re-gions. These consist of paired sequences of source and target tokens, where either sequence may be empty. Each error region serves as a candidate substitution rule. Additionally we consider refinements of these rules with varying amounts of contiguous context tokens on either side. Deviating from Peters and Drexel (2004) , in the special case of an empty target sequence, i.e. a deletion rule, we consider deleting all (non-empty) contiguous subsequences of the source sequence as well. For each candidate rule we accumulate two counts: the number of exactly matching error regions and the number of false alarms, i.e. when its left-hand-side matches a sequence of already correct tokens. Rules are ranked by the difference in these counts scaled by the number of errors corrected by a single rule application, which is the length of the corresponding error region. This is an approximation to the total number of errors corrected by a rule, ignoring rule interactions and non-local changes in the minimum edit distance alignment. A subset of the topranked non-overlapping rules satisfying frequency and minimum impact constraints are selected and the source sequences are updated by applying the selected rules. Again deviating from Peters and Drexel (2004) , we consider two rules as overlapping if the left-hand-side of one is a contiguous subsequence of the other. This procedure is iterated until no additional rules can be selected. The initial rule set is populated by a small sequence of hand-crafted rules (e.g. \"impression colon\" → \"IMPRESSION:\"). A user-independent baseline rule set is generated by applying the algorithm to data from a collection of users. We construct speaker-dependent models by initializing the algorithm with the speakerindependent rule set and applying it to data from the given user.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following Strzalkowski and Brandow (1997) and Peters and Drexel (2004) we have implemented a transformation-based learning (TBL) algorithm (Brill, 1995) . ", "after_sen": "This method iteratively improves the match (as measured by token error rate) of a collection of corresponding source and target token sequences by positing and applying a sequence of substitution rules. "}
{"citeStart": 55, "citeEnd": 80, "citeStartToken": 55, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003) , Mainichi news articles in 1995 that have been manually annotated with dependency relations. 6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively. The training samples generated from the training set included 150,064 positive and 146,712 negative samples.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.", "mid_sen": "For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003) , Mainichi news articles in 1995 that have been manually annotated with dependency relations. ", "after_sen": "6 The training, development, and test sets included 24,283, 4833, and 9284 sentences, and 234,685, 47,571, and 89,874 bunsetsus, respectively. "}
{"citeStart": 102, "citeEnd": 122, "citeStartToken": 102, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "We now have two probability distributions that we need to estimate, which we do using decision trees (Breiman et al., 1984; Bahl et al., 1989) . The decision tree algorithm has the advantage that it uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. The decision tree algorithm starts with all of the training data in a single leaf node. For each leaf node, it looks for the question to ask of the context such that splitting the node into two leaf nodes results in the biggest decrease in impurity, where tile impurity measures how well each leaf predicts the events in the node. After the tree is grown, a heldout dataset is used to smooth the probabilities of each node with its parent (Bahl et al., 1989) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Pr ( W1,N P1,N ) = H Pr (WiPilWl, i=l,N = 1-I Pr (WiIWl,i-lPl, i) Pr i=l,N", "mid_sen": "We now have two probability distributions that we need to estimate, which we do using decision trees (Breiman et al., 1984; Bahl et al., 1989) . ", "after_sen": "The decision tree algorithm has the advantage that it uses information theoretic measures to construct equivalence classes of the context in order to cope with sparseness of data. "}
{"citeStart": 136, "citeEnd": 148, "citeStartToken": 136, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "Although some of the parses can be ruled out using structural preferences during parsing (such as [,ate C'losure or Minimal Attachment (Frazier 1979) ), ex traction of the correct set of plausible readings requires use of real world knowledge. Incremental interpretation allows on-line semantic tiltering, i.e. parses of initial fragments which have an implausible or anolnalous interpretation are rqiected, thereby preven-*'.lPhis research was supported by the UK Science and Gnglneerlng l~.esearch Council, H, esearch Grant 1tR30718.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mothers' Day in the vase that you gave me for my birthday on the chest of drawers that you gave me lbr Armistice Day.", "mid_sen": "Although some of the parses can be ruled out using structural preferences during parsing (such as [,ate C'losure or Minimal Attachment (Frazier 1979) ), ex traction of the correct set of plausible readings requires use of real world knowledge. ", "after_sen": "Incremental interpretation allows on-line semantic tiltering, i.e. parses of initial fragments which have an implausible or anolnalous interpretation are rqiected, thereby preven-*'.lPhis research was supported by the UK Science and Gnglneerlng l~.esearch Council, H, esearch Grant 1tR30718."}
{"citeStart": 65, "citeEnd": 77, "citeStartToken": 65, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures. In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3. Note that in the 'reduce operation tree', the system first decides whether or not to perform a reduction before deciding on a specific reduction. Using our knowledge of similarity of parse actions and the exceptionality vs. generality of parse action groups, we can provide an overhead structure that helps prevent data fragmentation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We extended the standard ID3 model (Quinlan, 1986) to more general hybrid decision structures. ", "mid_sen": "In our tests, the best performing structure was a decision list (Rivest, 1987) of hierarchical decision trees, whose simplified basic structure is illustrated in figure 3. ", "after_sen": "Note that in the 'reduce operation tree', the system first decides whether or not to perform a reduction before deciding on a specific reduction. "}
{"citeStart": 145, "citeEnd": 176, "citeStartToken": 145, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "Following the format of Table 4 , the results for this bakeoff are shown in Table 5 . We chose the three models that achieved at least one best score in the closed tests from Emerson (2005) , as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison. Row \"Zh-a\" and \"Zh-b\" represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively. Again, our model achieved better overall accuracy than all the other models. The combination of character-based features improved the accuracy slightly again. The best score in each column and the best average in each row is in boldface. *Zhang 2007 with the (Carreras, Surdeanu, and Marquez 2006) method applied (see text for details).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following the format of Table 4 , the results for this bakeoff are shown in Table 5 . ", "mid_sen": "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005) , as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison. ", "after_sen": "Row \"Zh-a\" and \"Zh-b\" represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively. "}
{"citeStart": 136, "citeEnd": 159, "citeStartToken": 136, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "We experiment in the biological domain with the eight-category AZ scheme (Table 1) adapted from (Mizuta et al., 2006) and described in (Contractor et al., 2012) . The results show that our constrained model substantially outperforms a baseline unconstrained Maximum Entropy Model. While this type of constrained models have previously improved the feature-based model performance mostly in the weakly supervised and domain adaptation scenarios (e.g. (Mann and McCallum, 2007; Mann and Mc-Callum, 2008; Ganchev et al., 2010 )), we demonstrate substantial gains both when the Maximum En- ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This algorithm enables us to break the multi-class prediction into a pipeline of consecutive, simpler predictions which can be better assisted by the encoded knowledge.", "mid_sen": "We experiment in the biological domain with the eight-category AZ scheme (Table 1) adapted from (Mizuta et al., 2006) and described in (Contractor et al., 2012) . ", "after_sen": "The results show that our constrained model substantially outperforms a baseline unconstrained Maximum Entropy Model. "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Unfortunately, there are still cases where the inferred grammar is very different from the grammar we would expect, e.g. verbs become leaves instead of governing the sentences. Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, there are still cases where the inferred grammar is very different from the grammar we would expect, e.g. verbs become leaves instead of governing the sentences. ", "mid_sen": "Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed.", "after_sen": "The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the P stop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV. "}
{"citeStart": 84, "citeEnd": 108, "citeStartToken": 84, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas (Doan et al. 2002) . Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999) , (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003) . Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993) , or require human-annotated training data with relation information for each domain (Craven et al. 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The existing approaches to ontology induction include those that start from structured data, merging ontologies or database schemas (Doan et al. 2002) . ", "mid_sen": "Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999) , (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al. 2003) . ", "after_sen": "Information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge, e.g., (Craven and Kumlien 1999) (Hull and Gomez 1993) , or require human-annotated training data with relation information for each domain (Craven et al. 1998) ."}
{"citeStart": 111, "citeEnd": 128, "citeStartToken": 111, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Defining a task so as to maximize percent agreement can be difficult. The high and consistent levels of agreement for our task suggest that we have found a useful experimental formulation of the task of discourse segmentation. Furthermore, our percent agreement figures are comparable with the results of other segmentation studies discussed above. While studies of other tasks have achieved stronger results (e.g., 96.8% in a word-sense disambiguation study (Gale et al., 1992) ), the meaning of percent agreement in isolation is unclear. For example, a percent agreement figure of less than 90% could still be very meaningful if the probability of obtaining such a figure is low. In the next section we demonstrate the significance of our findings.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, our percent agreement figures are comparable with the results of other segmentation studies discussed above. ", "mid_sen": "While studies of other tasks have achieved stronger results (e.g., 96.8% in a word-sense disambiguation study (Gale et al., 1992) ), the meaning of percent agreement in isolation is unclear. ", "after_sen": "For example, a percent agreement figure of less than 90% could still be very meaningful if the probability of obtaining such a figure is low. "}
{"citeStart": 243, "citeEnd": 274, "citeStartToken": 243, "citeEndToken": 274, "sectionName": "UNKNOWN SECTION NAME", "string": "One contrast often taken for granted is the identification of a 'statistical-symbolic' distinction in language processing as an instance of the empirical vs. rational debate. I believe this contrast has been exaggerated though historically it has had some validity ill terms of accepted practice. Rule based approaches have become more empirical in a number of ways: First, a more empirical approach is being adopted to grammar development whereby the rule set is modified according to its performance against corpora of natural text (e.g. Taylor, Grovel and Briscoe 1989) . Second, there is a class of techniques for learning rules from text, a recent example being Brill 1993. Conversely, it is possible to imagine building a language model in which all probabilities are estimated according to intuition without reference to any real data, giving a probabilistic mod~,l that is not empirical.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I believe this contrast has been exaggerated though historically it has had some validity ill terms of accepted practice. ", "mid_sen": "Rule based approaches have become more empirical in a number of ways: First, a more empirical approach is being adopted to grammar development whereby the rule set is modified according to its performance against corpora of natural text (e.g. Taylor, Grovel and Briscoe 1989) . ", "after_sen": "Second, there is a class of techniques for learning rules from text, a recent example being Brill 1993. "}
{"citeStart": 117, "citeEnd": 131, "citeStartToken": 117, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda (Ushioda, 1996) . In this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When inspecting manually, the binary word tree representation appears to be the most easy to understand.", "mid_sen": "A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda (Ushioda, 1996) . ", "after_sen": "In this case it is necessary to use a hard-clustering method, such that a binary word tree can be constructed by the clustering process, as we did in the example in the previous sections."}
{"citeStart": 31, "citeEnd": 52, "citeStartToken": 31, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluate the extraction quality of our system (FL) against 7 different baselines. In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. The second baseline is the MEAD summarizer with all its settings set to default. The third baseline is LexRank (Erkan and Radev, 2004 ) run on the entire set of citation sentences of the target paper. The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. In one variation (FL-1), we remove the sentence filtering component. In another variation (FL-2), we remove the sentence classification component; so, all the sen-tences are assumed to come from one category in the subsequent components. In a third variation (FL-3), the clustering component is removed. To make the comparison of the extraction quality to those baselines fair, we remove the author name replacement component from our system and all its variations. Table 6 shows the average ROUGE-L scores (with 95% confidence interval) for the summaries of the 30 papers in dataset2 generated using our system and the different baselines. The two human summaries were used as models for comparison. The Human score reported in the table is the result of comparing the two human summaries to each others. Statistical significance was tested using a 2-tailed paired t-test. The results are statistically significant at the 0.05 level.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second baseline is the MEAD summarizer with all its settings set to default. ", "mid_sen": "The third baseline is LexRank (Erkan and Radev, 2004 ) run on the entire set of citation sentences of the target paper. ", "after_sen": "The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. "}
{"citeStart": 161, "citeEnd": 175, "citeStartToken": 161, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "Constrained type shifting avoids the problem with freely available categories in Eisner's normal form parsing scheme. However, some surface characteristics of the language, such as lack of case marking in certain constructions, puts the burden of type shifting on the processor (Bozsahin, 1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Constrained type shifting avoids the problem with freely available categories in Eisner's normal form parsing scheme. ", "mid_sen": "However, some surface characteristics of the language, such as lack of case marking in certain constructions, puts the burden of type shifting on the processor (Bozsahin, 1997) .", "after_sen": "Lower type arguments such as NP2 pose a different kind of ambiguity problem. "}
{"citeStart": 36, "citeEnd": 68, "citeStartToken": 36, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Following the work of, for example, Marslen-Wilson (1973), .lust and Carpenter (1980) and Altma.nn al]d Steedrnan (1988) , it has heroine widely accepted that semantic i11terpretation in hnman sentence processing can occur beibre sentence boundaries and even before clausal boundaries. It is less widely accepted that there is a need for incremental interpretation in computational applications.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following the work of, for example, Marslen-Wilson (1973), .lust and Carpenter (1980) and Altma.nn al]d Steedrnan (1988) , it has heroine widely accepted that semantic i11terpretation in hnman sentence processing can occur beibre sentence boundaries and even before clausal boundaries. ", "after_sen": "It is less widely accepted that there is a need for incremental interpretation in computational applications."}
{"citeStart": 140, "citeEnd": 158, "citeStartToken": 140, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "Our method to generate alteration candidates can be described as follows. First, we do word clustering using a Porter stemmer. All words in the vocabulary sharing the same root form are grouped together. Then we do corpus analysis to filter out the words which are clustered incorrectly, according to word distributional similarity, following (Xu and Croft, 1998; Lin 1998) . The rationale behind this is that words sharing the same meaning tend to occur in the same contexts.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All words in the vocabulary sharing the same root form are grouped together. ", "mid_sen": "Then we do corpus analysis to filter out the words which are clustered incorrectly, according to word distributional similarity, following (Xu and Croft, 1998; Lin 1998) . ", "after_sen": "The rationale behind this is that words sharing the same meaning tend to occur in the same contexts."}
{"citeStart": 139, "citeEnd": 152, "citeStartToken": 139, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used. But there is no question that semantic knowledge is essential for many problems in natural language processing. Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically. Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994) ). Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993) ). Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically. ", "mid_sen": "Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994) ). ", "after_sen": "Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993) ). "}
{"citeStart": 48, "citeEnd": 60, "citeStartToken": 48, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010) , G-index (Egghe, 2006) , and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. ", "mid_sen": "Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010) , G-index (Egghe, 2006) , and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "after_sen": "For example, the number of papers published by a researcher only tells how productive she or he is. "}
{"citeStart": 0, "citeEnd": 27, "citeStartToken": 0, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "A second potentially interesting application involves using the data and rules extracted by PROFILE for language regeneration. In (Radev and McKeown, 1998 to appear) we show how the conversion of extracted descriptions into components of a generation grammar allows for flexible (re)generation of new descriptions that don't appear in the source text. For example, a description can be replaced by a more general one, two descriptions can be combined to form a single one, or one long description can be deconstructed into its components, some of which can be reused as new descriptions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A second potentially interesting application involves using the data and rules extracted by PROFILE for language regeneration. ", "mid_sen": "In (Radev and McKeown, 1998 to appear) we show how the conversion of extracted descriptions into components of a generation grammar allows for flexible (re)generation of new descriptions that don't appear in the source text. ", "after_sen": "For example, a description can be replaced by a more general one, two descriptions can be combined to form a single one, or one long description can be deconstructed into its components, some of which can be reused as new descriptions."}
{"citeStart": 159, "citeEnd": 162, "citeStartToken": 159, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "Several natural language parser start with & pure Conte~zt. Free (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24, 28] ). In [28] , Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that ~ split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\". The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and ei~cient parsers [I,7] . A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences I. A natural solution *Address: INRIA, B.P. 105, 78153 Le Chesn~y, France. The work reported here was partially supported by the Eureka Software Factory project.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In [28] , Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that ~ split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing\". ", "mid_sen": "The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and ei~cient parsers [I,7] . ", "after_sen": "A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences I. "}
{"citeStart": 518, "citeEnd": 530, "citeStartToken": 518, "citeEndToken": 530, "sectionName": "UNKNOWN SECTION NAME", "string": "2The WARRANT having lhe desired cflect of getting the hearer io listen to l{amesh depends till ltle hearel previously believing or coming Io believe that hldians know of good Indian restaurants [Webber arid ] A Warrant IR/J such a.S that in 2 suggests tllat B's cognilive limitations [nay be a factor in what A cllooses to say, so thai even if B knows a wlu'rant for adopting A's proposed, what is critical is whetlm,' the warrant is salient lot B, i.e. whether the warnmt is ah'eady accessible in B's working memory [Prince, 1981; Baddelcy, 1986] . 11 the w~u'rant is not already salient, then B must either infer or retrieve the wlurant information or obtain it from an external source in order to evaluate A's proposed. Thus A's slrategy choice may depend oil A's model of B's alteutional stale, as well as the costs of retrieval ~md inference as opposed to communication. In other words, A may decide thal it is e~Lsier to just say the warrant rather than require B to infer or retrieve it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I The telalion between a WARRANT and the PR(II,OSI{ COlllmunicative act is similar to the MOTIVATION relation of [Moore and Paris, 1993; Mann and Thompson, 1987] . A WARRANT is always optional; this is consistent with the RST fl'anlewolk in which all satellites are optional information.", "mid_sen": "2The WARRANT having lhe desired cflect of getting the hearer io listen to l{amesh depends till ltle hearel previously believing or coming Io believe that hldians know of good Indian restaurants [Webber arid ] A Warrant IR/J such a.S that in 2 suggests tllat B's cognilive limitations [nay be a factor in what A cllooses to say, so thai even if B knows a wlu'rant for adopting A's proposed, what is critical is whetlm,' the warrant is salient lot B, i.e. whether the warnmt is ah'eady accessible in B's working memory [Prince, 1981; Baddelcy, 1986] . 11 the w~u'rant is not already salient, then B must either infer or retrieve the wlurant information or obtain it from an external source in order to evaluate A's proposed. ", "after_sen": "Thus A's slrategy choice may depend oil A's model of B's alteutional stale, as well as the costs of retrieval ~md inference as opposed to communication. "}
{"citeStart": 66, "citeEnd": 92, "citeStartToken": 66, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "We use a bottom-up hierarchical clustering algorithm to group together 514 verbs into K classes. The algorithm starts by finding the similarities between all the possible pairs of objects in the data according to a similarity measure S. After having established the distance between all the pairs, it links together the closest pairs of objects by a linkage method L, forming a binary cluster. The linking process is repeated iteratively over the newly created clusters until all the objects are grouped into one cluster. K, S and L are parameters that can be set for the clustering. For the similarity measure S, we choose the Euclidean distance. For the linkage method L, we choose the Ward linkage method (Ward, 1963) . Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003) . Applying a clustering method to the verbs in our data, we expect to find a natural division of the data that will be in accordance with the classification of verbs that we have set as our target classification. We perform different experiments with different values for K in order to test which of the different granularities yields better results.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For the linkage method L, we choose the Ward linkage method (Ward, 1963) . ", "mid_sen": "Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003) . ", "after_sen": "Applying a clustering method to the verbs in our data, we expect to find a natural division of the data that will be in accordance with the classification of verbs that we have set as our target classification. "}
{"citeStart": 145, "citeEnd": 170, "citeStartToken": 145, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "In the current implementation of LI[IP, compiled rules are interpreted depth-first and left-to-right by the standard Prolog theorem-prover, giving an anMyser that proceeds in a top-down, qeft-headcorner' fashion. Because of the reordering carried out during compilation, the situation regarding left-recursion is slightly more subtle than in a conventional DCG. The 's(conjunct(... ))' rule shown above is a case in point. While at first sight it appears left-recursive, inspection of its converted form shows its true leftmost subrule to be 'conjunction'. Naturally, compilation may induce left-recursion as well as eliminating it, in which case LIIIP will suffer from the same termination problems as an ordinary DCG formalism interpreted in this way. And as with an ordinary DCG formalism, it is possible to apply different parsing methods to LHIP in order to circumvent these problems (see e.g. Pereira and Shieber, 1987) . A related issue concerns the interpretation of embedded Prolog code. Reordering of lZHS clauses will result in code which precedes a head within a LtHP rule being evaluated after it; judicious freezing of goals and avoidance of unsafe cuts are therefore required. LHIP provides a number of ways of applying a grammar to input. The simplest allows one to enumerate the possible analyses of the input with the grammar. The order in which the results are produced wiU reflect the lexical ordering of the rules as they are converted by LHIP. With the threshold level set to 0, all analyses possible with the grammar by deletion of input terminals can be generated. Thus, supposing a suitable grammar, for the sentence John saw Mary and Mark saw them there would be analyses corresponding to the sentence itself, as well as John saw Mary, John saw Mark, John saw them, Mary saw them, Mary and Mark saw them, etc.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Naturally, compilation may induce left-recursion as well as eliminating it, in which case LIIIP will suffer from the same termination problems as an ordinary DCG formalism interpreted in this way. ", "mid_sen": "And as with an ordinary DCG formalism, it is possible to apply different parsing methods to LHIP in order to circumvent these problems (see e.g. Pereira and Shieber, 1987) . ", "after_sen": "A related issue concerns the interpretation of embedded Prolog code. "}
{"citeStart": 210, "citeEnd": 220, "citeStartToken": 210, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "The incremental parser learns while parsing, and it could, in principle, simply be evaluated for a single pass of the data. But, because the quality of the parses of the first sentences would be low, I first trained on the full corpus and then measured parsing accuracy on the corpus subset. By training on the full corpus, the procedure differs from that of Klein, Manning and Bod who only train on the subset of bounded length sentences. However, this excludes the induction of parts-of-speech for parsing from plain text. When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire (Klein and Manning, 2002) . The comparison between the algorithms remains, therefore, valid. Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (Klein and Manning, 2004) , U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a) . The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text. Results for the incremental parser are given for learning and parsing from left to right and from right to left.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The comparison between the algorithms remains, therefore, valid. ", "mid_sen": "Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (Klein and Manning, 2004) , U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a) . ", "after_sen": "The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text. "}
{"citeStart": 130, "citeEnd": 149, "citeStartToken": 130, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "One limitation of this study is that the dataset used is rather small, consisting of just 199 instances of the target construction. As discussed in Section 3.1, the extraction process we use to obtain our experimental items has low recall; in particular it misses variants of the target construction such as Nothing is too Y to Z and No X is too Y for Z. In the future we intend to expand our dataset by extracting such usages. Furthermore, the data used in the present study is primarily taken from news text. While we do not adopt the view of some that usages of the target construction having the \"no\" interpretation are errors, it could be the case that such usages are more frequent in less formal text. In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While we do not adopt the view of some that usages of the target construction having the \"no\" interpretation are errors, it could be the case that such usages are more frequent in less formal text. ", "mid_sen": "In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009) .", "after_sen": "Constructions other than No X is too Y to Z exhibit a similar ambiguity. "}
{"citeStart": 133, "citeEnd": 153, "citeStartToken": 133, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "If one assumes a pairwise linking from M't to right then the links between the two trees can be on,itted. Although such pairs of trees are reminiscent of synchronons trees in TAG's [Shieber and Schabes, 1991] , they are simpler in wtrious ways, in particular because we will nol make use of the adjunction operM.ion later on. In essence, pairs of trees are just a graphical notation for what has been put forward as the 'rule-to-rule'-hypothesis, el'. [Gazdar el al., 1985] , the fact that in the grammar each syntax rule is related with a semantic analysis rule. However, on the long run, the tree notation suggests a more general relation, e.g. more internal structure or additional, terminal leaf nodes in the local syntax tree. An obvkms way to implement a generation procedure (see Fig.l ) is to relate the inlmt semantics with the start symbol of the gramnuu\" and then to try to ex-l}and this node in a top-down manner accordiug to the rules specitied in tl,e grammar. This node expansion corresponds to an application of the (predicl)-rule in the following abstract specification of a Lop-down generator. Generation terminates successfully if all the leaf nodes are labeled with terminals (s.ucccss). The question is which method is used to nmke two, possibly complex symbols equal, l'~or the sake of simplicity, we assume that the open leaves at0 resp. X0 are matched by (feature) term unification with the corresponding mother nodes in the grammar rule. llowever, for the semantic form Xo, a decidable variant of higher order unification might be used instead, in order Lo inch, de the reduction of .\\-expressions. Of course, the. necessary precautions have to be taken in order to avoid the confusion between object-and meta-level wtriables, cf. [Shieber el al., 1990] .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "llowever, for the semantic form Xo, a decidable variant of higher order unification might be used instead, in order Lo inch, de the reduction of .\\-expressions. ", "mid_sen": "Of course, the. necessary precautions have to be taken in order to avoid the confusion between object-and meta-level wtriables, cf. [Shieber el al., 1990] .", "after_sen": "A clepth-first realization of this abstract top-down algorithm would work line as long ms tl,e semantic rep-all leaves of the syntax tree are labeled with terminals resentations of the leaves are always strictly smaller in size as the semantic form of the root node. "}
{"citeStart": 317, "citeEnd": 329, "citeStartToken": 317, "citeEndToken": 329, "sectionName": "UNKNOWN SECTION NAME", "string": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. ", "mid_sen": "With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . ", "after_sen": "One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . "}
{"citeStart": 112, "citeEnd": 136, "citeStartToken": 112, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "word sense disambiguation, information retrieval, natural language generation and so on. Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "word sense disambiguation, information retrieval, natural language generation and so on. ", "mid_sen": "Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). ", "after_sen": "However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) )."}
{"citeStart": 10, "citeEnd": 31, "citeStartToken": 10, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "In this study, conceptual association is used with groups consisting of all categories from the 1911 version of Roget's thesaurus. 4 Given two thesaurus categories tl and t~, there is a parameter which represents the degree of acceptability of the structure [nine] where nl is a noun appearing in tl and n2 appears in t2. By the assumption that words within a group behave similarly, this is constant given the two categories. Following Lauer and Dras (1994) we can formally write this parameter as Pr(tl ~ t2) where the event tl ~ t2 denotes the modification of a noun in t2 by a noun in tl.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By the assumption that words within a group behave similarly, this is constant given the two categories. ", "mid_sen": "Following Lauer and Dras (1994) we can formally write this parameter as Pr(tl ~ t2) where the event tl ~ t2 denotes the modification of a noun in t2 by a noun in tl.", "after_sen": ""}
{"citeStart": 151, "citeEnd": 163, "citeStartToken": 151, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "Words unknown to the lexicon present a substantial problem to part-of-speech (POS) tagging of realworld texts. Taggers assign a single POS-tag to a word-token, provided that it is known what partsof-speech this word can take on in principle. So, first words are looked up in the lexicon. However, 3 to 5% of word tokens are usually missing in the lexicon when tagging real-world texts. This is where word-Pos guessers take their place --they employ the analysis of word features, e.g. word leading and trailing characters, to figure out its possible POS categories. A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger (Kupiec, 1992) . A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g. (Brill, 1995; Weischedel et al., 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is where word-Pos guessers take their place --they employ the analysis of word features, e.g. word leading and trailing characters, to figure out its possible POS categories. ", "mid_sen": "A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger (Kupiec, 1992) . ", "after_sen": "A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. "}
{"citeStart": 10, "citeEnd": 22, "citeStartToken": 10, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008) , we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by \"diagand\" from Koehn et al. (2003) , and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006) , which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. ", "mid_sen": "Following Huang (2008) , we modify the parser to output a packed forest for each sentence. ", "after_sen": "Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. "}
{"citeStart": 101, "citeEnd": 122, "citeStartToken": 101, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "We discarded the unambiguous sentences in each corpus for both training and testing (as explained in Johnson et al. (1999) , pseudolikelihood estimation ignores unambiguous sentences), leaving us with a corpus of 324 ambiguous sentences in the Verbmobil corpus and 481 sentences in the Homecentre corpus; these sentences had a total of 3,245 and 3,169 parses respectively.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We chose the features of our SLFG based solely on the basis of the Verbmobil corpus, so the Homecentre corpus can be regarded as a held-out evaluation corpus.", "mid_sen": "We discarded the unambiguous sentences in each corpus for both training and testing (as explained in Johnson et al. (1999) , pseudolikelihood estimation ignores unambiguous sentences), leaving us with a corpus of 324 ambiguous sentences in the Verbmobil corpus and 481 sentences in the Homecentre corpus; these sentences had a total of 3,245 and 3,169 parses respectively.", "after_sen": "The (non-auxiliary) features used in were based on those described by Johnson et al. (1999) ."}
{"citeStart": 126, "citeEnd": 138, "citeStartToken": 126, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. ", "mid_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "after_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . "}
{"citeStart": 256, "citeEnd": 274, "citeStartToken": 256, "citeEndToken": 274, "sectionName": "UNKNOWN SECTION NAME", "string": "The ACL Anthology Network (AAN) 2 is a collection of papers from the ACL Anthology 3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009) . AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The ACL Anthology Network (AAN) 2 is a collection of papers from the ACL Anthology 3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009) . ", "after_sen": "AAN includes the citation network of the papers in the ACL Anthology. "}
{"citeStart": 68, "citeEnd": 100, "citeStartToken": 68, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Since we want to study the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant-absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since we want to study the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. ", "mid_sen": "Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant-absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. ", "after_sen": "What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference."}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "(21) Bill supported, and Hillary opposed, two trade bills. Partee and Rooth (1983) observe, and we agree, that the quantifier in such cases only scopes once, resulting in the reading where Bill supported and Hillary opposed the same two bills. 5 Our analysis predicts this fact in the same way as Partee and Rooth's analysis does. The meanings contributed by the lexieal items and f-structure dependencies are the same as in the previous example, except for that of the object NP. Following Dalrymple et al. (1994a) , the meaning derived using the contributions from an f-structure h for two trade bills is: two-trade-bills:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(21) Bill supported, and Hillary opposed, two trade bills. ", "mid_sen": "Partee and Rooth (1983) observe, and we agree, that the quantifier in such cases only scopes once, resulting in the reading where Bill supported and Hillary opposed the same two bills. ", "after_sen": "5 Our analysis predicts this fact in the same way as Partee and Rooth's analysis does. "}
{"citeStart": 149, "citeEnd": 160, "citeStartToken": 149, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "The problem of estimating parameters for log-linear models is not new. It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible. Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. In his seminal development of the MRF approach to spatial statistics, Besag introduced a \"pseudolikelihood\" estimator to address these difficulties (Besag, 1974; Besag, 1975) , and in fact our proposal here is an instance of his method. In general, the likelihood function is replaced by a more manageable product of conditional likelihoods (a pseudo-likelihood--hence the designation PL0), which is then optimized over the parameter vector, instead of the likelihood itself. In many cases, as in our case here, this substitution side steps much of the computational burden without sacrificing consistency (more on this shortly).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well. ", "mid_sen": "In his seminal development of the MRF approach to spatial statistics, Besag introduced a \"pseudolikelihood\" estimator to address these difficulties (Besag, 1974; Besag, 1975) , and in fact our proposal here is an instance of his method. ", "after_sen": "In general, the likelihood function is replaced by a more manageable product of conditional likelihoods (a pseudo-likelihood--hence the designation PL0), which is then optimized over the parameter vector, instead of the likelihood itself. "}
{"citeStart": 158, "citeEnd": 176, "citeStartToken": 158, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "The INTARC architecture as first presented by (Pyka 1992) is a distributed software system that allows for tile intcrconncction of NLSP modules under the principles of incrementality and interactivity. Figure 2 shows the modularization of INTARC-1.3: There is a main broad channel connecting all modules in bottom-up direction, i.e., from signal to interpretation. Furthermore, there are smaller channels connecting several modules, which are used for the top-down interactive disambiguation data flow. Inerementality is required for all modules. ICE assumes that each module has a local memory that is not directly accessible to other modules. Modules communicate explicitly with one another via messages sent over bidirectional channels. This kind of communication architecture is hardly new and eonl¥onts us directly with a large number of unresolved issues in distributed problem solving, ef. (Durfee et al. 1989 ). In the last 20 years there have been numerous architecture proposals for distributed problem solving among computing entities that exchange information explicitly via message passing. None of these models include explicit strategies or paradigms to tackle the problem of distributed control.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Modules communicate explicitly with one another via messages sent over bidirectional channels. ", "mid_sen": "This kind of communication architecture is hardly new and eonl¥onts us directly with a large number of unresolved issues in distributed problem solving, ef. (Durfee et al. 1989 ). ", "after_sen": "In the last 20 years there have been numerous architecture proposals for distributed problem solving among computing entities that exchange information explicitly via message passing. "}
{"citeStart": 178, "citeEnd": 199, "citeStartToken": 178, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems (e.g., (Bennacef et al., 1996; Stent et al., 1999) ). To instantiate an attribute, MIMIC adopts the lnfoSeek dialogue act to solicit the missing information. In contrast, when MIMIC has both initiatives, it plays a more active role by presenting the user with additional information comprising valid instantiations of the attribute (GiveOptions). Given an invalid query, MIMIC notifies the user of the failed query and provides an openended prompt when it only has dialogue initiative. When MIMIC has both initiatives, however, in addition to No-tifyFailure, it suggests an alternative close to the user's original query and provides a limited prompt. Finally, when MIMIC has neither initiative, it simply adopts No-tifyFailure, allowing the user to determine the next discourse goal.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1 .", "mid_sen": "The strategies employed when MIMIC has only dialogue initiative are similar to the mixed initiative dialogue strategies employed by many existing spoken dialogue systems (e.g., (Bennacef et al., 1996; Stent et al., 1999) ). ", "after_sen": "To instantiate an attribute, MIMIC adopts the lnfoSeek dialogue act to solicit the missing information. "}
{"citeStart": 30, "citeEnd": 62, "citeStartToken": 30, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "Many questions are left unanswered by these previous studies. In the word-level analyses of Fosler-Lussier and Morgan (1999) and Shinozaki and Furui (2001) , only substitution and deletion errors were considered, so we do not know how including insertions might affect the results. Moreover, these studies primarily analyzed lexical, rather than prosodic, factors. Hirschberg et al.'s (2004) work suggests that prosodic factors can impact error rates, but leaves open the question of which factors are important at the word level and how they influence recognition of natural conversational speech. Adda- Decker and Lamel's (2005) suggestion that higher rates of disfluency are a cause of worse recognition for male speakers presupposes that disfluencies raise error rates. While this assumption seems natural, it has yet to be carefully tested, and in particular we do not know whether disfluent words are associated with errors in adjacent words, or are simply more likely to be misrecognized themselves. Other factors that are often thought to affect a word's recognition, such as its status as a content or function word, and whether it starts a turn, also remain unexamined.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many questions are left unanswered by these previous studies. ", "mid_sen": "In the word-level analyses of Fosler-Lussier and Morgan (1999) and Shinozaki and Furui (2001) , only substitution and deletion errors were considered, so we do not know how including insertions might affect the results. ", "after_sen": "Moreover, these studies primarily analyzed lexical, rather than prosodic, factors. "}
{"citeStart": 121, "citeEnd": 147, "citeStartToken": 121, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase. 3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in Liberman and Prince (1977) , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251). This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our work on the prosodic phrase status of clause final prepositional phrases, which we discuss below, suggests the existence of a discourse-neutral phrasing that depends on syntactic constituency mediated by string adjacency and length of a potential prosodic phrase. ", "mid_sen": "3 Such phrasing provides us with a typical phrasing pattern analogous to the typical phrasal stress patterns examined in Liberman and Prince (1977) , which \"are often overwhelmed by the chiaroscuro of highlight and background in discourse, but retain the status of null-hypothesis patterns that emerge when there is no good reason to take some other option\" (p. 251). ", "after_sen": "This approach to prosodic phrase boundary determination brings us closer to a framework in which phonological, syntactic, and discourse features all contribute to prosodic phrasing."}
{"citeStart": 158, "citeEnd": 170, "citeStartToken": 158, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "The ability to deal with large amomlts of possibly ill-formed text is one of the principal objectives of current NLP research. Recent proposals include the use of probabilistic methods (see e.g. Briseoe and Carroll, 1993) and large robust deterministic systems like Hindle's Fidditch (Hindle, 1989) . 4 Experience so far suggests that systems like LIIIP may in the right circumstances provide an alternative to these approaches. It combines the advantages of Prolog-interpreted DCGs (ease of modification, parser output suitable for direct use by other programs, etc.) with the ability to relax tile adjacency constraints of that form&llsm in a flexible and dynamic manner.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ability to deal with large amomlts of possibly ill-formed text is one of the principal objectives of current NLP research. ", "mid_sen": "Recent proposals include the use of probabilistic methods (see e.g. Briseoe and Carroll, 1993) and large robust deterministic systems like Hindle's Fidditch (Hindle, 1989) . ", "after_sen": "4 Experience so far suggests that systems like LIIIP may in the right circumstances provide an alternative to these approaches. "}
{"citeStart": 112, "citeEnd": 138, "citeStartToken": 112, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "For training, we use 'CRL data', which was prepared for IREX (Information Retrieval and Extraction Exercise 1 , Sekine and Eriguchi (2000) ). It has about 19,000 NEs in 1,174 articles. We also use additional data by Isozaki (2001) . Both datasets are based on Mainichi Newspaper's 1994 and 1995 CD-ROMs. We use IREX's formal test data called GENERAL that has 1,510 named entities in 71 articles from Mainichi Newspaper of 1999. Systems are compared in terms of GENERAL's F-measure Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our system uses the Viterbi search (Allen, 1995) instead of sequential determination.", "mid_sen": "For training, we use 'CRL data', which was prepared for IREX (Information Retrieval and Extraction Exercise 1 , Sekine and Eriguchi (2000) ). ", "after_sen": "It has about 19,000 NEs in 1,174 articles. "}
{"citeStart": 45, "citeEnd": 57, "citeStartToken": 45, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "For comparison, a variety of other data has been collected. Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983) , using confusion matrices where appropriate (Kernighan, 1990) . Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). Suggestions have been made to look for low frequency words in corpora and news/mail archives, and to the Longmans learner corpus (not native speakers).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983) , using confusion matrices where appropriate (Kernighan, 1990) . ", "mid_sen": "Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). ", "after_sen": "Suggestions have been made to look for low frequency words in corpora and news/mail archives, and to the Longmans learner corpus (not native speakers)."}
{"citeStart": 1, "citeEnd": 23, "citeStartToken": 1, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Several approaches have been suggested to account for this behavior. [Litman and Allen, 1987] introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991 ] or Shared Plans [Grosz and Sidner, 1990] . While these accounts do help explain some discourse phenomena more satisfactorily, they still require a strong degree of cooperativity to account for dialogue coherence, and do not provide easy explanations of why an agent might act in cases which do not support high-level mutual goals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several approaches have been suggested to account for this behavior. ", "mid_sen": "[Litman and Allen, 1987] introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. ", "after_sen": "Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991 ] or Shared Plans [Grosz and Sidner, 1990] . "}
{"citeStart": 171, "citeEnd": 191, "citeStartToken": 171, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. ", "mid_sen": "In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . ", "after_sen": "In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) ."}
{"citeStart": 190, "citeEnd": 202, "citeStartToken": 190, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "Baselines We compared our model against four baselines, two with full supervision: Support Vector Machines (SVM) and Maximum Entropy Models (MaxEnt), and two with light supervision: Trans- (Vapnik, 1998; Jiao et al., 2006) . SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011) ) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This enables us to properly evaluate the impact of our modeling decision which augments a feature-based model with constraints.", "mid_sen": "Baselines We compared our model against four baselines, two with full supervision: Support Vector Machines (SVM) and Maximum Entropy Models (MaxEnt), and two with light supervision: Trans- (Vapnik, 1998; Jiao et al., 2006) . SVM and MaxEnt have proved successful in information structure analysis (e.g. (Merity et al., 2009; Guo et al., 2011) ) but, to the best of our knowledge, their semi-supervised versions have not been used for AZ of full articles.", "after_sen": "Parameter tuning The boundaries of the reference probabilities (a k and b k in Equation 8)were defined and optimized on the development data which consists of one third of the corpus. "}
{"citeStart": 186, "citeEnd": 205, "citeStartToken": 186, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of the NLU systems developed in the 70's indu(le(l a kind of error recovery Inechanisln ranging flom the treatment only of spelling e.rrors, PARRY (1)arkinson c 't al., 1977) , to tile inclusion also of incomplete int)ut containing some kind of ellipsis, LAD-DEll,/LIFEll (Hendrix et al., 1977) .", "mid_sen": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . ", "after_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. "}
{"citeStart": 70, "citeEnd": 83, "citeStartToken": 70, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "The complexity of the training procedure is a function of the number of features and the number of examples. For large datasets, we use an ensemble technique inspired by Bagging (Breiman, 1996) . Bagging is generally used to mitigate high variance in datasets by sampling, with replacement, from the training set. Given that we wish to include some of the less frequent examples and therefore are not necessarily avoiding high variance, we partition the data into disjoint sets.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The complexity of the training procedure is a function of the number of features and the number of examples. ", "mid_sen": "For large datasets, we use an ensemble technique inspired by Bagging (Breiman, 1996) . ", "after_sen": "Bagging is generally used to mitigate high variance in datasets by sampling, with replacement, from the training set. "}
{"citeStart": 23, "citeEnd": 43, "citeStartToken": 23, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "We could expect to achieve D1 from, say, a printed dictionary listing parts of speech in order of frequency. Perfect training is represented by case D0+T0. The Xerox experiments (Cutting et al., 1992) correspond to something between D1 and D2, and between TO and T1, in that there is some initial biasing of the probabilities. For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-B-J from parts B to J inclusive. Corpus LOB-B-J was used to train the model, and LOB-B. LOB-L and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data. In each case, the best accuracy (on ambiguous words, as usual) from the FB algorithm was noted. As an additional test, we tried assigning the most probable tag from the DO lexicon, completely ignoring tag-tag transitions. The results are summarised in table 1, for various corpora, where F denotes the \"most frequent tag\" test. As an example of how these figures relate to overall accuracies, LOB-B contains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J, and the overall accuracy in the D0+T0 case is hence 98.69%. The general pattern of the results is similar across the three test corpora, with the only difference of interest being that case D3+T0 does better for LOB-L than tbr the other two cases, and in particular does better than cases D0+T1 and DI+T1. A possible explanation is that in this case the test data does not overlap with the training data, and hence the good quality lexicons (DO and D1) have less of an influence. It is also interesting that D3+T1 does better than D2+T1. The reasons for this are unclear, and the results are not always the same with other corpora, which suggests that they are not statistically significant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Perfect training is represented by case D0+T0. ", "mid_sen": "The Xerox experiments (Cutting et al., 1992) correspond to something between D1 and D2, and between TO and T1, in that there is some initial biasing of the probabilities. ", "after_sen": "For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-B-J from parts B to J inclusive. "}
{"citeStart": 81, "citeEnd": 95, "citeStartToken": 81, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. For instance, Grimshaw (1990) defines the thematic hierarchy as:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . ", "mid_sen": "PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . ", "after_sen": "All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. "}
{"citeStart": 54, "citeEnd": 71, "citeStartToken": 54, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Though theorem 18 gives an interpretation free eharacterisation of a satisfiable feature structure, the characterisation still seems to admit of no effective algorithm to decide if a feature structure is satisfiable, tlowever, I use theorem 18 and resolve.d feature structures to yield a less general interpretation free characterisation of a satisfiable feature structure that admits of such an algorithm. Definition 19. R is a resolved feature structure itr R is a feature structure (Q, q, a, p}, p is a total function from Q to 6, and for each ~ E 91 and each q' G Q, if ~(q I, ct) is defined then ~(p(q'), ~r) is defined, and (~(p(q'), oz) ~_ p(a(q', c~)). Bach resolved feature structure is a well-typed (see [CARI'ENTF, R 1992] ) feature structure with output alphabet (%. Definition 20. I¢. is a resolvant off iff R is a resolved lbature structure (Q, q, 6, p), F is a feature structure (Q,q,~,O) , and rot each q' e Q, o(q') ~_ p(q').", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "R is a resolved feature structure itr R is a feature structure (Q, q, a, p}, p is a total function from Q to 6, and for each ~ E 91 and each q' G Q, if ~(q I, ct) is defined then ~(p(q'), ~r) is defined, and (~(p(q'), oz) ~_ p(a(q', c~)). ", "mid_sen": "Bach resolved feature structure is a well-typed (see [CARI'ENTF, R 1992] ) feature structure with output alphabet (%. Definition 20. ", "after_sen": "I¢. is a resolvant off iff R is a resolved lbature structure (Q, q, 6, p), F is a feature structure (Q,q,~,O) , and rot each q' e Q, o(q') ~_ p(q')."}
{"citeStart": 135, "citeEnd": 159, "citeStartToken": 135, "citeEndToken": 159, "sectionName": "UNKNOWN SECTION NAME", "string": "To evaluate our models, we constructed two di erent test corpora. We rst parsed with the LFG grammar 550 sentences which are used for illustrative purposes in the foreign language learner's grammar of Helbig and Buscha (1996) . In a next step, the correct parse was indicated by a human disambiguator, according to the reading intended in Helbig and Buscha (1996) . Thus a precise indication of correct c/f-structure pairs was possible. However, the average ambiguity of this corpus is only 5.4 parses per sentence, for sentences with on average 7.5 words. In order to evaluate on sentences with higher ambiguity rate, we manually disambiguated further 375 sentences of LFG-parsed newspaper text. The sentences of this corpus have on average 25 parses and 11.2 words.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To evaluate our models, we constructed two di erent test corpora. ", "mid_sen": "We rst parsed with the LFG grammar 550 sentences which are used for illustrative purposes in the foreign language learner's grammar of Helbig and Buscha (1996) . ", "after_sen": "In a next step, the correct parse was indicated by a human disambiguator, according to the reading intended in Helbig and Buscha (1996) . "}
{"citeStart": 59, "citeEnd": 71, "citeStartToken": 59, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "The obvious first task to consider is noun phrase bracketing itself. We implement a similar system to Table 5 : Lexical overlap Lauer (1995) , described in Section 3, and report on results from our own data and Lauer's original set. First, we extracted three word noun sequences from all the ambiguous NPs. If the last three children are nouns, then they became an example in our data set. If there is a NML node containing the first two nouns then it is left-branching, otherwise it is right-branching. Because we are only looking at the right-most part of the NP, we know that we are not extracting any nonsensical items. We also remove all items where the nouns are all part of a named entity to eliminate flat structure cases.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The obvious first task to consider is noun phrase bracketing itself. ", "mid_sen": "We implement a similar system to Table 5 : Lexical overlap Lauer (1995) , described in Section 3, and report on results from our own data and Lauer's original set. ", "after_sen": "First, we extracted three word noun sequences from all the ambiguous NPs. "}
{"citeStart": 46, "citeEnd": 71, "citeStartToken": 46, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "The most similar work to ours was proposed by Somasundaran et al.(2009) . They proposed to use iterative classification algorithm to capture discourselevel associations. However different to us, they focused on pairwise relationships between opinion expressions. In this paper, we used MLN framework to capture another different discourse-level relation, which exists between subject clauses or subject clause and objective clause. Richardson and Domingos (2006) proposed Markov Logic Networks, which combines firstorder logic and probabilistic graphical models. In recent years, MLN has been adopted for several natural language processing tasks and achieved a certain level of success (Singla and Domingos, 2006; Riedel and Meza-Ruiz, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Huang et al., 2012) . Singla and Domingos (2006) modeled the entity resolution problem with MLN. They demonstrated the capability of MLN to seamlessly combine a number of previous approaches. Poon and Domingos (2008) proposed to use MLN for joint unsupervised coreference resolution. Yoshikawa et al. (2009) proposed to use Markov logic to incorporate both local features and global constraints that hold between temporal relations. Andrzejewski et al. (2011) introduced a framework for incorporating general domain knowledge, which is represented by First-Order Logic (FOL) rules, into LDA inference to produce topics shaped by both the data and the rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Zirn et al. (2011) proposed to use MLN framework to capture the context information in analysing (sub-)sentences.", "mid_sen": "The most similar work to ours was proposed by Somasundaran et al.(2009) . ", "after_sen": "They proposed to use iterative classification algorithm to capture discourselevel associations. "}
{"citeStart": 13, "citeEnd": 32, "citeStartToken": 13, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "state probabilities. In contrast, Liang et al. (2007) define a global DP over sequences, with the base measure defined over the global state probabilities, β; locally, each state has an HDP, with this global DP as the base measure. We believe our choice to be more linguistically sensible: in our model, for a particular state, dependent sequences which are similar to one another increase one another's likelihood. Additionally, their modeling decision made it difficult to define a Gibbs sampler, and instead they use variational inference. Earlier, Johnson et al. (2007) presented adaptor grammars, which is a very similar model to the HDP-PCFG. However they did not confine themselves to a binary branching structure and presented a more general framework for defining the process for splitting the states.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "state probabilities. ", "mid_sen": "In contrast, Liang et al. (2007) define a global DP over sequences, with the base measure defined over the global state probabilities, β; locally, each state has an HDP, with this global DP as the base measure. ", "after_sen": "We believe our choice to be more linguistically sensible: in our model, for a particular state, dependent sequences which are similar to one another increase one another's likelihood. "}
{"citeStart": 0, "citeEnd": 19, "citeStartToken": 0, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "Many variants of distributional similarity have been used in NLP (Lee 1999; Lee and Pereira 1999) . Dekang Lin's (1998) method is used here. In contrast to many implementations, which focus exclusively on verb-noun relationships, Lin's method incorporates a variety of syntactic relations. This is important for subjectivity recognition, because PSEs are not limited to verb-noun relationships. In addition, Lin's results are freely available.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many variants of distributional similarity have been used in NLP (Lee 1999; Lee and Pereira 1999) . ", "mid_sen": "Dekang Lin's (1998) method is used here. ", "after_sen": "In contrast to many implementations, which focus exclusively on verb-noun relationships, Lin's method incorporates a variety of syntactic relations. "}
{"citeStart": 67, "citeEnd": 81, "citeStartToken": 67, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. It is well-known in the machine learning community, however, that the success of a learning algorithm depends critically on the representation used to describe the training and test instances (Almuallim and Dietterich, 1991, Langley and Sage, in press ). Unfortunately, the task of designing an appropriate instance representation --also known as feature set selection --can be extraordinarily difficult, time-consuming, and knowledge-intensive (Quinlan, 1983) . This poses a problem for current statistical and machine learning approaches to natural language understanding where a new instance representation is typically required for each linguistic task tackled.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Standard symbolic machine learning techniques have been successfully applied to a number of tasks in natural language processing (NLP). ", "mid_sen": "Examples include the use of decision trees for syntactic analysis (Magerman, 1995) , coreference (Aone and Bennett, 1995; McCarthy and Lehnert, 1995) , and cue phrase identification (Litman, 1994) ; the use of inductive logic programming for learning semantic grammars and building prolog parsers (Zelle and Mooney, 1994; Zelle and Mooney, 1993) ; the use of conceptual clustering algorithms for relative pronoun resolution (Cardie, 1992a; Cardie 1992b) , and the use of case-based learning techniques for lexical tagging tasks (Cardie, 1993a; Daelemans et al., submitted) . ", "after_sen": "In theory, both statistical and machine learning techniques can significantly reduce the knowledge-engineering effort for building large-scale NLP systems: they offer an automatic means for acquiring robust heuristics for a host of lexical and structural disambiguation tasks. "}
{"citeStart": 173, "citeEnd": 184, "citeStartToken": 173, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to handle the non-linear phenomenon of Arabic, our model adopts the two-level formalism presented by (Pulman and Hepple, 1993) , with the multi tape extensions in (Kiraz, 1994) . Their forrealism appears in (2).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In order to handle the non-linear phenomenon of Arabic, our model adopts the two-level formalism presented by (Pulman and Hepple, 1993) , with the multi tape extensions in (Kiraz, 1994) . ", "after_sen": "Their forrealism appears in (2)."}
{"citeStart": 310, "citeEnd": 327, "citeStartToken": 310, "citeEndToken": 327, "sectionName": "UNKNOWN SECTION NAME", "string": "The Winnow update rule has, in addition to the threshold 9t at the target t, two update parameters: a promotion parameter a > 1 and a demotion parameter 0 < j3 < 1. These are being used to update the current representation of the target t (the set of weights w~) only when a mistake in prediction is made. Let .At --(Q,... ,ira} be the set of active features that are linked to the target node t. If the algot < St) and rithm predicts 0 (that is, ~ieAt withe received label is 1, the active weights in the current example are promoted in a multit If the plicative fashion: Vi E ,At, wl t +--o~ • w i. t 0t) and the algorithm predicts 1 (~ie.~t wi > received label is 0, the active weights in the curt t rent example are demoted Vi E .At, wi ~ 8 \" wi. All other weights are unchanged. The key feature of the Winnow update rule is that the number of examples required to learn a linear function grows linearly with the number of relevant features and only logarithmically with the total number of features. This property seems crucial in domains in which the number of potential features is vast, but a relatively small number of them is relevant. Winnow is known to learn efficiently any linear threshold function and to be robust in the presence of various kinds of noise and in cases where no linear-threshold function can make perfect classifications, while still maintaining its abovementioned dependence on the number of total and relevant attributes (Littlestone, 1991; Kivinen and Warmuth, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This property seems crucial in domains in which the number of potential features is vast, but a relatively small number of them is relevant. ", "mid_sen": "Winnow is known to learn efficiently any linear threshold function and to be robust in the presence of various kinds of noise and in cases where no linear-threshold function can make perfect classifications, while still maintaining its abovementioned dependence on the number of total and relevant attributes (Littlestone, 1991; Kivinen and Warmuth, 1995) .", "after_sen": "Once target subnetworks have been learned and the network is being evaluated, a decision support mechanism is employed, which selects the dominant active target node in the SNoW unit via a winner-take-all mechanism to produce a final prediction. "}
{"citeStart": 121, "citeEnd": 143, "citeStartToken": 121, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Both attentional (Cf) and propositional (mutual beliefs) structures are updated throughout. However, unlike attentional structures which are ephemeral in various time scales and empty at the end of the discourse (Grosz and Sidner, 1986) , mutual beliefs persist throughout the conversation, preserving at the end the semantic and pragmatic outcome of the discourse.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Both attentional (Cf) and propositional (mutual beliefs) structures are updated throughout. ", "mid_sen": "However, unlike attentional structures which are ephemeral in various time scales and empty at the end of the discourse (Grosz and Sidner, 1986) , mutual beliefs persist throughout the conversation, preserving at the end the semantic and pragmatic outcome of the discourse.", "after_sen": "In addition, while propositions can be excluded from the mutual beliefs because they fail to meet some inclusion criterion, no lexical denotation is excluded from Cf regardless of its propositional value. "}
{"citeStart": 166, "citeEnd": 186, "citeStartToken": 166, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "This problem is analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach. We were led to seek a solution for this problem within DRT, because of DRT's advantages as a general theory of discourse, and its choice as the underlying formalism in another research project of ours, which deals with sentences such as 1-4, in the context of natural language specifications of computerized systems. In this paper, we propose such a solution, based on a careful distinction between different roles of Reichenbach's reference time (Reichenbach, 1947) , adapted from (Kamp and Reyle, 1993) . Figure 1 shows a 'minimal pair' of DRS's for sentence 1, one according to Partee's(1984) analysis and one according to ours.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We were led to seek a solution for this problem within DRT, because of DRT's advantages as a general theory of discourse, and its choice as the underlying formalism in another research project of ours, which deals with sentences such as 1-4, in the context of natural language specifications of computerized systems. ", "mid_sen": "In this paper, we propose such a solution, based on a careful distinction between different roles of Reichenbach's reference time (Reichenbach, 1947) , adapted from (Kamp and Reyle, 1993) . ", "after_sen": "Figure 1 shows a 'minimal pair' of DRS's for sentence 1, one according to Partee's(1984) analysis and one according to ours."}
{"citeStart": 178, "citeEnd": 202, "citeStartToken": 178, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "The underlying principle of most language proficiency tests is the concept of reduced redundancy testing (Spolsky, 1969) . It is based on the idea that \"natural language is redundant\" and that more advanced learners can be dis-tinguished from beginners by their ability to deal with reduced redundancy. For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test. The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For language testing, redundancy can be reduced by eliminating words from a text and asking the learner to fill in the gap, also known as the cloze test. ", "mid_sen": "The C-test is a variant of the cloze test which contains more gaps but provides part of the solution as a hint and has been found to be a good estimate for language proficiency (Eckes and Grotjahn, 2006) .", "after_sen": "We present an approach for determining the difficulty of C-tests that overcomes the mentioned drawbacks of subjective evaluation by teachers. "}
{"citeStart": 29, "citeEnd": 44, "citeStartToken": 29, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions (Friedl, 1997) . For example, Emacs uses the special brackets \\( and \\) to capture strings along with the notation \\n to recall the nth such string. The expression \\(a*\\)b\\l matches strings of the form anba n. Unrestricted use of backreferencing thus can introduce non-regular languages. For NLP finite state calculi van Noord, 1997) this is unacceptable. The form of backreferences introduced in this paper will therefore be restricted.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The expression \\(a*\\)b\\l matches strings of the form anba n. Unrestricted use of backreferencing thus can introduce non-regular languages. ", "mid_sen": "For NLP finite state calculi van Noord, 1997) this is unacceptable. ", "after_sen": "The form of backreferences introduced in this paper will therefore be restricted."}
{"citeStart": 39, "citeEnd": 60, "citeStartToken": 39, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . EventMine-MK is available as a component of the U-Compare interoperable text mining system 4 (Kano et al., 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . ", "mid_sen": "In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. ", "after_sen": "The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . "}
{"citeStart": 41, "citeEnd": 64, "citeStartToken": 41, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "At this point, one may wonder if all statistical tests make such an independence assumption. The answer is no, but those tests that do not measure how much two techniques interact do need to make some assumption about that interaction and typically, that assumption is independence. Those tests that notice in some way how much two techniques interact can use those observations instead of relying on assumptions. One way to measure how two techniques interact is to compare how similarly the two techniques react to various parts of the test set. This is done in the matched-pair t test (Harnett, 1982, Sec. 8.7) . This test finds the difference between how techniques 1 and 2 perform on each test set sample. The t distribution and a form of equation 1 are used. The null hypothesis is still that the numerator d has a 0 mean, but d is now the sum of these difference values (divided by the number of samples), instead of being x 1 − x 2 . Similarly, the denominator s d is now estimating the standard deviation of these difference values, instead of being a function of s 1 and s 2 . This means for example, that even if the values from techniques 1 and 2 vary on different test samples, s d will now be 0 if on each test sample, technique 1 produces a value that is the same constant amount more than the value from technique 2.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One way to measure how two techniques interact is to compare how similarly the two techniques react to various parts of the test set. ", "mid_sen": "This is done in the matched-pair t test (Harnett, 1982, Sec. 8.7) . ", "after_sen": "This test finds the difference between how techniques 1 and 2 perform on each test set sample. "}
{"citeStart": 132, "citeEnd": 149, "citeStartToken": 132, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003) , Shen et al. (2003) . However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001) . UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS. For example, for the phrase immediate systemic anticoagulants, MetaMap identifies immediate as a TEMPORAL CONCEPT, systemic as a FUNCTIONAL CONCEPT, and anticoagulants as a PHARMACOLOGIC SUBSTANCE. More than one semantic category in UMLS may correspond to MED-ICATION or DISEASE. For example, either a PHAR-MACOLOGIC SUBSTANCE or a THERAPEUTIC OR PREVENTIVE PROCEDURE can be a MEDICATION; either a DISEASE OR SYNDROME or a PATHOLOGIC FUNCTION can be a DISEASE.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003) , Shen et al. (2003) . ", "after_sen": "However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. "}
{"citeStart": 117, "citeEnd": 121, "citeStartToken": 117, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "A featnre-based tag grammar was written for this investigation (based loosely on one written by Briscoe and Waegner [1992] ), and used in conjunction with tile parser inchlded in the Alvey Tools' Grammar Development Environment (ODE) [Carroll etal, 1991] , which allows for rapid prototyping aud e,~sy analysis of parses. It should be stressed that this grammar is solely one of tags, aild so is not very detailed syntactically.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "S 4 lip (eonllna) s.", "mid_sen": "A featnre-based tag grammar was written for this investigation (based loosely on one written by Briscoe and Waegner [1992] ), and used in conjunction with tile parser inchlded in the Alvey Tools' Grammar Development Environment (ODE) [Carroll etal, 1991] , which allows for rapid prototyping aud e,~sy analysis of parses. ", "after_sen": "It should be stressed that this grammar is solely one of tags, aild so is not very detailed syntactically."}
{"citeStart": 115, "citeEnd": 132, "citeStartToken": 115, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to maintain structural coherence, the new word attached via tree-lowering must be preceded by all other words previously attached into the description. We can guarantee this by requiring the lowered node to dominate the last word to be attached. We also need to ensure that, to avoid crossing branches, the lowered node does not dominate any unsaturated attachment sites (or \"dangling nodes\") We therefore define accessibility for tree-lowering as follows: It will be noticed that tree-lowering is similar in spirit to the adjunction operation of Tree Adjoining Grammars (Joshi et al, 1975) . The difference is that the foot and root nodes of an auxiliary tree in TAG, (corresponding to the \"lowered\" node and the node that replaces it respectively) must be of the same syntactic category, whereas, as we have seen in this example, in the model proposed here, the two nodes may be of different categories, so long as the resulting structure is licensed by the grammar.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also need to ensure that, to avoid crossing branches, the lowered node does not dominate any unsaturated attachment sites (or \"dangling nodes\") We therefore define accessibility for tree-lowering as follows: ", "mid_sen": "It will be noticed that tree-lowering is similar in spirit to the adjunction operation of Tree Adjoining Grammars (Joshi et al, 1975) . ", "after_sen": "The difference is that the foot and root nodes of an auxiliary tree in TAG, (corresponding to the \"lowered\" node and the node that replaces it respectively) must be of the same syntactic category, whereas, as we have seen in this example, in the model proposed here, the two nodes may be of different categories, so long as the resulting structure is licensed by the grammar."}
{"citeStart": 17, "citeEnd": 38, "citeStartToken": 17, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "We use RFTagger (Schmid and Laws, 2008) for POS tagging. Despite the good quality of tagging provided by RFTagger, some errors seem to be unavoidable due to the ambiguity of certain grammatical forms in Russian. A good example of this is neuter nouns that have the same form in all cases, or feminine nouns, which have identical forms in singular genitive and plural nominative (Sharoff et al., 2008) . Since Russian sentences have free word order, and the case of nouns cannot be determined on that basis, this imperfection can not be corrected during tagging or by postprocessing the tagger output.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use RFTagger (Schmid and Laws, 2008) for POS tagging. ", "after_sen": "Despite the good quality of tagging provided by RFTagger, some errors seem to be unavoidable due to the ambiguity of certain grammatical forms in Russian. "}
{"citeStart": 226, "citeEnd": 248, "citeStartToken": 226, "citeEndToken": 248, "sectionName": "UNKNOWN SECTION NAME", "string": "Before explaining our CLIR system, we classify existing CLIR into three approaches in terms of the implementation of the translation phase. The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . We prefer the first approach, the \"query translation\", to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. At the same time, we concede that other CLIR approaches are worth further exploration. Figure 1 depicts the overall design of our CLIR system, where most components are the same as those for monolingual IR, excluding \"translator\".", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . ", "mid_sen": "The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . ", "after_sen": "We prefer the first approach, the \"query translation\", to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. "}
{"citeStart": 21, "citeEnd": 35, "citeStartToken": 21, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Research has been done within this domain, mainly for English medical documents. Systems developed by Pakhomov (2002; , Yu et al. (2003) and Gaudan et al. (2005) achieved 84% to 98% accuracy. These systems used various machine learning (ML) methods, e.g.: Maximum Entropy, SVM and C5.0.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Research has been done within this domain, mainly for English medical documents. ", "mid_sen": "Systems developed by Pakhomov (2002; , Yu et al. (2003) and Gaudan et al. (2005) achieved 84% to 98% accuracy. ", "after_sen": "These systems used various machine learning (ML) methods, e.g.: Maximum Entropy, SVM and C5.0."}
{"citeStart": 117, "citeEnd": 131, "citeStartToken": 117, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996) , a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998) . The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? What is the effect of cascading? Will errors at a lower level percolate to higher modules?", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. ", "mid_sen": "Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996) , a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998) . ", "after_sen": "The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? "}
{"citeStart": 68, "citeEnd": 98, "citeStartToken": 68, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "Several measures of distributional similarity have been proposed in the literature (Dagan, Lee, and Pereira 1999; . We used two measures, the Jensen-Shannon divergence and the confusion probability. The choice of these two measures was motivated by work described in Dagan, Lee, and Pereira (1999) , in which the Jensen-Shannon divergence outperforms related similarity measures (such as the confusion probability or the L 1 norm) on a pseudodisambiguation task that uses verb-object pairs. The confusion probability has been used by several authors to smooth word co-occurrence probabilities (Essen and Steinbiss 1992; Grishman and Sterling 1994) and shown to give promising performance. Grishman and Sterling (1994) in particular employ the confusion probability to re-create the frequencies of verb-noun co-occurrences in which the noun is the object or the subject of the verb in question. In the following we describe these two similarity measures and show how they can be used to re-create the frequencies for unseen verb-argument tuples (for a more detailed description see Dagan, Lee, and Pereira (1999) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used two measures, the Jensen-Shannon divergence and the confusion probability. ", "mid_sen": "The choice of these two measures was motivated by work described in Dagan, Lee, and Pereira (1999) , in which the Jensen-Shannon divergence outperforms related similarity measures (such as the confusion probability or the L 1 norm) on a pseudodisambiguation task that uses verb-object pairs. ", "after_sen": "The confusion probability has been used by several authors to smooth word co-occurrence probabilities (Essen and Steinbiss 1992; Grishman and Sterling 1994) and shown to give promising performance. "}
{"citeStart": 132, "citeEnd": 155, "citeStartToken": 132, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999) . This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not simply the result of parser error or noise. 1", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string-and token-based features.", "mid_sen": "Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999) . ", "after_sen": "This data consists of triples (v, n, n ′ ) where v, n is a predicateargument pair observed in the corpus and v, n ′ has not been observed. "}
{"citeStart": 190, "citeEnd": 218, "citeStartToken": 190, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "The 'magic-compiled grammar' in figure 2 is the result of applying the algorithm in the previous section to the head-recursive example grammar and subsequently performing two optimizations (Beeri and Ramakrishnan, 1991) : All (calls to) magic predicates corresponding to lexical entries are removed. Furthermore, data-flow analysis is used to fine-tune the magic predicates for the specific processing task at hand, i.e., generation3 Given a user-specified abstract query, i.e., a specification of the intended input (Beeri and Ramakrishnan, 1991) those arguments which are not bound and which therefore serve no filtering purpose are removed. The modified versions of the original rules in the grammar are adapted accordingly. The effect of taking data-flow into account can be observed by comparing the rules for mag±c_vp and mag±c_np in the previous section with rule 12 and 14 in figure 2, respectively. Figure 3 shows the results from generation of the sentence \"John buys Mary a book\". In the case of this example the seed looks as follows: magic_sentence (decl (buys (john, a (book) ,mary) ) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The use of such a semantic filter in bottom-up evaluation requires the grammar to obey the semantic monotonicity constraint in order to ensure completeness (Shieber, 1988 ) (see below).", "mid_sen": "The 'magic-compiled grammar' in figure 2 is the result of applying the algorithm in the previous section to the head-recursive example grammar and subsequently performing two optimizations (Beeri and Ramakrishnan, 1991) : ", "after_sen": "All (calls to) magic predicates corresponding to lexical entries are removed. "}
{"citeStart": 109, "citeEnd": 117, "citeStartToken": 109, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ( [Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 162, "citeEnd": 191, "citeStartToken": 162, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. , Poznanski & Sanfilippo (1993) , Resnik (1993) , Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. ", "mid_sen": "It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. , Poznanski & Sanfilippo (1993) , Resnik (1993) , Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. ", "after_sen": "Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. "}
{"citeStart": 86, "citeEnd": 104, "citeStartToken": 86, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "The culling of irrelevant features also has the benefit of reducing node training times and facil- Class similarity measures. As mentioned above, Table 1 shows that the Tanimoto coefficient, coupled with feature culling, yields marginally better results than the centroid/noculling option for most authors in the three-class dataset, and significantly better results for all the authors in the four-class dataset. The Tanimoto coefficient generally matches or outperforms the centroid/Euclidean-distance measure both with feature culling (Columns 4 and 5 in Table 1 ) and without feature culling (Columns 2 and 3). However, without feature culling, these improvements are not statistically significant. For most cases in the three-star dataset, the tree structures found using the Tanimoto coefficient are identical to those found using the Euclideancentroid option, hence the performance of the classifier is unchanged. For some validation folds, the Tanimoto coefficient discovered tree structures that differed from those found by the Euclidean-centroid option, generally yielding small accuracy improvements (e.g., 0.98% for Author A in the three-star dataset, with feature culling). The Tanimoto coefficient provides a greater benefit for the four-class dataset. Specifically, when feature culling is used (Columns 4 and 5 in Table 1 ), accuracy improves by 2.63% and 2.27% for Authors A and B respectively (statistically significant), and by 1.29% and 0.70% for Authors C and D respectively. This may be explained by the fact that there are many more tree structures possible for the four-class case than the three-class case, thereby increasing the impact of the inter-class similarity measure for the four-class case. However, this impact is significant only in conjunction with feature culling. Pang and Lee (2005) Figure 2 compares the performance of the algorithms presented in (Pang and Lee, 2005) against the performance of the best MCST-SVM variant, which employs feature culling and uses the Tanimoto coefficient to compute inter-class similarity (Section 4.2). As per (Pang and Lee, 2005) , REG indicates SVM-R, which is the baseline ordinal regression method. The suffix \"+PSP\" denotes methods that use the metric labeling scheme. We excluded DAGSVM from our results to maintain consistency with Pang and Lee's experiments. However, according to (Platt et al., 2000) , the performance difference between DAGSVM and OVA is not statistically significant.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, this impact is significant only in conjunction with feature culling. ", "mid_sen": "Pang and Lee (2005) Figure 2 compares the performance of the algorithms presented in (Pang and Lee, 2005) against the performance of the best MCST-SVM variant, which employs feature culling and uses the Tanimoto coefficient to compute inter-class similarity (Section 4.2). ", "after_sen": "As per (Pang and Lee, 2005) , REG indicates SVM-R, which is the baseline ordinal regression method. "}
{"citeStart": 0, "citeEnd": 18, "citeStartToken": 0, "citeEndToken": 18, "sectionName": "UNKNOWN SECTION NAME", "string": "To counter problem (1), we use the compound word translation method we proposed (Fujii and Ishikawa, 1999) , which selects appropriate translations based on the probability of occurrence of each combination of base words in the target language. For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. However, since their method needs large-scale phoneme inventories, we propose a simpler approach using surface mapping between English and katakana characters, rather than sounds. Section 2 overviews our CLIR system, and Section 3 elaborates on the translation module focusing on compound word translation and transliteration. Section 4 then evaluates the effectiveness of our CLIR system by way of the standardized IR evaluation method used in TREC programs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For problem (2), we use \"transliteration\" (Chen et al., 1998; Knight and Graehl, 1998; Wan and Verspoor, 1998) . ", "mid_sen": "Chen et al. (1998) and Wan and Verspoor (1998) proposed English-Chinese transliteration methods relying on the property of the Chinese phonetic system, which cannot be directly applied to transliteration between English and Japanese. ", "after_sen": "Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. "}
{"citeStart": 11, "citeEnd": 23, "citeStartToken": 11, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002) . Following (Chiang, 2005) , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the \"diag-and\" method of . Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002) . ", "mid_sen": "Following (Chiang, 2005) , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. ", "after_sen": "First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. "}
{"citeStart": 36, "citeEnd": 55, "citeStartToken": 36, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "To better determine stemming rules, Xu and Croft (1998) propose a selective stemming method based on corpus analysis. They refine the Porter stemmer by means of word clustering: words are first clustered according to their co-occurrences in the text collection. Only word variants belonging to the same cluster will be conflated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A standard stemmer such as Porter's will wrongly stem them.", "mid_sen": "To better determine stemming rules, Xu and Croft (1998) propose a selective stemming method based on corpus analysis. ", "after_sen": "They refine the Porter stemmer by means of word clustering: words are first clustered according to their co-occurrences in the text collection. "}
{"citeStart": 162, "citeEnd": 175, "citeStartToken": 162, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) . The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures ( feature sets, but then efficiency becomes a problem. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on re-ranking using a finite set of features (Collins, 2000) . In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000) . The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999) . Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. ", "mid_sen": "The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) . ", "after_sen": "The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. "}
{"citeStart": 103, "citeEnd": 124, "citeStartToken": 103, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "The reordering models we describe follow our previous work using function word models for translation (Setiawan et al., 2007; Setiawan et al., 2009) . The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. To make this insight useful for alignment, we develop features that score the alignment configuration of the neighboring phrases of a function word (which functions as an anchor) using two kinds of information: 1) the relative ordering of the phrases with respect to the function word anchor; and 2) the span of the phrases. This section provides a high level overview of our reordering model, which attempts to leverage this information.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The reordering models we describe follow our previous work using function word models for translation (Setiawan et al., 2007; Setiawan et al., 2009) . ", "after_sen": "The core hypothesis in this work is that function words provide robust clues to the reordering patterns of the phrases surrounding them. "}
{"citeStart": 99, "citeEnd": 113, "citeStartToken": 99, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Current interfaces to cable and satellite television services typically use direct manipulation of a graphical user interface using a remote control. In order to find content, users generally have to either navigate a complex, pre-defined, and often deeply embedded menu structure or type in titles or other key phrases using an onscreen keyboard or triple tap input on a remote control keypad. These interfaces are cumbersome and do not scale well as the range of content available increases (Berglund, 2004; Mitchell, 1999) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to find content, users generally have to either navigate a complex, pre-defined, and often deeply embedded menu structure or type in titles or other key phrases using an onscreen keyboard or triple tap input on a remote control keypad. ", "mid_sen": "These interfaces are cumbersome and do not scale well as the range of content available increases (Berglund, 2004; Mitchell, 1999) .", "after_sen": ""}
{"citeStart": 88, "citeEnd": 110, "citeStartToken": 88, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "Unlike in (Jiang and Ng, 2006) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data. Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al., 2005) . After the features of every constituent are extracted, each constituent is simply classified independently as either argument or non-argument.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unlike in (Jiang and Ng, 2006) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data. ", "mid_sen": "Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al., 2005) . ", "after_sen": "After the features of every constituent are extracted, each constituent is simply classified independently as either argument or non-argument."}
{"citeStart": 141, "citeEnd": 145, "citeStartToken": 141, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "Language Production Model ~).r bmguage model can be viewed in terms of a probabihstic generative process based on the choice of lexical \"heads\" of phrases and the recursive generation of sub-;,bra~es and their ordering. For this purpose, we can de-(ira, tho head word of a phrase to be the word that most strongly influences the way the phrase may be combiucd with other phrases. This notion has been central to a number of approaches to grammar for some time, including theories like dependency grammar (Hudson I!~7 (;, 1990) and HPSG (Pollard and Sag 1987) . More ;,'~,.t,l. ly, the statistical properties of associations be-Iw,.,'n words, and more particularly heads of phrases, JL:t.~ J~,~'~,l|lql, all a.el.iw; area of research (e.g. Chang, l,uo, aml Su 1992; Ilindlc and R.ooth 1993) . 'l'h,' language model factors the statistical derivation ,,f a .~'ul.ence with word string W as follows: I'(ll) = ~,: P(C) P (WIC) where C ranges over relation graphs. The content model, P(C), and generation model, P(WIC), are components of the overall statistical model for spoken language translation given earlier. This decomposition of P(W) can be viewed as first deciding on the content of a sentence, formulated as a set of relation edges according to a statistical model for P(C), and then deciding on word order according to P(WIC ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For this purpose, we can de-(ira, tho head word of a phrase to be the word that most strongly influences the way the phrase may be combiucd with other phrases. ", "mid_sen": "This notion has been central to a number of approaches to grammar for some time, including theories like dependency grammar (Hudson I!~7 (;, 1990) and HPSG (Pollard and Sag 1987) . ", "after_sen": "More ;,'~,.t,l. ly, the statistical properties of associations be-Iw,.,'n words, and more particularly heads of phrases, JL:t.~ J~,~'~,l|lql, all a.el.iw; area of research (e.g. Chang, l,uo, aml Su 1992; Ilindlc and R.ooth 1993) . 'l'h,' language model factors the statistical derivation ,,f a .~'ul."}
{"citeStart": 168, "citeEnd": 184, "citeStartToken": 168, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agn~ et al, 1994) . Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). When a sentence was analysed successfully, several semantic analyses were, in general, created, and a selection was made from among these on the basis of trained preference functions (Alshawi and Carter, 1994) . For the purpose of the experiment, clustering and hypothesis selection were performed on the basis not of the words in a sentence but of the grammar rules used to construct its most preferred analysis.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the second experiment, each training sentence and each test sentence hypothesis was analysed by the Core Language Engine (Alshawi, 1992) trained on the ATIS domain (Agn~ et al, 1994) . ", "after_sen": "Unanalysable sentences were discarded, as were sentences of over 15 words in length (the ATIS adaptation had concentrated on sentences of 15 words or under, and analysis of longer sentences was less reliable and slower). "}
{"citeStart": 39, "citeEnd": 63, "citeStartToken": 39, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "The work presented in this section highlights a number of issues associated with the evaluation of automatically induced subcategorization frames against an existing external gold standard, in this case COMLEX. While this evaluation approach is arguably less labor-intensive than the manual construction of a custom-made gold standard, it does introduce a number of difficulties into the evaluation procedure. It is a nontrivial task to convert both the gold standard and the induced resource to a common format in order to facilitate evaluation. In addition, as our results show, the choice of common format and mapping to it can affect the results. In COMLEX-LFG Mapping I (Section 6.2), we found that mapping from the induced lexicon to COMLEX resulted in higher recall scores than those achieved when we (effectively) reversed the mapping ). The first mapping is essentially a conflation of our more fine-grained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible. Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above, it is well documented (Roland and Jurafsky 1998 ) that subcategorization frames (and their frequencies) vary across domains. We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt, for example, the overgeneration of subcategorized-for directional prepositional phrases. This is because the aim of the COMLEX project was to construct as complete a set of subcategorization frames as possible, even for infrequent verbs. Lexicographers were allowed to extrapolate from the citations found, a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples. As a generalization, Briscoe (2001) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall. Briscoe and Carroll (1997) report on manually analyzing an open-class vocabulary of 35,000 head words for predicate subcategorization information and comparing the results against the subcategorization details in COMLEX. Precision was quite high (95%), but recall was low (84%). This has an effect on both the precision and recall scores of our system against COMLEX. In order to ascertain the effect of using COMLEX as a gold standard for our induced lexicon, we carried out some more-detailed error analysis, the results of which are summarized in Table 26 . We randomly selected 80 false negatives (fn) and 80 false positives (fp) across a range of active frame types containing prepositional and particle detail taken from Penn-III and manually examined them in order to classify them as \"correct\" or \"incorrect.\" Of the 80 fps, 33 were manually judged to be legitimate subcategorization frames. For example, as Table 26 shows, there are a number of correct transitive verbs ([subj,obj] ) in our automatically induced lexicon which are not included in COMLEX. This examination was also useful in highlighting to us the frame types on which the lexical extraction procedure was performing poorly, in our case, those containing XCOMPs and those containing OBJ2S. Out of 80 fns, 14 were judged to be incorrect when manually examined. These can be broken down as follows: one intransitive frame, three ditransitive frames, three frames containing a COMP, and seven frames containing an oblique were found to be invalid.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. ", "mid_sen": "As noted above, it is well documented (Roland and Jurafsky 1998 ) that subcategorization frames (and their frequencies) vary across domains. ", "after_sen": "We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. "}
{"citeStart": 168, "citeEnd": 182, "citeStartToken": 168, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "The results of the evaluation are exlremely encouraging, especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs (Hearst, 1991; Cowie et al., 1992) . A note worth adding: it is not clear that the \"exact match\" criterion --that is, evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline --is the right task. In particular, in many tasks it is at least as important to avoid inappropriate senses than to select exactly the right one. This would be the case in query expansion for information retrieval, for example, where indiscriminately adding inappropriate words to a query can degrade performance (Voorhees, 1994) . The examples presented in Section 3 are encouraging in this regard: in addition to performing well at the task of assigning a high score to the best sense, it does a good job of assigning low scores to senses that are clearly inappropriate.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In particular, in many tasks it is at least as important to avoid inappropriate senses than to select exactly the right one. ", "mid_sen": "This would be the case in query expansion for information retrieval, for example, where indiscriminately adding inappropriate words to a query can degrade performance (Voorhees, 1994) . ", "after_sen": "The examples presented in Section 3 are encouraging in this regard: in addition to performing well at the task of assigning a high score to the best sense, it does a good job of assigning low scores to senses that are clearly inappropriate."}
{"citeStart": 139, "citeEnd": 164, "citeStartToken": 139, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "We have applied majority voting and nine other combination methods to the output of the learning systems that were applied to the three tasks. Nine combination methods were originally suggested by Van Halteren et al. (1998) . Five of them, including majority voting, are so-called voting methods. Apart from majority voting, all assign weights to the predictions of the different systems based on their performance on non-used training data, the tuning data. TOTPRECISION uses classifier weights based on their accuracy. TAG-PRECISION applies classification weights based on the accuracy of the classifier for that classification. PRECISION-RECALL uses classification weights that combine the precision of the classification with the recall of the competitors. And finally, TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair (van Halteren et al., 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "PRECISION-RECALL uses classification weights that combine the precision of the classification with the recall of the competitors. ", "mid_sen": "And finally, TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair (van Halteren et al., 1998) .", "after_sen": "The remaining four combination methods are so-called STACKED CLASSIFIERS. "}
{"citeStart": 88, "citeEnd": 110, "citeStartToken": 88, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The relative success of the Gemini system suggests a new question. Uni cation grammars have been used many times to build substantial general grammars for English and other natural languages, but the language model oriented grammars so far developed for Gemini including the one for CommandTalk have all been domain-speci c. One naturally wonders how feasible it is to take y et another step in the direction of increased generality; roughly, what we w ant to do is start with a completely general, linguistically motivated grammar, combine it with a domain-speci c lexicon, and compile the result down to a domain-speci c contextfree grammar that can be used as a language model. If this programme can be realized, it is easy to believe that the result would be an extremely useful methodology for rapid construction of language models. It is important to note that there are no obvious theoretical obstacles in our way. The claim that English is contextfree has been respectable since at least the early 80s Pullum and Gazdar 1982 , and the idea of using uni cation grammar as a compact way of representing an underlying context-free language is one of the main motivations for GPSG Gazdar et al 1985 and other formalisms based on it. The real question is whether the goal is practically achievable, given the resource limitations of current technology.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is important to note that there are no obvious theoretical obstacles in our way. ", "mid_sen": "The claim that English is contextfree has been respectable since at least the early 80s Pullum and Gazdar 1982 , and the idea of using uni cation grammar as a compact way of representing an underlying context-free language is one of the main motivations for GPSG Gazdar et al 1985 and other formalisms based on it. ", "after_sen": "The real question is whether the goal is practically achievable, given the resource limitations of current technology."}
{"citeStart": 241, "citeEnd": 260, "citeStartToken": 241, "citeEndToken": 260, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998) . Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998 ), which we leave for future work. rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1 .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. ", "mid_sen": "Since task initiative models contribution to domain/problemsolving goals, while dialogue initiative affects the cur-5An alternative strategy to step (4) is to perform a database lookup based on the ambiguous query and summarize the results (Litman et al., 1998 ), which we leave for future work. ", "after_sen": "rent discourse goal, we developed alternative strategies for achieving the goals in Figure 4 based on initiative distribution, as shown in Table 1 ."}
{"citeStart": 100, "citeEnd": 123, "citeStartToken": 100, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "In most social tagging sites, many documents have been annotated by few tags and most users have only annotated few documents. In this case, existing tag-based summarization methods will fail to produce a personalized summary (Boydell and Smyth, 2007; Zhu et al., 2009) , since the user-related tags may be absent for that document. To address it, we propose to expand both the target document and the intended user with appropriate social context so that the important parts in the document that the intended user may care about can be identified from context.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In most social tagging sites, many documents have been annotated by few tags and most users have only annotated few documents. ", "mid_sen": "In this case, existing tag-based summarization methods will fail to produce a personalized summary (Boydell and Smyth, 2007; Zhu et al., 2009) , since the user-related tags may be absent for that document. ", "after_sen": "To address it, we propose to expand both the target document and the intended user with appropriate social context so that the important parts in the document that the intended user may care about can be identified from context."}
{"citeStart": 216, "citeEnd": 233, "citeStartToken": 216, "citeEndToken": 233, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we will address the problem of sense discrimination as defined above. That is, we will not be concerned with the sense-labeling component of word sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. ", "mid_sen": "Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "after_sen": "What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses."}
{"citeStart": 154, "citeEnd": 167, "citeStartToken": 154, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "Phrase-based decoding (Koehn et al., 2003) is a dominant formalism in statistical machine translation. Contiguous segments of the source are translated and placed in the target, which is constructed from left to right. The process iterates within a beam search until each word from the source has been covered by exactly one phrasal translation. Candidate translations are scored by a linear combination of models, weighted according to Minimum Error Rate Training or MERT (Och, 2003) . Phrasal SMT draws strength from being able to memorize noncompositional and context-specific translations, as well as local reorderings. Its primary weakness is in movement modeling; its default distortion model applies a flat penalty to any deviation from source order, forcing the decoder to rely heavily on its language model. Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Its primary weakness is in movement modeling; its default distortion model applies a flat penalty to any deviation from source order, forcing the decoder to rely heavily on its language model. ", "mid_sen": "Recently, a number of data-driven distortion models, based on lexical features and relative distance, have been proposed to compensate for this weakness (Tillman, 2004; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006) .", "after_sen": "There have been a number of proposals to incorporate syntactic information into phrasal decoding. "}
{"citeStart": 186, "citeEnd": 208, "citeStartToken": 186, "citeEndToken": 208, "sectionName": "UNKNOWN SECTION NAME", "string": "The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words. This is in contrast to many other measures, e.g., Lin (1998) , which use the co-occurrences of features with other words to compute a weighting function such as mutual information (MI) (Church and Hanks, 1989 ). Since we only have corpus data for the target phrases, it is not possible for us to use such a measure. However, the α-skew divergence measure has been shown (Weeds, 2003) to perform comparably with measures which use MI, particularly for lower frequency target words.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words. ", "mid_sen": "This is in contrast to many other measures, e.g., Lin (1998) , which use the co-occurrences of features with other words to compute a weighting function such as mutual information (MI) (Church and Hanks, 1989 ). ", "after_sen": "Since we only have corpus data for the target phrases, it is not possible for us to use such a measure. "}
{"citeStart": 146, "citeEnd": 147, "citeStartToken": 146, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "It remains to I)e seeit how bottom-up Barley deduction compares with (and can be combined with) the improved top-down Barley deduction of l)hrre [3] , Johnson [7] mud Neumann [91, and to head-driven methods with well-formed substring tables [1] , and which methods are best suited for which kinds of problems (e.g. parsing, generation, noisy input, incremental processing etc.).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, it is not clear how well the directed methods are applicable to grammars which do not depend on concatenation and have no unique 'left cornet\" which should be connected to the start symbol.", "mid_sen": "It remains to I)e seeit how bottom-up Barley deduction compares with (and can be combined with) the improved top-down Barley deduction of l)hrre [3] , Johnson [7] mud Neumann [91, and to head-driven methods with well-formed substring tables [1] , and which methods are best suited for which kinds of problems (e.g. parsing, generation, noisy input, incremental processing etc.).", "after_sen": ""}
{"citeStart": 82, "citeEnd": 93, "citeStartToken": 82, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "The CoreSC scheme is accompanied by 47-page annotation guidelines, and has been used by 16 domain experts to annotate a corpus of 265 full papers from physical chemistry & biochemistry (Liakata & Soldatova, 2009; Liakata et al., 2010) . This corpus consists of 40,000 sentences, containing over 1 million words and was developed in three phases (for details see Liakata et al. (2012) ). Inter-annotator agreement between experts was measured in terms of Cohen's kappa (Cohen, 1960) on 41 papers and ranged between 0.5 and 0.7. Machine learning classifiers have been trained on the CoreSC corpus, achieving > 51% accuracy across the eleven categories. The most accurately predicted category is Experiment, the category describing experimental methods (Liakata et al., 2012) . Classifiers trained on 1000 Biology abstracts annotated with CoreSC have obtained an accuracy of over 80% (Guo et al., 2010) . Models trained on the CoreSC corpus papers have been used to create automatic summaries of the papers, which have been evaluated in a question answering task (Liakata et al., 2012) . Lastly, the CoreSC scheme was used to annotate 50 papers from Pubmed Central pertaining to Cancer Risk Assessment. A web tool (SAPIENTA 1 ) allows users to annotate their full papers with Core Scientific concepts, and can be combined with manual annotation. A UIMA framework 2 implementation of this code for large-scale annotation of CoreSC concepts is in progress.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This corpus consists of 40,000 sentences, containing over 1 million words and was developed in three phases (for details see Liakata et al. (2012) ). ", "mid_sen": "Inter-annotator agreement between experts was measured in terms of Cohen's kappa (Cohen, 1960) on 41 papers and ranged between 0.5 and 0.7. ", "after_sen": "Machine learning classifiers have been trained on the CoreSC corpus, achieving > 51% accuracy across the eleven categories. "}
{"citeStart": 55, "citeEnd": 70, "citeStartToken": 55, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "This approach is somehow similar to those proposed by (Qi et al., 2004) and (Xu y Schuurmans, 2005) . Nonetheless, the 2-steps-SVM approach uses the same method for both the first and second steps. A supervised multiclass SVM is used to increase the labeled set and, after that, to classify the test set.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the second step, now with a fully labeled training set, the usual supervised classification process is done, learning with the training documents and predicting the documents in the test set.", "mid_sen": "This approach is somehow similar to those proposed by (Qi et al., 2004) and (Xu y Schuurmans, 2005) . ", "after_sen": "Nonetheless, the 2-steps-SVM approach uses the same method for both the first and second steps. "}
{"citeStart": 88, "citeEnd": 110, "citeStartToken": 88, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "To implement the method mentioned in Section 2, we use the weights W of topics and foci, the distance D, the definiteness P, and the semantic similarity S (in R4 of Section 3.2) to determine points. The weights W oftopics and foci are given in Table 3 and  Table 4 respectively in Section 2, and represent the preferability of the desired antecedent. In this work, a topic is defined as a theme which is described, and a focus is defined as a word which is stressed by the speaker (or the writer). But we cannot detect topics and foci correctly. Therefore we approximated them as shown in Table 3 and Table 4 . The distance D is the number of the topics (foci) between the anaphor and a possible antecedent which is a topic (focus). The value P is given by the score of the definiteness in referential property analysis (Murata and Nagao, 1993) . This is because it is easier for a definite noun phrase to have an antecedent than for an indefinite noun phrase to have one. The value S is the semantic similarity between a possible antecedent and Noun X of \"Noun X no Noun Y.\" Semantic similarity is shown by level in Bunrui Goi Hyou (NLRI, 1964) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The distance D is the number of the topics (foci) between the anaphor and a possible antecedent which is a topic (focus). ", "mid_sen": "The value P is given by the score of the definiteness in referential property analysis (Murata and Nagao, 1993) . ", "after_sen": "This is because it is easier for a definite noun phrase to have an antecedent than for an indefinite noun phrase to have one. "}
{"citeStart": 200, "citeEnd": 220, "citeStartToken": 200, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Neither Kamp nor Kehler extend their copying/substitution mechanism to anything besides pronouns, as we have done. In Kehler's case, it is hard to see how his role assignment functions can be extended to deal with non-referential terms in the desired manner. DRT's use of discourse referents to indicate scope suggests that Kamp's treatment may be more readily extended in this manner; lists of discourse referents at the top of DRS boxes are highly reminiscent of the index lists in scope nodes. Figure 1 defines a valuation relation for the QLF fragment used above, derived from Cooper et al., 1994a) . If a QLF expression contains uninstantiated recta-variables, the valuation relation can associate more than one value with the expression. In the case of formulas, they may be given both the values true and false, corresponding to the formula being true under one possible resolution and false under another. A subsumption ordering over QLFS, ~, is employed in the evaluation rules, in effect to propose possible instantiations for meta-variables (the rule fragment only allows for scope meta-variables, but (Cooper et al., 1994a) describes the more general case where other kinds of meSa-variable are permitted). A partially instantiated QLF therefore effectively specifies a set of possible evaluations (or semantic compositions).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the case of formulas, they may be given both the values true and false, corresponding to the formula being true under one possible resolution and false under another. ", "mid_sen": "A subsumption ordering over QLFS, ~, is employed in the evaluation rules, in effect to propose possible instantiations for meta-variables (the rule fragment only allows for scope meta-variables, but (Cooper et al., 1994a) describes the more general case where other kinds of meSa-variable are permitted). ", "after_sen": "A partially instantiated QLF therefore effectively specifies a set of possible evaluations (or semantic compositions)."}
{"citeStart": 217, "citeEnd": 229, "citeStartToken": 217, "citeEndToken": 229, "sectionName": "UNKNOWN SECTION NAME", "string": "While such systems have been useful in limited contexts, they have also been criticized as cumbersome: by forcing users to conform to a particular formal system, they constrain communication and make it less natural (Schoop, 2001) ; in short, users often prefer unstructured email interactions (Camino et al. 1998) . We note that these difficulties are avoided if messages can be automatically annotated by intent, rather than soliciting a statement of intent from the user. Murakoshi et al. (1999) proposed an email annotation scheme broadly similar to ours, called a \"deliberation tree\", and an algorithm for constructing deliberation trees automatically, but their approach was not quantitatively evaluated. The approach is based on recognizing a set of hand-coded linguistic \"clues\". A limitation of their approach is that these hand-coded linguistic \"clues\" are language-specific (and in fact limited to Japanese text.)", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Coordinator (Winograd, 1987) was one such system, in which users augmented email messages with additional annotations indicating intent.", "mid_sen": "While such systems have been useful in limited contexts, they have also been criticized as cumbersome: by forcing users to conform to a particular formal system, they constrain communication and make it less natural (Schoop, 2001) ; in short, users often prefer unstructured email interactions (Camino et al. 1998) . ", "after_sen": "We note that these difficulties are avoided if messages can be automatically annotated by intent, rather than soliciting a statement of intent from the user. "}
{"citeStart": 115, "citeEnd": 136, "citeStartToken": 115, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006) , is a promising way forward in this regard.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. ", "mid_sen": "Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006) , is a promising way forward in this regard.", "after_sen": ""}
{"citeStart": 274, "citeEnd": 287, "citeStartToken": 274, "citeEndToken": 287, "sectionName": "UNKNOWN SECTION NAME", "string": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992) , we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c ; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). ", "mid_sen": "Like Cowie, Guthrie, and Guthrie (1992) , we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c ; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions.", "after_sen": "In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2)."}
{"citeStart": 1, "citeEnd": 34, "citeStartToken": 1, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Three papers mention having used the memorybased learning method IBI-IG. (Veenstra, 1998) introduced cascaded chunking, a two-stage process in which the first stage classifications are used to improve the performance in a second processing stage. This approach reaches the same performance level as Argamon et al. but it requires lexical information. (Daelemans et al., 1999a ) report a good performance for baseNP recognition but they use a different data set and do not mention precision and recall rates. (Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. Their baseNP results are slightly better than those of Ramshaw and Marcus (F~=1=92.37).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Daelemans et al., 1999a ) report a good performance for baseNP recognition but they use a different data set and do not mention precision and recall rates. ", "mid_sen": "(Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. ", "after_sen": "Their baseNP results are slightly better than those of Ramshaw and Marcus (F~=1=92.37)."}
{"citeStart": 158, "citeEnd": 177, "citeStartToken": 158, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "ing co-occurrence statistics. Since then, there haw\" been some promising results from using co-occurrence vectors, such as word sense disambiguation (Schiitze [993) , and word clustering (Pereira eL al. 1993 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ing co-occurrence statistics. ", "mid_sen": "Since then, there haw\" been some promising results from using co-occurrence vectors, such as word sense disambiguation (Schiitze [993) , and word clustering (Pereira eL al. 1993 ).", "after_sen": "llowever, using the co-occurrence statistics requires a huge corpus that covers even most rare words. "}
{"citeStart": 127, "citeEnd": 139, "citeStartToken": 127, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) , which makes the creation of a general sentiment classifier a difficult task. Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. While the authors did make use of the context in their annotation, their focus was on the task of determining the author's reason for citing a given paper. This task differs from citation sentiment detection, which is in a sense a \"lower level\" of analysis.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources and NLP tools.", "mid_sen": "A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . ", "after_sen": "However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) , which makes the creation of a general sentiment classifier a difficult task. "}
{"citeStart": 79, "citeEnd": 100, "citeStartToken": 79, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 111, "citeEnd": 136, "citeStartToken": 111, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "The reasoning behind this scheme is that a paper is the human-readable representation of a scientific investigation. Therefore, the goal of the annotation is to retrieve the content model of scientific investigations as reflected within scientific discourse. The hypothesis is that there is a set of core scientific concepts (CoreSC), which constitute the key components of a scientific investigation. CoreSCs consist of 11 concepts originating from the CISP (Core Information about Scientific Papers) meta-data (Soldatova & Liakata, 2007) , which are a subset of classes from the EXPO ontology for the description of scientific experiments (Soldatova & King, 2006 The CoreSC scheme (Liakata et al., 2010; Liakata et al., 2012) implements the abovementioned concepts as a 3-layered sentence-based annotation scheme. This means that each sentence in a document is assigned one of the 11 CoreSC concepts. The scheme also considers a layer designated to properties of the concepts (e.g. New Method vs Old Method) as well as identifiers which link instances of the same concept across sentences. A short definition of CoreSC categories and their properties can be found in Table 1 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The hypothesis is that there is a set of core scientific concepts (CoreSC), which constitute the key components of a scientific investigation. ", "mid_sen": "CoreSCs consist of 11 concepts originating from the CISP (Core Information about Scientific Papers) meta-data (Soldatova & Liakata, 2007) , which are a subset of classes from the EXPO ontology for the description of scientific experiments (Soldatova & King, 2006 The CoreSC scheme (Liakata et al., 2010; Liakata et al., 2012) implements the abovementioned concepts as a 3-layered sentence-based annotation scheme. ", "after_sen": "This means that each sentence in a document is assigned one of the 11 CoreSC concepts. "}
{"citeStart": 24, "citeEnd": 42, "citeStartToken": 24, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "Distributional cluster (Brown et al., 1992) : head, body, hands, eye, voice, arm, seat, hair, mouth", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Distributional cluster (Brown et al., 1992) : head, body, hands, eye, voice, arm, seat, hair, mouth", "after_sen": "Word 'head' (17 alternatives) 0.0000 crown, peak, summit, head, top: subconceptofupperbound 0.0000 principaL school principal, head teacher, head: educator who has executive authority 0.0000 head, chief, top dog: subeoncept of leader 0.0000 head: a user of (usually soft) drugs 0.1983 head: \"the head of the page\"; \"the head of the fist\" 0.1983 beginning, head, origin, root, source: the point or place where something begins 0.0000 pass, head, straits: a difficult juncture; \"a pretty pass\" 0.0000 headway, head: subconcept of progress, progression, advance 0.0903 point, hod: a V-shaped mark at one end of an arrow pointer 0.0000 heading, head: a line of text serving to indicate what the passage below it is about 0.0000 mind, head, intellect, psyche: that which is responsible for your thoughts and feelings 0.5428 head: the upper or front part of the body that contains the faee and brains 0.0000 toilet, lavatory, can, head, facility, john, privy, bathroom 0.0000 head: the striking part of a tool; \"hammerhead\" 0.1685 head: a part that projects out from the rest; \"the head of the nail\", \"pinhead\" 0.0000 drumhead, head: stretched taut 0.0000 oral sex, head: oral-genital stimulation Word 'body' (8 alternatives) 0.0000 body: an individual 3-dimensional object that has mass 0.0000 gathering, assemblage, assembly, body, confluence: group of people together in one place 0.0000 body: people associated by some common tie or occupation 0.0000 body: the centralmessage of a communication 0.9178 torso, trunk, body: subconcept of body part, member 0.0000 body, organic structure: the entire physical structure of an animal or human being (10 alternatives) hand: subconeept of linear unit hired hand, hand, hired man: a hired laborer on a farm or ranch bridge player, hand: \"we need a 4th hand for bridge\" hand, deal: the cards held in a card game by a given player at any given time hand: a round of applause to signify approval; \"give the little lady a great big hand\" handwriting, cursive, hand, script: something written by hand hand: ability; \"he wanted to try his hand at singing\" hand, manus, hook, mauler, mitt, paw: the distal extremity of the superior limb hand: subconcept of pointer hand: physical assistance; \"give me a hand with the chores\" voice: the relation of the subject of a verb to the action that the verb denotes spokesperson, spokesman, interpreter, representative, mouthpiece, voice voice, vocalization: the sound made by the vibration of vocal folds articulation, voice: expressing in coherent verbal form; \"I gave voice to my feelings\" part, voice: the melody carried by a particular voice or instrument in polyphonic music voice: the ability to speak; \"he lost his voice\" voice: the distinctive sound of a person's speech; \"I recognized her voice\""}
{"citeStart": 101, "citeEnd": 112, "citeStartToken": 101, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "Words unknown to the lexicon present a substantial problem to part-of-speech (POS) tagging of realworld texts. Taggers assign a single POS-tag to a word-token, provided that it is known what partsof-speech this word can take on in principle. So, first words are looked up in the lexicon. However, 3 to 5% of word tokens are usually missing in the lexicon when tagging real-world texts. This is where word-Pos guessers take their place --they employ the analysis of word features, e.g. word leading and trailing characters, to figure out its possible POS categories. A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger (Kupiec, 1992) . A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g. (Brill, 1995; Weischedel et al., 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. ", "mid_sen": "The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g. (Brill, 1995; Weischedel et al., 1993) .", "after_sen": "The major topic in the development of word-Pos guessers is the strategy which is to be used for the acquisition of the guessing rules. "}
{"citeStart": 223, "citeEnd": 244, "citeStartToken": 223, "citeEndToken": 244, "sectionName": "UNKNOWN SECTION NAME", "string": "Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000) . The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts Other factors such as student confidence could be considered as well (Callaway et al., 2007) . of the answer. Once the complete answer has been accumulated, the system accepts it and moves on. Tutor hints can contribute parts of the answer to the cumulative state as well, allowing the system to jointly construct the solution with the student.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Interaction between components is coordinated by the dialogue manager which uses the informationstate approach (Larsson and Traum, 2000) . ", "mid_sen": "The dialogue state is represented by a cumulative answer analysis which tracks, over multiple turns, the correct, incorrect, and not-yet-mentioned parts Other factors such as student confidence could be considered as well (Callaway et al., 2007) . of the answer. ", "after_sen": "Once the complete answer has been accumulated, the system accepts it and moves on. "}
{"citeStart": 267, "citeEnd": 288, "citeStartToken": 267, "citeEndToken": 288, "sectionName": "UNKNOWN SECTION NAME", "string": "The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993) , word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b ), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993) , and statistical translation (Brown et al. 1993 ). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it is less related to ours, addressing, for example, word sense disambiguation in the source language by statistically examining context (e.g., collocations) in the source language, thus allowing appropriate word selection in the target language. (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993) , word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b ), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993) , and statistical translation (Brown et al. 1993 ). ", "after_sen": "Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. "}
{"citeStart": 46, "citeEnd": 65, "citeStartToken": 46, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "We are concerned with the nature of the rule set extracted, and how it can be improved, with regard both to linguistic criteria and processing efficiency. Inwhat follows, we report the worrying observation that the growth of the rule set continues at a square root rate throughout processing of the entire treebank (suggesting, perhaps that the rule set is far from complete). Our results are similar to those reported in (Krotov et al., 1994) . 1 We discuss an alternative possible source of this rule growth phenomenon, partial bracketting, and suggest that it can be alleviated by compaction, where rules that are redundant (in a sense to be defined) are eliminated from the grammar.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Inwhat follows, we report the worrying observation that the growth of the rule set continues at a square root rate throughout processing of the entire treebank (suggesting, perhaps that the rule set is far from complete). ", "mid_sen": "Our results are similar to those reported in (Krotov et al., 1994) . ", "after_sen": "1 We discuss an alternative possible source of this rule growth phenomenon, partial bracketting, and suggest that it can be alleviated by compaction, where rules that are redundant (in a sense to be defined) are eliminated from the grammar."}
{"citeStart": 256, "citeEnd": 272, "citeStartToken": 256, "citeEndToken": 272, "sectionName": "UNKNOWN SECTION NAME", "string": "The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data. This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features that have been shown to be optimal for word sense disambiguation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . ", "mid_sen": "A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). ", "after_sen": "In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. "}
{"citeStart": 91, "citeEnd": 109, "citeStartToken": 91, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "The PCC10 is a sub-corpus of 10 commentaries that serves as \"testbed\" for further developing the annotation levels. On the one hand, we are applying recent guidelines on annotation of information structure (Götze et al., 2007) . On the other hand, based on experiences with the RST annotation, we are replacing the rhetorical trees with a set of distinct, simpler annotation layers: thematic structure, conjunctive relations (Martin, 1992) , and argumentation structure (Freeman, 1991); these are complemented by the other levels mentioned above for the PCC176. The primary motivation for this step is the high degree of arbitrariness that annotators reported when producing the RST trees (see (Stede, 2007) ). By separating the thematic from the intentional information, and accounting for the surface-oriented conjunctive relations (which are similar to what is annotated in the PDTB, see Section 6), we hope to", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The PCC10 is a sub-corpus of 10 commentaries that serves as \"testbed\" for further developing the annotation levels. ", "mid_sen": "On the one hand, we are applying recent guidelines on annotation of information structure (Götze et al., 2007) . ", "after_sen": "On the other hand, based on experiences with the RST annotation, we are replacing the rhetorical trees with a set of distinct, simpler annotation layers: thematic structure, conjunctive relations (Martin, 1992) , and argumentation structure (Freeman, 1991); these are complemented by the other levels mentioned above for the PCC176. "}
{"citeStart": 179, "citeEnd": 203, "citeStartToken": 179, "citeEndToken": 203, "sectionName": "UNKNOWN SECTION NAME", "string": "Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional techniques become increasingly widespread (e.g. (Bensch and Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1994; McKcown and Hatzivassiloglou, 1993; Pereira et al., 1993; Schtltze, 1993) ). However, for many tasks, one is interested in relationships among word senses, not words. Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown et al. (1992) to illustrate a \"semantically sticky\" group of words. As is often the case where sense ambiguity is involved, we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so. Yet a computational system has no choice but to consider other, more awkward possibilities --for example, this cluster might be capturing a distributional relationship between advice (as one sense of counsel) and royalty (as one sense of court). This would be a mistake for many applications, such as query expansion in information retrieval, where a surfeit of false connections can outweigh the benefits obtained by using lexical knowledge.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional techniques become increasingly widespread (e.g. (Bensch and Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1994; McKcown and Hatzivassiloglou, 1993; Pereira et al., 1993; Schtltze, 1993) ). ", "after_sen": "However, for many tasks, one is interested in relationships among word senses, not words. "}
{"citeStart": 163, "citeEnd": 182, "citeStartToken": 163, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "To answer these questions, we have performed experiments that compare the parsing qualities of grammars induced under different training conditions using both adaptation and direct induction. We vary the number of labeled brackets and the linguistic classes of the labeled brackets. The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We vary the number of labeled brackets and the linguistic classes of the labeled brackets. ", "mid_sen": "The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993) .", "after_sen": "Our results show that the training examples do not need to be fully parsed for either strategy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data. "}
{"citeStart": 43, "citeEnd": 64, "citeStartToken": 43, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus. One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM). The model is defined by two collections of parameters: the transition probabilities, which express the probability that a tag follows the preceding one (or two for a second order model); and the lexical probabilities, giving the probability that a word has a given tag without regard to words on either side of it. To tag a text, the tags with non-zero probability are hypothesised for each word, and the most probable sequence of tags given tbe sequence of words is determined from the probabilities. Two algorithms are commonly used, known as the Forward-Backward (FB) and Viterbi algorithms. FB assigns a probability to every tag on every word. while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency. For an introduction to the algorithms, see Cutting et al. (1992) , or the lucid description by Sharman (1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency. ", "mid_sen": "For an introduction to the algorithms, see Cutting et al. (1992) , or the lucid description by Sharman (1990) .", "after_sen": "There are two principal sources for the parameters of the model. "}
{"citeStart": 111, "citeEnd": 133, "citeStartToken": 111, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "First, \"tokenizer\" processes \"documents\" in a given collection to produce an inverted file (\"surrogates\"). Since our system is bidirectional, tokenization differs depending on the target language. In the case where documents are in English, tokenization involves eliminating stopwords and identifying root forms for inflected words, for which we used \"Word-Net\" (Miller et al., 1993) . On the other hand, we segment Japanese documents into lexical units using the \"ChaSen\" morphological analyzer (Matsumoto et al., 1997) and discard stopwords. In the current implementation, we use word-based uni-gram indexing for both English and Japanese documents. In other words, compound words are decomposed into base words in the surrogates. Note that indexing and retrieval methods are theoretically independent of the translation method.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the case where documents are in English, tokenization involves eliminating stopwords and identifying root forms for inflected words, for which we used \"Word-Net\" (Miller et al., 1993) . ", "mid_sen": "On the other hand, we segment Japanese documents into lexical units using the \"ChaSen\" morphological analyzer (Matsumoto et al., 1997) and discard stopwords. ", "after_sen": "In the current implementation, we use word-based uni-gram indexing for both English and Japanese documents. "}
{"citeStart": 154, "citeEnd": 174, "citeStartToken": 154, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. ", "mid_sen": "Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "after_sen": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. "}
{"citeStart": 240, "citeEnd": 249, "citeStartToken": 240, "citeEndToken": 249, "sectionName": "UNKNOWN SECTION NAME", "string": "The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints (Fox, 2002) , and (ii) we can model broad syntactic reordering phenomena, such as subject-verb-object constructions translating into subject-object-verb ones, as is generally the case for English and Japanese.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) .", "mid_sen": "The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints (Fox, 2002) , and (ii) we can model broad syntactic reordering phenomena, such as subject-verb-object constructions translating into subject-object-verb ones, as is generally the case for English and Japanese.", "after_sen": "On the other hand, there is also significant amount of information in the surface strings of the source and target and their alignment. "}
{"citeStart": 48, "citeEnd": 62, "citeStartToken": 48, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "Exhaustive search for the global optimum is not an option when the search space is prohibitively large. In the present context, say for a sequence of 20 tones, the search space contains 6 ~° ~ 10 is possible tone transcriptions, and for each of these there are thousands of possible parameter settings, too large a search space for exhaustive search in a reasonable amount of compu- Non-deterministic search methods have been devised as a way of tackling large-scale combinatorial optimisation problems, problems that involve fin(ling optima of functions of discrete variables. 'I'hcse methods are only designed to yield an approximate solution, but they do so in a reasonable amount of computation time. The best known such methods are genetic search (Goldberg, 1989) and annealing search (van Laarhoven & Aarts, 1987) . Recently, annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata (Ellison, 1993 ). In the following sections I describe a genetic algorithm and an annealing algorithm for the tone transcription problem.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "'I'hcse methods are only designed to yield an approximate solution, but they do so in a reasonable amount of computation time. ", "mid_sen": "The best known such methods are genetic search (Goldberg, 1989) and annealing search (van Laarhoven & Aarts, 1987) . ", "after_sen": "Recently, annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata (Ellison, 1993 ). "}
{"citeStart": 25, "citeEnd": 49, "citeStartToken": 25, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "Objective information. The objective portions of a review do not contain the author's opinion; hence features extracted from objective sentences and phrases are irrelevant with respect to the polarity classification task and their presence may complicate the learning task. Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003) ) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004) ). Motivated by the work of Koppel and Schler (2005) , we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003) ) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004) ). ", "mid_sen": "Motivated by the work of Koppel and Schler (2005) , we identify and extract objective material from nonreviews and show how to exploit such information in polarity classification.", "after_sen": "Finally, previous work has also investigated features that do not fall into any of the above categories. "}
{"citeStart": 99, "citeEnd": 126, "citeStartToken": 99, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "We argue that aspects of both analyses are necessary to account for the recovery of temporal relations. To demonstrate our approach we will address the following examples; passages (la-b) are taken from Lascarides and Asher (1993) :", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We argue that aspects of both analyses are necessary to account for the recovery of temporal relations. ", "mid_sen": "To demonstrate our approach we will address the following examples; passages (la-b) are taken from Lascarides and Asher (1993) :", "after_sen": "(1) a. Max slipped. "}
{"citeStart": 73, "citeEnd": 83, "citeStartToken": 73, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "The McCarthy et al. system needs to re-train (create a new thesaurus) every time it is to determine predominant senses in data from a different domain. This requires large amounts of partof-speech-tagged and chunked data from that domain. Further, the target text must be large enough to learn a thesaurus from (Lin (1998) used a 64million-word corpus), or a large auxiliary text with a sense distribution similar to the target text must be provided (McCarthy et al. (2004) separately used 90-, 32.5-, and 9.1-million-word corpora).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This requires large amounts of partof-speech-tagged and chunked data from that domain. ", "mid_sen": "Further, the target text must be large enough to learn a thesaurus from (Lin (1998) used a 64million-word corpus), or a large auxiliary text with a sense distribution similar to the target text must be provided (McCarthy et al. (2004) separately used 90-, 32.5-, and 9.1-million-word corpora).", "after_sen": "By contrast, in this paper we present a method that accurately determines sense dominance even in relatively small amounts of target text (a few hundred sentences); although it does use a corpus, it does not require a similarly-sense-distributed corpus. "}
{"citeStart": 56, "citeEnd": 71, "citeStartToken": 56, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech repair terminology used here follows that of Shriberg (1994) . A speech repair consists of a reparandum, an interruption point, and the alteration. The reparandum contains the words that the speaker means to replace, including both words that are in error and words that will be retraced. The interruption point is the point in time where the stream of speech is actually stopped, and the repairing of the mistake can begin. The alteration contains the words that are meant to replace the words in the reparandum.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because of the relatively high frequency of this phenomenon, spontaneous speech recognition systems will need to be able to deal with repairs to achieve high levels of accuracy.", "mid_sen": "The speech repair terminology used here follows that of Shriberg (1994) . ", "after_sen": "A speech repair consists of a reparandum, an interruption point, and the alteration. "}
{"citeStart": 136, "citeEnd": 148, "citeStartToken": 136, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "Generic target lexicons were constructed by starting with a small sample of the kind of reviews that the lexicon would apply to. We examined these manually to find generic words referring to appraised things to serve as seed terms for the lexicon and used WordNet (Miller, 1995) to suggest additional terms to add to the lexicon.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Generic target lexicons were constructed by starting with a small sample of the kind of reviews that the lexicon would apply to. ", "mid_sen": "We examined these manually to find generic words referring to appraised things to serve as seed terms for the lexicon and used WordNet (Miller, 1995) to suggest additional terms to add to the lexicon.", "after_sen": "Since movie reviews often refer to the specific contents of the movie under review by proper names (of actors, the director, etc.), we also automatically constructed a specific target lexicon for each movie in the corpus, based on lists of actors, characters, writers, directors, and companies listed for the film at imdb.com. "}
{"citeStart": 54, "citeEnd": 88, "citeStartToken": 54, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "The ambiguity factor 2.27 attributed to Dagan and Itai's 1994 experiment is calculated by dividing their average of 3.27 alternative translations by their average of 1.44 correct translations. Furthermore, we calculated the ambiguity factor 3.51 for Resnik's 1997 experiment shows the random baselines cited for the respective experiments, ranging from ca. 11 to 50 . Precision values are given in column 5. In order to compare these results which were computed for di erent a m biguity factors, we standardized the measures to an evaluation for binary ambiguity. This is achieved by calculating p 1= log 2 amb for precision p and ambiguity factor amb. The consistency of this binarization can be seen by a standardization of the di erent random baselines which yields a value of ca. 50 for all approaches 5 . The standardized precision of our approach is ca. 79 on all test corpora. The most direct point of comparison is the method of Dagan and Itai 1994 which gives 91.4 precision 92.7 standardized and 62.1 e ectiveness 66.8 standardized on 103 test examples for target word selection in the transfer of Hebrew to English. However, compensating this high precision measure for the low e ectiveness gives values comparable to our results. Dagan and Itai's 1994 method is based on a large variety of grammatical relations for verbal, nominal, and adjectival predicates, but no class-based information or slot-labeling is used. Resnik 1997 presented a disambiguation method which yields 44.3 precision 63.8 standardized for a test set of 88 verb-object tokens. His approach i s comparable to ours in terms of informedness of the disambiguator. He also uses a class-based selection measure, but based on WordNet classes. However, the task of his evaluation was to select WordNet-senses for the objects rather than the objects themselves, so the results cannot becompared directly. The same is true for the Senseval evaluation exercise Kilgarri and Rosenzweig, 2000there word senses from the Hector-dictionary had to be disambiguated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the task of his evaluation was to select WordNet-senses for the objects rather than the objects themselves, so the results cannot becompared directly. ", "mid_sen": "The same is true for the Senseval evaluation exercise Kilgarri and Rosenzweig, 2000there word senses from the Hector-dictionary had to be disambiguated.", "after_sen": "The precision results for the ten unsupervised systems taking part in the competitive evaluation ranged from 20-65 at e ciency values from 3-54. "}
{"citeStart": 45, "citeEnd": 57, "citeStartToken": 45, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1 . This data has since been used in studies by (Mooney, 1996) , (Towell and Voorhees, 1998) , and (Leacock et al., 1998) . In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These senses and their frequency distribution are shown in Table 1 . ", "mid_sen": "This data has since been used in studies by (Mooney, 1996) , (Towell and Voorhees, 1998) , and (Leacock et al., 1998) . ", "after_sen": "In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. "}
{"citeStart": 151, "citeEnd": 174, "citeStartToken": 151, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of linear logic provides a flexible mechanism for deducing meanings of sentences based on their f-structure representations. Accounts of various linguistic phenomena have been developed within the framework on which our extension is based, including quantifiers and anaphora (Dalrymple et al., 1994a) , intensional verbs (Dalrympie et al., 1994b) , and complex predicates (Dalrymple et al., !993b). The logic fits well with the 'resource-sensitivity' of natural language semantics: there is a one-to-one correspondence between f-structure relationships and meanings; the multiple use of resources arises from multiple paths to them in the f-structure. In the next section, we show how this system applies to several cases of right-node raising.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The use of linear logic provides a flexible mechanism for deducing meanings of sentences based on their f-structure representations. ", "mid_sen": "Accounts of various linguistic phenomena have been developed within the framework on which our extension is based, including quantifiers and anaphora (Dalrymple et al., 1994a) , intensional verbs (Dalrympie et al., 1994b) , and complex predicates (Dalrymple et al., !993b). ", "after_sen": "The logic fits well with the 'resource-sensitivity' of natural language semantics: there is a one-to-one correspondence between f-structure relationships and meanings; the multiple use of resources arises from multiple paths to them in the f-structure. "}
{"citeStart": 62, "citeEnd": 87, "citeStartToken": 62, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Corpus-based Methods. Lexicographic methods are necessarily confined within the underlying resources. Much greater coverage can be had with syntactic or co-occurrence patterns across large corpora. Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. A popular, more general unsupervised method was introduced in Turney and Littman (2003) which induces the polarity of a word from its Pointwise Mutual Information (PMI) or Latent Semantic Analysis (LSA) scores obtained from a web search engine against a few paradigmatic (+) and (-) seeds. Kaji and Kitsuregawa (2007) describe a method for harvesting sentiment words from non-neutral sentences extracted from Japanese web documents based on structural layout clues. Strong adjectival subjectivity clues were mined in Wiebe (2000) with a distributional similarity-based word clustering method seeded by hand-labelled annotation. Riloff et al. (2003) mined subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. ", "mid_sen": "A popular, more general unsupervised method was introduced in Turney and Littman (2003) which induces the polarity of a word from its Pointwise Mutual Information (PMI) or Latent Semantic Analysis (LSA) scores obtained from a web search engine against a few paradigmatic (+) and (-) seeds. ", "after_sen": "Kaji and Kitsuregawa (2007) describe a method for harvesting sentiment words from non-neutral sentences extracted from Japanese web documents based on structural layout clues. "}
{"citeStart": 63, "citeEnd": 80, "citeStartToken": 63, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997) , similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992) , suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (>1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. ", "mid_sen": "We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997) , similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. ", "after_sen": "The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). "}
{"citeStart": 291, "citeEnd": 323, "citeStartToken": 291, "citeEndToken": 323, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we focus on the problem of computationally estimating similarity or relatedness between two natural-language documents. A novel technique is proposed for computing semantic similarity by spreading activation over the hyperlink structure of Wikipedia, the largest free online encyclopaedia. New measures for computing similarity between individual concepts (inter-concept similarity, such as \"France\" and \"Great Britain\"), as well as between documents (inter-document similarity) are proposed and tested. It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively. Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity. Furthermore, we use the same background data as for WLM, which is less than 10% of the data required for ESA.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "New measures for computing similarity between individual concepts (inter-concept similarity, such as \"France\" and \"Great Britain\"), as well as between documents (inter-document similarity) are proposed and tested. ", "mid_sen": "It will be demonstrated that the proposed techniques can achieve comparable inter-concept and inter-document similarity accuracy on similar datasets as compared to the current state of the art Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) methods respectively. ", "after_sen": "Our methods outperform WLM in computing inter-concept similarity, and match ESA for inter-document similarity. "}
{"citeStart": 50, "citeEnd": 64, "citeStartToken": 50, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "The present work evaluates the performance on Arabic documents of the Naïve Bayes algorithm (NB) -one of the simplest algorithms applied to English document categorization (Mitchell, 1997) . The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. This work is a continuation of that initiated in (Yahyaoui, 2001) , which reports an overall NB classification correctness of 75.6%, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories (the data set is collected from different Arabic portals). A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). The present work expands the work in (Yahyaoui, 2001 ) by experimenting with the use of a better root extraction algorithm (El Kourdi, 2004) for document preprocessing, and using a different data set, collected from the largest Arabic site on the web: aljazeera.net.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of this work is to gain some insight as to whether Arabic document categorization (using NB) is sensitive to the root extraction algorithm used or to different data sets. ", "mid_sen": "This work is a continuation of that initiated in (Yahyaoui, 2001) , which reports an overall NB classification correctness of 75.6%, in cross validation experiments, on a data set that consists of 100 documents for each of 12 categories (the data set is collected from different Arabic portals). ", "after_sen": "A 50% overall classification accuracy is also reported when testing with a separately collected evaluation set (3 documents for each of the 12 categories). "}
{"citeStart": 57, "citeEnd": 77, "citeStartToken": 57, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "OPINE extracts explicit features for the given product class from parsed review data. First, the system recursively identifies both the parts and the properties of the given product class and their parts and properties, in turn, continuing until no candidates are found. Then, the system finds related concepts as described in (Popescu et al., 2004) and extracts their parts and properties. Table 1 shows that each feature type contributes to the set of final features (averaged over 7 product classes). In order to find parts and properties, OPINE first extracts the noun phrases from reviews and retains those with frequency greater than an experimentally set threshold. OPINE's Feature Assessor, which is an instantiation of KnowItAll's Assessor, evaluates each noun phrase by computing the PMI scores between the phrase and meronymy discriminators associated with the product class (e.g., \"of scanner\", \"scanner has\", \"scanner comes with\", etc. for the Scanner class). OPINE distinguishes parts from properties using WordNet's IS-A hierarchy (which enumerates different kinds of properties) and morphological cues (e.g., \"-iness\", \"-ity\" suffixes).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, the system recursively identifies both the parts and the properties of the given product class and their parts and properties, in turn, continuing until no candidates are found. ", "mid_sen": "Then, the system finds related concepts as described in (Popescu et al., 2004) and extracts their parts and properties. ", "after_sen": "Table 1 shows that each feature type contributes to the set of final features (averaged over 7 product classes). "}
{"citeStart": 274, "citeEnd": 285, "citeStartToken": 274, "citeEndToken": 285, "sectionName": "UNKNOWN SECTION NAME", "string": "One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 2 5 < 50 < 2 6 ), and many subtrees are repeated across different parses (Huang, 2008) . It is thus inefficient either to decode separately with each of these very similar trees. Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. ", "mid_sen": "However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 2 5 < 50 < 2 6 ), and many subtrees are repeated across different parses (Huang, 2008) . ", "after_sen": "It is thus inefficient either to decode separately with each of these very similar trees. "}
{"citeStart": 250, "citeEnd": 275, "citeStartToken": 250, "citeEndToken": 275, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ( [Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 128, "citeEnd": 154, "citeStartToken": 128, "citeEndToken": 154, "sectionName": "UNKNOWN SECTION NAME", "string": "The applications of the framework discussed here are many and varied both for cognitive science and NLP. We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994) . NLP tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (Coccaro and Jurafsky, 1998) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994) . ", "mid_sen": "NLP tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (Coccaro and Jurafsky, 1998) .", "after_sen": "http://www.ru.nl/celex/"}
{"citeStart": 227, "citeEnd": 245, "citeStartToken": 227, "citeEndToken": 245, "sectionName": "UNKNOWN SECTION NAME", "string": ".There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990 ]. The second is The words finding automaton based on the Aho-Corasick Algorithm [Hong-I and Lua] . The former requires three scans of the input character string. In addition, during each scan, backtracking has to be performed in cases where a dictionary search fails. After that, the word recognition is built based on the candidates. The second requires building up a state chart, is difficult to combine with other algorithms.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": ".", "mid_sen": "There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990 ]. ", "after_sen": "The second is The words finding automaton based on the Aho-Corasick Algorithm [Hong-I and Lua] . "}
{"citeStart": 145, "citeEnd": 157, "citeStartToken": 145, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic underlying hypothesis is that intrasentential candidates are more salient than intersentential candidates as proposed, for example, in Hobbs (1978) and Kameyama (in press) , and that fine-grained syntax-based salience fades with time. Since fine-grained syntax with grammatical functions is unavailable, the syntactic prominence of subjects and left-dislocation is approximated by the left-right linear ordering.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The following ordering approximates the relative salience of entities.", "mid_sen": "The basic underlying hypothesis is that intrasentential candidates are more salient than intersentential candidates as proposed, for example, in Hobbs (1978) and Kameyama (in press) , and that fine-grained syntax-based salience fades with time. ", "after_sen": "Since fine-grained syntax with grammatical functions is unavailable, the syntactic prominence of subjects and left-dislocation is approximated by the left-right linear ordering."}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993) ; however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents. Sidner (1992; formulated an artificial language for modeling collaborative discourse using propo~acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, ff so, selects appropriate evidence from the system's private beliefs to support the claim.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sidner (1992; formulated an artificial language for modeling collaborative discourse using propo~acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. ", "mid_sen": "Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. ", "after_sen": "They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. "}
{"citeStart": 173, "citeEnd": 210, "citeStartToken": 173, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998) . Five are so-called voting methods. They assign weights to the output of the individual systems and use these weights to determine the most probable output tag. Since the classifiers generate different output formats, all classifier output has been converted to the O and the C representations. The most simple voting method assigns uniform weights and picks the tag that occurs most often (Majority). A more advanced method is to use as a weight the accuracy of the classifier on some held-out part of the training data, the tuning data (Tot-Precision). One can also use the precision obtained by a classifier for a specific output value as a weight (TagPrecision). Alternatively, we use as a weight a combination of the precision score for the output tag in combination with the recall score for competing tags (Precision-Recall). The most advanced voting method examines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag-Pair, Van Halteren et al., (1998) ).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Alternatively, we use as a weight a combination of the precision score for the output tag in combination with the recall score for competing tags (Precision-Recall). ", "mid_sen": "The most advanced voting method examines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag-Pair, Van Halteren et al., (1998) ).", "after_sen": "Apart from these voting methods we have also applied two memory-based learners t;o the output of the five chunkers: IBI-IG and IGTREE, a decision tree variant of IBI-IG (Daelemans et al., 1999) . "}
{"citeStart": 197, "citeEnd": 228, "citeStartToken": 197, "citeEndToken": 228, "sectionName": "UNKNOWN SECTION NAME", "string": "We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991) , 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs 3 (Finkelstein et al., 2001) . We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. We measure the correlation with Pearson's Correlation Coefficient. A preliminary experiment set out to determine whether there is any advantage to indexing the words in a phrase separately, for example, whether the phrase \"change of direction\" should be indexed only as a whole, or as all of \"change\", \"of\", \"direction\" and \"change of direction\". The outcome of this experiment appears in Table 4 . There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget's. In the remaining experiments, we have each word in a phrase indexed.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. ", "mid_sen": "The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991) , 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs 3 (Finkelstein et al., 2001) . ", "after_sen": "We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. "}
{"citeStart": 24, "citeEnd": 42, "citeStartToken": 24, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "As in statistical machine translation, we make modelling assumptions. We use the IBM Model 1 (Brown et al., 1993 ) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996) ) to estimate the alignment model. The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As in statistical machine translation, we make modelling assumptions. ", "mid_sen": "We use the IBM Model 1 (Brown et al., 1993 ) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996) ) to estimate the alignment model. ", "after_sen": "The lexicon probability of a sentence pair is modelled as a product of single-word based probabilities of the aligned words."}
{"citeStart": 226, "citeEnd": 247, "citeStartToken": 226, "citeEndToken": 247, "sectionName": "UNKNOWN SECTION NAME", "string": "While the TF measurement concerns the importance of a term in a given document, IDF seeks to measure the relative importance of a term in a collection of documents. The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. (Salton and Yang, 1973) proposed the combination of TF and IDF as weighting schemes, and it has been shown that their product gave better performance. Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The importance of each term is assumed to be inversely proportional to the number of documents that contain that term. ", "mid_sen": "TF is given by TF D,t , and it denotes frequency of term t in document D. IDF is given by IDF t = log(N/df t ), where N is the number of documents in the collection, and df t is the number of documents containing the term t. (Salton and Yang, 1973) proposed the combination of TF and IDF as weighting schemes, and it has been shown that their product gave better performance. ", "after_sen": "Thus, the weight of each term/root in a document is given by w D,t = TF D,t * IDF t ."}
{"citeStart": 16, "citeEnd": 38, "citeStartToken": 16, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "The MUC scorer (Vilain et al., 1995) is a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004) , if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score -significantly higher than any published system. The b 3 scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and b 3 scorers, we also evaluate using cluster f-measure (Ghosh, 2003) , which is the standard f-measure computed over true/false coreference decisions for pairs of mentions; the Rand index (Rand, 1971) , which is pairwise accuracy of the clustering; and variation of information (Meila, 2003) , which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As observed by Luo et al. (2004) , if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score -significantly higher than any published system. ", "mid_sen": "The b 3 scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. ", "after_sen": "However, coreference resolution is a clustering task, and many cluster scorers already exist. "}
{"citeStart": 170, "citeEnd": 189, "citeStartToken": 170, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "Portage's model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (s|t) are smoothed using the Good-Turing technique (Foster et al., 2006) . The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Portage's model for P (t|s) is a log-linear combination of four main components: one or more ngram target-language models, one or more phrase translation models, a distortion (word-reordering) model, and a sentence-length feature. ", "mid_sen": "The phrasebased translation model is similar to that of Koehn, with the exception that phrase probability estimates P (s|t) are smoothed using the Good-Turing technique (Foster et al., 2006) . ", "after_sen": "The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings."}
{"citeStart": 433, "citeEnd": 443, "citeStartToken": 433, "citeEndToken": 443, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ([Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes \"these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 73, "citeEnd": 102, "citeStartToken": 73, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto (2002) . They constructed an SVM model for PoS tagging, and considered Support Vectors with high α values to be indicative of suspicious corpus locations. These locations can be either outliers, or correctly labeled locations similar to an outlier. They then looked for similar corpus locations with a different label, to point out right-wrong pairs with high precision.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They can be either corpus errors, or genuinely hard cases.", "mid_sen": "This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto (2002) . ", "after_sen": "They constructed an SVM model for PoS tagging, and considered Support Vectors with high α values to be indicative of suspicious corpus locations. "}
{"citeStart": 88, "citeEnd": 107, "citeStartToken": 88, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated our appraisal extraction system on two corpora. The first is the standard publicly available collection of movie reviews constructed by Pang and Lee (2004) . This standard testbed consists of 1000 positive and 1000 negative reviews, taken from the IMDb movie review archives 2 . Reviews with 'neutral' scores (such as three stars out of five) were removed by Pang and Lee, giving a data set with only clearly positive and negative reviews. The average document length in this corpus is 764 words, and 1107 different movies are reviewed.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We evaluated our appraisal extraction system on two corpora. ", "mid_sen": "The first is the standard publicly available collection of movie reviews constructed by Pang and Lee (2004) . ", "after_sen": "This standard testbed consists of 1000 positive and 1000 negative reviews, taken from the IMDb movie review archives 2 . Reviews with 'neutral' scores (such as three stars out of five) were removed by Pang and Lee, giving a data set with only clearly positive and negative reviews. "}
{"citeStart": 185, "citeEnd": 200, "citeStartToken": 185, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "The Compiled-Earley (CE) parser is based on a predictive chart-based CF parsing algorithm devised by Schabes (1991) which is driven by a table compiling out the predictive component of Earley's (1970) parser. The size of the table is related linearly to the size of the grammar (unlike the LR technique). Schabes demonstrates that this parser always takes fewer steps than Earley's, although its time complexity is the same: O(n3). The space complexity is also cubic, since the parser uses Earley's representation of parse forests.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A weaker kind of cache on partial analyses (and thus unification results) was found to be necessary in the implementation, though, to avoid duplication of unifications; this sped the parser up by a factor of about three, at little space cost.", "mid_sen": "The Compiled-Earley (CE) parser is based on a predictive chart-based CF parsing algorithm devised by Schabes (1991) which is driven by a table compiling out the predictive component of Earley's (1970) parser. ", "after_sen": "The size of the table is related linearly to the size of the grammar (unlike the LR technique). "}
{"citeStart": 232, "citeEnd": 240, "citeStartToken": 232, "citeEndToken": 240, "sectionName": "UNKNOWN SECTION NAME", "string": "The prol)lem has boon recognized and seine possihle remedies have been prol)osed. They all try to minimize or to elhninMe the intel'r:tce betweell word and sontoiic(: low4 processing. One stop is the descriptiml of word fl)rmation ill terms of a unification-based gl'all/lnai' to make the result (~1' morphological l)rocessing dir(,ctly ~wMhd)le to syntax and vice w~rsa, an :g)l)roa(-h ah'eady ti~ken in X2Moltl,' (Trost 90, '['rost 91) , an extension of two-hwel nmrphohlgy.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They all try to minimize or to elhninMe the intel'r:tce betweell word and sontoiic(: low4 processing. ", "mid_sen": "One stop is the descriptiml of word fl)rmation ill terms of a unification-based gl'all/lnai' to make the result (~1' morphological l)rocessing dir(,ctly ~wMhd)le to syntax and vice w~rsa, an :g)l)roa(-h ah'eady ti~ken in X2Moltl,' (Trost 90, '['rost 91) , an extension of two-hwel nmrphohlgy.", "after_sen": "The harder probhml is the integration of morphol)honology which is traditionally formalized in it way not easily t,':mshmdlle into the fei~ture formalisnx. "}
{"citeStart": 84, "citeEnd": 101, "citeStartToken": 84, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . EventMine-MK is available as a component of the U-Compare interoperable text mining system 4 (Kano et al., 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . ", "mid_sen": "In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. ", "after_sen": "The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . "}
{"citeStart": 0, "citeEnd": 12, "citeStartToken": 0, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "Arabic is a morphologically rich language: in our training corpus of about 288,000 words we find 3279 distinct morphological tags, with up to 100,000 possible tags. 1 Because of the large number of tags, it is clear that morphological tagging cannot be construed as a simple classification task. Hajič (2000) is the first to use a dictionary as a source of possible morphological analyses (and hence tags) for an inflected word form. He redefines the tagging task as a choice among the tags proposed by the dictionary, using a log-linear model trained on specific ambiguity classes for individual morphological features. Hajič et al. (2005) implement the approach of Hajič (2000) for Arabic. In previous work, we follow the same approach (Habash and Rambow, 2005) , using SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. Since the dictionary we use, BAMA (Buckwalter, 2004) , also includes diacritics (orthographic marks not usually written), we extend this approach to the diacritization task in (Habash and Rambow, 2007) . The work presented in this paper differs from this previous work in that (a) we introduce a new task for Arabic, namely lemmatization; (b) we use an explicit modeling of lexemes as a component in all tasks discussed in this paper (morphological tagging, diacritization, and lemmatization); and (c) we tune the weights of the feature classifiers on a tuning corpus (different tuning for different tasks).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 Because of the large number of tags, it is clear that morphological tagging cannot be construed as a simple classification task. ", "mid_sen": "Hajič (2000) is the first to use a dictionary as a source of possible morphological analyses (and hence tags) for an inflected word form. ", "after_sen": "He redefines the tagging task as a choice among the tags proposed by the dictionary, using a log-linear model trained on specific ambiguity classes for individual morphological features. "}
{"citeStart": 248, "citeEnd": 267, "citeStartToken": 248, "citeEndToken": 267, "sectionName": "UNKNOWN SECTION NAME", "string": "At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005) . However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on. FOL-based systems that have attained high precision (Bos and Markert, 2006) have done so at the cost of very poor recall.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the premise.", "mid_sen": "At the other extreme, textual inference can be approached as deduction, building on work in formal computational semantics to translate sentences into first-order logic (FOL), and then applying a theorem prover or a model builder (Akhmatova, 2005; Fowler et al., 2005) . ", "after_sen": "However, such approaches tend to founder on the difficulty of accurately translating natural language in FOL-tricky issues include idioms, intensionality and propositional attitudes, modalities, temporal and causal relations, certain quantifiers, and so on. "}
{"citeStart": 58, "citeEnd": 82, "citeStartToken": 58, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "The perfect is analyzed by using the notion of a nucleus (Moens and Steedman, 1988) to account for the inner structure of an eventuality. A nucleus is defined as a structure containing a preparatory process, culmination and consequent state. The categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to. The perfect is seen in (Kamp and Reyle, 1993) as an aspectual operator. The eventualities described by the perfect of a verb refer to the consequent state of its nucleus. For example, the following sentence 14 denotes the state, s, holding at the present, that Mary has met the president. This state is a result of the event e, in which Mary met the president.\" Temporally, the state s starts just when e ends, or as it is put in (Kamp and Reyle, 1993 ):e and s abut, (represented as e DCs). 14Mary has met the president.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that in a 'terminal' DRS (ready for an embedding test), all the auxiliary Rpts 'disappear' (do not participate in the embedding).", "mid_sen": "The perfect is analyzed by using the notion of a nucleus (Moens and Steedman, 1988) to account for the inner structure of an eventuality. ", "after_sen": "A nucleus is defined as a structure containing a preparatory process, culmination and consequent state. "}
{"citeStart": 23, "citeEnd": 48, "citeStartToken": 23, "citeEndToken": 48, "sectionName": "UNKNOWN SECTION NAME", "string": "The Cite-Sum system (Kaplan and Tokunaga, 2008) also aims at knowledge reduction through use of citations. It receives a paper title as a query and attempts to generate a summary of the paper by finding citing papers 1 and extracting citations in the running-text that refer to the paper. Before outputting a summary, it also classifies extracted citation text, and removes citations with redundant content. Another similar study (Qazvinian and Radev, 2008) aims at using the content of citations within citing papers to generate summaries of fields of research.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Before outputting a summary, it also classifies extracted citation text, and removes citations with redundant content. ", "mid_sen": "Another similar study (Qazvinian and Radev, 2008) aims at using the content of citations within citing papers to generate summaries of fields of research.", "after_sen": "It is clear that merit exists behind extraction of citations in running text. "}
{"citeStart": 81, "citeEnd": 100, "citeStartToken": 81, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "The first experiments in Argumentative Zoning used Naïve Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independence of the features. However, this assumption is rarely true for the kinds of rich feature representations we want to use for most NLP tasks.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Argumentative Zoning can enable tailored summarizations depending on the needs of the user, e.g. a layperson versus a domain expert.", "mid_sen": "The first experiments in Argumentative Zoning used Naïve Bayes (NB) classifiers (Kupiec et al., 1995; Teufel, 1999) which assume conditional independence of the features. ", "after_sen": "However, this assumption is rarely true for the kinds of rich feature representations we want to use for most NLP tasks."}
{"citeStart": 259, "citeEnd": 280, "citeStartToken": 259, "citeEndToken": 280, "sectionName": "UNKNOWN SECTION NAME", "string": "Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework. We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies. Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models. 2", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies. ", "mid_sen": "Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models. ", "after_sen": "2"}
{"citeStart": 68, "citeEnd": 83, "citeStartToken": 68, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "reorder-score = 1 − # chunks − 1 # unigrams matched − 1 reorder-cost = 1 − reorder-score All reordering augmented-loss experiments are run with the same treebank data as the baseline (the training portions of PTB, Brown, and QTB). The extrinsic reordering training data consists of 10930 examples of English sentences and their correct Japanese word-order. We evaluate our results on an evaluation set of 6338 examples of similarly created reordering data. The reordering cost, evaluation Our rules are similar to those from Xu et al. (2009 . Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We evaluate our results on an evaluation set of 6338 examples of similarly created reordering data. ", "mid_sen": "The reordering cost, evaluation Our rules are similar to those from Xu et al. (2009 . ", "after_sen": "Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates."}
{"citeStart": 225, "citeEnd": 259, "citeStartToken": 225, "citeEndToken": 259, "sectionName": "UNKNOWN SECTION NAME", "string": "We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , some Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and some subjectivity clues listed in (Wiebe, 1990) . We represented each set as a three-valued feature based on the presence of 0, 1, or ≥ 2 members of the set. We will refer to these as the manual features.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also added manually-developed features found by other researchers. ", "mid_sen": "We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , some Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and some subjectivity clues listed in (Wiebe, 1990) . ", "after_sen": "We represented each set as a three-valued feature based on the presence of 0, 1, or ≥ 2 members of the set. "}
{"citeStart": 120, "citeEnd": 143, "citeStartToken": 120, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "To detect CKUs, we assume that these are indicated minimally by two co-occurring concepts: a first concept, which we call DEICTIC, and which conveys reference to the current work (here, we, our, these), and a second concept, which is a subclass of what we call MENTAL_OPERATION (identify, demonstrate, find, etc.). This specific subclass is a list of verbs and their nominalizations that belong to the category of \"certainty verbs\" in Thomas and Hawes (1994) . This minimal pattern detects expressions like \"we identify\" or \"our finding\". In expressions like \"these results indicate\" or \"our data demonstrate\", the DEICTIC concept is linked to the certainty verb in an indirect way, since it is the modifier of the subject of the certainty verb.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To detect CKUs, we assume that these are indicated minimally by two co-occurring concepts: a first concept, which we call DEICTIC, and which conveys reference to the current work (here, we, our, these), and a second concept, which is a subclass of what we call MENTAL_OPERATION (identify, demonstrate, find, etc.). ", "mid_sen": "This specific subclass is a list of verbs and their nominalizations that belong to the category of \"certainty verbs\" in Thomas and Hawes (1994) . ", "after_sen": "This minimal pattern detects expressions like \"we identify\" or \"our finding\". "}
{"citeStart": 0, "citeEnd": 14, "citeStartToken": 0, "citeEndToken": 14, "sectionName": "UNKNOWN SECTION NAME", "string": "(1) 3ohn revised his paper before the teacher did, and so did Bill. Kehler (1993a) has convincingly argued that this problem arises because DSP do not distinguish between merely co-referential and co-indexed (in his terminology, role-linked) expressions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(1) 3ohn revised his paper before the teacher did, and so did Bill. ", "mid_sen": "Kehler (1993a) has convincingly argued that this problem arises because DSP do not distinguish between merely co-referential and co-indexed (in his terminology, role-linked) expressions.", "after_sen": "Third, though perhaps less importantly, higherorder unification going beyond second-order matching is required for resolving ellipses involving quart-tification. "}
{"citeStart": 266, "citeEnd": 288, "citeStartToken": 266, "citeEndToken": 288, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexicalist approaches to MT, particularly those incorporating the technique of Shake-and-Bake generation (Beaven, 1992a; Beaven, 1992b; Whitelock, 1994) , combine the linguistic advantages of transfer (Arnold et al., 1988; Allegranza et al., 1991) and interlingual (Nirenburg et al., 1992; Dorr, 1993) approaches. Unfortunately, the generation algorithms described to date have been intractable. In this paper, we describe an alternative generation component which has polynomial time complexity.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Lexicalist approaches to MT, particularly those incorporating the technique of Shake-and-Bake generation (Beaven, 1992a; Beaven, 1992b; Whitelock, 1994) , combine the linguistic advantages of transfer (Arnold et al., 1988; Allegranza et al., 1991) and interlingual (Nirenburg et al., 1992; Dorr, 1993) approaches. ", "after_sen": "Unfortunately, the generation algorithms described to date have been intractable. "}
{"citeStart": 210, "citeEnd": 211, "citeStartToken": 210, "citeEndToken": 211, "sectionName": "UNKNOWN SECTION NAME", "string": "This assumption is challenged by a number of cases in naturally occurring dialogues where inferences that follow from what has been said are made explicit. I restrict the inferences that I discuss to those that are (a) based on information explicitly provided in the dialogue or, (b) licensed by applications of Gricean Maxims such as scalar implicature inferences [9] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This assumption is challenged by a number of cases in naturally occurring dialogues where inferences that follow from what has been said are made explicit. ", "mid_sen": "I restrict the inferences that I discuss to those that are (a) based on information explicitly provided in the dialogue or, (b) licensed by applications of Gricean Maxims such as scalar implicature inferences [9] .", "after_sen": "For example the logical omniscience assumption would mean that if l(a) and (b) below are in the context, then (c) will be as well since it is entailed from (a) and (b)."}
{"citeStart": 58, "citeEnd": 81, "citeStartToken": 58, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "Even moderately long documents typically address sew~ral topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of linear text segmentation is to discover the topic boundaries. ", "mid_sen": "The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "after_sen": "This paper focuses on domain independent methods for segmenting written text. "}
{"citeStart": 212, "citeEnd": 223, "citeStartToken": 212, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988) . It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995) . To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n. We can compute the per-utterance posterior DA probabilities by summing:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "U* = argmaxP(UIE ) (4) u", "mid_sen": "The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988) . ", "after_sen": "It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995) . "}
{"citeStart": 59, "citeEnd": 80, "citeStartToken": 59, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "We collect 100 sets of tweets, each of which is related to a trending topic. For each set of tweets, we manually select representative tweets as the summarization, forming the gold-standard dataset. We show that our system compares favorably to the LexRank (Erkan and Radev, 2004) baseline in terms of ROUGE-1 and ROUGE-2. We also show the positive effects of considering social signals, readability and user diversity, respectively. To understand how well our method performs in a real scenario, we apply our system to summarize Twitter search results, and consistently observe an improvement of user's satisfaction of Twitter search in a serials of user studies. It is worth mentioning that our proposed summarization system can be easily adapted to other social contents that are short and noisy but with rich social evidences, e.g., Facebook updates or short messages shared through Facebook.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each set of tweets, we manually select representative tweets as the summarization, forming the gold-standard dataset. ", "mid_sen": "We show that our system compares favorably to the LexRank (Erkan and Radev, 2004) baseline in terms of ROUGE-1 and ROUGE-2. ", "after_sen": "We also show the positive effects of considering social signals, readability and user diversity, respectively. "}
{"citeStart": 144, "citeEnd": 149, "citeStartToken": 144, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "To explore the relationship of control to planning, we compare the TODs with both types of ADs (financial and support). We would expect these dialogues to differ in terms of initiative. In the ADs, the objective is to develop a collaborative plan through a series of conversational exchanges. Both discourse participants believe that the expert has knowledge about the domain, but only has partial information about the situation. They also believe that the advisee must contribute both the problem description and also constraints as to how the problem can be solved. This information must be exchanged, so that the mutual beliefs necessary to develop the collaborative plan are established in the conversation [Jos82] . The situation is different in the TODs. Both participants here believe at the outset that the expert has sufficient information about the situation and complete and correct knowledge about how to execute the Task. Since the apprentice has no need to assert information to change the expert's beliefs or to ask questions to verify the expert's beliefs or to issue commands, we should not expect the apprentice to have control. S/he is merely present to execute the actions indicated by the knowledgeable participant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They also believe that the advisee must contribute both the problem description and also constraints as to how the problem can be solved. ", "mid_sen": "This information must be exchanged, so that the mutual beliefs necessary to develop the collaborative plan are established in the conversation [Jos82] . ", "after_sen": "The situation is different in the TODs. "}
{"citeStart": 49, "citeEnd": 61, "citeStartToken": 49, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Four similarity measures were examined. The cosine coefficient (R98(s,co,) ) and dot density measure (R98(m,(lot)) yield similar results. Our spread activation based semantic measure (R98( ..... ,)) improved a.ccura(:y. This confirms that although Kozima's apl) roaeh (Kozima, 1993) is computationally expensive, it does produce more precise segmentation. Tile most significant improvement was due to our ranking scheme which linearises the cosine coefficient. Our exl)eriments demonstrate that given insuffi-(:lent data, tile qualitative behaviour of the cosine m(,asul'e is indeed more reliable than the actual val-II(~S.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our spread activation based semantic measure (R98( ..... ,)) improved a.ccura(:y. ", "mid_sen": "This confirms that although Kozima's apl) roaeh (Kozima, 1993) is computationally expensive, it does produce more precise segmentation. ", "after_sen": "Tile most significant improvement was due to our ranking scheme which linearises the cosine coefficient. "}
{"citeStart": 107, "citeEnd": 138, "citeStartToken": 107, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed a compiler for off-line optimization of phrase structure rule-based typed feature structure grammars which generalizes the techniques developed in the context of the DIA, and we advanced a typed extension of the Earley-style generator of Gerdemann (1991) . Off-line compilation (section 3) is used to produce grammars for the Earley-style generator (section 2). We show that our use of offline grammar optimization overcomes problems with empty or displaced heads. The developed techniques are extensively tested with a large HPSG grammar for partial vP topicallzation in German (iiinrichs et al., 1994) . This uncovered some important constraints on the form of the phrase structure rules (phrase structure rules) in a grammar imposed by the compiler (section 4).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We show that our use of offline grammar optimization overcomes problems with empty or displaced heads. ", "mid_sen": "The developed techniques are extensively tested with a large HPSG grammar for partial vP topicallzation in German (iiinrichs et al., 1994) . ", "after_sen": "This uncovered some important constraints on the form of the phrase structure rules (phrase structure rules) in a grammar imposed by the compiler (section 4)."}
{"citeStart": 144, "citeEnd": 163, "citeStartToken": 144, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "In Zaidan et al. (2007) , we introduced the \"Movie Review Polarity Dataset Enriched with Annotator Rationales.\" 8 It is based on the dataset of Pang and Lee (2004) , 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F 0 -F 9 ). All our experiments use F 9 as their final blind test set.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In Zaidan et al. (2007) , we introduced the \"Movie Review Polarity Dataset Enriched with Annotator Rationales.\" 8 It is based on the dataset of Pang and Lee (2004) , 9 which consists of 1000 positive and 1000 negative movie reviews, tokenized and divided into 10 folds (F 0 -F 9 ). ", "after_sen": "All our experiments use F 9 as their final blind test set."}
{"citeStart": 105, "citeEnd": 121, "citeStartToken": 105, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "With the linguistics-based approach, candidate terminology is filtered by linguistic features using morphological analysis, such as part of speech (POS). Complex terms are extracted using shallow parsing and dependency analysis between words in the sentence (Bourigault, 1992) . (Dagan and Church, 1994) limited the candidate terminology to a string that represents the pattern of noun sequences. Good results can be achieved in small corpora using linguistic methods, but due to the shortage of patterns, recall can be low and it is difficult to generalize these techniques across fields and languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With the linguistics-based approach, candidate terminology is filtered by linguistic features using morphological analysis, such as part of speech (POS). ", "mid_sen": "Complex terms are extracted using shallow parsing and dependency analysis between words in the sentence (Bourigault, 1992) . (Dagan and Church, 1994) limited the candidate terminology to a string that represents the pattern of noun sequences. ", "after_sen": "Good results can be achieved in small corpora using linguistic methods, but due to the shortage of patterns, recall can be low and it is difficult to generalize these techniques across fields and languages."}
{"citeStart": 37, "citeEnd": 64, "citeStartToken": 37, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "So far, we have only evaluated models trained on gold POS tag set and morphological feature values. Some researchers, however, including Goldberg and Elhadad (2010) , train on predicted feature values instead. It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. But we argue that it also makes sense that training on a combination of gold and predicted features (one copy of each) might do even better, because good predictions of feature values are reinforced (since they repeat the gold patterns), whereas noisy predicted feature values are still represented in training (in patterns that do not repeat the gold). 21 To test our hypothesis, we start this section by comparing three variations:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So far, we have only evaluated models trained on gold POS tag set and morphological feature values. ", "mid_sen": "Some researchers, however, including Goldberg and Elhadad (2010) , train on predicted feature values instead. ", "after_sen": "It makes sense that training on predicted features yields better scores for evaluation on predicted features, since the training better resembles the test. "}
{"citeStart": 7, "citeEnd": 20, "citeStartToken": 7, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "tences. It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996) . Unlike Erbach (1990) , however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure. Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It focusses on extracting linguistic properties, as compared to e.g. general concept learning (Hahn, Klenner & Schnattinger 1996) . ", "mid_sen": "Unlike Erbach (1990) , however, it is not confined to simple morpho-syntactic information but can also handle selectional restrictions, semantic types and argument structure. ", "after_sen": "Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context."}
{"citeStart": 117, "citeEnd": 138, "citeStartToken": 117, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "Proposition 5. REGD wn .k/ D LCCFL.k C 1/ As a special case, Coupled-Context-Free Grammars with fan-out 2 are equivalent to Tree Adjoining Grammars (tags) (Hotz and Pitsch, 1996) . This enables us to generalize a previous result on the class of dependency structures generated by lexicalized tags (Bodirsky et al., 2005) to the class of generated dependency languages, LTAL.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Proposition 5. REGD wn .k/ D LCCFL.k C 1/ As a special case, Coupled-Context-Free Grammars with fan-out 2 are equivalent to Tree Adjoining Grammars (tags) (Hotz and Pitsch, 1996) . ", "mid_sen": "This enables us to generalize a previous result on the class of dependency structures generated by lexicalized tags (Bodirsky et al., 2005) to the class of generated dependency languages, LTAL.", "after_sen": "Proposition 6. REGD wn .1/ D LTAL"}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Named entity recognition (NER) provides information that is particularly relevant for NP parsing, simply because entities are nouns. For example, knowing that Air Force is an entity tells us that Air Force contract is a left-branching NP. Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NERbased features will be helpful in a statistical model. There has also been recent work combining NER and parsing in the biomedical field. Lewin (2007) experiments with detecting base-NPs using NER information, while Buyko et al. (2007) use a CRF to identify a guest comedian Victor Borge", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, knowing that Air Force is an entity tells us that Air Force contract is a left-branching NP. ", "mid_sen": "Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NERbased features will be helpful in a statistical model. ", "after_sen": "There has also been recent work combining NER and parsing in the biomedical field. "}
{"citeStart": 45, "citeEnd": 59, "citeStartToken": 45, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "It was, for instance, essential for the writing of categorial grammars to allow category variables in the context-free phrase structure part of the rules. How else could one formulate the rules of functional application. The implementation of this facility through Stuart Shieber, however, raised interesting problems in connection with the prediction aspect of the Earley-parser. Original Earley prediction works on category symbols. An answer to these problems was presented by Shieber (1985) who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Original Earley prediction works on category symbols. ", "mid_sen": "An answer to these problems was presented by Shieber (1985) who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer.", "after_sen": "Another example for the influence of the CUG efforts on the development of PATR is a new template notation introduced by Lauri Karttunen in his Interlisp-D version of PATR."}
{"citeStart": 125, "citeEnd": 144, "citeStartToken": 125, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "To account for this difference, we can estimate separate values of X + and A-for different ranges of n (u,v) . Similarly, the hidden parameters can be conditioned on the linked parts of speech. Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences. Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not. This method of incorporating dictionary information seems simpler than the method proposed by Brown et ai. for their models (Brown et al., 1993b) . When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not. ", "mid_sen": "This method of incorporating dictionary information seems simpler than the method proposed by Brown et ai. for their models (Brown et al., 1993b) . ", "after_sen": "When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class."}
{"citeStart": 57, "citeEnd": 69, "citeStartToken": 57, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexicai forms of words. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.", "mid_sen": "For example, in the context of syntactic disambiguation, Black (1993) and Magerman (1995) proposed statistical parsing models based-on decisiontree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees. ", "after_sen": "As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexicai forms of words. "}
{"citeStart": 216, "citeEnd": 234, "citeStartToken": 216, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004) , the Polarity data set 5 created by (Pang and Lee, 2004) , and the MPQA data set created by (Wiebe et al., 2005) . 6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves sentence-level classification.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004) , the Polarity data set 5 created by (Pang and Lee, 2004) , and the MPQA data set created by (Wiebe et al., 2005) . 6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves sentence-level classification.", "after_sen": "The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993) . "}
{"citeStart": 107, "citeEnd": 122, "citeStartToken": 107, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2007a) , with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009) . However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. ", "mid_sen": "With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009) . ", "after_sen": "However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model."}
{"citeStart": 149, "citeEnd": 166, "citeStartToken": 149, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "The aim of this paper is to give a detailed account of the techniques used in TnT. Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993) . ", "mid_sen": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996) . ", "after_sen": "For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999) ."}
{"citeStart": 170, "citeEnd": 197, "citeStartToken": 170, "citeEndToken": 197, "sectionName": "UNKNOWN SECTION NAME", "string": "Distance-weighted averaging induces classes of similar words from word co-occurrences without making reference to a taxonomy. A key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies. Several measures of distributional similarity have been proposed in the literature (Dagan et al., 1999; Lee, 1999) . We used two measures, the Jensen-Shannon divergence and the confusion probability. Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs (Dagan et al., 1999; Grishman and Sterling, 1994; Lapata, 2000; Lee, 1999) . In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used two measures, the Jensen-Shannon divergence and the confusion probability. ", "mid_sen": "Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs (Dagan et al., 1999; Grishman and Sterling, 1994; Lapata, 2000; Lee, 1999) . ", "after_sen": "In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs."}
{"citeStart": 87, "citeEnd": 114, "citeStartToken": 87, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "The two log-linear models for the MEMM tagger and reranker are estimated using a limited-memory BFGS algorithm implemented in an open-source software Amis 3 . In both models, Gaussian prior distributions are used to avoid overfitting (Chen and Rosenfeld, 1999) , and the standard deviations of the Gaussian distributions are optimized to maximize the performance on the development set. We also used a thresholding technique which discards features with low frequency. This is also optimized using the development set, and the best threshold was 4 for the MEMM tagger, and 50 for the reranker 4 . For both of the MEMM tagger and reranker, combinations of feature classes are manually selected to improve the accuracies on the development set. Our final models include 49 and 148 feature class combinations for the MEMM tagger and reranker, respectively. Table 3 shows the performance of the MEMM tagger on the development set. As reported in many of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006) , features of shallow parsers had a large contribution to the performance. The information of the previous labels was also quite effective, which indicates that label unigram models (i.e. 0th order Markov models, so to speak) would have been insufficient for good performance.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 3 shows the performance of the MEMM tagger on the development set. ", "mid_sen": "As reported in many of the previous studies (Kim et al., 2004; Okanohara et al., 2006; Tzong-Han Tsai et al., 2006) , features of shallow parsers had a large contribution to the performance. ", "after_sen": "The information of the previous labels was also quite effective, which indicates that label unigram models (i.e. 0th order Markov models, so to speak) would have been insufficient for good performance."}
{"citeStart": 10, "citeEnd": 37, "citeStartToken": 10, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin's original taxonomy (Stevenson and Joanis, 2003) . We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods. Following Stevenson and Joanis (2003) , we selected 20 verbs from each class which occur at least 100 times in our corpus. This gave us 260 verbs in total.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods. ", "mid_sen": "Following Stevenson and Joanis (2003) , we selected 20 verbs from each class which occur at least 100 times in our corpus. ", "after_sen": "This gave us 260 verbs in total."}
{"citeStart": 66, "citeEnd": 84, "citeStartToken": 66, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Generalizing the interpretation of our data in the light of these findings, we recognize a substantial increase of retrieval performance when query and text tokens are segmented according to the principles of the subword model. The gain is still not overwhelming. With regard to orthographic normalization, we expected a higher performance benefit because of the well-known spelling problems for German medical terms of Latin or Greek origin (such as in 'Zäkum ', 'Cäkum', 'Zaekum', 'Caekum', 'Zaecum', 'Caecum') . For our experiments, however, we used quite a homogeneous document collection following the spelling standards of medical publishers. The same standards apparently applied to the original multiple choice questions, by which the acquisition of expert queries was guided (cf. Section 3). In the layman queries, there were only few Latin or Greek terms, and, therefore, they did not take advantage of the spelling normalization. However, the experience with medical text retrieval (especially on medical reports which exhibit a high rate of spelling variations) shows that orthographic normalization is a desider- Table 5 : Query / Document Mismatch atum for enhanced retrieval quality. The proximity (adjacency) of search terms as a crucial parameter for output ranking proved useful, so we use it as default for subword and synonym class indexing. Whereas the usefulness of Subword Indexing became evident, we could not provide sufficient evidence for Synonym Class Indexing, so far. However, synonym mapping is still incomplete in the current state of our subword dictionary. A question we have to deal with in the future is an alternative way to evaluate the comparative value of synonym class indexing. We have reason to believe that precision cannot be taken as the sole measure for the advantages of a query expansion in cases where the subword approach is already superior (for all layman and expert queries this method retrieved relevant documents, whereas word-based methods failed in 29.6% of the layman queries and 8% of the expert queries, cf. Figure 5 ). It would be interesting to evaluate the retrieval effectiveness (in terms of precision and recall) of different versions of the synonym class indexing approach in those cases where retrieval using word or subword indexes fails due to a complete mismatch between query and documents. This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001 )) are incorporated into our system. Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus (MeSH, (NLM, 2001 )) are incorporated into our system. ", "mid_sen": "Alternatively, we may think of user-centered comparative studies (Hersh et al., 1995) .", "after_sen": ""}
{"citeStart": 32, "citeEnd": 52, "citeStartToken": 32, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that in our original work (Zhang and Chai, 2009) , only development data were used to show some initial observations. Here we trained our models on the development data and results shown are from the testing data.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The hypotheses The data is available for download at http: //links.cse.msu.edu:8000/lair/projects/ conversationentailment_data.html. were categorized into four types: (1) fact: profile and social relations of conversation participants (accounted for 47% of the development data and 49% of the testing data); (2) belief: participants' beliefs and opinions (34% and 35%); (3) desire: participants' desire of certain actions or outcomes (11% and 4%); (4) intent: communicative intent that captures some perlocutionary force from one participant to the other (e.g. A stops B from doing something; A disagreees with B on something, 8% and 12%)", "mid_sen": "Note that in our original work (Zhang and Chai, 2009) , only development data were used to show some initial observations. ", "after_sen": "Here we trained our models on the development data and results shown are from the testing data."}
{"citeStart": 115, "citeEnd": 130, "citeStartToken": 115, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.", "mid_sen": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "after_sen": "We have a different perspective than these lines of inquiry. "}
{"citeStart": 157, "citeEnd": 181, "citeStartToken": 157, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we should mention the problem of complexity. A critical reader might see the nine steps in our algorithm and conclude that the algorithm is overly complex. This would be a false conclusion. To begin with, the problem itself is complex. It is easy to create examples where the resulting transducer created by any algorithm would become unmanageably large. But there exist strategies for keeping the transducers smaller. For example, it is not necessary for all nine steps to be composed. They can also be cascaded. In that case it will be possible to implement different steps by different strategies, e.g. by deterministic or non-deterministic transducers or bimachines (Roche and Schabes, 1997b) . The range of possibilities leaves plenty of room for future research.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They can also be cascaded. ", "mid_sen": "In that case it will be possible to implement different steps by different strategies, e.g. by deterministic or non-deterministic transducers or bimachines (Roche and Schabes, 1997b) . ", "after_sen": "The range of possibilities leaves plenty of room for future research."}
{"citeStart": 155, "citeEnd": 176, "citeStartToken": 155, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "Mention detection. Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006) ). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004) ); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mention detection. ", "mid_sen": "Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006) ). ", "after_sen": "Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). "}
{"citeStart": 71, "citeEnd": 91, "citeStartToken": 71, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "The root extraction process is concerned with the transformation of all Arabic word derivatives to their single common root or canonical form. This process is very useful in terms of reducing and compressing the indexing structure, and in taking advantage of the semantic/conceptual relationships between the different forms of the same root. In this work, we use the Arabic root extraction technique in (El Kourdi, 2004) . It compares favorably to other stemming or root extraction algorithms (Yates and Neto, 1999; Al-Shalabi and Evens, 1998; and Houmame, 1999) , with a performance of over 97% for extracting the correct root in web documents, and it addresses the challenge of the Arabic broken plural and hollow verbs. In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this work, we use the Arabic root extraction technique in (El Kourdi, 2004) . ", "mid_sen": "It compares favorably to other stemming or root extraction algorithms (Yates and Neto, 1999; Al-Shalabi and Evens, 1998; and Houmame, 1999) , with a performance of over 97% for extracting the correct root in web documents, and it addresses the challenge of the Arabic broken plural and hollow verbs. ", "after_sen": "In the remainder of this paper, we will use the term \"root\" and \"term\" interchangeably to refer to canonical forms obtained through this root extraction process."}
{"citeStart": 78, "citeEnd": 103, "citeStartToken": 78, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "For our experiments, we use the opinion frame annotations from previous work (Somasundaran et al., 2008) . These annotations consist of the opinion spans that reveal opinions, their targets, the polarity information for opinions, the labeled links between the targets and the frame links between the opinions. The annotated data consists of 7 scenario-based, multi-party meetings from the AMI meeting corpus (Carletta et al., 2005) . The manual Dialog Act (DA) annotations, provided by AMI, segment the meeting transcription into separate dialog acts. We use these DAs as nodes or instances in our opinion graph.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For our experiments, we use the opinion frame annotations from previous work (Somasundaran et al., 2008) . ", "after_sen": "These annotations consist of the opinion spans that reveal opinions, their targets, the polarity information for opinions, the labeled links between the targets and the frame links between the opinions. "}
{"citeStart": 58, "citeEnd": 72, "citeStartToken": 58, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "where σ is an arbitrary threshold used to control the precision/recall balance. For comparison purposes, we also use Joachims' SVM light (Joachims, 1999) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "where σ is an arbitrary threshold used to control the precision/recall balance. ", "mid_sen": "For comparison purposes, we also use Joachims' SVM light (Joachims, 1999) .", "after_sen": "10 Experimental Evaluation"}
{"citeStart": 26, "citeEnd": 36, "citeStartToken": 26, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "apply (l\\run(l), bill) ) (Park, 1992) proposes a solution within first-order unification that can handle not only sentence (la), but also more complex examples with determiners. The method used is to introduce spurious bindings that subsequently get removed. For example, the semantics of (4a) would be (4b), which would then get simplified to (4c).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(3) and (apply (I\\run(X), j ohn).", "mid_sen": "apply (l\\run(l), bill) ) (Park, 1992) proposes a solution within first-order unification that can handle not only sentence (la), but also more complex examples with determiners. ", "after_sen": "The method used is to introduce spurious bindings that subsequently get removed. "}
{"citeStart": 74, "citeEnd": 101, "citeStartToken": 74, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "I will discuss two w ays of improving the e ciency of parsing a sublanguage, given an all-purpose unication grammar. The rst consists in deleting unused disjuncts, while the second uses a staged parsing process. The experiments are only sketched, to indicate the applicability of the instrumentation technique, and not to directly compete with other proposals on grammar specialization. For example, the work reported in Rayner and Samuelsson, 1994; Samuelsson, 1994 di ers from the one presented below in several aspects: They induce a grammar from a treebank, while I propose to annotate the grammar based on all solutions it produces. No criteria for tree decomposition and category specialization are needed here, and the standard parsing algorithm can be used. On the other hand, the e ciency gains are not as big as those reported by Rayner and Samuelsson, 1994. ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "No criteria for tree decomposition and category specialization are needed here, and the standard parsing algorithm can be used. ", "mid_sen": "On the other hand, the e ciency gains are not as big as those reported by Rayner and Samuelsson, 1994. ", "after_sen": ""}
{"citeStart": 129, "citeEnd": 146, "citeStartToken": 129, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "(4) The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The following sentences are two examples.", "mid_sen": "(4) The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965 ).", "after_sen": "(5) This type of model has been used by, among others, Eisner (1996) ."}
{"citeStart": 182, "citeEnd": 196, "citeStartToken": 182, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994 ). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et al., 1995) . 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993 Carroll, , 1994 , using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994 , 1995 Carroll & Briscoe, 1996) . 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Description of the System 2.1 Overview The system consists of the following six components which are applied in sequence to sentences containing a specific predicate in order to retrieve a set of subcategorization classes for that predicate:", "mid_sen": "1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994 ). ", "after_sen": "2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. "}
{"citeStart": 117, "citeEnd": 137, "citeStartToken": 117, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al., 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988) . Even though these approaches often accomplish considerable improvements with respect to efficiency or termination behavior, it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering. By bringing filtering into the logic underlying the grammar it is possible to show in a perspicuous and logically clean way how and why filtering can be optimized in a particular fashion and how various approaches relate to each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many approaches focus on exploiting specific knowledge about grammars and/or the computational task(s) that one is using them for by making filtering explicit and extending the processing strategy such that this information can be made effective.", "mid_sen": "In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al., 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988) . ", "after_sen": "Even though these approaches often accomplish considerable improvements with respect to efficiency or termination behavior, it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering. "}
{"citeStart": 89, "citeEnd": 100, "citeStartToken": 89, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Decoder The decoder used in this work is Moses, a log-linear decoder similar to Pharaoh (Koehn, 2004) , modified to accommodate supertag phrase probabilities and supertag language models.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Nevertheless, this mechanism provides a simple, efficient integration of a global compositionality (grammaticality) measure into the n-gram language model over supertags.", "mid_sen": "Decoder The decoder used in this work is Moses, a log-linear decoder similar to Pharaoh (Koehn, 2004) , modified to accommodate supertag phrase probabilities and supertag language models.", "after_sen": ""}
{"citeStart": 87, "citeEnd": 98, "citeStartToken": 87, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "Pronoun Resolution in LaSIE Our implementation makes use of the algorithm proposed in (Azzam, 1996) , where elementary events (EEs, effectively simple clauses) are used as basic processing units, rather than sentences. Updating the focus registers and the application of interpretation rules (IRs) for pronoun resolution then takes place after each EE, permitting intrasentential references3 In addition, an initial 'expected focus' is determined based on the first EE in a text, providing a potential antecedent for any pronoun within the first EE.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Pronoun Resolution in LaSIE Our implementation makes use of the algorithm proposed in (Azzam, 1996) , where elementary events (EEs, effectively simple clauses) are used as basic processing units, rather than sentences. ", "after_sen": "Updating the focus registers and the application of interpretation rules (IRs) for pronoun resolution then takes place after each EE, permitting intrasentential references3 In addition, an initial 'expected focus' is determined based on the first EE in a text, providing a potential antecedent for any pronoun within the first EE."}
{"citeStart": 41, "citeEnd": 67, "citeStartToken": 41, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001) . Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving. Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4 ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to establish a shared terminology, learner strategies for C-test solving have been categorized as micro-level and macro-level processing strategies (Babaii and Ansary, 2001) . ", "mid_sen": "Psycholinguistic analyses (Sigott, 2006; Grotjahn and Stemmer, 2002) discuss in detail that both strategies are required for successful C-test solving. ", "after_sen": "Therefore, we developed a model for C-test difficulty that incorporates features from both processing levels (see Figure 4 )."}
{"citeStart": 14, "citeEnd": 37, "citeStartToken": 14, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "The delicacy of testsuite construction is acknowledged in EAGLES, 1996, p.37 . Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. ", "mid_sen": "Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. ", "after_sen": "The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category."}
{"citeStart": 57, "citeEnd": 66, "citeStartToken": 57, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we will address the problem of sense discrimination as defined above. That is, we will not be concerned with the sense-labeling component of word sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. ", "mid_sen": "Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "after_sen": "What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses."}
{"citeStart": 22, "citeEnd": 34, "citeStartToken": 22, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern mateher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern mateher and a stopword list. ", "mid_sen": "A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. ", "after_sen": "A dictionary of word stem frequencies is constructed for each sentence. "}
{"citeStart": 133, "citeEnd": 149, "citeStartToken": 133, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993) . Tag probabilities are set according to the word's ending. The suffix is a strong predictor for word classes, e.g., words in the Wall Street Journal part of the Penn Treebank ending in able are adjectives (JJ) in 98% of the cases (e.g. fashionable, variable), the rest of 2% are nouns (e.g. cable, variable).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Currently, the method of handling unknown words that seems to work best for inflected languages is a suffix analysis as proposed in (Samuelsson, 1993) . ", "after_sen": "Tag probabilities are set according to the word's ending. "}
{"citeStart": 121, "citeEnd": 123, "citeStartToken": 121, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "In grammar formalisms like DCG or IIPSG, the complex nonterminals have an argument or a feature (PtION) that represents the covered substring explicitly. The combination of the substrings is explicit in the rules of the grammar. As a consequence, Earley deduction does not need to make use of string positions for its clauses, as Pereira and Warren [10] point out.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The combination of the substrings is explicit in the rules of the grammar. ", "mid_sen": "As a consequence, Earley deduction does not need to make use of string positions for its clauses, as Pereira and Warren [10] point out.", "after_sen": "Moreover, the use of string positions known from chart parsing is too inflexible because it el= lows only concatenation of adjacent contiguous substrings. "}
{"citeStart": 168, "citeEnd": 196, "citeStartToken": 168, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system . In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006) , Callison-Burch et al. (2007) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system . ", "mid_sen": "In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006) , Callison-Burch et al. (2007) .", "after_sen": "We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task. "}
{"citeStart": 1, "citeEnd": 15, "citeStartToken": 1, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "Our basic parsing and interactive training paradigm is based on (Simmons and Yu, 1992) . We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. (Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER. with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993) . Questioning the traditional n-grams, Magerman already advocates a heavier reliance on contextual information. Going beyond Magerman's still relatively rigid set of 36 features, we propose a yet richer, basically unlimited feature language set. Our parse action sequences are too complex to be derived from a treebank like Penn's. Not only do our parse trees contain semantic annotations, roles and more syntactic detail, we also rely on the more informative parse action sequence. While this necessitates the involvement of a parsing supervisor for training, we are able to perform deterministic parsing and get already very good test results for only 256 training sentences. (Collins, 1996) focuses on bigram lexical dependencies (BLD). Trained on the same 40,000 sentences as Spatter, it relies on a much more limited type of context than our system and needs little background knowledge. words. Table 6 compares our results with SPATTER, and BLD. The results have to be interpreted cautiously since they are not based on the exact same sentences and detail of bracketing. Due to lexical restrictions, our average sentence length (17.1) is below the one used in SPATTER and BLD (22.3), but some of our test sentences have more than 40 words; and while the Penn Treebank leaves many phrases such as \"the New York Stock Exchange\" without internal structure, our system performs a complete bracketing, thereby increasing the risk of crossing brackets.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have extended their work by significantly increasing the expressiveness of the parse action and feature languages, in particular by moving far beyond the few simple features that were limited to syntax only, by adding more background knowledge and by introducing a sophisticated machine learning component. ", "mid_sen": "(Magerman, 1995) uses a decision tree model similar to ours, training his system SPATTER. ", "after_sen": "with parse action sequences for 40,000 Wall Street Journal sentences derived from the Penn Treebank (Marcus et al., 1993) . "}
{"citeStart": 188, "citeEnd": 202, "citeStartToken": 188, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "Here, the first rewrite rule is applied iteratively (bottom-up on the tree) to flatten all right recursion, using incomplete constituents to record the original nonterminal ordering. The second rule is then applied to generate left recursive structure, preserving this ordering. The incomplete constituent categories created by the right corner transform are similar in form and meaning to non-constituent categories used in Combinatorial Categorial Grammars (CCGs) (Steedman, 2000) . Unlike CCGs, however, a right corner transformed grammar does not allow backward function application, composition, or raising. As a result, it does not introduce spurious ambiguity between forward and backward operations, but cannot be taken to explicitly encode argument structure, as CCGs can. ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second rule is then applied to generate left recursive structure, preserving this ordering. ", "mid_sen": "The incomplete constituent categories created by the right corner transform are similar in form and meaning to non-constituent categories used in Combinatorial Categorial Grammars (CCGs) (Steedman, 2000) . ", "after_sen": "Unlike CCGs, however, a right corner transformed grammar does not allow backward function application, composition, or raising. "}
{"citeStart": 186, "citeEnd": 206, "citeStartToken": 186, "citeEndToken": 206, "sectionName": "UNKNOWN SECTION NAME", "string": "number of features used in our complex models -in the several hundreds of thousands, is extremely high in comparison with the data set size and the number of features used in other machine learning domains. We describe two sets of experiments aimed at comparing models with and without regularization. One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features. The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in Chen and Rosenfeld (2000) , and used in all the stochastic LFG work (Johnson et al., 1999) . However, until recently, its role and importance have not been widely understood. For example, Zhang and Oles (2001) attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization. At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996) , Toutanova and Manning (2000) , and Collins (2002) all present unregularized models. Indeed, the result of Collins (2002) that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized. Table 5 shows results on the development set from two pairs of experiments. The first pair of models use common word templates t 0 , w 0 , t 0 , t −1 , t −2 and the same rare word templates as used in the models in table 2. The second pair of models use the same features as model BEST with a higher frequency cutoff of 5 for common word features.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features. ", "mid_sen": "The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in Chen and Rosenfeld (2000) , and used in all the stochastic LFG work (Johnson et al., 1999) . ", "after_sen": "However, until recently, its role and importance have not been widely understood. "}
{"citeStart": 53, "citeEnd": 70, "citeStartToken": 53, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "This representation of context is a variation on the bag-of-words feature set, where a single window of context includes words that occur to both the left and right of the ambiguous word. An early use of this representation is described in (Gale et al., 1992) , where word sense disambiguation is performed with a Naive Bayesian classifier. The work in this paper differs in that there are two windows of context, one representing words that occur to the left of the ambiguous word and another for those to the right.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This representation of context is a variation on the bag-of-words feature set, where a single window of context includes words that occur to both the left and right of the ambiguous word. ", "mid_sen": "An early use of this representation is described in (Gale et al., 1992) , where word sense disambiguation is performed with a Naive Bayesian classifier. ", "after_sen": "The work in this paper differs in that there are two windows of context, one representing words that occur to the left of the ambiguous word and another for those to the right."}
{"citeStart": 100, "citeEnd": 122, "citeStartToken": 100, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "With our new data set, we began running experiments similar to those carried out in the literature (Nakov and Hearst, 2005) . We implemented both an adjacency and dependency model, and three different association measures: raw counts, bigram probability, and", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "With our new data set, we began running experiments similar to those carried out in the literature (Nakov and Hearst, 2005) . ", "after_sen": "We implemented both an adjacency and dependency model, and three different association measures: raw counts, bigram probability, and"}
{"citeStart": 295, "citeEnd": 309, "citeStartToken": 295, "citeEndToken": 309, "sectionName": "UNKNOWN SECTION NAME", "string": "Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news domain. For example, the best F-score in the shared task of Bio-NER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1 , whereas the best performance at MUC-6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news domain. ", "mid_sen": "For example, the best F-score in the shared task of Bio-NER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1 , whereas the best performance at MUC-6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995) .", "after_sen": "Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al., 1997) , the dictionary HMM model (Kou et al., 2005) and Maximum Entropy Markov Models (MEMMs) (Finkel et al., 2004) . "}
{"citeStart": 155, "citeEnd": 172, "citeStartToken": 155, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992) ). A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986) , RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988) . We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992) ). ", "mid_sen": "A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986) , RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988) . ", "after_sen": "We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion."}
{"citeStart": 29, "citeEnd": 50, "citeStartToken": 29, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "In the rest of this paper I will lay out a proposal for handling reduplication with finite-state methods. As a starting point, I adopt Bird & Ellison (1994) 's One-Level Phonology, a monostratal constraintbased framework where phonological representations, morphemes and generalizations are all finitestate automata (FSAs) and constraint combination is accomplished via automata intersection. While it is possible to transfer much of the present proposal to the transducer-based setting that is often preferred nowadays, the monostratal approach still offers an attractive alternative due to its easy blend with monostratal grammars such as HPSG and the good prospects for machine learning of its surfacetrue constraints (Ellison (1992) , Belz (1998)) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the rest of this paper I will lay out a proposal for handling reduplication with finite-state methods. ", "mid_sen": "As a starting point, I adopt Bird & Ellison (1994) 's One-Level Phonology, a monostratal constraintbased framework where phonological representations, morphemes and generalizations are all finitestate automata (FSAs) and constraint combination is accomplished via automata intersection. ", "after_sen": "While it is possible to transfer much of the present proposal to the transducer-based setting that is often preferred nowadays, the monostratal approach still offers an attractive alternative due to its easy blend with monostratal grammars such as HPSG and the good prospects for machine learning of its surfacetrue constraints (Ellison (1992) , Belz (1998)) ."}
{"citeStart": 51, "citeEnd": 66, "citeStartToken": 51, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "The algorithm is implemented in the FSA Utilities (van Noord, 1997) . We use the notation provided by the toolbox throughout this paper. Ta macro (vowel, {a, e, i,o,u}) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The algorithm is implemented in the FSA Utilities (van Noord, 1997) . ", "after_sen": "We use the notation provided by the toolbox throughout this paper. "}
{"citeStart": 98, "citeEnd": 117, "citeStartToken": 98, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "Well-typcdness requirements restrict the space of valid feature structures (cf. Carl)enter 92): Every feature structure must I)e associated with a type, and every type restricts its associated feature structure in that only certain features are allowed and the values of these features must be of a certain type. Appropriateness and value restrictions are inherited along the type hierarchy. The second source of constraints, in order to admit only linguistically valid feature structures, are the principles of grammar. Pollard ,~ Sag 87 allow general implicative and negative constraints in the form of conditional feature structures. In Pollard h Sag in press principles are given only in verbal form. Recent work on formalizing the basis of IIPSG models them as constraints attached to types (e.g., Carpenter et al. 91) . Iiowever, these distinctions affect only how the applicability of a principle is specified. More iml)ortant for our present purpose is the form which the constraints expressed by a principle may take. Besides constraints enforcing simple structure sharing (e.g., the Head Featnre Principle given in Fig.2 ) there are also complex relational dependencies (e.g., in the Subcategorization Principlea). Constraints tions a and d. The rule ha.s an empty phonological context but a morphological filter. This is an example for the treatmeat of non-concatenative phenomena in X2MonF.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Pollard h Sag in press principles are given only in verbal form. ", "mid_sen": "Recent work on formalizing the basis of IIPSG models them as constraints attached to types (e.g., Carpenter et al. 91) . ", "after_sen": "Iiowever, these distinctions affect only how the applicability of a principle is specified. "}
{"citeStart": 184, "citeEnd": 206, "citeStartToken": 184, "citeEndToken": 206, "sectionName": "UNKNOWN SECTION NAME", "string": "This research raises many questions about the relationship between syntactic context and verb semantics. An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (Pedersen, et al., 2004) . Particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context. A strong syntactic/semantic correlation would suggest that further gains in the use of surrogate annotation data could be gained if syntactic similarity was computed between rolesets rather than their verbs. However, this would first require accurate word-sense disambiguation both for the test sentences as well as for the parsed corpora used to calculate parse tree path frequencies. Alternatively, parse tree path profiles associated with rolesets may be useful for word sense disambiguation, where the probability of a sense is computed as the likelihood that an ambiguous verb's parse tree paths are sampled from the distributions associated with each verb sense. These topics will be the focus of our future work in this area.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This research raises many questions about the relationship between syntactic context and verb semantics. ", "mid_sen": "An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (Pedersen, et al., 2004) . ", "after_sen": "Particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context. "}
{"citeStart": 114, "citeEnd": 127, "citeStartToken": 114, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "We first present our soft cohesion constraint's effect on BLEU score (Papineni et al., 2002) for both our dev-test and test sets. We compare against an unmodified baseline decoder, as well as a decoder enhanced with a lexical reordering model (Tillman, 2004; . For each phrase pair in our translation table, the lexical reordering model tracks statistics on its reordering behavior as observed in our word-aligned training text. The lexical reordering model provides a good comparison point as a non-syntactic, and potentially orthogonal, improvement to phrase-based movement modeling. We use the implementation provided in Moses, with probabilities conditioned on bilingual phrases and predicting three orientation bins: straight, inverted and disjoint. Since adding features to the decoder's log-linear model is straight-forward, we also experiment with a combined system that uses both the cohesion constraint and a lexical reordering model.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We first present our soft cohesion constraint's effect on BLEU score (Papineni et al., 2002) for both our dev-test and test sets. ", "mid_sen": "We compare against an unmodified baseline decoder, as well as a decoder enhanced with a lexical reordering model (Tillman, 2004; . ", "after_sen": "For each phrase pair in our translation table, the lexical reordering model tracks statistics on its reordering behavior as observed in our word-aligned training text. "}
{"citeStart": 168, "citeEnd": 193, "citeStartToken": 168, "citeEndToken": 193, "sectionName": "UNKNOWN SECTION NAME", "string": "Another set of approaches for computing distance was based on the phonetics. This computed the Levenshtein distance between phonetic strings. The Levenshtein distance is the cost of the least expensive set of insertions, deletions, or substitutions that would be needed to transform one string into the other (Sankoff and Kruskal, 1983 tie') would get a much larger score, 5.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This computed the Levenshtein distance between phonetic strings. ", "mid_sen": "The Levenshtein distance is the cost of the least expensive set of insertions, deletions, or substitutions that would be needed to transform one string into the other (Sankoff and Kruskal, 1983 tie') would get a much larger score, 5.", "after_sen": "In the above technique, very small phonetic differences, such as that between a moderately palatalized and a very palatalized [t], count the same as major differences, such as that between a [t] and an [e]. "}
{"citeStart": 69, "citeEnd": 89, "citeStartToken": 69, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "While the combined MBL method achieves reasonably good results even given the limitations of the evaluation method, there is still clearly room for improvement. Future work will pursue at least two directions for improving the results. First, while semantic information is not available for all adjectives, it is clearly available for some. Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. More generally, distributional clustering techniques (Schütze, 1992; Pereira et al., 1993) could be applied to extract semantic classes from the corpus itself. Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results. The second area where the methods described here could be improved is in the way that multiple information sources are integrated. The technique method described in section 3.7 is a fairly crude method for combining frequency information with symbolic data. It would be worthwhile to investigate applying some of the more sophisticated ensemble learning techniques which have been proposed in the literature (Dietterich, 1997) . In particular, boosting (Schapire, 1999; Abney et al., 1999) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, any realistic dialog system would make use of some limited vocabulary for which semantic information would be available. ", "mid_sen": "More generally, distributional clustering techniques (Schütze, 1992; Pereira et al., 1993) could be applied to extract semantic classes from the corpus itself. ", "after_sen": "Since the constraints on adjective ordering in English depend largely on semantic classes, the addition of semantic information to the model ought to improve the results. "}
{"citeStart": 50, "citeEnd": 67, "citeStartToken": 50, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "The input to the system consists of the terminal symbols from the trees in the corpus section mentioned above. The terminal symbol strings are first pre-processed by stripping punctuation and other non-vocalized terminal symbols, which could not be expected from the output of a speech recognizer. Crucially, any information about repair is stripped from the input, including partial words, repair symbols 3 , and interruption point information. While an integrated system for processing and parsing speech may use both acoustic and syntactic information to find repairs, and thus may have access to some of this information about where interruptions occur, this experiment is intended to evaluate the use of the right corner transform and syntactic information on parsing speech repair. To make a fair comparison to the CYK baseline of (Hale et al., 2006) , the recognizer was given correct part-of-speech tags as input along with words. The results presented here use two standard metrics for assessing accuracy of transcribed speech with repairs. The first metric, Parseval F-measure, takes into account precision and recall of all nonterminal (and non pre-terminal) constituents in a hypothesized tree relative to the gold standard. The second metric, EDIT-finding F, measures precision and recall of the words tagged as EDITED in the hypothesized tree relative to those tagged EDITED in the gold standard. F score is defined as usual, 2pr/(p + r) for precision p and recall r.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While an integrated system for processing and parsing speech may use both acoustic and syntactic information to find repairs, and thus may have access to some of this information about where interruptions occur, this experiment is intended to evaluate the use of the right corner transform and syntactic information on parsing speech repair. ", "mid_sen": "To make a fair comparison to the CYK baseline of (Hale et al., 2006) , the recognizer was given correct part-of-speech tags as input along with words. ", "after_sen": "The results presented here use two standard metrics for assessing accuracy of transcribed speech with repairs. "}
{"citeStart": 51, "citeEnd": 68, "citeStartToken": 51, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. In a first stage, only MEDLINE abstracts are used (Yang et al., 2004) , later other-anaphora, a very specific sub-task, are investigated using full paper content (Chen et al., 2008) . Gasperin (2009) presents a full annotation of anaphora and coreference in biomedical text, but only noun phrases referring to biomedical entities are considered. On the basis of this annotation, she implements a probabilistic anaphora resolution system. In contrast, Cohen et al. (2010) build a corpus of 97 full-text journal articles in the biomedical domain where every co-referring noun phrase is annotated (CRAFT -Colorado Richly Annotated Full Text). Their annotation guidelines follow those of the OntoNotes project (Hovy et al., 2006) , adapted to the biomedical domain. OntoNotes itself is a text corpus of approx. one million words from mainly news texts (newswire, magazines, broadcast conversations, web pages). It also contains general anaphoric coreference annotations (Pradhan et al., 2007) : events and (like in our annotation) unlimited noun phrase entity types. Kim and Webber (2006) investigate a special aspect, citation sentences where a pronoun such as \"they\" refers to a previous citation. The study is performed on astronomy journal articles and a maximum-entropy classifier is trained. Kaplan et al. (2009) investigate coreferences and citations as well, but only at a very small scale (4 articles from the Computational Linguistics journal). They focus on so-called c-sites which are the sentences following a citation that also refer to the same paper (typically by anaphora). The authors train a specific coreference model for this phenomenon. They show that exploitation of coreference chains improves the extraction of citation contexts which they then use for research paper summarization.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. ", "mid_sen": "In a first stage, only MEDLINE abstracts are used (Yang et al., 2004) , later other-anaphora, a very specific sub-task, are investigated using full paper content (Chen et al., 2008) . Gasperin (2009) presents a full annotation of anaphora and coreference in biomedical text, but only noun phrases referring to biomedical entities are considered. ", "after_sen": "On the basis of this annotation, she implements a probabilistic anaphora resolution system. "}
{"citeStart": 46, "citeEnd": 65, "citeStartToken": 46, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "All logical notions that we are going to consider, such as theory or model, will be finitary. For example, a model would typically contain fewer than a hundred elements of different logical sorts. Therefore these notions, and all other constructs we are going to define (axioms, metarules, definitions etc.) are computational, although usually we will not provide explicit algorithms for computing them. The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. This Principle of Finitism is also assumed by Johnson-Laird (1983 ), Jackendoff (1983 , Kamp (1981) , and implicitly or explicitly by almost all researchers in computational linguistics. As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The issues of control are not so important for us at this point; we restrict ourselves to describing the logic. ", "mid_sen": "This Principle of Finitism is also assumed by Johnson-Laird (1983 ), Jackendoff (1983 , Kamp (1981) , and implicitly or explicitly by almost all researchers in computational linguistics. ", "after_sen": "As a logical postulate it is not very radical; it is possible within a finitary framework to develop that part of mathematics that is used or has potential applications in natural science, such as mathematical analysis (cf. Mycielski 1981) ."}
{"citeStart": 1, "citeEnd": 25, "citeStartToken": 1, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "The text corpus tasks parsing, information extraction and information retrieval can benefit from dividing sentences in chunks of words. (Ramshaw and Marcus, 1995) describe an error-driven transformation-based learning (TBL) method for finding NP chunks in texts. NP chunks (or baseNPs) are non-overlapping, non-recursive noun phrases. In their experiments they have modeled chunk recognition as a tagging task: words that are inside a baseNP were marked I, words outside a baseNP received an 0 tag and a special tag B was used for the first word inside a baseNP immediately following another baseNP. A text example:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The text corpus tasks parsing, information extraction and information retrieval can benefit from dividing sentences in chunks of words. ", "mid_sen": "(Ramshaw and Marcus, 1995) describe an error-driven transformation-based learning (TBL) method for finding NP chunks in texts. ", "after_sen": "NP chunks (or baseNPs) are non-overlapping, non-recursive noun phrases. "}
{"citeStart": 108, "citeEnd": 127, "citeStartToken": 108, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009) , where specified meter or rhyme schemes are enforced. In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The majority of NLG focuses on the satisfaction of a communicative goal, with examples such as Belz (2008) which produces weather reports from structured data or Mitchell et al. (2013) which generates descriptions of objects from images. ", "mid_sen": "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry (Greene et al., 2010) (Colton et al., 2012) (Jiang and Zhou, 2008) or song lyrics (Wu et al., 2013) (Ramakrishnan A et al., 2009) , where specified meter or rhyme schemes are enforced. ", "after_sen": "In these papers soft semantic goals are sometimes also introduced that seek responses to previous lines of poetry or lyric."}
{"citeStart": 257, "citeEnd": 269, "citeStartToken": 257, "citeEndToken": 269, "sectionName": "UNKNOWN SECTION NAME", "string": "Other content citation analysis research which is rel-evant to our work concentrates on relating textual spans to authors' descriptions of other work. For example, in O'Connor's (1982) experiment, citing statements (one or more sentences referring to other researchers' work) were identified manually. The main problem encountered in that work is the fact that many instances of citation context are linguistically unmarked. Our data confirms this: articles often contain large segments, particularly in the central parts, which describe other people's research in a fairly neutral way. We would thus expect many citations to be neutral (i.e., not to carry any function relating to the argumentation per se). Many of the distinctions typically made in content citation analysis are immaterial to the task considered here as they are too sociologically orientated, and can thus be difficult to operationalise without deep knowledge of the field and its participants (Swales, 1986) . In particular, citations for general reference (background material, homage to pioneers) are not part of our analytic interest here, and so are citations \"in passing\", which are only marginally related to the argumentation of the overall paper (Ziman, 1968) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We would thus expect many citations to be neutral (i.e., not to carry any function relating to the argumentation per se). ", "mid_sen": "Many of the distinctions typically made in content citation analysis are immaterial to the task considered here as they are too sociologically orientated, and can thus be difficult to operationalise without deep knowledge of the field and its participants (Swales, 1986) . ", "after_sen": "In particular, citations for general reference (background material, homage to pioneers) are not part of our analytic interest here, and so are citations \"in passing\", which are only marginally related to the argumentation of the overall paper (Ziman, 1968) ."}
{"citeStart": 20, "citeEnd": 38, "citeStartToken": 20, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been many approaches to automatic detection of similar words from text corpora. Ours is similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There have been many approaches to automatic detection of similar words from text corpora. ", "mid_sen": "Ours is similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed.", "after_sen": "Evaluation of automatically generated lexical resources is a difficult problem. "}
{"citeStart": 93, "citeEnd": 104, "citeStartToken": 93, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. , Poznanski & Sanfilippo (1993) , Resnik (1993) , Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. In future work, we intend to extend the system in this direction. The ability to recognize that argument slots of different subcategorization classes for the same predicate share semantic restrictions/preferences would assist recognition that the predicate undergoes specific alternations, this in turn assisting inferences about control, equi and raising (e.g. .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our system needs further refinement to narrow some subcategorization classes, for example, to choose between differing control options with predicative complements. ", "mid_sen": "It also needs supplementing with information about diathesis alternation possibilities (e.g. Levin, 1993) and semantic selection preferences on argument heads. , Poznanski & Sanfilippo (1993) , Resnik (1993) , Ribas (1994) and others have shown that it is possible to acquire selection preferences from (partially) parsed data. ", "after_sen": "Our system already gathers head lemmas in patterns, so any of these approaches could be applied, in principle. "}
{"citeStart": 108, "citeEnd": 139, "citeStartToken": 108, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "The two wu'iables that determine a parser's coml)utational complexity are the grammar and the input string (Barton, Berwick &: Ristad, 1987) . These are considered separately in the next two sections.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The two wu'iables that determine a parser's coml)utational complexity are the grammar and the input string (Barton, Berwick &: Ristad, 1987) . ", "after_sen": "These are considered separately in the next two sections."}
{"citeStart": 172, "citeEnd": 183, "citeStartToken": 172, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "We first evaluate our model on the task of ordering target sentences, given correct (reference) unordered target dependency trees. Our results show that combining features derived from the source and target dependency trees, distortion surface order-based features (like the distortion used in Pharaoh (Koehn, 2004) ) and language model-like features results in a model which significantly outperforms models using only some of the information sources.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We first evaluate our model on the task of ordering target sentences, given correct (reference) unordered target dependency trees. ", "mid_sen": "Our results show that combining features derived from the source and target dependency trees, distortion surface order-based features (like the distortion used in Pharaoh (Koehn, 2004) ) and language model-like features results in a model which significantly outperforms models using only some of the information sources.", "after_sen": "We also evaluate the contribution of our model to the performance of an MT system. "}
{"citeStart": 24, "citeEnd": 45, "citeStartToken": 24, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "The DSO corpus (Ng and Lee, 1996) contains 192,800 annotated examples for 121 nouns and 70 verbs, drawn from BC and WSJ. While the BC is built as a balanced corpus, containing texts in various categories such as religion, politics, humanities, fiction, etc, the WSJ corpus consists primarily of business and financial news. Exploiting the difference in coverage between these two corpora, Escudero et al. (2000) separated the DSO corpus into its BC and WSJ parts to investigate the domain dependence of several WSD algorithms. Following the setup of (Escudero et al., 2000) , we similarly made use of the DSO corpus to perform our experiments on domain adaptation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Exploiting the difference in coverage between these two corpora, Escudero et al. (2000) separated the DSO corpus into its BC and WSJ parts to investigate the domain dependence of several WSD algorithms. ", "mid_sen": "Following the setup of (Escudero et al., 2000) , we similarly made use of the DSO corpus to perform our experiments on domain adaptation.", "after_sen": "Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al., 1994) is the most widely used. "}
{"citeStart": 113, "citeEnd": 135, "citeStartToken": 113, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "• Techniques used in this paper can be used to determine consistency for other probability models based on TAGs (Carroll and Weir, 1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the initial assignment causes the grammar to be inconsistent, then iterative re-estimation might converge to an inconsistent grammar 1.", "mid_sen": "• Techniques used in this paper can be used to determine consistency for other probability models based on TAGs (Carroll and Weir, 1997) .", "after_sen": ""}
{"citeStart": 217, "citeEnd": 238, "citeStartToken": 217, "citeEndToken": 238, "sectionName": "UNKNOWN SECTION NAME", "string": "The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation, e.g latent semantic indexing (LSI) (Littman et al, 1998) , or the General Vector space model (GVSM), (Carbonell et al, 1997) . We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997 . Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.", "mid_sen": "The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation, e.g latent semantic indexing (LSI) (Littman et al, 1998) , or the General Vector space model (GVSM), (Carbonell et al, 1997) . ", "after_sen": "We believe our approach is computationally less costly than (LSI and GVSM) and assumes less resources (WordNet in Diekema et al., 1999) ."}
{"citeStart": 139, "citeEnd": 152, "citeStartToken": 139, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004) , for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998) , Soon et al. (2001) , Strube and Muller (2003) ). Could the relatively noisy semantic knowledge give us further system improvement?", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Kehler et al. (2004) , for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. ", "mid_sen": "Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998) , Soon et al. (2001) , Strube and Muller (2003) ). ", "after_sen": "Could the relatively noisy semantic knowledge give us further system improvement?"}
{"citeStart": 50, "citeEnd": 75, "citeStartToken": 50, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of product review mining, sentiments and features (aspects or targets) have been mined (for example, Yi et al. (2003) , Popescu and Etzioni (2005) , and Hu and Liu (2006) ). More recently there has been work on creating joint models of topic and sentiments (Mei et al., 2007; Titov and McDonald, 2008) to improve topic-sentiment summaries. We do not model topics; instead we directly model the relations between targets. The focus of our work is to jointly model opinion polarities via target relations. The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The focus of our work is to jointly model opinion polarities via target relations. ", "mid_sen": "The task of finding coreferent opinion topics by (Stoyanov and Cardie, 2008) is similar to our target link classification task, and we use somewhat similar features. ", "after_sen": "Even though their genre is different, we plan to experiment with their full feature set for improving our TLC system."}
{"citeStart": 117, "citeEnd": 139, "citeStartToken": 117, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "• We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006) .", "mid_sen": "• We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root.", "after_sen": "Cube pruning and cube growing are collectively called forest rescoring since they both approximately rescore the packed forest of derivations from −LM decoding. "}
{"citeStart": 130, "citeEnd": 151, "citeStartToken": 130, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French treebank (Abeillé et al., 2003) . French forms are then simplified according to the rules given in Due to tractability problems with word alignment, the 10 9 French-English corpus and the UN corpus were filtered to a more manageable size. The filtering criteria are sentence length (between 15 and 25 words), as well as strings indicating that a sentence is neither French nor English, or otherwise not well-formed, aiming to obtain a subset of good-quality sentences. In total, we use 9M parallel sentences. For the English language model we use large training data with 287.3M true-cased sentences (including the LDC Giga-word data).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The pre-processing of the French input consists of two steps: (1) normalizing not well-formed data (cf. table 1) and (2) morphological simplification.", "mid_sen": "In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French treebank (Abeillé et al., 2003) . ", "after_sen": "French forms are then simplified according to the rules given in Due to tractability problems with word alignment, the 10 9 French-English corpus and the UN corpus were filtered to a more manageable size. "}
{"citeStart": 24, "citeEnd": 39, "citeStartToken": 24, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981) , which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981) , which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. ", "mid_sen": "The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. ", "after_sen": "Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. "}
{"citeStart": 129, "citeEnd": 158, "citeStartToken": 129, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "The MUC-6 system took the destructive option. The nondestructive option has been implemented in a more recent system. These basic steps of \"COLLECT, FILTER, and ORDER by salience\" are analogous to Lappin and Leass's (1994) pronoun resolution algorithm, but each step in FASTUS relies on considerably poorer syntactic input. The present algorithm thus provides an interesting case of what happens with extremely poor syntactic input, even poorer than in Kennedy and Boguraev's (1996) system. This comparison will be discussed later.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These basic steps of \"COLLECT, FILTER, and ORDER by salience\" are analogous to Lappin and Leass's (1994) pronoun resolution algorithm, but each step in FASTUS relies on considerably poorer syntactic input. ", "mid_sen": "The present algorithm thus provides an interesting case of what happens with extremely poor syntactic input, even poorer than in Kennedy and Boguraev's (1996) system. ", "after_sen": "This comparison will be discussed later."}
{"citeStart": 124, "citeEnd": 149, "citeStartToken": 124, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "After cycle removal, incorporating relevant indexing and the collapsing of redundant magic predicates the magic-compiled grammar from figure 2 looks as displayed in figure 5. Figure 6 shows the chart resulting from generation of the sentence \"John buys Mary a book\" .4 The seed is identical to the one used for the example in the previous section. The facts in the chart resulted from not-so-naive bottom-up evaluation: semi-naive evaluation without subsumption checking (Ramakrishnan et al., 1992) . The resulting processing behavior is similar to the behavior that would result from head corner generation except that the different filtering steps are performed in a bottom-up fashion. The head corner approach jumps top-down from pivot to pivot in order to satisfy its assumptions concerning the flow of semantic information, i.e., semantic chaining, and subsequently generates starting from the semantic head in a bottom-up fashion. In the example, the seed is used without any delay to apply the base case of the vp-procedure, thereby jumping over all intermediate chain and non-chain rules. In this respect the initial reordering of rule 2 which led to rule 2 in the final grammar in figure 5 is crucial (see section 4).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figure 6 shows the chart resulting from generation of the sentence \"John buys Mary a book\" .4 The seed is identical to the one used for the example in the previous section. ", "mid_sen": "The facts in the chart resulted from not-so-naive bottom-up evaluation: semi-naive evaluation without subsumption checking (Ramakrishnan et al., 1992) . ", "after_sen": "The resulting processing behavior is similar to the behavior that would result from head corner generation except that the different filtering steps are performed in a bottom-up fashion. "}
{"citeStart": 97, "citeEnd": 101, "citeStartToken": 97, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "In our analysis we argued for hierarchical organization of the control segments on the basis of specific examples of interruptions. We also believe that there are other levels of structure in discourse that are not captured by the control rules, e.g. control shifts do not always correspond with task boundaries. There can be topic shifts without change of initiation, change of control without a topic shift [WS88] . The relationship of cue words, intonational contour[PH90] and the use of modal subordination[Rob86] to the segments derived from the control rules is a topic for future research.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also believe that there are other levels of structure in discourse that are not captured by the control rules, e.g. control shifts do not always correspond with task boundaries. ", "mid_sen": "There can be topic shifts without change of initiation, change of control without a topic shift [WS88] . ", "after_sen": "The relationship of cue words, intonational contour[PH90] and the use of modal subordination[Rob86] to the segments derived from the control rules is a topic for future research."}
{"citeStart": 40, "citeEnd": 58, "citeStartToken": 40, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "We employ the Gibbs sampling algorithm (Gilks et al., 1996) . Unlike in (Mareček andŽabokrtský, 2012) , where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming. The algorithm works as follows:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We employ the Gibbs sampling algorithm (Gilks et al., 1996) . ", "after_sen": "Unlike in (Mareček andŽabokrtský, 2012) , where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming. "}
{"citeStart": 86, "citeEnd": 110, "citeStartToken": 86, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "One approach to convert existing AL algorithms into cost-conscious algorithms is to normalize the results of the original algorithm by the estimated cost. It should be somewhat obvious that many selection algorithms are inherently length-biased for sequence labeling tasks. For instance, since QBU is the sum of entropy over all words, longer sentences will tend to have higher uncertainty. The easiest solution is to normalize by sentence length, as has been done previously (Engelson and Dagan, 1996; Tomanek et al., 2007) . This of course assumes that annotators are paid by the word, which may or may not be true. Nevertheless, this approach can be justified by the hourly cost model. Replacing the number of words needing correction, c, with the product of l (the sentence length) and the accuracy p of the model, equation 1 can be re-written as the estimate:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For instance, since QBU is the sum of entropy over all words, longer sentences will tend to have higher uncertainty. ", "mid_sen": "The easiest solution is to normalize by sentence length, as has been done previously (Engelson and Dagan, 1996; Tomanek et al., 2007) . ", "after_sen": "This of course assumes that annotators are paid by the word, which may or may not be true. "}
{"citeStart": 64, "citeEnd": 86, "citeStartToken": 64, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "Similar results for German intransitive scalar motion verbs are shown in Fig. 9 . The data for these experiments were extracted from the maximal-probability parses of a 4.1 million word :1.1.1111\"11 : The lexicalized probabilistic grammar for German used is described in Beil et al. (1999) . We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of \"einfache Anderungsverben\" (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there at all. Fig. i0 shows the most probable pair of classes for increase as a transitive verb, together with estimated frequencies for the head filler pair. Note that the object label 17 is the class found with intransitive scalar motion verbs; this correspondence is exploited in the next section. one-place predicate in the structure on the left in Fig. 11 . Linguistically, such representations are motivated by argument alternations (diathesis), case linking and deep word order, language acquistion, scope ambiguity, by the desire to represent aspects of lexical meaning, and by the fact that in some languages, the postulated decomposed representations are overt, with each primitive predicate corresponding to a morpheme. For references and recent discussion of this kind of theory see Hale and Keyser (1993) and Kural (1996) . We will sketch an understanding of the lexical representations induced by latent-class labeling in terms of the linguistic theories mentioned above, aiming at an interpretation which combines computational leaxnability, linguistic motivation, and denotational-semantic adequacy. The basic idea is that latent classes are computational models of the atomic relation symbols occurring in lexical-semantic representations. As a first implementation, consider replacing the relation symbols in the first tree in Fig. 11 with relation symbols derived from the latent class labeling. In the second tree in Fig 11, R17 and R8 are relation symbols with indices derived from the labeling procedure of Sect. 4. Such representations can be semantically interpreted in standard ways, for instance by interpreting relation symbols as denoting relations between events and individuals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "one-place predicate in the structure on the left in Fig. 11 . Linguistically, such representations are motivated by argument alternations (diathesis), case linking and deep word order, language acquistion, scope ambiguity, by the desire to represent aspects of lexical meaning, and by the fact that in some languages, the postulated decomposed representations are overt, with each primitive predicate corresponding to a morpheme. ", "mid_sen": "For references and recent discussion of this kind of theory see Hale and Keyser (1993) and Kural (1996) . ", "after_sen": "We will sketch an understanding of the lexical representations induced by latent-class labeling in terms of the linguistic theories mentioned above, aiming at an interpretation which combines computational leaxnability, linguistic motivation, and denotational-semantic adequacy. "}
{"citeStart": 200, "citeEnd": 220, "citeStartToken": 200, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we explore various ways to integrate new perceptual information through novel computational modeling of this grounded knowledge into a multimodal distributional model of word meaning. The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. This model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity (Andrews et al., 2009; Silberer and Lapata, 2012) . While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. We also show how simple, unsupervised clusters of images can act as a semantically useful and qualitatively interesting set of features. Finally, we describe two ways to extend the model by incorporating three or more modalities. We find that each modality provides useful but disjoint information for describing word meaning, and that a hybrid integration of multiple modalities provides significant improvements in the representations of word meaning. We release both our code and data to the community for future research. 1", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The model we rely on was originally developed by Andrews et al. (2009) and is based on a generalization of Latent Dirichlet Allocation. ", "mid_sen": "This model has previously been shown to provide excellent performance on multiple tasks, including prediction of association norms, word substitution errors, semantic inferences, and word similarity (Andrews et al., 2009; Silberer and Lapata, 2012) . ", "after_sen": "While prior work has used the model only with feature norms and visual attributes, we show that low-level image features are directly compatible with the model and provide improved representations of word meaning. "}
{"citeStart": 114, "citeEnd": 129, "citeStartToken": 114, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "However, it turned out that off-the-shelf SVM classifiers are too inefficient for NE recognition. The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001) ) can process several kilobytes in a second. The major reason is the inefficiency of SVM classifiers. There are other reports on the slowness of SVM classifiers. Another SVM-based NE recognizer (Yamada and Mat-sumoto, 2001 ) is 0.8 sentences/sec on a Pentium III 933 MHz PC. An SVM-based part-of-speech (POS) tagger (Nakagawa et al., 2001 ) is 20 tokens/sec on an Alpha 21164A 500 MHz processor. It is difficult to use such slow systems in practical applications.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, it turned out that off-the-shelf SVM classifiers are too inefficient for NE recognition. ", "mid_sen": "The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001) ) can process several kilobytes in a second. ", "after_sen": "The major reason is the inefficiency of SVM classifiers. "}
{"citeStart": 174, "citeEnd": 192, "citeStartToken": 174, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "Supervised methods usually rely on the information from previously sense tagged corpora to determine the senses of words in unseen texts. Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994) , or sense-tagged seed examples (Yarowsky, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in the learning procedure with the need of predefined sense inventories for target words. ", "mid_sen": "The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994) , or sense-tagged seed examples (Yarowsky, 1995) .", "after_sen": "Some observations can be made on the previous supervised and semi-supervised methods. "}
{"citeStart": 136, "citeEnd": 164, "citeStartToken": 136, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "As mentioned in Section 3.1, the construction of an MCST-SVM classifier requires the computation of a similarity measure between classes. The MCST-SVM method may use any measure of inter-class similarity during the tree construction stage, and many such methods exist (e.g., linear discriminant analysis to order a tree of classifiers (Li et al., 2007) ). We elected to use class prototypes to calculate similarity since they have achieved good performance in previous MCST-SVM applications (Lorena and de Carvalho, 2005; Bickerstaffe et al., 2007) , and are fast to compute over many documents with a large feature space.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The MCST-SVM method may use any measure of inter-class similarity during the tree construction stage, and many such methods exist (e.g., linear discriminant analysis to order a tree of classifiers (Li et al., 2007) ). ", "mid_sen": "We elected to use class prototypes to calculate similarity since they have achieved good performance in previous MCST-SVM applications (Lorena and de Carvalho, 2005; Bickerstaffe et al., 2007) , and are fast to compute over many documents with a large feature space.", "after_sen": "Algorithm 1 Constructing the MCST-SVM 1: Let V be a set of graph vertices, where each vertex v i ∈ V represents rating class i and its available training samples. "}
{"citeStart": 167, "citeEnd": 169, "citeStartToken": 167, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithm. Some of the published algorithms produce only a chart as described by Kay in [14] , which only associates nonterminal categories to segments of the analyzed sentence [11, 39, 13, 3, 9] , and which thus still requires non-trivial proceasing to extract parse-trees [26] . The worst size complexity of such a chart is only a square function of the size of the input 2.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithm. ", "mid_sen": "Some of the published algorithms produce only a chart as described by Kay in [14] , which only associates nonterminal categories to segments of the analyzed sentence [11, 39, 13, 3, 9] , and which thus still requires non-trivial proceasing to extract parse-trees [26] . ", "after_sen": "The worst size complexity of such a chart is only a square function of the size of the input 2."}
{"citeStart": 189, "citeEnd": 207, "citeStartToken": 189, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "For word sentiment classification we developed two models. The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two lists-positive and negative-and then to grow this by adding words obtained from WordNet (Miller et al. 1993; Fellbaum et al. 1993) . We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word \"good\" has synonyms \"virtuous, honorable, righteous\" and antonyms \"evil, disreputable, unrighteous\".", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For word sentiment classification we developed two models. ", "mid_sen": "The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two lists-positive and negative-and then to grow this by adding words obtained from WordNet (Miller et al. 1993; Fellbaum et al. 1993) . ", "after_sen": "We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word \"good\" has synonyms \"virtuous, honorable, righteous\" and antonyms \"evil, disreputable, unrighteous\"."}
{"citeStart": 145, "citeEnd": 158, "citeStartToken": 145, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990 Alshawi ( , 1992 , and implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. Just as here, these QLFs represent the basic predicate argument structure of the sentence, and contain constructs which represent those aspects of the meaning of the sentence that are dependent on context.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The starting point for the approach followed here was a dissatisfaction with certain aspects of the theory of quasi-logical form as described in Alshawi (1990 Alshawi ( , 1992 , and implemented in SRI's Core Language Engine (CLE). ", "after_sen": "In the CLE-QLF approach, as ra-tionally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994) , the context-independent meaning of a sentence is given by one or more QLFs that are built directly from syntactic and semantic rules. "}
{"citeStart": 101, "citeEnd": 117, "citeStartToken": 101, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "A similar solution to the nontermination problem with unification grammars in Prolog is proposed in (Samuelsson, 1993) . In this method, an operation called anti-unification (often referred to as generalization as the counterpart of unification) is applied to the root and leaf terms of a cyclic propagation, and the resulting term is stored in the reachablity table as the result of applying restriction on both terms. Another approach taken in (Haas, 1989) eliminates the cyclic propagation by replacing the features in the root and leaf terms with new variables. The method proposed in this paper is more general than the above approaches: if the Selection ordering is imposed in the detection function, features in .restrictor. can be collected incrementally as the cyclic propagations are repeated. Thus, this method 7This scheme may be rather conservative. 8Note the cases in this section do not represent all possible situations. is able to create a less restrictive *restrictor. than these other approaches.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A similar solution to the nontermination problem with unification grammars in Prolog is proposed in (Samuelsson, 1993) . ", "after_sen": "In this method, an operation called anti-unification (often referred to as generalization as the counterpart of unification) is applied to the root and leaf terms of a cyclic propagation, and the resulting term is stored in the reachablity table as the result of applying restriction on both terms. "}
{"citeStart": 155, "citeEnd": 168, "citeStartToken": 155, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given this interest in the valency concept, and the fact that word order is one of the main difference between phrase-structure based approaches (henceforth PSG) and dependency grammar (DG), it is valid to ask whether DG can capture word order phenomena without recourse to phrasal nodes, traces, slashed categories, etc. ", "mid_sen": "A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . ", "after_sen": "This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997) ."}
{"citeStart": 151, "citeEnd": 178, "citeStartToken": 151, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "Language Production Model ~).r bmguage model can be viewed in terms of a probabihstic generative process based on the choice of lexical \"heads\" of phrases and the recursive generation of sub-;,bra~es and their ordering. For this purpose, we can de-(ira, tho head word of a phrase to be the word that most strongly influences the way the phrase may be combiucd with other phrases. This notion has been central to a number of approaches to grammar for some time, including theories like dependency grammar (Hudson I!~7 (;, 1990) and HPSG (Pollard and Sag 1987) . More ;,'~,.t,l. ly, the statistical properties of associations be-Iw,.,'n words, and more particularly heads of phrases, JL:t.~ J~,~'~,l|lql, all a.el.iw; area of research (e.g. Chang, l,uo, aml Su 1992; Ilindlc and R.ooth 1993) . 'l'h,' language model factors the statistical derivation ,,f a .~'ul.ence with word string W as follows: I'(ll) = ~,: P(C) P (WIC) where C ranges over relation graphs. The content model, P(C), and generation model, P(WIC), are components of the overall statistical model for spoken language translation given earlier. This decomposition of P(W) can be viewed as first deciding on the content of a sentence, formulated as a set of relation edges according to a statistical model for P(C), and then deciding on word order according to P(WIC ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For this purpose, we can de-(ira, tho head word of a phrase to be the word that most strongly influences the way the phrase may be combiucd with other phrases. ", "mid_sen": "This notion has been central to a number of approaches to grammar for some time, including theories like dependency grammar (Hudson I!~7 (;, 1990) and HPSG (Pollard and Sag 1987) . ", "after_sen": "More ;,'~,.t,l. ly, the statistical properties of associations be-Iw,.,'n words, and more particularly heads of phrases, JL:t.~ J~,~'~,l|lql, all a.el.iw; area of research (e.g. Chang, l,uo, aml Su 1992; Ilindlc and R.ooth 1993) . 'l'h,' language model factors the statistical derivation ,,f a .~'ul."}
{"citeStart": 69, "citeEnd": 87, "citeStartToken": 69, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "As a baseline classifier we use the substring matching technique of (Light et al., 2004) , which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As a baseline classifier we use the substring matching technique of (Light et al., 2004) , which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose.", "after_sen": "To provide a comparison for our learning model, we implement a more traditional self-training procedure in which at each iteration a committee of five SVMs is trained on randomly generated overlapping subsets of the training data and their cumulative confidence is used to select items for augmenting the labelled training data. "}
{"citeStart": 12, "citeEnd": 28, "citeStartToken": 12, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "Similar to (Li et al., 2013a ), our summarization system is , which consists of three key components: an initial sentence pre-selection module to select some important sentence candidates; the above compression model to generate n-best compressions for each sentence; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Similar to (Li et al., 2013a ), our summarization system is , which consists of three key components: an initial sentence pre-selection module to select some important sentence candidates; the above compression model to generate n-best compressions for each sentence; and then an ILP summarization method to select the best summary sentences from the multiple compressed sentences.", "after_sen": "The sentence pre-selection model is a simple supervised support vector regression (SVR) model that predicts a salience score for each sentence and selects the top ranked sentences for further processing (compression and summarization). "}
{"citeStart": 148, "citeEnd": 171, "citeStartToken": 148, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "An understanding system can recognize the aspectual transformations that have affected a clause only after establishing the clause's fundamental aspectual category. Linguistic models motivate the division between a module that first detects fundamental aspect and a second that detects aspectual transformations (Hwang and Schubert 1991; Schubert and Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman 1988; Hitzeman, Moens, and Grover 1994) . In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An understanding system can recognize the aspectual transformations that have affected a clause only after establishing the clause's fundamental aspectual category. ", "mid_sen": "Linguistic models motivate the division between a module that first detects fundamental aspect and a second that detects aspectual transformations (Hwang and Schubert 1991; Schubert and Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman 1988; Hitzeman, Moens, and Grover 1994) . ", "after_sen": "In principle, it is possible for this second module to detect aspectual transformations that apply to any input clause, independent of the manner in which the core constituents interact to produce its fundamental aspectual class."}
{"citeStart": 22, "citeEnd": 38, "citeStartToken": 22, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "On the 5 datasets in (Hu and Liu, 2004) , OPINE's precision is 22% higher than Hu's at the cost of a 3% recall drop. There are two important differences between OPINE and Hu's system: a) OPINE's Feature Assessor uses PMI assessment to evaluate each candidate feature and b) OPINE incorporates Web PMI statistics in addition to review data in its assessment. In the following, we quantify the performance gains from a) and b). a) In order to quantify the benefits of OPINE's Feature Assessor, we use it to evaluate the features extracted by Hu's algorithm on review data (Hu+A/R). The Feature Assessor improves Hu's precision by 6%. b) In order to evaluate the impact of using Web PMI statistics, we assess OPINE's features first on reviews (OP/R) and then on reviews in conjunction with the Web (the corresponding methods are Hu+A/R+W and OPINE). Web PMI statistics increase precision by an average of 14.5%.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system only extracts explicit features.", "mid_sen": "On the 5 datasets in (Hu and Liu, 2004) , OPINE's precision is 22% higher than Hu's at the cost of a 3% recall drop. ", "after_sen": "There are two important differences between OPINE and Hu's system: a) OPINE's Feature Assessor uses PMI assessment to evaluate each candidate feature and b) OPINE incorporates Web PMI statistics in addition to review data in its assessment. "}
{"citeStart": 88, "citeEnd": 111, "citeStartToken": 88, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "We have used the memory-based learning algorithm IBI-IG which is part of TiMBL package (Daelemans et al., 1999b) . In memory-based learning the training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. In IBI-IG each feature receives a weight which is based on the amount of information which it provides for computing the classification of the items in the training data. These feature weights are used for computing the distance between a pair of data items (Daelemans et al., 1999b) . ml-IG has been used successfully on a large variety of natural language processing tasks.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We have used the memory-based learning algorithm IBI-IG which is part of TiMBL package (Daelemans et al., 1999b) . ", "after_sen": "In memory-based learning the training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item. "}
{"citeStart": 55, "citeEnd": 91, "citeStartToken": 55, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "The sentence pyramid precision is a relative precision based on the GSValue. Since this idea is borrowed from the pyramid metric by Nenkova et al.(Nenkova et al., 2007) , we call it the sentence pyramid precision. In this paper, we simplify it as the pyramid precision. As we have discussed above, with the reviewers' selections, we get a GSValue for each sentence, which ranges from 0 to 15. With this GSValue, we rank all sentences in a descendant order. We also group all sentences with the same GSValue together as one tier T i , where i is the corre-sponding GSValue; i is called the level of the tier T i . In this way, we organize all sentences into a pyramid: a sequence of tiers with a descendant order of levels. With the pyramid of sentences, the accuracy of a summary is evaluated over the best summary we can achieve under the same summary length. The best summary of k sentences are the top k sentences in terms of GSValue.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The sentence pyramid precision is a relative precision based on the GSValue. ", "mid_sen": "Since this idea is borrowed from the pyramid metric by Nenkova et al.(Nenkova et al., 2007) , we call it the sentence pyramid precision. ", "after_sen": "In this paper, we simplify it as the pyramid precision. "}
{"citeStart": 4, "citeEnd": 16, "citeStartToken": 4, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "Defeasibili~y is a notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it. The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world. For Frege (1892) , Russell (1905) , and Quine (1949) \"everything exists\"; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994) . We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first observers of the phenomenon preferred to keep defeasibility outside the mathematical world. ", "mid_sen": "For Frege (1892) , Russell (1905) , and Quine (1949) \"everything exists\"; therefore, in their logical systems, it is impossible to formalize the cancellation of the presupposition that definite referents exist (Hirst, 1991; Marcu and Hirst, 1994) . ", "after_sen": "We can taxonomize previous approaches to defea~ible pragmatic inferences into three categories (we omit here work on defeasibility related to linguistic phenomena such as discourse, anaphora, or speech acts)."}
{"citeStart": 162, "citeEnd": 178, "citeStartToken": 162, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "A more elaborate definition of dependency structures and £~ defines two more dimensions, a feature graph mapped off the dependency tree much like the proposal of Blackburn (1994) , and a conceptual representation based on terminological logic, linking content words with reference objects and dependencies with conceptual roles.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For the precedence predicates <. and <~, there are inverses >. and >~. For presentation, the relation places C 142 x 142 has been introduced, which holds between two words iff the first argument is the positional head of the second argument.", "mid_sen": "A more elaborate definition of dependency structures and £~ defines two more dimensions, a feature graph mapped off the dependency tree much like the proposal of Blackburn (1994) , and a conceptual representation based on terminological logic, linking content words with reference objects and dependencies with conceptual roles.", "after_sen": ""}
{"citeStart": 137, "citeEnd": 158, "citeStartToken": 137, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "Before describing the algorithm, it will be helpful to have at our disposal a few general tools, most of which were described already in Kaplan and Kay (1994) . These tools, however, have been modified so that they work with our approach of distinguishing markers from ordinary symbols. So to begin with, we provide macros to describe the alphabet and the alphabet extended with marker symbols:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Before describing the algorithm, it will be helpful to have at our disposal a few general tools, most of which were described already in Kaplan and Kay (1994) . ", "after_sen": "These tools, however, have been modified so that they work with our approach of distinguishing markers from ordinary symbols. "}
{"citeStart": 248, "citeEnd": 265, "citeStartToken": 248, "citeEndToken": 265, "sectionName": "UNKNOWN SECTION NAME", "string": "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001) ). While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. * This work was partly supported by UK EPSRC project GR/N36462/93: 'Robust Accurate Statistical Parsing (RASP)'.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001) ). ", "after_sen": "While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. * This work was partly supported by UK EPSRC project GR/N36462/93: 'Robust Accurate Statistical Parsing (RASP)'."}
{"citeStart": 76, "citeEnd": 101, "citeStartToken": 76, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "The dialogue manager must keep track of the current state of the dialogue, determine the effects of observed conversation acts, generate utterances back, and send commands to the domain plan reasoner and domain plan executor when appropriate. Conversational action is represented using the theory of Conversation Acts [Traum and Hinkelman, 1992] which augments traditional Core Speech Acts with levels of acts for turn-taking, grounding [Clark and Schaefer, 1989] , and argumentation. Each utterance will generally contain acts (or partial acts) at each of these levels.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The dialogue manager must keep track of the current state of the dialogue, determine the effects of observed conversation acts, generate utterances back, and send commands to the domain plan reasoner and domain plan executor when appropriate. ", "mid_sen": "Conversational action is represented using the theory of Conversation Acts [Traum and Hinkelman, 1992] which augments traditional Core Speech Acts with levels of acts for turn-taking, grounding [Clark and Schaefer, 1989] , and argumentation. ", "after_sen": "Each utterance will generally contain acts (or partial acts) at each of these levels."}
{"citeStart": 0, "citeEnd": 25, "citeStartToken": 0, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "Given a stem such as brother, Toutanova et. al's system might generate the \"stem and inflection\" corresponding to and his brother. Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required. The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008) , which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Both efforts were ineffective on large data sets. ", "mid_sen": "Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. ", "after_sen": "Our CRF framework allows us to use more complex context features."}
{"citeStart": 101, "citeEnd": 126, "citeStartToken": 101, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "We have presented an algorithm for the extraction of semantic forms (or subcategorization frames) from the Penn-II and Penn-III Treebanks, automatically annotated with LFG f-structures. In contrast to many other approaches, ours does not predefine the subcategorization frames we extract. We have applied the algorithm to the WSJ sections of Penn-II (50,000 trees) (O'Donovan et al. 2004) and to the parse-annotated Brown corpus of Penn-III (almost 25,000 additional trees). We extract syntactic-function-based subcategorization frames (LFG semantic forms) and traditional CFG category-based frames, as well as mixed-function-category-based frames. Unlike many other approaches to subcategorization frame extraction, our system properly reflects the effects of long-distance dependencies. Also unlike many approaches, our method distinguishes between active and passive frames. Finally, our system associates conditional probabilities with the frames we extract. Making the distinction between the behavior of verbs in active and passive contexts is particularly important for the accurate assignment of probabilities to semantic forms. We carried out an extensive evaluation of the complete induced lexicon against the full COMLEX resource. To our knowledge, this is the most extensive qualitative evaluation of subcategorization extraction in English. The only evaluation of a similar scale is that carried out by Schulte im Walde (2002b) for German. The results reported here for Penn-II compare favorably against the baseline and, in fact, are an improvement on those reported in O'Donovan et al. (2004) . The results for the larger, more domain-diverse Penn-III lexicon are very encouraging, in some cases almost 15% above the baseline. We believe our semantic forms are fine-grained, and by choosing to evaluate against COMLEX, we set our sights high: COMLEX is considerably more detailed than the OALD or LDOCE used for other earlier evaluations. Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. In the future, we hope to evaluate the automatic annotations and extracted lexicon against Propbank (Kingsbury and Palmer 2002) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our error analysis also revealed some interesting issues associated with using an external standard such as COMLEX. ", "mid_sen": "In the future, we hope to evaluate the automatic annotations and extracted lexicon against Propbank (Kingsbury and Palmer 2002) .", "after_sen": "Apart from the related approach of Miyao, Ninomiya, and Tsujii (2004) , which does not distinguish between argument and adjunct prepositional phrases, our treebank and automatic f-structure annotation-based architecture for the automatic acquisition of detailed subcategorization frames is quite unlike any of the architectures presented in the literature. "}
{"citeStart": 29, "citeEnd": 50, "citeStartToken": 29, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "where f~(Yi) is the set of all syntactic structures in f~ with yield yi (i.e., all parses of Yi generated by the base UBG). It turns out that calculating the pseudo-likelihood of a corpus only involves integrations over the sets of parses of its yields f~(Yi), which is feasible for many interesting UBGs. Moreover, the maximum pseudolikelihood estimator is asymptotically consistent for the conditional distribution P(w]y). For the reasons explained in Johnson et al. (1999) we actually estimate )~ by maximizing a regularized version of the log pseudo-likelihood (5), where aj is 7 times the maximum value of fj found in the training corpus:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, the maximum pseudolikelihood estimator is asymptotically consistent for the conditional distribution P(w]y). ", "mid_sen": "For the reasons explained in Johnson et al. (1999) we actually estimate )~ by maximizing a regularized version of the log pseudo-likelihood (5), where aj is 7 times the maximum value of fj found in the training corpus:", "after_sen": "m ~2 logPL~(~) -~ 2\"~2 (5) j=l vj"}
{"citeStart": 76, "citeEnd": 94, "citeStartToken": 76, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists, but more like composition with respect to the wh-list. This is very similar to the way in which wh-movement is dealt with in GPSG (Gazdar et al. 1985) and HPSG, where wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is then possible to specify operations which act as purely applicative operations with respect to the left and right arguments lists, but more like composition with respect to the wh-list. ", "mid_sen": "This is very similar to the way in which wh-movement is dealt with in GPSG (Gazdar et al. 1985) and HPSG, where wh-arguments are treated using slash mechanisms or feature inheritance principles which correspond closely to function composition.", "after_sen": "Given that our arguments have produced a categorial grammar which looks very similar to HPSG, why not use HPSG rather than Applicative CG? "}
{"citeStart": 21, "citeEnd": 35, "citeStartToken": 21, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Meaning-Text Theory (Melc'fik, 1988) assumes seven strata of representation. The rules mapping fi'om the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Melc'~tk & Pertsov, 1987p.1870 . Word Grammar (WG, Hudson (1990 ) is based on general graphs instead of trees. The ordering of two linked words is specified together with their dependency relation, as in the proposition \"object of verb follows it\". Extraction of, e.g., objects is analyzed by establishing an additional dependency called visitor between the verb and the extractee, which requires the reverse order, as in \"visitor of verb precedes it\". This results in inconsistencies, since an extracted object must follow the verb (being its object) and at the same time precede it (being its visitor). The approach compromises the semantic motivation of dependencies by adding purely order-induced dependencies. WG is similar to our proposal in that it also distinguishes a propositional meta language describing the graph-based analysis structures.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure~ which stratifies the theory and makes incremental processing difficult.", "mid_sen": "Meaning-Text Theory (Melc'fik, 1988) assumes seven strata of representation. ", "after_sen": "The rules mapping fi'om the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. "}
{"citeStart": 109, "citeEnd": 128, "citeStartToken": 109, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields. ", "mid_sen": "In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992) .", "after_sen": "Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly ) are as useful (if not more useful) as the full probabilities P0(w), at least in those cases for which the ultimate goal is syntactic analysis. "}
{"citeStart": 111, "citeEnd": 126, "citeStartToken": 111, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "Many of the loanwords exist in today's Chinese through semantic transliteration, which has been well received (Hu and Xu, 2003; Hu, 2004) by the people because of many advantages. Here we just name a few. (1) It brings in not only the sound, but also the meaning that fills in the semantic blank left by phonetic transliteration. This also reminds people that it is a loanword and avoids misleading;", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Xu Guangqi 2 translated geo-in geometry into Chinese as 几何 /Ji-He/, which carries the pronunciation of geo-and expresses the meaning of \"a science concerned with measuring the earth\".", "mid_sen": "Many of the loanwords exist in today's Chinese through semantic transliteration, which has been well received (Hu and Xu, 2003; Hu, 2004) by the people because of many advantages. ", "after_sen": "Here we just name a few. "}
{"citeStart": 114, "citeEnd": 132, "citeStartToken": 114, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Considerable efforts have been made in the NLP community in the study of Chinese word segmentation. The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003) . Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004) ; (Asahara et al., 2005) ; (Zhang and Clark, 2007) ; (Zhao et al., 2010) ). However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. Thus, the scale of available manually labeled data has placed considerable limitations on the further enhancement of supervised CWS methods.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The most popular supervised approach treats word segmentation as a sequence labeling problem, as first proposed by (Xue et al., 2003) . ", "mid_sen": "Most previous systems have addressed this task using linear statistical models with carefully designed features ((Peng et al., 2004) ; (Asahara et al., 2005) ; (Zhang and Clark, 2007) ; (Zhao et al., 2010) ). ", "after_sen": "However, the primary shortcoming of these approaches is that they rely heavily on a large amount of labeled data, which is very timeconsuming and expensive to produce. "}
{"citeStart": 72, "citeEnd": 95, "citeStartToken": 72, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tuples while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computer vision (e.g., model-based matching). OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983) , in order to solve the three subtasks described above.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tuples while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computer vision (e.g., model-based matching). ", "mid_sen": "OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983) , in order to solve the three subtasks described above.", "after_sen": ""}
{"citeStart": 51, "citeEnd": 66, "citeStartToken": 51, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "However, again in retrospect, it is in the work of Magerman (1995) that we see the greatest similarity. Rather than talking about clustering nodes, as we do, Magerman creates a decision tree, but the differences between clustering and decision trees are small. Perhaps a more substantial difference is that by not casting his problem as one of learning phrasal categories Magerman loses all of the free PCFG technology that we can leverage. For instance, Magerman must use heuristic search to find his parses and incurs search errors because of it. We use an efficient CKY algorithm to do exhaustive search in reasonable time. Belz (2002) considers the problem in a manner more similar to our approach. Beginning with both a non-annotated grammar and a parent annotated grammar, using a beam search they search the space of grammars which can be attained via merging nonterminals. They guide the search using the performance on parsing (and several other tasks) of the grammar at each stage in the search. In contrast, our approach explores the space of grammars by starting with few nonterminals and splitting them. We also consider a much wider range of contextual information than just parent phrase-markers.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We would like to infer the number of annotations for each nonterminal automatically.", "mid_sen": "However, again in retrospect, it is in the work of Magerman (1995) that we see the greatest similarity. ", "after_sen": "Rather than talking about clustering nodes, as we do, Magerman creates a decision tree, but the differences between clustering and decision trees are small. "}
{"citeStart": 197, "citeEnd": 209, "citeStartToken": 197, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "Pronouns: Of the four pronoun resolution errors, one is due to a parse error (American in 7-1 was incorrectly parsed as a person entity, to which she in 8-1 was resolved), that in 6-2 is a discourse deixis (Webber, 1988) , beyond the scope of the current approach, and two errors (it in 3-1 and its in 7-1) were due to the left-right ordering of intrasentential candidates. Recognition of par-51 allelism among clause conjuncts and a stricter locality preference for possessive pronouns may help here.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Pronouns: ", "mid_sen": "Of the four pronoun resolution errors, one is due to a parse error (American in 7-1 was incorrectly parsed as a person entity, to which she in 8-1 was resolved), that in 6-2 is a discourse deixis (Webber, 1988) , beyond the scope of the current approach, and two errors (it in 3-1 and its in 7-1) were due to the left-right ordering of intrasentential candidates. ", "after_sen": "Recognition of par-51 allelism among clause conjuncts and a stricter locality preference for possessive pronouns may help here."}
{"citeStart": 154, "citeEnd": 180, "citeStartToken": 154, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "Modeling the sentiment of a word has been a popular approach in sentiment analysis. There are many publicly available lexicon resources. The size, format, specificity, and reliability differ in all these lexicons. For example, lexicon sizes range from a few hundred to several hundred thousand. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) . There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, lexicon sizes range from a few hundred to several hundred thousand. ", "mid_sen": "Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) . ", "after_sen": "There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006) ."}
{"citeStart": 56, "citeEnd": 68, "citeStartToken": 56, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "2.2 The algorithm Our algorithm, described in detail by Marcu (1994) , takes as input a set of first-order stratified formulas • that represents an adequate knowledge base that expresses semantic knowledge and the necessary conditions for triggering pragmatic inferences, and the translation of an utterance or set of utterances uttered(u). The Mgorithm builds the set of all possible interpretations for a given utterance, using a generalization of the semantic tableau technique. The model-ordering relation filters the optimistic interpretations. Among them, the defeasible inferences that have been triggered on pragmatic grounds are checked to see whether or not they are cancelled in any optimistic interpretation. Those that are not cancelled are labelled as pragmatic inferences for the given utterance or set of utterances.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In both cases, W uttered(u) is u-consistent.", "mid_sen": "2.2 The algorithm Our algorithm, described in detail by Marcu (1994) , takes as input a set of first-order stratified formulas • that represents an adequate knowledge base that expresses semantic knowledge and the necessary conditions for triggering pragmatic inferences, and the translation of an utterance or set of utterances uttered(u). ", "after_sen": "The Mgorithm builds the set of all possible interpretations for a given utterance, using a generalization of the semantic tableau technique. "}
{"citeStart": 201, "citeEnd": 212, "citeStartToken": 201, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "When there is \"Noun X\" in another case component of the verb which has the analyzed case component (the analyzed zero pronoun), {(Noun X, -20)} Rule using empathy This rule is based on empathy theory (Kameyama 86) . When an anaphor is a ga-case zero pronoun whose verb is followed by an auxiliary verb such as \"kureru\" or \"kudasaru,\" the n/-case zero pronoun is analyzed first, and doru souba-wa kitai-kara (210/26S) The points given in each nile are manually adjusted by using the training sentences. Training sentences (example sentences (43 sentences), a folk tale \"kobutori jiisan\" (Nakao 85 ) (93 sentences), an essay in \"tenseijingo\" (26 sentences), an editorial (26 sentences), an article in \"Scientific American (in Japanese)\"(16 sentences)} Test sentences {a folk tale \"tsuru no ongaeshi\" (Nakao 85) (91 sentences), two essays in \"tenseijingo\" (50 sentences), an editorial (30 sentences), articles in \"Scientific American (in Japanese)\" (13 sentences)} it is filled with the noun phrase that has high empathy such as the topic, and a ga-case zero pronoun is filled with another noun phrase.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "When there is \"Noun X\" in another case component of the verb which has the analyzed case component (the analyzed zero pronoun), {(Noun X, -20)} Rule using empathy This rule is based on empathy theory (Kameyama 86) . ", "after_sen": "When an anaphor is a ga-case zero pronoun whose verb is followed by an auxiliary verb such as \"kureru\" or \"kudasaru,\" the n/-case zero pronoun is analyzed first, and doru souba-wa kitai-kara (210/26S) The points given in each nile are manually adjusted by using the training sentences. "}
{"citeStart": 211, "citeEnd": 242, "citeStartToken": 211, "citeEndToken": 242, "sectionName": "UNKNOWN SECTION NAME", "string": "To make sure the annotations are comparable, we mark the true citation sentiment to be the last sentiment mentioned in a 4-sentence context window, as this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984) . The window length is motivated by recent research (Qazvinian and Radev, 2010) which favours a foursentence boundary for detecting non-explicit citations. Analysis of our data shows that more than 60% of the subjective citations lie in this window. We include the implicit citations predicted by the method described in the previous section in the context. The results of the single-sentence baseline system are compared with this context enhanced system in Table 3 Table 3 : F -scores for citation sentiment detection.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It would also make it impossible to compare the sentiment annotated in the previous work with our annotation.", "mid_sen": "To make sure the annotations are comparable, we mark the true citation sentiment to be the last sentiment mentioned in a 4-sentence context window, as this is pragmatically most likely to be the real intention (MacRoberts and MacRoberts, 1984) . ", "after_sen": "The window length is motivated by recent research (Qazvinian and Radev, 2010) which favours a foursentence boundary for detecting non-explicit citations. "}
{"citeStart": 8, "citeEnd": 25, "citeStartToken": 8, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper uses the ACE RDC 2003 corpus provided by LDC to train and evaluate the hierarchical learning strategy. Same as Zhou et al (2005) , we only model explicit relations and explicitly model the argument order of the two mentions involved. Table 1 : Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus (Note: According to frequency, all the subtypes are divided into three bins: large/ middle/ small, with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin). The training data consists of 674 documents (~300k words) with 9683 relation examples while the held-out testing data consists of 97 documents (~50k words) with 1386 relation examples. All the experiments are done five times on the 24 relation subtypes in the ACE corpus, except otherwise specified, with the final performance averaged using the same re-sampling with replacement strategy as the one in the bagging technique. Table 1 lists various types and subtypes of relations for the ACE RDC 2003 corpus, along with their occurrence frequency in the training data. It shows that this corpus suffers from a small amount of annotated data for a few subtypes such as the subtype \"Founder\" under the type \"ROLE\".", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper uses the ACE RDC 2003 corpus provided by LDC to train and evaluate the hierarchical learning strategy. ", "mid_sen": "Same as Zhou et al (2005) , we only model explicit relations and explicitly model the argument order of the two mentions involved. ", "after_sen": "Table 1 : Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus (Note: According to frequency, all the subtypes are divided into three bins: large/ middle/ small, with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin). "}
{"citeStart": 26, "citeEnd": 51, "citeStartToken": 26, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "We define an annotation task that requires explicit definitions that distinguish between phrases that represent the same or different information units. Unfortunately, there is little consensus in the literature on such definitions. Therefore, we follow (van Halteren and Teufel, 2003) and make the following distinction. We define a nugget to be a phrasal information unit. Different nuggets may all represent the same atomic semantic unit, which we call as a factoid. In the following headlines, which are randomly extracted from the redsox dataset, nuggets are manually underlined.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, there is little consensus in the literature on such definitions. ", "mid_sen": "Therefore, we follow (van Halteren and Teufel, 2003) and make the following distinction. ", "after_sen": "We define a nugget to be a phrasal information unit. "}
{"citeStart": 319, "citeEnd": 343, "citeStartToken": 319, "citeEndToken": 343, "sectionName": "UNKNOWN SECTION NAME", "string": "Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007) , or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006) . These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006) , who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Past work on discriminative SMT only address some of these problems. ", "mid_sen": "To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007) , or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006) . ", "after_sen": "These systems all include regularisation, thereby addressing Problem 2. "}
{"citeStart": 123, "citeEnd": 149, "citeStartToken": 123, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words. The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004) . Both identify product features from reviews, but OPINE significantly improves on both. (Hu and Liu, 2004) doesn't assess candidate features, so its precision is lower than OPINE's. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE's focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. ", "mid_sen": "OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . ", "after_sen": "Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . "}
{"citeStart": 257, "citeEnd": 279, "citeStartToken": 257, "citeEndToken": 279, "sectionName": "UNKNOWN SECTION NAME", "string": "The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006) . However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. Parse tree paths features can be used to achieve high precision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006) . ", "after_sen": "However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. "}
{"citeStart": 162, "citeEnd": 181, "citeStartToken": 162, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar extraction algorithm Systemic Functional Grammar (SFG) (Halliday, 1985) is based on the assumption that the differentiation of syntactic phenomena is always deter-mined by its function in the communicative context. This functional orientation has lead to the creation of detailed linguistic resources that are characterized by an integrated treatment of content-related, textual and pragmatic aspects. Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects--such as, for example, PENMAN (Mann, 1983) , COMMUNAL (Fawcett and Tucker, 1990) , TECHDOC (KSsner and Stede, 1994) , Drafter (Paris and Vander Linden , 1996) , and Gist (Not and Stock, 1994) . For our present purposes, however, it is the formal characteristics of systemic grammar and its implementations that are more important. Systemic grammar assumes multifunctional constituent structuresrepresentable as feature structures with coreferences. As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This functional orientation has lead to the creation of detailed linguistic resources that are characterized by an integrated treatment of content-related, textual and pragmatic aspects. ", "mid_sen": "Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects--such as, for example, PENMAN (Mann, 1983) , COMMUNAL (Fawcett and Tucker, 1990) , TECHDOC (KSsner and Stede, 1994) , Drafter (Paris and Vander Linden , 1996) , and Gist (Not and Stock, 1994) . ", "after_sen": "For our present purposes, however, it is the formal characteristics of systemic grammar and its implementations that are more important. "}
{"citeStart": 11, "citeEnd": 36, "citeStartToken": 11, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "In spite of the permanent interest in German word order in the linguistics community, most studies have limited their scope to the order of verb arguments and few researchers have implemented -and even less evaluated -a generation algorithm. In this paper, we present an algorithm, which orders not only verb arguments but all kinds of constituents, and evaluate it on a corpus of biographies. For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text. We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones. We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. Similar to Langkilde & Knight (1998) we utilize statistical methods. Unlike overgeneration approaches (Varges & Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. ", "mid_sen": "Similar to Langkilde & Knight (1998) we utilize statistical methods. ", "after_sen": "Unlike overgeneration approaches (Varges & Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation."}
{"citeStart": 24, "citeEnd": 42, "citeStartToken": 24, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "Distributional cluster (Brown et al., 1992) : cost, expense, risk, profitability, deferral, earmarks, capstone, cardinality, mintage, reseller 0.3183 test, trial: the act ofundergoingtesting; \"he survived the great test of battle\" 0.3183 test, trial run: the act of testing something Word 'mean' (1 alternatives) 1.0000 mean: an average ofn numbers computed by...", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This cluster was derived by Brown et al. using a modification of their algorithm, designed to uncover \"semantically sticky\" clusters.", "mid_sen": "Distributional cluster (Brown et al., 1992) : cost, expense, risk, profitability, deferral, earmarks, capstone, cardinality, mintage, reseller 0.3183 test, trial: the act ofundergoingtesting; \"he survived the great test of battle\" 0.3183 test, trial run: the act of testing something Word 'mean' (1 alternatives) 1.0000 mean: an average ofn numbers computed by...", "after_sen": "Word 'proeedure' (4 alternatives) 1.0000 procedure, process: a particular course of action intended to achieve a results 1.0000 operation, procedure: a process or series of acts ,.. involved in a particular form of work 0.0000 routine, subroutine, subprogram, procedure, function 0.0000 procedure: a mode of conducting legal and parliamentary proceedings Word 'technique' (2 alternatives) 1.0000 technique: a tecfiniealmethod 0.0000 profieieney, facility, technique: skillfulness deriving from practice and familiarity I chose this grouping at random from a thesaurus created automatically by Grefenstette's syntacticodistributional methods, using the MED corpus of medical abstracts as its source. "}
{"citeStart": 151, "citeEnd": 175, "citeStartToken": 151, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": ". That is, every feature is an intersection of the occurrence of a particular predicate,v, and some feature of the argument f (n). For example, a feature for a verb-object pair might be, \"the verb is eat and the object is lower-case.\" In this representation, features for one predicate will be completely independent from those for every other predicate. Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003) 's conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n). Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. MI was also recently used for inference-rule SPs by Pantel et al. (2007) . classifier for each predicate independently. The prediction becomes", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, a feature for a verb-object pair might be, \"the verb is eat and the object is lower-case.\" In this representation, features for one predicate will be completely independent from those for every other predicate. ", "mid_sen": "Thus rather than a single training procedure, we can actually partition the examples by predicate, and train a For a fixed verb, MI is proportional to Keller and Lapata (2003) 's conditional probability scores for pseudodisambiguation of (v, n, n ′ ) triples: Pr(v|n) = Pr(v, n)/Pr(n), which was shown to be a better measure of association than co-occurrence frequency f (v, n). ", "after_sen": "Normalizing by Pr(v) (yielding MI) allows us to use a constant threshold across all verbs. "}
{"citeStart": 14, "citeEnd": 43, "citeStartToken": 14, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "The research on general paraphrase fragment extraction at the sub-sentential level is mainly based on phrase pair extraction techniques from the MT literature. Munteanu and Marcu (2006) extract subsentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. Quirk et al. (2007) extract fragments using a generative model of noisy translations. Our own work (Wang and Callison-Burch, 2011 ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments. Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Quirk et al. (2007) extract fragments using a generative model of noisy translations. ", "mid_sen": "Our own work (Wang and Callison-Burch, 2011 ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora. ", "after_sen": "Our current approach also uses word-word alignment, however, we use syntactic dependency trees to compute grammatical fragments. "}
{"citeStart": 37, "citeEnd": 64, "citeStartToken": 37, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "Performance would probably be improved by better models of morphology and/or phonology. An ngram model of morpheme sequences (e.g. like Goldwater uses) might avoid some of the mistakes mentioned in Section 8. Feature-based or gestural phonology (Browman and Goldstein, 1992) might help model segmental variation. Finite-state models (Belz, 2000) might be more compact. Prosody, stress, and other sub-phonemic cues might disambiguate some problem situations (Hockema, 2006; Rytting, 2007; Salverda et al., 2003) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An ngram model of morpheme sequences (e.g. like Goldwater uses) might avoid some of the mistakes mentioned in Section 8. ", "mid_sen": "Feature-based or gestural phonology (Browman and Goldstein, 1992) might help model segmental variation. ", "after_sen": "Finite-state models (Belz, 2000) might be more compact. "}
{"citeStart": 164, "citeEnd": 169, "citeStartToken": 164, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "is exchanged by MIXED-INITIATIVE. Each participant will, on occasion, take the conversational lead. Conversational partners not only respond to what others say, but feel free to volunteer information that is not requested and sometimes ask questions of their own [Nic76] . As INITIATIVE passes back and forth between the discourse participants, we say that CONTROL over the conversation gets transferred from one discourse participant to another.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each participant will, on occasion, take the conversational lead. ", "mid_sen": "Conversational partners not only respond to what others say, but feel free to volunteer information that is not requested and sometimes ask questions of their own [Nic76] . ", "after_sen": "As INITIATIVE passes back and forth between the discourse participants, we say that CONTROL over the conversation gets transferred from one discourse participant to another."}
{"citeStart": 95, "citeEnd": 116, "citeStartToken": 95, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "The production problems are related to the different character combinations and the lower frequency of words with Latin stem. In addition, these words might not be part of the students active vocabulary and are only guessed because they occur as cognates in the students L1. This is supported by the fact that many of the cognate answers resemble orthographic principles from other languages, e.g. for skeletons we find *skellets, *skelleton(s), *skelets, *skelletts, *skeletton(s), *skeltons, *skeletes, and *skelette(s). 11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists. We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013) . In addition, we consult the COCA list of academic words 12 and a list of words with latin roots. 13 Inflection Many errors are caused by wrong morphological inflection as in this example:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "11 In order to account for this phenomenon, we estimate the cognateness of words by gathering data from four different lists. ", "mid_sen": "We retrieve cognates from UBY using string similarity and from a cognate production algorithm (Beinborn et al., 2013) . ", "after_sen": "In addition, we consult the COCA list of academic words 12 and a list of words with latin roots. "}
{"citeStart": 111, "citeEnd": 124, "citeStartToken": 111, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "If this doesn't succeed, it backtracks to try an error rule at an earlier point in the analysis. At present it will not apply more than one error rule per word, in keeping with findings on error frequencies (Pollock, 1983) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If this doesn't succeed, it backtracks to try an error rule at an earlier point in the analysis. ", "mid_sen": "At present it will not apply more than one error rule per word, in keeping with findings on error frequencies (Pollock, 1983) .", "after_sen": "As an alternative, a program was developed which uses positional binary trigrams (Riseman,1974) (p.b.t.'s) to spot the error position and to check candidate corrections generated by reverse Damerau transformations. "}
{"citeStart": 60, "citeEnd": 83, "citeStartToken": 60, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "Latent Semantic Analysis (LSA) is a theory and method for extracting and representing the meaning of words by statistically analyzing word contextual usages in a collection of text. It provides a method by which to calculate the similarity of meaning of given words and documents. LSA has been successfully applied to information retrieval (Deerwester et al., 1990) , statistical langauge modeling (Bellegarda, 2000) and etc. We explore LSA techniques in bilingual environment to derive semantic constraints as prior knowledge for guiding a word alignment model training. The idea is to find semantic representation of source words and target words in the so-called lowdimensional LSA-space, and then to use their similarities to quantitatively establish semantic consistencies. We propose two different approaches.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It provides a method by which to calculate the similarity of meaning of given words and documents. ", "mid_sen": "LSA has been successfully applied to information retrieval (Deerwester et al., 1990) , statistical langauge modeling (Bellegarda, 2000) and etc. ", "after_sen": "We explore LSA techniques in bilingual environment to derive semantic constraints as prior knowledge for guiding a word alignment model training. "}
{"citeStart": 42, "citeEnd": 60, "citeStartToken": 42, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we have presented a novel application of Alternating Structure Optimization (ASO) to the Semantic Role Labeling (SRL) task on NomBank. The possible auxiliary problems are categorized and tested extensively. Our results outperform those reported in (Jiang and Ng, 2006) . To the best of our knowledge, we achieve the highest SRL accuracy published to date on the English NomBank.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The possible auxiliary problems are categorized and tested extensively. ", "mid_sen": "Our results outperform those reported in (Jiang and Ng, 2006) . ", "after_sen": "To the best of our knowledge, we achieve the highest SRL accuracy published to date on the English NomBank."}
{"citeStart": 44, "citeEnd": 55, "citeStartToken": 44, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "The SMT system was trained on freely avail-Source: On the other hand <zone> <x translation=\"ou1-P\">x</x> <wall/> a big advantage <wall/> <x translation=\"/ou1\">x</x> </zone> of the hostel is its placement Translation: por otra parte <ou1-P>una gran ventaja</ou1> del hostal es su colocación Figure 3 : Source text with reordering constraint mark-up as well as code to pass tags, and its translation. able data from the 2013 workshop on Statistical Machine Translation 6 (WMT 2013). We also crawled monolingual data in the hotel booking domain, from booking.com and TripAdvisor.com. From these in-domain data we extracted 100k and 50k word corpora, respectively for data selection and language model (LM) interpolation tuning. We selected the data closest to the domain in the English-Spanish parallel corpora via a crossentropy-based method (Moore and Lewis, 2010), using the open source XenC tool (Rousseau, 2013) . The size of available and selected corpora are indicated in the first 4 rows of Table 2 . The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2). We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system. 7", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The LM was an interpolation of LMs trained with the target part of the parallel corpora and with the rest of the Booking and Trip Advisor data (last 2 rows of Table 2). ", "mid_sen": "We used Moses Experiment Management System (Koehn, 2010) with all default options to build the SMT system. ", "after_sen": "7"}
{"citeStart": 238, "citeEnd": 248, "citeStartToken": 238, "citeEndToken": 248, "sectionName": "UNKNOWN SECTION NAME", "string": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992) , (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003) , (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. ", "mid_sen": "They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992) , (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003) , (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995) .", "after_sen": "As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. "}
{"citeStart": 102, "citeEnd": 114, "citeStartToken": 102, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "Common Topic Inference Understanding segments of utterances standing in a Common Topic relation requires the determination of points of commonality (parallelism) and departure (contrast) between sets of corresponding entities and properties within the utterances. This process is reliant on performing comparison and generalization operations on the corresponding representations (Scha and Polanyi, 1988; Hobbs, 1990; Priist, 1992; Asher, 1993) . Table 2 sketches definitions for some Common Topic relations, some taken from and others adapted from Hobbs (1990) . In each case, the hearer is to understand the relation by inferring po (al,..., a,) from sentence So and inferring p1(bl, ..., bn) from sentence $1 under the listed constraints. 9 In order to meet these constraints, the identification of p0 and Pl may require arbitrary levels of generalization from the relations explicitly stated in the utterances. Examples of these relations are given in sentences (13a-d). 13cis likewise coherent by virtue of the inferences resulting from identifying parallel elements and properties, including that John is a young aspiring politician and that he's a Democrat (since Clinton is identified with his party's candidate). The characteristic that Common Topic relations share is that they require the identification of parallel entities (i.e., the al and bi) and relations (P0 and Px) as arguments to the constraints. We posit that the syntactic representation is used both to guide the identification of parallel elements and to retrieve their semantic representations.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This process is reliant on performing comparison and generalization operations on the corresponding representations (Scha and Polanyi, 1988; Hobbs, 1990; Priist, 1992; Asher, 1993) . ", "mid_sen": "Table 2 sketches definitions for some Common Topic relations, some taken from and others adapted from Hobbs (1990) . ", "after_sen": "In each case, the hearer is to understand the relation by inferring po (al,..., a,) from sentence So and inferring p1(bl, ..., bn) from sentence $1 under the listed constraints. "}
{"citeStart": 49, "citeEnd": 71, "citeStartToken": 49, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Schutze, 1998) . In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human judges has been made by McCarthy, Keller and Caroll (McCarthy et al., 2003) for verb-particle constructions. (See Section for more details). Some preliminary work on recognition of V-N collocations was presented in (Venkatapathy and Joshi, 2004) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Various statistical measures have been suggested for ranking expressions based on their compositionality. ", "mid_sen": "Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Schutze, 1998) . ", "after_sen": "In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. ", "mid_sen": "Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser.", "after_sen": "The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below:"}
{"citeStart": 107, "citeEnd": 133, "citeStartToken": 107, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training. Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008) , although training would be much slower compared to using generative models, as in our case.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We conjecture based on our analysis that the EM training algorithm is able to exploit the information available in both gold and automatically labeled data with more complex grammars while being less affected by over-fitting. ", "mid_sen": "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches (Charniak and Johnson, 2005; Huang, 2008) for self training. ", "after_sen": "Self-training should also benefit other discriminatively trained parsers with latent annotations (Petrov and Klein, 2008) , although training would be much slower compared to using generative models, as in our case."}
{"citeStart": 150, "citeEnd": 161, "citeStartToken": 150, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, the method is applied to a French corpus on computing to nd noun-verb combinations in which verbs convey a meaning of realization. The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information (L 'Homme, 2004) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, the method is applied to a French corpus on computing to nd noun-verb combinations in which verbs convey a meaning of realization. ", "mid_sen": "The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information (L 'Homme, 2004) .", "after_sen": "Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. "}
{"citeStart": 34, "citeEnd": 61, "citeStartToken": 34, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "We adopt a similar strategy and resort to the notion of polarity neutral input to control the well formedness of the enriched input. The proposal draws on ideas from (Koller and Striegnitz, 2002; Gardent and Kow, 2005) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics), syntactic requirements and resources cancel out. More specifically, the aim is to determine whether given the input set of elementary trees, each substitution and each adjunction requirement is satisfied by exactly one elementary tree of the appropriate syntactic category and semantic index.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We adopt a similar strategy and resort to the notion of polarity neutral input to control the well formedness of the enriched input. ", "mid_sen": "The proposal draws on ideas from (Koller and Striegnitz, 2002; Gardent and Kow, 2005) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics), syntactic requirements and resources cancel out. ", "after_sen": "More specifically, the aim is to determine whether given the input set of elementary trees, each substitution and each adjunction requirement is satisfied by exactly one elementary tree of the appropriate syntactic category and semantic index."}
{"citeStart": 12, "citeEnd": 26, "citeStartToken": 12, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "A computational treatment expanding out the lexicon cannot be used for the increasing number of HPSG analyses that propose lexical rules that would result in an infinite lexicon. Most current HPSG analyses of Dutch, German, Italian, and French fall into that category. 1 Furthermore, since lexical rules in such an approach only serve in a precompilation step, the generalizations captured by the lexical rules cannot be used at run-time. Finally, all such treatments of lexical rules currently available presuppose a fully explicit notation of lexical rule specifications that transfer properties not changed by the lexical rules to the newly created lexical entry. This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. As shown in Meurers (1994) this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This conflicts with the standard assumption made in HPSG that only the properties changed by a lexical rule need be mentioned. ", "mid_sen": "As shown in Meurers (1994) this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries.", "after_sen": "• The authors are listed alphabetically. "}
{"citeStart": 167, "citeEnd": 188, "citeStartToken": 167, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997) . However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization. The parallel parsers in past work are implemented on multicore systems, where the limited parallelization possibilities provided by the systems restrict the speedups that can be achieved. For example, van Lohuizen (1999) reports a 1.8× speedup, while Manousopoulou et al. (1997) claims a 7-8× speedup. In contrast, our parallel parser is implemented on a manycore system with an abundant number of threads and pro-cessors to parallelize upon. We exploit the massive fine-grained parallelism inherent in natural language parsing and achieve a speedup of more than an order of magnitude.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A substantial body of related work on parallelizing natural language parsers has accumulated over the last two decades (van Lohuizen, 1999; Giachin and Rullent, 1989; Pontelli et al., 1998; Manousopoulou et al., 1997) . ", "after_sen": "However, none of this work is directly comparable to ours, as GPUs provide much more fine-grained possibilities for parallelization. "}
{"citeStart": 35, "citeEnd": 51, "citeStartToken": 35, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the main challenges facing hyponymy extraction is that comparatively few of the correct relations that might be found in text are expressed overtly by the simple lexicosyntactic patterns used in Section 2, as was apparent in the results presented in that section. This problem has been addressed by Caraballo (1999) , who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information. The leaves of this hierarchy (corresponding to nouns) are assigned hypernyms using Hearst-style lexicosyntactic patterns. Internal nodes in the hierarchy are then labelled with hypernyms of the leaves they subsume according to a vote of these subsumed leaves.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One of the main challenges facing hyponymy extraction is that comparatively few of the correct relations that might be found in text are expressed overtly by the simple lexicosyntactic patterns used in Section 2, as was apparent in the results presented in that section. ", "mid_sen": "This problem has been addressed by Caraballo (1999) , who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information. ", "after_sen": "The leaves of this hierarchy (corresponding to nouns) are assigned hypernyms using Hearst-style lexicosyntactic patterns. "}
{"citeStart": 91, "citeEnd": 111, "citeStartToken": 91, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we have only scratched the surface of what can be be done with the computation of best configurations in Section 5. The algorithms generalize easily to weights that are taken from an arbitrary ordered semiring (Golan, 1999; Borchardt and Vogler, 2003) and to computing minimal-weight rather than maximal-weight configurations. It is also useful in applications beyond semantic construction, e.g. in discourse parsing (Regneri et al., 2008) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The algorithms generalize easily to weights that are taken from an arbitrary ordered semiring (Golan, 1999; Borchardt and Vogler, 2003) and to computing minimal-weight rather than maximal-weight configurations. ", "mid_sen": "It is also useful in applications beyond semantic construction, e.g. in discourse parsing (Regneri et al., 2008) .", "after_sen": ""}
{"citeStart": 68, "citeEnd": 79, "citeStartToken": 68, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "3.2.4 Hidden Markov Models. In a Hidden Markov Model, the tagging task is viewed as finding the maximum probability sequence of states in a stochastic finite-state machine. The transitions between states emit the words of a sentence with a probability P(w [ St) , the states St themselves model tags or sequences of tags. The transitions are controlled by Markovian state transition probabilities P(Stl ] Sti_l ). Because a sentence could have been generated by a number of different state sequences, the states are considered to be \"Hidden.\" Although methods for unsupervised training of HMM's do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data. The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because a sentence could have been generated by a number of different state sequences, the states are considered to be \"Hidden.\" Although methods for unsupervised training of HMM's do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data. ", "mid_sen": "The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993) .", "after_sen": "In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM's, which turned out to have the worst accuracy of the four competing methods. "}
{"citeStart": 115, "citeEnd": 132, "citeStartToken": 115, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratnaparkhi, 1996) . For each potential sentence boundary token (., ?, and !), we estimate a joint probability distribution p of the token and it.s surrounding context, both of which are denoted by c, occurring as an actual sentence I)oundary. The (list, ribul.ioll is given by: 2A token in the training data is considered an abbreviation if it is preceded and followed by whitespace, and it contains a . that is not a sentence boundary. the ctj's are the unknown parameters of the model, and where each c U corresponds to a fj, or a feature.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratnaparkhi, 1996) . ", "after_sen": "For each potential sentence boundary token (., ?, and !), we estimate a joint probability distribution p of the token and it.s surrounding context, both of which are denoted by c, occurring as an actual sentence I)oundary. "}
{"citeStart": 60, "citeEnd": 83, "citeStartToken": 60, "citeEndToken": 83, "sectionName": "UNKNOWN SECTION NAME", "string": "Viewed another way, we must show how to esti-mate the average word length. Conversational English has short words (about 3 phones), because most grammatical morphemes are free-standing. Languages with many affixes have longer words, e.g. my Arabic data averages 5.6 phones per word. Pauses are vital for deciding what is an affix. Attempts to segment transcriptions without pauses, e.g. (Christiansen et al., 1998) , have worked poorly. Claims that humans can extract words without pauses seem to be based on psychological experiments such as (Saffran, 2001; Jusczyk and Aslin, 1995) which conflate words and morphemes. Even then, explicit boundaries seem to improve performance (Seidl and Johnson, 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Claims that humans can extract words without pauses seem to be based on psychological experiments such as (Saffran, 2001; Jusczyk and Aslin, 1995) which conflate words and morphemes. ", "mid_sen": "Even then, explicit boundaries seem to improve performance (Seidl and Johnson, 2006) .", "after_sen": "Another significant part of this task is finding syllable boundaries. "}
{"citeStart": 89, "citeEnd": 113, "citeStartToken": 89, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus, in order to predict the most likely word in a given context, a global estimation of the sentence probability is derived which, in turn, is computed by estimating the probability of each word given its local context or history. Estimating terms of the form Pr(wlh ) is done by assuming some generative probabilistic model, typically using Markov or other independence assumptions, which gives rise to estimating conditional probabilities of n-grams type features (in the word or POS space). Machine learning based classifiers and maximum entropy models which, in principle, are not restricted to features of these forms have used them nevertheless, perhaps under the influence of probabilistic methods (Brill, 1995; Yarowsky, 1994; Ratnaparkhi et al., 1994) . It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn better classifiers and language models. Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996) , and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999) . Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (Chelba and Jelinek, 1998; Rosenfeld, 1996) . We believe that the main reason for that is that incorporating information sources in NLP needs to be coupled with a learning approach that is suitable for it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has been argued that the information available in the local context of each word should be augmented by global sentence information and even information external to the sentence in order to learn better classifiers and language models. ", "mid_sen": "Efforts in this directions consists of (1) directly adding syntactic information, as in (Chelba and Jelinek, 1998; Rosenfeld, 1996) , and (2) indirectly adding syntactic and semantic information, via similarity models; in this case n-gram type features are used whenever possible, and when they cannot be used (due to data sparsity), additional information compiled into a similarity measure is used (Dagan et al., 1999) . ", "after_sen": "Nevertheless, the efforts in this direction so far have shown very insignificant improvements, if any (Chelba and Jelinek, 1998; Rosenfeld, 1996) . "}
{"citeStart": 102, "citeEnd": 125, "citeStartToken": 102, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Perhaps the most influential and widely-adopted semantic treatment of coordination is the approach of Partee and Rooth (1983) . They propose a generalized conjunction scheme in which conjuncts of the same type can be combined...ks is the case with Steedman's operators, contraction inherent in the schema allows for a single shared argument to be distributed as an argument of each conjunct. Type-lifting is allowed to produce like types when necessary; the combination of the coordination scheme and type-lifting can have the effect of 'copying' an argument of higher type, such as a quantifier in the case of coordinated intensional verbs. They propose a 'processing strategy' requiring that expressions are interpreted a! the lowest possible type, with type-raising taking place only where necessary.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Perhaps the most influential and widely-adopted semantic treatment of coordination is the approach of Partee and Rooth (1983) . ", "after_sen": "They propose a generalized conjunction scheme in which conjuncts of the same type can be combined...ks is the case with Steedman's operators, contraction inherent in the schema allows for a single shared argument to be distributed as an argument of each conjunct. "}
{"citeStart": 81, "citeEnd": 101, "citeStartToken": 81, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to optimize the number of topics K, we run five trials of each modality for 2000 iterations for K = {50, 100, 150, 200, 250} (a total of 25 runs per setup). We select the value or K for each model which minimizes the average perplexity estimate over the five trials. Table 1 shows our results for each of our selected models with our compositionality evaluation. The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). This result is consistent with other works using this model with these features (Andrews et al., 2009; Silberer and Lapata, 2012) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The 2D models employing feature norms and association norms do significantly better than the text-only model (two-tailed t-test). ", "mid_sen": "This result is consistent with other works using this model with these features (Andrews et al., 2009; Silberer and Lapata, 2012) .", "after_sen": ""}
{"citeStart": 39, "citeEnd": 51, "citeStartToken": 39, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "(2) Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(1) Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role.", "mid_sen": "(2) Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.", "after_sen": "In many cases, it is useful to know which arguments were confirmed and accepted by the research community and which ones where disputed or even rejected. "}
{"citeStart": 35, "citeEnd": 44, "citeStartToken": 35, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "whose idea came from Lin's method (Lin, 1998) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "EQUATION", "mid_sen": "whose idea came from Lin's method (Lin, 1998) .", "after_sen": ""}
{"citeStart": 25, "citeEnd": 50, "citeStartToken": 25, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words. The semantic space we used in our experiments was built on a lemmatised version of the BNC. Following previous work (Bullinaria and Levy, 2007) , we optimized its parameters on a word-based semantic similarity task. The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values. We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability). We used WordSim353, a benchmark dataset (Finkelstein et al., 2002) , consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The semantic space we used in our experiments was built on a lemmatised version of the BNC. ", "mid_sen": "Following previous work (Bullinaria and Levy, 2007) , we optimized its parameters on a word-based semantic similarity task. ", "after_sen": "The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values. "}
{"citeStart": 187, "citeEnd": 207, "citeStartToken": 187, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model (Weischedel et al., 1993 , Merialdo, 1994 or Statistical Decision Tree (Jelinek et al., 1994 The Maximum Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods. It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques. 5The mapping from article to annotator is in the file doc/wsj .wht on the Treebank CDROM.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Most of the recent corpus-based POS taggers in the literature are either statistically based, and use Markov Model (Weischedel et al., 1993 , Merialdo, 1994 or Statistical Decision Tree (Jelinek et al., 1994 The Maximum Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods. ", "after_sen": "It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques. "}
{"citeStart": 39, "citeEnd": 54, "citeStartToken": 39, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "In the Top Twenty set there are a fewer number of individually non-discriminative feature making the pool of features \"better\". In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of \"better\" features, cf. (Shen and Joshi, 2004) . If the number of the non-discriminative features is large enough, the data set becomes unsplittable. We have tried using the ¹ trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the number of the non-discriminative features is large enough, the data set becomes unsplittable. ", "mid_sen": "We have tried using the ¹ trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.", "after_sen": "We achieve similar results with Algorithm 2, the ordinal regression with uneven margin. "}
{"citeStart": 292, "citeEnd": 305, "citeStartToken": 292, "citeEndToken": 305, "sectionName": "UNKNOWN SECTION NAME", "string": "but this led to unwanted effects elsewhere in the grammar (Bouma 1987) . Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \\ for normal args, ? for wh-args (Moortgat 1988) , or have introduced so called modal operators on the whargument (Morrill et al. 1990 ). Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted effects elsewhere in the grammar. However, there are problems with having just composition, the most basic of the non-applicative operations. In CGs which contain functions of functions (such as very, or slowly), the addition of composition adds both new analyses of sentences, and new strings to the language. This is due to the fact that composition can be used to form a function, which can then be used as an argument to a function of a function. For example, if the two types, n/n and n/n are composed to give the type n/n, then this can be modified by an adjectival modifier of type (n/n)/(n/n).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "but this led to unwanted effects elsewhere in the grammar (Bouma 1987) . ", "mid_sen": "Subsequent treatments of non-peripheral extraction based on the Lambek Calculus (where standard composition is built in: it is a rule which can be proven from the calculus) have either introduced an alternative to the forward and backward slashes i.e. / and \\ for normal args, ? for wh-args (Moortgat 1988) , or have introduced so called modal operators on the whargument (Morrill et al. 1990 ). ", "after_sen": "Both techniques can be thought of as marking the wh-arguments as requiring special treatment, and therefore do not lead to unwanted effects elsewhere in the grammar. "}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "At the core of our work lies the notion of distrib utional similarity (Harris, 1968) , which states that similar words occur in similar contexts. In various sources, the notion of context ranges from bag-ofwords-like approaches to more structured ones in which syntax plays a role. Schutze (1998) used bag-of-words contexts for sense discrimination. Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts. Our approach is similar with the difference that we do not group noun arguments into finite categories, but instead leave the category boundaries blurry and allow overlaps.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Schutze (1998) used bag-of-words contexts for sense discrimination. ", "mid_sen": "Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts. ", "after_sen": "Our approach is similar with the difference that we do not group noun arguments into finite categories, but instead leave the category boundaries blurry and allow overlaps."}
{"citeStart": 65, "citeEnd": 78, "citeStartToken": 65, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The BOW approach is different from the parsing based approaches (Melamed, 2004; Zhang and Gildea, 2005; Cowan et al., 2006) where the translation model tightly couples the syntactic and lexical items of the two languages. The decoupling of the two steps in our model has the potential for generating paraphrased sentences not necessarily isomorphic to the structure of the source sentence.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, if we set the threshold to 0.3, both t 1 and t 2 will be detected in the target sentence, and we found this to be a major source of undesirable insertions.", "mid_sen": "The BOW approach is different from the parsing based approaches (Melamed, 2004; Zhang and Gildea, 2005; Cowan et al., 2006) where the translation model tightly couples the syntactic and lexical items of the two languages. ", "after_sen": "The decoupling of the two steps in our model has the potential for generating paraphrased sentences not necessarily isomorphic to the structure of the source sentence."}
{"citeStart": 51, "citeEnd": 66, "citeStartToken": 51, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "The formalism should be fine-grained, ie. responsive to the behaviour of individual words (as n-gram models are). This suggests a radically lexiealist approach (cf. Karttunen, 1990) in which all rules are encoded in the lexicon, there being no phrase structure rules which do not introduce lexical items.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "responsive to the behaviour of individual words (as n-gram models are). ", "mid_sen": "This suggests a radically lexiealist approach (cf. Karttunen, 1990) in which all rules are encoded in the lexicon, there being no phrase structure rules which do not introduce lexical items.", "after_sen": "It should be capable of capturing fully the linguistic intuitions of language users. "}
{"citeStart": 73, "citeEnd": 85, "citeStartToken": 73, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "The Intra-AFL is first initialised with all (non-pronominal) candidate foci in the EE: Intra-AFL = witnesses The IRs are then applied to the first pronoun, them, and, in this case, propose the current AF, State Police, as the antecedent. The Intra-AFL is immediately updated to add the antecedent: Intra-AFL = State Police, witnesses EE-2 has a pronoun in 'thematic' position, 'theme' being either the object of a transitive verb, or the subject of an intransitive or the copula (following (Gruber, 1976) ). Its antecedent therefore becomes the new CF, with the previous value moving to the FS. EE-2 has an 'agent', where this is an animate verb subject (again as in (Gruber, 1976) ), and this becomes the new AF. Because the old AF is now the CF, it is not added to the AFS as it would be otherwise. After each EE the Intra-AFL is added to the current AFL, excluding the CF. Non-agent pronouns:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Its antecedent therefore becomes the new CF, with the previous value moving to the FS. ", "mid_sen": "EE-2 has an 'agent', where this is an animate verb subject (again as in (Gruber, 1976) ), and this becomes the new AF. ", "after_sen": "Because the old AF is now the CF, it is not added to the AFS as it would be otherwise. "}
{"citeStart": 71, "citeEnd": 90, "citeStartToken": 71, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "There is one parameter β in the query expansion method presented in Section 3. We tune the value of β and report the best performance. The parameter sensitivity is similar to the observations described in (Fang and Zhai, 2006) and will not be discussed in this paper. In all the result tables, ‡ and † indicate that the performance difference is statistically significant according to Wilcoxon signed rank test at the level of 0.05 and 0.1 respectively.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We tune the value of β and report the best performance. ", "mid_sen": "The parameter sensitivity is similar to the observations described in (Fang and Zhai, 2006) and will not be discussed in this paper. ", "after_sen": "In all the result tables, ‡ and † indicate that the performance difference is statistically significant according to Wilcoxon signed rank test at the level of 0.05 and 0.1 respectively."}
{"citeStart": 6, "citeEnd": 25, "citeStartToken": 6, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "In our example sentence in section 2.1, noun phrases are represented by bracket structures. Both (Mufioz et al., 1999) and (Tjong Kim Sang and Veenstra, 1999) have shown how classifiers can process bracket structures. One classifier can be trained to recognize open brackets (O) while another will process close brackets (C). Their results can be converted to baseNPs by making pairs of open and close brackets with large probability scores (Mufioz et al., 1999) or by regarding only the shortest phrases between open and close brackets as baseNPs (Tjong Kim Sang and Veenstra, 1999) . We have used the bracket representation (O+C) in combination with the second baseNP construction method.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our example sentence in section 2.1, noun phrases are represented by bracket structures. ", "mid_sen": "Both (Mufioz et al., 1999) and (Tjong Kim Sang and Veenstra, 1999) have shown how classifiers can process bracket structures. ", "after_sen": "One classifier can be trained to recognize open brackets (O) while another will process close brackets (C). "}
{"citeStart": 40, "citeEnd": 60, "citeStartToken": 40, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. ", "mid_sen": "The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. ", "after_sen": "Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. "}
{"citeStart": 80, "citeEnd": 98, "citeStartToken": 80, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ( (Turney, 2002; Pang et al., 2002) ). Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999) . Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in arti-cles that are not editorials or reviews) were subjective.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). ", "mid_sen": "In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999) . ", "after_sen": "Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. "}
{"citeStart": 95, "citeEnd": 111, "citeStartToken": 95, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic framework on which the implementation is built is similar to Tree Adjoining Grammar (Joshi et al 1975) . Each lexical category is associated with a set of structural relations, which determine its lexical subtree. We call this set the subtree projection of that lexical category. For example, the subtree projection for verbs in the English grammar is as follows, where Lex is a variable which will be instantiated to the actual verb found in the input.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The basic framework on which the implementation is built is similar to Tree Adjoining Grammar (Joshi et al 1975) . ", "after_sen": "Each lexical category is associated with a set of structural relations, which determine its lexical subtree. "}
{"citeStart": 69, "citeEnd": 106, "citeStartToken": 69, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting befiefs; therefore we employ a re, cursive model for collaboration that captures extended negotiation and represents the structure of the discourse. Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. In addition, we provide a process for selecting among multiple possible pieces of evidence.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . ", "mid_sen": "However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting befiefs; therefore we employ a re, cursive model for collaboration that captures extended negotiation and represents the structure of the discourse. ", "after_sen": "Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. "}
{"citeStart": 104, "citeEnd": 118, "citeStartToken": 104, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "With all its strong points, there are a number of restrictions to the proposed approach. First, in its present form it is suitable only for processing of reasonably \"wellbehaved\" texts that consistently use capitalization (mixed case) and do not contain much noisy data. Thus, for instance, we do not expect our system to perform well on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on optical character reader-generated texts. We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. This is where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger reported in Mikheev (2000) , which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We noted in Section 8 that very short documents of one to three sentences also present a difficulty for our approach. ", "mid_sen": "This is where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger reported in Mikheev (2000) , which do not heavily rely on word capitalization and are not sensitive to document length, have an advantage.", "after_sen": "Our DCA uses information derived from the entire document and thus can be used as a complement to approaches based on the local context. "}
{"citeStart": 14, "citeEnd": 34, "citeStartToken": 14, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "There are various strands of future research. Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior. Secondly, as (Blunsom et al., 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect. Apart from a new decoder, it will be worthwhile adapting the prior probability in our model to allow for consistent estimation. Finally, it would be interesting to study properties of the penalized Deleted Estimation used in this paper.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Firstly, we plan to explore our estimator on other language pairs in order to obtain more evidence on its behavior. ", "mid_sen": "Secondly, as (Blunsom et al., 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. ", "after_sen": "We plan to build our own decoder (based on ITG) where different ideas can be tested including tractable ways for achieving a marginalization effect. "}
{"citeStart": 150, "citeEnd": 174, "citeStartToken": 150, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we address both of these shortcomings by proposing regular tree grammars (RTGs) as a novel underspecification formalism. Regular tree grammars (Comon et al., 2007) are a standard approach for specifying sets of trees in theoretical computer science, and are closely related to regular tree transducers as used e.g. in recent work on statistical MT (Knight and Graehl, 2005) and grammar formalisms (Shieber, 2006) . We show that the \"dominance charts\" proposed by Koller and Thater (2005b) can be naturally seen as regular tree grammars; using their algorithm, classical underspecified descriptions (dominance graphs) can be translated into RTGs that describe the same sets of readings. However, RTGs are trivially expressively complete because every finite tree language is also regular. We exploit this increase in expressive power in presenting a novel redundancy elimination algorithm that is simpler and more powerful than the one by Koller and Thater (2006) ; in our algorithm, redundancy elimination amounts to intersection of regular tree languages. Furthermore, we show how to define a PCFG-style cost model on RTGs and compute best readings of deterministic RTGs efficiently, and illustrate this model on a machine learning based model of scope preferences (Higgins and Sadock, 2003) . To our knowledge, this is the first efficient algorithm for computing best readings of a scope ambiguity in the literature.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, RTGs are trivially expressively complete because every finite tree language is also regular. ", "mid_sen": "We exploit this increase in expressive power in presenting a novel redundancy elimination algorithm that is simpler and more powerful than the one by Koller and Thater (2006) ; in our algorithm, redundancy elimination amounts to intersection of regular tree languages. ", "after_sen": "Furthermore, we show how to define a PCFG-style cost model on RTGs and compute best readings of deterministic RTGs efficiently, and illustrate this model on a machine learning based model of scope preferences (Higgins and Sadock, 2003) . "}
{"citeStart": 117, "citeEnd": 142, "citeStartToken": 117, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "number of features used in our complex models -in the several hundreds of thousands, is extremely high in comparison with the data set size and the number of features used in other machine learning domains. We describe two sets of experiments aimed at comparing models with and without regularization. One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features. The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in Chen and Rosenfeld (2000) , and used in all the stochastic LFG work (Johnson et al., 1999) . However, until recently, its role and importance have not been widely understood. For example, Zhang and Oles (2001) attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization. At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996) , Toutanova and Manning (2000) , and Collins (2002) all present unregularized models. Indeed, the result of Collins (2002) that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized. Table 5 shows results on the development set from two pairs of experiments. The first pair of models use common word templates t 0 , w 0 , t 0 , t −1 , t −2 and the same rare word templates as used in the models in table 2. The second pair of models use the same features as model BEST with a higher frequency cutoff of 5 for common word features.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features. ", "mid_sen": "The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in Chen and Rosenfeld (2000) , and used in all the stochastic LFG work (Johnson et al., 1999) . ", "after_sen": "However, until recently, its role and importance have not been widely understood. "}
{"citeStart": 113, "citeEnd": 123, "citeStartToken": 113, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "This type of learners constructs a representation for document vectors belonging to a certain class during the learning phase, e.g. decision trees, decision rules or probability weightings. During the categorization phase, the representation is used to assign the appropriate class to a new document vector. Several pruning or specialization heuristics can be used to control the amount of generalization. We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. Support Vector Machines (SVMs): SVMs are described in (Vapnik, 1995) . SVMs are binary learners in that they distinguish positive and negative examples for each class. Like eager learners, they construct a representation during the learning phase, namely a hyper plane supported by vectors of positive and negative examples. For each class, a categorizer is built by computing such a hyper plane. During the categorization phase, each categorizer is applied to the new document vector, yielding the probabilities of the document belonging to a class. The probability increases with the distance of thevector from the hyper plane. A document is said to belong to the class with the highest probability. We chose SVM_Light (Joachims, 1998) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several pruning or specialization heuristics can be used to control the amount of generalization. ", "mid_sen": "We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ", "after_sen": "ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. "}
{"citeStart": 102, "citeEnd": 123, "citeStartToken": 102, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002) . We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1 . The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002) . ", "after_sen": "We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). "}
{"citeStart": 184, "citeEnd": 194, "citeStartToken": 184, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "It can be shown that the computation of the intersection of a FSA and a CFG requires only a rain-imal generalization of existing parsing algorithms. We simply replace the usual string positions with the names of the states in the FSA. It is also straightforward to show that the complexity of this process is cubic in the number of states of the FSA (in the case of ordinary parsing the number of states equals n + 1) (Lang, 1974; Billot and Lang, 1989 ) (assuming the right-hand-sides of grammar rules have at most two categories).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We simply replace the usual string positions with the names of the states in the FSA. ", "mid_sen": "It is also straightforward to show that the complexity of this process is cubic in the number of states of the FSA (in the case of ordinary parsing the number of states equals n + 1) (Lang, 1974; Billot and Lang, 1989 ) (assuming the right-hand-sides of grammar rules have at most two categories).", "after_sen": "In this paper we investigate whether the same techniques can be applied in case the grammar is a constraint-based grammar rather than a CFG. "}
{"citeStart": 153, "citeEnd": 171, "citeStartToken": 153, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "The classification results show that our method is powerful, and suited to the classification of unknown verbs. However, we have not yet addressed the problem of verbs that can have multiple classifications. We think that many cases of ambiguous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al. (1998) . This is an important concept, which proposes that \"regular\" ambiguity in classification--i.e., sets of verbs that have the same multi-way classifications according to Levin (1993) --can be captured with a finer-grained notion of lexical semantic classes. Thus, subsets of verbs that occur in the intersection of two or more Levin classes form in themselves a coherent semantic (sub)class. Extending our work to exploit this idea requires only defining the classes appropriately; the basic approach will remain the same. Given the current demonstration of our method on fine-grained classes that share subcategoriza-tion alternations, we are optimistic regarding its future performance on intersective sets.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, we have not yet addressed the problem of verbs that can have multiple classifications. ", "mid_sen": "We think that many cases of ambiguous classification of the lexical entry for a verb can be addressed with the notion of intersective sets introduced by Dang et al. (1998) . ", "after_sen": "This is an important concept, which proposes that \"regular\" ambiguity in classification--i.e., sets of verbs that have the same multi-way classifications according to Levin (1993) --can be captured with a finer-grained notion of lexical semantic classes. "}
{"citeStart": 143, "citeEnd": 155, "citeStartToken": 143, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "The present work uses the time spent looking at a word during reading as an empirical measure of sentence processing difficulty. From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre (2004) ; these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. The fact that any member of this parser family derives a useful predictor shows that at least some syntactic properties are reflected in readers' eye fixation durations. From a cognitive perspective, the utility of small k parsers for modeling comprehension difficulty lends credence to the view that the human processor is a single-path analyzer (Frazier and Fodor, 1978) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The present work uses the time spent looking at a word during reading as an empirical measure of sentence processing difficulty. ", "mid_sen": "From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre (2004) ; these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings. ", "after_sen": "We find that predictions derived even from very narrow-beamed parsers improve a baseline eye-fixation duration model. "}
{"citeStart": 271, "citeEnd": 298, "citeStartToken": 271, "citeEndToken": 298, "sectionName": "UNKNOWN SECTION NAME", "string": "word sense disambiguation, information retrieval, natural language generation and so on. Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). ", "mid_sen": "However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "after_sen": "We have been concerned with investigating the lexical . ['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates. "}
{"citeStart": 27, "citeEnd": 41, "citeStartToken": 27, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "Primary Term > Secondary Term > Tertiary Term > Oblique Term. Primacy of a term over another is defined by the former having a wider range of syntactic features than the latter. In an accusative language, subjects are less marked (hence primary) than objects; all verbs take subjects but only transitive verbs take objects. Terms (=arguments) can be denoted by the genotype indices on NPs, such as NP1, NP2 for primary and secondary termsJ An NP2 would be a direct object (NPacc) in an accusative language, or an ergativemarked NP (NPerg) in an ergative language. This level of description also simplifies the formulation of grammatical function changing; the primary term of a passivized predicate (PASS p) is the secondary term of the active p. I follow Shaumyan and Steedman (1996) also in the ordered representation of the PAS (1). The reader is referred to (Shaumyan, 1987) for linguistic justification of this ordering.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This level of description also simplifies the formulation of grammatical function changing; the primary term of a passivized predicate (PASS p) is the secondary term of the active p. I follow Shaumyan and Steedman (1996) also in the ordered representation of the PAS (1). ", "mid_sen": "The reader is referred to (Shaumyan, 1987) for linguistic justification of this ordering.", "after_sen": "(1) Pred... <Sec. Term> <Primary Term> Given this representation, the surface order of constituents is often in conflict with the order in the PAS. "}
{"citeStart": 178, "citeEnd": 187, "citeStartToken": 178, "citeEndToken": 187, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we re-examine the problem of query expansion using lexical resources with the recently proposed axiomatic approaches (Fang and Zhai, 2006) . The major advantage of axiomatic approaches in query expansion is to provide guidance on how to weight related terms based on a given term similarity function. In our previous study, a cooccurrence-based term similarity function was proposed and studied. In this paper, we study several term similarity functions that exploit various information from two lexical resources, i.e., WordNet and dependency-thesaurus constructed by Lin (Lin, 1998) , and then incorporate these similarity functions into the axiomatic retrieval framework. We conduct empirical experiments over several TREC standard collections to systematically evaluate the effectiveness of query expansion based on these similarity functions. Experiment results show that all the similarity functions improve the retrieval performance, although the performance improvement varies for different functions. We find that the most effective way to utilize the information from Word-Net is to compute the term similarity based on the overlap of synset definitions. Using this similarity function in query expansion can significantly improve the retrieval performance. According to the retrieval performance, the proposed similarity function is significantly better than simple mutual information based similarity function, while it is comparable to the function proposed in (Fang and Zhai, 2006) . Furthermore, we show that the retrieval performance can be further improved if the proposed similarity function is combined with the similarity function derived from co-occurrence-based resources.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our previous study, a cooccurrence-based term similarity function was proposed and studied. ", "mid_sen": "In this paper, we study several term similarity functions that exploit various information from two lexical resources, i.e., WordNet and dependency-thesaurus constructed by Lin (Lin, 1998) , and then incorporate these similarity functions into the axiomatic retrieval framework. ", "after_sen": "We conduct empirical experiments over several TREC standard collections to systematically evaluate the effectiveness of query expansion based on these similarity functions. "}
{"citeStart": 101, "citeEnd": 113, "citeStartToken": 101, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Such techniques might be of use both in the case of written and spoken language input. In the latter case another possible application concerns the treatment of phenomena such as repairs (Carter, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such techniques might be of use both in the case of written and spoken language input. ", "mid_sen": "In the latter case another possible application concerns the treatment of phenomena such as repairs (Carter, 1994) .", "after_sen": "Note that we allow the input to be a full FSA (possibly including cycles, etc.) since some of the above-mentioned techniques indeed result in cycles. "}
{"citeStart": 147, "citeEnd": 167, "citeStartToken": 147, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "Applicative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994) , it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994) , and by Lambek Categorial Grammars (Lambek 1958) . It is therefore worth giving some brief indications of how it fits in with these developments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. ", "mid_sen": "Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994) , it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994) , and by Lambek Categorial Grammars (Lambek 1958) . ", "after_sen": "It is therefore worth giving some brief indications of how it fits in with these developments."}
{"citeStart": 32, "citeEnd": 56, "citeStartToken": 32, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "Hillary: wanted: found: supported: and: and2:go \"~ Hillary VX, Y. (fl SUBJ)~ ~'\" X (Vs,p. (VX. (fl susJ)~'--*X -o s--~p(X)) --o s-,~ Y(p)) ---o flz\"'* wanted (X, \"}\") VX, Y. (f2 sUBJ)~-~.¥ '.9 (f20BJ)a\"\"~ Y --,o f2~-.-~found(X, Y) VX, Y. (f3 SUBJ),,---+X ® (f30BJ)o\"--~Y --<, f3o-. --supported(X, Y) VX, Y. (f CONJ)a\",~X @ (f CONJ)o.\",.-~ Y ---o fo. -.-+and(X, Y) ! (VX, Y. (f CONJ) cy\"c*X @ fa-.~ Y --o fo-.~and(X, Y)) two-,:andidates:VH, S. (Vz. h~X --o lf~S(z)) --o H-.-*two(z, candidate(z) , S(z)) 23constructor and2 is introduced by and in order to handle cases where there are more than two conjuncts; this contribution will be used once in the derivation of the meaning for sentence (23). The following R-relations result from the f-structural relationships:Following the analysis given in Dalrymple et al. (1994b) , the lexical entry for want takes a quantified NP as an argument. This requires that the quantified NP meaning be duplicated, since otherwise no readings result. We provide a special rule for duplicating quantified NPs when necessary:In the interest of space, again we only show a few steps of the derivation. Combining the meanings for Hillary, found, supported, and and, Axiom I, and R-relations (ii), (iii), (v), (vi), (viii), and (ix), we can derive: -,-*and(found( Hillary, x) , supported( Hillary, x ) ) )We duplicate the meaning of two candidates using QNP Duplication, and combine one copy with the foregoing formula to yield: .... t wo( z, candidate(z), and(found( Hillary, z) , supported( Hillary, z ) ) )We then combine the other meaning of two candidates with the meanings of Hillary and wanted. and using Axiom I and R-relations (i), (iv), and (vii) we obtain:Finally, using and2 with the two foregoing formulae, we deduce the desired result:f~ . -~ and(wanted( Hillary, \"AQ.two( x, candidate(x) , I-Q] (x))). two (z, candidate(z), and(found( Hillary, z ) , suppo~ted( HiUa~y, z))))We can now specify a Partee and Rooth style processing strategy, which is to prefer readings which require the least use of QNP duplication. This strategy predicts the readings generated for the examples in Section 4. It also predicts the desired reading for sentence (23), since that reading requires two quantifiers. While the reading generated by Partee and Rooth is derivable, it requires three quantifiers and thus uses QNP duplication twice, which is less preferred than the reading requiring two quantifiers which uses QNP duplication once. Also, it allows some flexibility in cases where pragmatics strongly suggests that quantitiers are copied and distributed for multiple extensional verbs; unlike the Partee and Rooth account, this would apply equally to the case where there are also intensional verbs and the case where there are not. Finally, our account readily applies to cases of intensional verbs without coordination as in example (10), since it applies more generally to cases of resource sharing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The following R-relations result from the f-structural relationships:", "mid_sen": "Following the analysis given in Dalrymple et al. (1994b) , the lexical entry for want takes a quantified NP as an argument. ", "after_sen": "This requires that the quantified NP meaning be duplicated, since otherwise no readings result. "}
{"citeStart": 202, "citeEnd": 223, "citeStartToken": 202, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "We present a new method for generating linguistic variation projecting multiple personality traits continuously, by combining and extending previous research in statistical natural language generation (Paiva and Evans, 2005; Rambow et al., 2001; Isard et al., 2006; . While handcrafted rule-based approaches are limited to variation along a small number of discrete points (Hovy, 1988; Walker et al., 1997; Lester et al., 1997; Power et al., 2003; Cassell and Bickmore, 2003; Piwek, 2003; Rehm and André, in press), we learn models that predict parameter values for any arbitrary value on the variation dimension scales. Additionally, our data-driven approach can be applied to any dimension that is meaningful to human judges, and it provides an elegant way to project multiple dimensions simultaneously, by including the relevant dimensions as features of the parameter models' training data. Isard et al. (2006) and also propose a personality generation method, in which a data-driven personality model selects the best utterance from a large candidate set. Isard et al.'s technique has not been evaluated, while Mairesse and Walker's overgenerate and score approach is inefficient. Paiva and Evans' technique does not overgenerate (2005), but it requires a search for the optimal generation decisions according to the learned models. Our approach does not require any search or overgeneration, as parameter estimation models predict the generation decisions directly from the target variation dimensions. This technique is therefore beneficial for real-time generation. Moreover the variation dimensions of Paiva and Evans' data-driven technique are extracted from a corpus: there is thus no guarantee that they can be easily interpreted by humans, and that they generalise to other corpora. Previous work has shown that modeling the relation between personality and language is far from trivial (Pennebaker and King, 1999; Argamon et al., 2005; Oberlander and Nowson, 2006; , suggesting that the control of personality is a harder problem than the control of data-driven variation dimensions.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We present a new method for generating linguistic variation projecting multiple personality traits continuously, by combining and extending previous research in statistical natural language generation (Paiva and Evans, 2005; Rambow et al., 2001; Isard et al., 2006; . ", "after_sen": "While handcrafted rule-based approaches are limited to variation along a small number of discrete points (Hovy, 1988; Walker et al., 1997; Lester et al., 1997; Power et al., 2003; Cassell and Bickmore, 2003; Piwek, 2003; Rehm and André, in press), we learn models that predict parameter values for any arbitrary value on the variation dimension scales. "}
{"citeStart": 56, "citeEnd": 79, "citeStartToken": 56, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994 ). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et al., 1995) . 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993 Carroll, , 1994 , using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994 , 1995 Carroll & Briscoe, 1996) . 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. ", "mid_sen": "We use an enhanced version of the GATE project stemmer (Cunningham et al., 1995) . ", "after_sen": "3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993 Carroll, , 1994 , using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994 , 1995 Carroll & Briscoe, 1996) . "}
{"citeStart": 137, "citeEnd": 162, "citeStartToken": 137, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003) . In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model. All adaptations were done based on the utterances of a single speaker and pertained to that speaker only, i.e., it was not incremental or cumulative. Since a second decoding run is needed after the actual MLLR adaptations, the recognizer's response time more than doubles when this method is employed. The unsupervised speaker adaptation led to an additional increase of approximately 2% for the Picture and Opinion tasks (see Table 1 ). There were large differences between different speakers in terms of the performance gain of MLLR adaptation on our data set, however. There was also a large variation of word accuracies between speakers (13-100%). The variation in accuracy across speakers can be due to many different factors, including the degree of accent, the grammaticality of the response, the voice quality and the recording quality.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used unsupervised maximum likelihood linear regression (MLLR) AM adaptation on top of the previous adaptation and optimization steps (Tomokiyo and Waibel, 2001; Wang et al., 2003) . ", "after_sen": "In this step, all words whose confidence score was higher than a pre-set threshold were collected and their acoustic information was used to adapt the acoustic model. "}
{"citeStart": 143, "citeEnd": 166, "citeStartToken": 143, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005) , which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order.", "mid_sen": "Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005) , which are obtained using a parser trained to determine linguistic constituency. ", "after_sen": "Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) ."}
{"citeStart": 48, "citeEnd": 70, "citeStartToken": 48, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "In the initial annotation phase, 13 % of our corpus was annotated twice by different annotators in order to measure inter-annotator agreement. We did this measurement as it was done for MUC (Hirschman et al., 1997) and in the same way as a coreference resolution system is evaluated against some gold standard: one annotation was set to be the gold standard (\"key\") and the second annotation was set to be the \"response\". Herewith we reached an inter-annotator agreement of 49.5 MUC points (for MUC score calculation see Vilain et al. (1995) ). Although the MUC measure is questionable (Luo, 2005) and the task is difficult, this number is too low and asked for improvements.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the initial annotation phase, 13 % of our corpus was annotated twice by different annotators in order to measure inter-annotator agreement. ", "mid_sen": "We did this measurement as it was done for MUC (Hirschman et al., 1997) and in the same way as a coreference resolution system is evaluated against some gold standard: one annotation was set to be the gold standard (\"key\") and the second annotation was set to be the \"response\". ", "after_sen": "Herewith we reached an inter-annotator agreement of 49.5 MUC points (for MUC score calculation see Vilain et al. (1995) ). "}
{"citeStart": 108, "citeEnd": 127, "citeStartToken": 108, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the intersection of two well-known measures of significance, mutual information scores and t-scores (Church et al., 1994) , to determine if a (first-order) cooccurrence relation should be included in the network;", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given a root word, connect it to all the words that significantly co-occur with it in the training corpus; 1 then, recursively connect these words to their significant cooccurring words up to some specified depth.", "mid_sen": "We use the intersection of two well-known measures of significance, mutual information scores and t-scores (Church et al., 1994) , to determine if a (first-order) cooccurrence relation should be included in the network;", "after_sen": "however, we use just the t-scores in computing significance scores for all the relations. "}
{"citeStart": 15, "citeEnd": 25, "citeStartToken": 15, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ( (Turney, 2002; Pang et al., 2002) ). Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999) . Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in arti-cles that are not editorials or reviews) were subjective.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ( (Turney, 2002; Pang et al., 2002) ). ", "mid_sen": "Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. ", "after_sen": "Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). "}
{"citeStart": 100, "citeEnd": 125, "citeStartToken": 100, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Inspired by research in the. areas of semantic relations, semantic distance, concept clustering, and using (,once I tual (Ji a l hs (Sowa, 1984) as our knowledge representation, we introduce (;oncept (?lustering I{nowledge Graphs (CCKGs). Each (JCKG will start as a Conceptual Graph representation of a trigger word and will expaud following a search algorit, hm to incorporate related words and ibrm a C'oncept Cn,s(,er. The concept chlstcr in itself is interesting for tasks such as word disambiguation, but the C(~K(] will give more to that cluster. It will give the relations between the words, making the graph in some aspects similar to a script (Schank and Abelson, 11975) . llowever, a CCK(I is generated automaticMly and does not rely on prin,itives but on an unlimited number of concel)ts , showing objects, persons, and actions interacting with each other. This interaction will be set, within a lmrtieular domain, and the trigger word should be a key word of the domain to represent. 11' that process would be done for the whole dictionary, we would obtain an l,l( II divided into multiple clusters of words, each represented by a CCK(]. Then during text processing fin: example, a portion of text could be analyzed using the appropriate CCK(] to lind implicit relations and hell) understanding the text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The concept chlstcr in itself is interesting for tasks such as word disambiguation, but the C(~K(] will give more to that cluster. ", "mid_sen": "It will give the relations between the words, making the graph in some aspects similar to a script (Schank and Abelson, 11975) . llowever, a CCK(I is generated automaticMly and does not rely on prin,itives but on an unlimited number of concel)ts , showing objects, persons, and actions interacting with each other. ", "after_sen": "This interaction will be set, within a lmrtieular domain, and the trigger word should be a key word of the domain to represent. "}
{"citeStart": 70, "citeEnd": 90, "citeStartToken": 70, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared our model to the original HMM model, identical in implementation to our syntactic HMM model save the distortion component. Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations. Both models were then combined with an independently trained HMM model in the opposite direction: f → e. 4 Table 1 summarizes the results; the two models perform similarly. The main benefit of our model is the effect on rule extraction, discussed below. We also compared our French results to the public baseline GIZA++ using the script published for the NAACL 2006 Machine Translation Workshop Shared Task. 5 Similarly, we compared our Chinese results to the GIZA++ results in Ayan and Dorr (2006) . Our models substantially outperform GIZA++, confirming results in Liang et al. (2006) . Table 2 shows the effect on AER of competitive thresholding and different combination functions.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also compared our French results to the public baseline GIZA++ using the script published for the NAACL 2006 Machine Translation Workshop Shared Task. ", "mid_sen": "5 Similarly, we compared our Chinese results to the GIZA++ results in Ayan and Dorr (2006) . ", "after_sen": "Our models substantially outperform GIZA++, confirming results in Liang et al. (2006) . "}
{"citeStart": 74, "citeEnd": 102, "citeStartToken": 74, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "Other previous NLP research has used features similar to ours for other NLP tasks. Low-frequency words have been used as features in information extraction (Weeber, Vos, and Baayen 2000) and text categorization (Copeck et al. 2000) . A number of researchers have worked on mining collocations from text to extend lexicographic resources for machine translation and word sense disambiguation (e.g., Smajda 1993; Lin 1999; Biber 1993) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other previous NLP research has used features similar to ours for other NLP tasks. ", "mid_sen": "Low-frequency words have been used as features in information extraction (Weeber, Vos, and Baayen 2000) and text categorization (Copeck et al. 2000) . ", "after_sen": "A number of researchers have worked on mining collocations from text to extend lexicographic resources for machine translation and word sense disambiguation (e.g., Smajda 1993; Lin 1999; Biber 1993) ."}
{"citeStart": 87, "citeEnd": 104, "citeStartToken": 87, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "The rather spectacular looking precision and recall differences in unlexicalized training con-firm what was observed for the full frame set. From the first trained unlexicalized model throughout unlexicalized training, we find a steady increase in precision (70% first trained model to 78% final model) against a sharp drop in recall (78% peek in the second model vs. 50% in the final). Considering our above remarks on the difficulties of frame recognition in unlexicalized training, the sharp drop in recall is to be expected: Since recall measures the correct parser guesses against the annotator's baseline, the tendency to favor PP arguments over PP-adjuncts leads to a loss in guesses when PP-frames are abandoned. Similarly, the rise in precision is mainly explained by the decreasing number of guesses when cutting out non-PP frames. For further discussion of what happens with individual frames, we refer the reader to (Beil et al., 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similarly, the rise in precision is mainly explained by the decreasing number of guesses when cutting out non-PP frames. ", "mid_sen": "For further discussion of what happens with individual frames, we refer the reader to (Beil et al., 1998) .", "after_sen": "One systematic result in these plots is that performance of lexicalized training stabilizes after a few iterations. "}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Figure 1: Two analysis models and the associations they compare of left and right-branching compounds. Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time). In the test set used here and in that of Resnik (1993) , the proportion of leftbranching compounds is 67% and 64% respectively. In contrast, the adjacency model appears to predict a proportion of 50%.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figure 1: Two analysis models and the associations they compare of left and right-branching compounds. ", "mid_sen": "Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time). ", "after_sen": "In the test set used here and in that of Resnik (1993) , the proportion of leftbranching compounds is 67% and 64% respectively. "}
{"citeStart": 54, "citeEnd": 80, "citeStartToken": 54, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Magic makes filtering explicit through characterizing it as definite clauses. Intuitively understood, filtering is reversed as binding information that normally becomes available as a result of top-down evaluation is derived by bottom-up evaluation of the definite clause characterization of filtering. The following is the basic Magic algorithm taken from Ramakrishnan et al. (1992) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Intuitively understood, filtering is reversed as binding information that normally becomes available as a result of top-down evaluation is derived by bottom-up evaluation of the definite clause characterization of filtering. ", "mid_sen": "The following is the basic Magic algorithm taken from Ramakrishnan et al. (1992) .", "after_sen": "Let P be a program and q(E) a query on the program. "}
{"citeStart": 6, "citeEnd": 26, "citeStartToken": 6, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Oepen and Flickinger, 1998 stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Corpora thus are used only as an inspiration.", "mid_sen": "Oepen and Flickinger, 1998 stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "after_sen": ""}
{"citeStart": 1, "citeEnd": 13, "citeStartToken": 1, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. (Quinlan 1986; Bahl et al. 1989 ) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994) . We tested the Satz system using the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. These results are discussed in Section 4.10.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. ", "mid_sen": "(Quinlan 1986; Bahl et al. 1989 ) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994) . ", "after_sen": "We tested the Satz system using the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. "}
{"citeStart": 135, "citeEnd": 140, "citeStartToken": 135, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "We analyse the performance of the algorithms on three types of data. Two of the samples are those that Hobbs used when developing his algorithm. One is an excerpt from a novel and the other a sample of journalistic writing. The remaining sample is a set of 5 human-human, keyboard-mediated, task-oriented dialogues about the assembly of a plastic water pump [Coh84] . This covers only a subset of the above types. Obviously it would be instructive to conduct a similar analysis on other textual types.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One is an excerpt from a novel and the other a sample of journalistic writing. ", "mid_sen": "The remaining sample is a set of 5 human-human, keyboard-mediated, task-oriented dialogues about the assembly of a plastic water pump [Coh84] . ", "after_sen": "This covers only a subset of the above types. "}
{"citeStart": 20, "citeEnd": 39, "citeStartToken": 20, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . A number of other systems have addressed part of the task. Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. ", "mid_sen": "Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. ", "after_sen": "Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . "}
{"citeStart": 79, "citeEnd": 103, "citeStartToken": 79, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980) , Categorial Grammar (Ades and Steedman, 1982) , PATR-II (Shieber, 1986) , and lexicalized TAG (Schabes et al, 1988) all deal with complementation by including in one form or another a notion of \"subcategorization frame\" that specifies a sequence of complement phrases and constraints on them. Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could \"apply\" to.", "mid_sen": "Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980) , Categorial Grammar (Ades and Steedman, 1982) , PATR-II (Shieber, 1986) , and lexicalized TAG (Schabes et al, 1988) all deal with complementation by including in one form or another a notion of \"subcategorization frame\" that specifies a sequence of complement phrases and constraints on them. ", "after_sen": "Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain."}
{"citeStart": 42, "citeEnd": 63, "citeStartToken": 42, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "The chunker is based on our earlier work (Whitelaw et al., 2005) , which finds attitude groups and targets using a hand-built lexicon (Sec. 4). This lexi-con contains head adjectives (which specify values for the attributes attitude type, force, polarity, and orientation), and appraisal modifiers (which specify transformations to the four attributes). Some head adjectives are ambiguous, having multiple entries in the lexicon with different attribute values. In all cases, different entries for a given word have different attitude types. If the head adjective is ambiguous, multiple groups are created, to be disambiguated later. See our previous work (Whitelaw et al., 2005) for a discussion of the technique. Target groups are found by matching phrases in the lexicon with corresponding phrases in the text and assigning the target type listed in the lexicon.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The chunker is based on our earlier work (Whitelaw et al., 2005) , which finds attitude groups and targets using a hand-built lexicon (Sec. 4). ", "after_sen": "This lexi-con contains head adjectives (which specify values for the attributes attitude type, force, polarity, and orientation), and appraisal modifiers (which specify transformations to the four attributes). "}
{"citeStart": 68, "citeEnd": 93, "citeStartToken": 68, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "The ability to deal with large amomlts of possibly ill-formed text is one of the principal objectives of current NLP research. Recent proposals include the use of probabilistic methods (see e.g. Briseoe and Carroll, 1993) and large robust deterministic systems like Hindle's Fidditch (Hindle, 1989) . 4 Experience so far suggests that systems like LIIIP may in the right circumstances provide an alternative to these approaches. It combines the advantages of Prolog-interpreted DCGs (ease of modification, parser output suitable for direct use by other programs, etc.) with the ability to relax tile adjacency constraints of that form&llsm in a flexible and dynamic manner.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ability to deal with large amomlts of possibly ill-formed text is one of the principal objectives of current NLP research. ", "mid_sen": "Recent proposals include the use of probabilistic methods (see e.g. Briseoe and Carroll, 1993) and large robust deterministic systems like Hindle's Fidditch (Hindle, 1989) . ", "after_sen": "4 Experience so far suggests that systems like LIIIP may in the right circumstances provide an alternative to these approaches. "}
{"citeStart": 4, "citeEnd": 20, "citeStartToken": 4, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "The biggest problem for assigning semantic categories to words lies in the incompleteness of dictionaries. It is impractical to construct a dictionary that will contain all words that may occur in some previously unseen corpora. This issue is particularly problematic for natural language processing applications that work with Chinese texts. Specifically, for the Sinica Corpus 1 , Bai, Chen and Chen (1998) found that articles contain on average 3.51% words that were not listed in the Chinese Electronic Dictionary 2 of 80,000 words. Because novel words are created daily, it is impossible to collect them all. Furthermore, across most of the corpora, many of these newly coined words seem to be used only once, and thus they may not even be worth collecting. However, the occurrence of unknown words makes a number of NLP (Natural Language Processing) tasks such as segmentation and word sense disambiguation more difficult. Consequently, it would be valuable to have some means of automatically assigning meaning to unknown words. This paper describes a classifier that assigns semantic thesaurus categories to unknown Chinese words. The Caraballo (1999) 's system adopted the contextual information to assign nouns to their hyponyms. Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. While context is clearly an important feature, this paper focuses on non-contextual features, which may play a key role for unknown words that occur only once The Sinica Corpus is a balanced corpus contained five million part-of-speech words in Mandarin Chinese. The Chinese Electronic Dictionary is from the Computational Linguistics Society of R.O.C. and hence have limited context. The feature I focus on, following Ciaramita (2002) , is morphological similarity to words whose semantic category is known. Ciaramita (2002) boosted the lexical acquisition system by simple morphological rules and found a significant improvement. Such a finding suggests that a reliable source of semantic information lies in the morphology used to construct the unknown words. In Chinese morphology, the two ways to generate new words are compounding and affixation. Orthographically, such compounding and affixation is represented by combinations of characters, and as a result, the character combinations and the morpho-syntactic relationship used to link them together can be clues for classification. Furthermore, my analysis of the Sinica Corpus indicates that only 49.68% monosyllabic 3 words have one word class, but 91.67% multisyallabic words have one word class in Table 1 . Once characters merge together, only 8.33% words remain ambiguous. It implies that as characters are combined together, the degree of ambiguity tends to decrease.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper describes a classifier that assigns semantic thesaurus categories to unknown Chinese words. ", "mid_sen": "The Caraballo (1999) 's system adopted the contextual information to assign nouns to their hyponyms. ", "after_sen": "Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. "}
{"citeStart": 27, "citeEnd": 37, "citeStartToken": 27, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "A similar solution to the nontermination problem with unification grammars in Prolog is proposed in (Samuelsson, 1993) . In this method, an operation called anti-unification (often referred to as generalization as the counterpart of unification) is applied to the root and leaf terms of a cyclic propagation, and the resulting term is stored in the reachablity table as the result of applying restriction on both terms. Another approach taken in (Haas, 1989) eliminates the cyclic propagation by replacing the features in the root and leaf terms with new variables. The method proposed in this paper is more general than the above approaches: if the Selection ordering is imposed in the detection function, features in .restrictor. can be collected incrementally as the cyclic propagations are repeated. Thus, this method 7This scheme may be rather conservative. 8Note the cases in this section do not represent all possible situations. is able to create a less restrictive *restrictor. than these other approaches.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this method, an operation called anti-unification (often referred to as generalization as the counterpart of unification) is applied to the root and leaf terms of a cyclic propagation, and the resulting term is stored in the reachablity table as the result of applying restriction on both terms. ", "mid_sen": "Another approach taken in (Haas, 1989) eliminates the cyclic propagation by replacing the features in the root and leaf terms with new variables. ", "after_sen": "The method proposed in this paper is more general than the above approaches: if the Selection ordering is imposed in the detection function, features in .restrictor. "}
{"citeStart": 4, "citeEnd": 25, "citeStartToken": 4, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "While high-quality NLP corpora and tools are available in English, such resources are difficult to obtain in most other languages. Three challenges must be met when adapting results established in English to another language: (1) acquiring high quality annotated data; (2) adapting the English task definition to the nature of a different language, and (3) adapting the algorithm to the new language. This paper presents a case study in the adaptation of a well known task to a language with few NLP resources available. Specifically, we deal with SVM based Hebrew NP chunking. In (Goldberg et al., 2006) , we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify 'suspicious' data points in the corpus, and identify how robust the model is in the presence of noise;", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Specifically, we deal with SVM based Hebrew NP chunking. ", "mid_sen": "In (Goldberg et al., 2006) , we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. ", "after_sen": "We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify 'suspicious' data points in the corpus, and identify how robust the model is in the presence of noise;"}
{"citeStart": 79, "citeEnd": 89, "citeStartToken": 79, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "18 Division and subtraction are also possible: −(p, v) = (−p, −v) and (p, v) −1 = (p −1 , −p −1 vp −1 ). Division is commonly used in defining f θ (for normalization). 19 Multiple edges from j to k are summed into a single edge. (Mohri, 2002) . Efficient hardware implementation is also possible via chip-level parallelism (Rote, 1985) .• In many cases of interest, T i is an acyclic graph. 20 Then Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations. For HMMs (footnote 11), T i is the familiar trellis, and we would like this computation of t i to reduce to the forwardbackward algorithm (Baum, 1972) . But notice that it has no backward pass. In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities. This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together. We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing t i (so they are needed only to construct T i ). This speedup also works for cyclic graphs and for any V . Write w jk as (p jk , v jk ), and let w 1 jk = (p 1 jk , v 1 jk ) denote the weight of the edge from j to k. 19 Then it can be shown thatThe forward and backward probabilities, p 0j and p kn , can be computed using single-source algebraic path for the simpler semiring (R, +, ×, * )-or equivalently, by solving a sparse linear system of equations over R, a much-studied problem at O(n) space, O(nm) time, and faster approximations (Greenbaum, 1997) .• A Viterbi variant of the expectation semiring exists:Here, the forward and backward probabilities can be computed in time only O(m + n log n) (Fredman and Tarjan, 1987) . k-best variants are also possible.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Mohri, 2002) . ", "mid_sen": "Efficient hardware implementation is also possible via chip-level parallelism (Rote, 1985) .• In many cases of interest, T i is an acyclic graph. ", "after_sen": "20 Then Tarjan's method computes w 0j for each j in topologically sorted order, thereby finding t i in a linear number of ⊕ and ⊗ operations. "}
{"citeStart": 103, "citeEnd": 121, "citeStartToken": 103, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we present a novel method that automatically extracts relations between full-form phrases and their abbreviations from monolingual corpora, and induces translation entries for these abbreviations by using their full-form as a bridge. Our method is scalable enough to handle large amount of monolingual data, and is essentially unsupervised as it does not require any additional annotated data than the baseline translation system. Our method exploits the data co-occurrence phenomena that is very useful for relation extractions. We integrate our method into a state-of-the-art phrase-based baseline translation system, i.e., Moses (Koehn et al., 2007) , and show that the integrated system consistently improves the performance of the baseline system on various NIST machine translation test sets.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our method exploits the data co-occurrence phenomena that is very useful for relation extractions. ", "mid_sen": "We integrate our method into a state-of-the-art phrase-based baseline translation system, i.e., Moses (Koehn et al., 2007) , and show that the integrated system consistently improves the performance of the baseline system on various NIST machine translation test sets.", "after_sen": "However, it is not a strict comparison because the dataset is different and the recall may also be different."}
{"citeStart": 406, "citeEnd": 417, "citeStartToken": 406, "citeEndToken": 417, "sectionName": "UNKNOWN SECTION NAME", "string": "The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993) , word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b ), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993) , and statistical translation (Brown et al. 1993 ). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it is less related to ours, addressing, for example, word sense disambiguation in the source language by statistically examining context (e.g., collocations) in the source language, thus allowing appropriate word selection in the target language. (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993) , word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b ), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993) , and statistical translation (Brown et al. 1993 ). ", "after_sen": "Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. "}
{"citeStart": 70, "citeEnd": 99, "citeStartToken": 70, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "100 sentences were selected from the DUC 2001 corpus with the topics \"illegal alien\", \"term limits\", \"gun control\", and \"NAFTA\". Two humans annotated the 100 sentences with three categories (positive, negative, and N/A). To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988 ). The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Two humans annotated the 100 sentences with three categories (positive, negative, and N/A). ", "mid_sen": "To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988 ). ", "after_sen": "The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable."}
{"citeStart": 56, "citeEnd": 78, "citeStartToken": 56, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "Evaluation. To analyze the impact of smoothing, we evaluate the coverage of models and the quality of their predictions separately. In both tasks, coverage is the percentage of items for which we make a prediction. We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items . For synonym choice, we follow the method established by Mohammad et al. (2007) , measuring accuracy over covered items, with partial credit for ties.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items . ", "mid_sen": "For synonym choice, we follow the method established by Mohammad et al. (2007) , measuring accuracy over covered items, with partial credit for ties.", "after_sen": "Results for Semantic Similarity. "}
{"citeStart": 82, "citeEnd": 100, "citeStartToken": 82, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily. Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. Luhn, 1958; Edmundson, 1969) or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g. Kupiec et al., 1995; Mani and Bloedorn, 1998) . Neither approach is very satisfactory. Relying only on your own intuitions inevitably creates a biased resource; indeed, Rath et al. (1961) report low agreement between human judges carrying out this kind of task. On the other hand, using abstracts as targets is not necessarily a good gold standard for comparison of the systems' results, although abstracts are the only kind of gold standard that comes for free with the papers. Even if the abstracts are written by professional abstractors, there are considerable differences in length, structure, and information content. This is due to differences in the common abstract presentation style in different disciplines and to the projected use of the abstracts (cf. Liddy, 1991) . In the case of our corpus, an additional problem was the fact that the abstracts are written by the authors themselves and thus susceptible to differences in individual writing style.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Neither approach is very satisfactory. ", "mid_sen": "Relying only on your own intuitions inevitably creates a biased resource; indeed, Rath et al. (1961) report low agreement between human judges carrying out this kind of task. ", "after_sen": "On the other hand, using abstracts as targets is not necessarily a good gold standard for comparison of the systems' results, although abstracts are the only kind of gold standard that comes for free with the papers. "}
{"citeStart": 36, "citeEnd": 51, "citeStartToken": 36, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "The same methodology can be applied to modeling conversational impIicatures in indirect replies (Green, 1992 ). Green's algorithm makes use of discourse expectations, discourse plans, and discourse relations. The following dialog is considered (Green, 1992, p. Answer (17) conveys a \"yes\", but a reply consisting only of (17)a would implicate a \"no\". As Green notices, in previous models of implicatures (Gazdar, 1979; Hirschberg, 1985) , processing (17)a will block the implicature generated by (17)c. Green solves the problem by extending the boundaries of the analysis to discourse units. Our approach does not exhibit these constraints. As in the previous example, the one dealing with a sequence of utterances, we obtain a different interpretation after each step. When the question is asked, there is no conversational implicature. Answer (17)a makes the necessary conditions for implicating \"no\" true, and the implication is computed. Answer (17)b reinforces a previous condition. Answer (17)c makes the preconditions for implicating a \"no\" false, and the preconditions for implicating a \"yes\" true. Therefore, the implicature at the end of the dialogue is that the conversant who answered went shopping.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Green's algorithm makes use of discourse expectations, discourse plans, and discourse relations. ", "mid_sen": "The following dialog is considered (Green, 1992, p. Answer (17) conveys a \"yes\", but a reply consisting only of (17)a would implicate a \"no\". ", "after_sen": "As Green notices, in previous models of implicatures (Gazdar, 1979; Hirschberg, 1985) , processing (17)a will block the implicature generated by (17)c. "}
{"citeStart": 75, "citeEnd": 89, "citeStartToken": 75, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "It is interesting to compare this technique to the restriction proposal in Shieber (1985) . Both approaches select functional features to be moved forward in processing order in the hope that some processing will be pruned. Shieber's approach changes the processing order of functional constraints so that some of them are processed top-down instead of bottom-up. Our approach takes a different tack, actually converting some of the functional constraints into phrasal constraints. Thus Shieber's does its pruning using functional mechanisms whereas our approach prunes via standard phrasal operations.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, if features are carefully selected so as to increase the amount of pruning done by the chart, the net effect may be that even though the grammar allows more types of constituents, the chart may end up with fewer instances.", "mid_sen": "It is interesting to compare this technique to the restriction proposal in Shieber (1985) . ", "after_sen": "Both approaches select functional features to be moved forward in processing order in the hope that some processing will be pruned. "}
{"citeStart": 22, "citeEnd": 43, "citeStartToken": 22, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "The most similar efforts to ours, mainly (DeNero et al., 2006) , conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. Based on this advise (Moore and Quirk, 2007) exclude the latent segmentation variables and opt for a heuristic training procedure. In this work we also start out from a generative model with latent segmentation variables. However, we find out that concentrating the learning effort on smoothing is crucial for good performance. For this, we devise ITG-based priors over segmentations and employ a penalized version of Deleted Estimation working with EM at its core. The fact that our results (at least) match the heuristic estimates on a reasonably sized data set (947k parallel sentence pairs) is rather encouraging.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The most similar efforts to ours, mainly (DeNero et al., 2006) , conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. ", "mid_sen": "Based on this advise (Moore and Quirk, 2007) exclude the latent segmentation variables and opt for a heuristic training procedure. ", "after_sen": "In this work we also start out from a generative model with latent segmentation variables. "}
{"citeStart": 99, "citeEnd": 119, "citeStartToken": 99, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "In the initial annotation phase, 13 % of our corpus was annotated twice by different annotators in order to measure inter-annotator agreement. We did this measurement as it was done for MUC (Hirschman et al., 1997) and in the same way as a coreference resolution system is evaluated against some gold standard: one annotation was set to be the gold standard (\"key\") and the second annotation was set to be the \"response\". Herewith we reached an inter-annotator agreement of 49.5 MUC points (for MUC score calculation see Vilain et al. (1995) ). Although the MUC measure is questionable (Luo, 2005) and the task is difficult, this number is too low and asked for improvements.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We did this measurement as it was done for MUC (Hirschman et al., 1997) and in the same way as a coreference resolution system is evaluated against some gold standard: one annotation was set to be the gold standard (\"key\") and the second annotation was set to be the \"response\". ", "mid_sen": "Herewith we reached an inter-annotator agreement of 49.5 MUC points (for MUC score calculation see Vilain et al. (1995) ). ", "after_sen": "Although the MUC measure is questionable (Luo, 2005) and the task is difficult, this number is too low and asked for improvements."}
{"citeStart": 1, "citeEnd": 21, "citeStartToken": 1, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Several groups have continued working with the Ramshaw and Marcus data sets for base noun phrases. (Argamon et al., 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. This method records POS tag sequences which contain chunk boundaries and uses these sequences to classify the test data. Its performance is somewhat worse than that of Ramshaw and Marcus (F~=1=91.6 vs. 92.0) but it is the best result obtained without using lexical information 6. (Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data. This approach performs worse than the method of Argamon et al. (F~=1=90.9).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several groups have continued working with the Ramshaw and Marcus data sets for base noun phrases. ", "mid_sen": "(Argamon et al., 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. ", "after_sen": "This method records POS tag sequences which contain chunk boundaries and uses these sequences to classify the test data. "}
{"citeStart": 333, "citeEnd": 350, "citeStartToken": 333, "citeEndToken": 350, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ( [Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 177, "citeEnd": 203, "citeStartToken": 177, "citeEndToken": 203, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to capture the agents' intentions conveyed by their utterances, our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in (Lambert and Carberry, 1991) to represent the current status of the interaction. The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for the user's later execution, the problem-solving level which contains the actions being performed to construct the do-n~n plan, the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions, and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994) . This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for engaging in collaborative negotiations.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In order to capture the agents' intentions conveyed by their utterances, our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in (Lambert and Carberry, 1991) to represent the current status of the interaction. ", "after_sen": "The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for the user's later execution, the problem-solving level which contains the actions being performed to construct the do-n~n plan, the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions, and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994) . "}
{"citeStart": 79, "citeEnd": 100, "citeStartToken": 79, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The biggest difference between this work and theirs is in what the links represent linguistically. ", "mid_sen": "Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). ", "after_sen": "Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . "}
{"citeStart": 118, "citeEnd": 138, "citeStartToken": 118, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "A distinctive aspect of CCG is that it provides a very flexible notion of constituency. This supports elegant analyses of several phenomena (e.g., coordination, long-distance extraction, and intonation) and allows incremental parsing with the competence grammar (Steedman, 2000) . Here, we argue that even with its flexibility, CCG as standardly defined is not permissive enough for certain linguistic constructions and greater incrementality. Following Wittenburg (1987) , we remedy this by adding a set of rules based on the D combinator of combinatory logic (Curry and Feys, 1958) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here, we argue that even with its flexibility, CCG as standardly defined is not permissive enough for certain linguistic constructions and greater incrementality. ", "mid_sen": "Following Wittenburg (1987) , we remedy this by adding a set of rules based on the D combinator of combinatory logic (Curry and Feys, 1958) .", "after_sen": "(1) x/(y/z) :f y/w : g ⇒ x/(w/z): λh.f (λx.ghx)"}
{"citeStart": 153, "citeEnd": 177, "citeStartToken": 153, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . A number of other systems have addressed part of the task. Com-mandTalk (Moore et al., 1997) , Circuit Fix-It Shop (Smith, 1997) and (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999 ) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995) . In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is converted into text. Text either from the recognizer or directly input by the user is then converted into some kind of logical formula, which abstractly represents the user's intended command; this formula is then fed into a command interpreter, which executes the command.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. ", "mid_sen": "More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999) . ", "after_sen": "A number of other systems have addressed part of the task. "}
{"citeStart": 225, "citeEnd": 239, "citeStartToken": 225, "citeEndToken": 239, "sectionName": "UNKNOWN SECTION NAME", "string": "The most relevant prior work is (Wiebe et al. 98) , who dealt with meeting scheduling dialogs (see also (Alexandersson et al. 97) , (Busemann et al. 97)) , where the goal is to schedule a time for the meeting. The temporal references in meeting scheduling are somewhat more constrained than in news, where (e.g., in a historical news piece on toxic dumping) dates and times may be relatively unconstrained. In addition, their model requires the maintenance of a focus stack. They obtained roughly .91 Precision and .80 Recall on one test set, and .87 Precision and .68 Recall on another. However, they adjust the reference time during processing, which is something that we have not yet addressed. More recently, (Setzer and Gaizauskas 2000) have independently developed an annotation scheme which represents both time values and more fine-grained interevent and event-time temporal relations. Although our work is much more limited in scope, and doesn't exploit the internal structure of events, their annotation scheme may be leveraged in evaluating aspects of our work. The MUC-7 task (MUC-7 98) did not require VALs, but did test TIMEX recognition accuracy. Our 98 F-measure on NYT can be compared for just TIMEX with MUC-7 (MUC-7 1998) results on similar news stories, where the best performance was .99 Precision and .88 Recall. (The MUC task required recognizing a wider variety of TIMEXs, including event-dependent ones. However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. ) Finally, there is a large body of work, e.g., (Moens and Steedman 1988) , (Passoneau 1988) , (Webber 1988) , (Hwang 1992) , (Song and Cohen 1991) , that has focused on a computational analysis of tense and aspect. While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(The MUC task required recognizing a wider variety of TIMEXs, including event-dependent ones. ", "mid_sen": "However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. ) Finally, there is a large body of work, e.g., (Moens and Steedman 1988) , (Passoneau 1988) , (Webber 1988) , (Hwang 1992) , (Song and Cohen 1991) , that has focused on a computational analysis of tense and aspect. ", "after_sen": "While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work."}
{"citeStart": 20, "citeEnd": 31, "citeStartToken": 20, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods are known that can parse languages generated by Tree Adjoining Grammars (TAGs) in worst case time O(n~), where n is the length of the input string (see (Schabes and Joshi, 1991) and references therein). Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995) , these methods are not of practical interest, due to large hidden constants. More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although asymptotically faster methods can be constructed, as discussed in (Rajasekaran and Yooseph, 1995) , these methods are not of practical interest, due to large hidden constants. ", "mid_sen": "More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants.", "after_sen": "A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). "}
{"citeStart": 23, "citeEnd": 51, "citeStartToken": 23, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules. We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product). We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences. Let b, a 1 , a 2 , . . . a n or b, A denote the contents of an itemset, and c ( b, A ) denote the support for this itemset. For a given item b, π(b) denotes its immediate parent its value taxonomy, or 'root' for flat sets. For each item set, we collect rules b, A and compute their interestingness relative to the itemset π(b), A . Interestingness is defined as follows:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We generally look for rules that contain attitude type, orientation, thing type, and a product name, when these rules occur more frequently than expected.", "mid_sen": "The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules. ", "after_sen": "We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product). "}
{"citeStart": 59, "citeEnd": 79, "citeStartToken": 59, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "In Mel'~nk's Explanatory Combinatory Dictionary (ECD, see (Mel'~uk et al., 1984) ), expressions such as uneJerme intention, une rdsistance acharnde, un argument de poids, un bruit it~fernal and donner une lefon, faire un pas, commetre un crime are described in the lexical combinatorics zone. These \"expressions plus ou moins fig6es\" will be called 'collocations'. They are considered to consist of two parts --the base and the collocate. In the examples above, the nouns are the bases and the adjectives and the verbs are the collocates. The idea that all adjective collocates and all the verb 2Head Drivt~n Phrase SlltlCItlrc granllllar, see (Pollard and Sag, 1987) , (Pollard and Sag, to appear) . For another treatment ,:ff collo cations in HPSG, see (Krenn and ltrbach, to appear) . collocates share an important meaning component --roughly paraphrasable as intense and do respectively -and the fact that the adjectives and verbs are not interchangeable but are restricted with this meaning to the accompanying nouns, is coded in the dictionary using lcxical functions (in this case Magn and Oper).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In Mel'~nk's Explanatory Combinatory Dictionary (ECD, see (Mel'~uk et al., 1984) ), expressions such as uneJerme intention, une rdsistance acharnde, un argument de poids, un bruit it~fernal and donner une lefon, faire un pas, commetre un crime are described in the lexical combinatorics zone. ", "after_sen": "These \"expressions plus ou moins fig6es\" will be called 'collocations'. "}
{"citeStart": 31, "citeEnd": 65, "citeStartToken": 31, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "As mentioned above, we intend to be able to compare a clustering generated by a system against one provided by an expert. Since a word can occur in more than one class, it is important to find some kind of mapping between the classes generated by the system and the classes given by the expert. Such a mapping tells us which class in the system's clustering maps to which one in the expert's clustering, and an overall comparison of the clusterings is based on the comparison of the mutually mapping classes. Before we delve deeper into the evaluation process, we must decide on some measure of \"closeness\" between a pair of classes. We have adopted the F-measure (Hatzivassiloglou and McKeown, 1993; Chincor, 1992) . In our computation of the Fmeasure, we construct a contingency table based on the presence or absence of individual elements in the two classes being compared, as opposed to basing it on pairs of words. For example, suppose that Class A is generated by the system and Class B is provided by an expert (as shown in Table 1 ). The contingency table obtained for this pair of classes is shown in Table 2 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Before we delve deeper into the evaluation process, we must decide on some measure of \"closeness\" between a pair of classes. ", "mid_sen": "We have adopted the F-measure (Hatzivassiloglou and McKeown, 1993; Chincor, 1992) . ", "after_sen": "In our computation of the Fmeasure, we construct a contingency table based on the presence or absence of individual elements in the two classes being compared, as opposed to basing it on pairs of words. "}
{"citeStart": 10, "citeEnd": 36, "citeStartToken": 10, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "Following Lekakos and Giaglis (2007) , one approach for achieving this objective consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases. However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). Therefore, such an approach would require the utilization of subjective heuristics for creating labels, which would significantly influence what is being learned. Instead, we adopt an unsupervised approach that finds patterns in the data-confidence values coupled with performance scores (Section 6.1)-and then attempts to fit unseen data to these patterns (Section 6.2). Heuristics are still needed in order to decide which response-generation method to apply to an unseen case, but they are applied only after the learning is complete (Section 6.3). In other words, the subjective process of setting performance criteria (which should be conducted by the organization running the helpdesk) does not influence the machine learning process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These predictions enable our system to recommend a particular method for handling a new (unseen) request (Marom, Zukerman, and Japkowicz 2007) .", "mid_sen": "Following Lekakos and Giaglis (2007) , one approach for achieving this objective consists of applying supervised learning, where a winning method is selected for each case in the training set, all the training cases are labeled accordingly, and then the system is trained to predict a winner for unseen cases. ", "after_sen": "However, in our situation, there is not always one single winner (two methods can perform similarly well for a given request), and there are different ways to pick winners (for example, based on F-score or precision). "}
{"citeStart": 55, "citeEnd": 73, "citeStartToken": 55, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "In Table 4 we compare the results of CR-CNN and CNN+Softmax. CR-CNN outperforms CNN+Softmax in both precision and recall, and improves the F1 by 1.6. The third line in Table 4 shows the result reported by Zeng et al. (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax). We believe that the word embeddings employed by them is the main reason their result is much worse than that of CNN+Softmax. We use word embeddings of size 400 while they use word embeddings of size 50, which were trained using much less unlabeled data than we did.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "CR-CNN outperforms CNN+Softmax in both precision and recall, and improves the F1 by 1.6. ", "mid_sen": "The third line in Table 4 shows the result reported by Zeng et al. (2014) when only word embeddings and WPEs are used as input to the network (similar to our CNN+Softmax). ", "after_sen": "We believe that the word embeddings employed by them is the main reason their result is much worse than that of CNN+Softmax. "}
{"citeStart": 92, "citeEnd": 93, "citeStartToken": 92, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "Another focus of current research has been [3] the modelling of speaker and listener goals (1, 3) but there has been little research on real dialogues investigating how goals are communicated and inferred. This study identifies surface linguistic phenomena which reflect the", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finally there was evidence for high level structures in these dialogues as evidenced by topic initiation and control, with early topics being initiated and dominated by the client and the opposite being true for the later parts.", "mid_sen": "Another focus of current research has been [3] the modelling of speaker and listener goals (1, 3) but there has been little research on real dialogues investigating how goals are communicated and inferred. ", "after_sen": "This study identifies surface linguistic phenomena which reflect the"}
{"citeStart": 198, "citeEnd": 212, "citeStartToken": 198, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997 ) with the Alembic system (Aberdeen et al. 1995) : a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989) , who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000) . We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989) , who trained a decision tree classifier on a 25-million-word corpus. ", "mid_sen": "In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000) . ", "after_sen": "We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus."}
{"citeStart": 69, "citeEnd": 97, "citeStartToken": 69, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section, we describe each of the components of our SFST system shown in Figure 1 . The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005 ).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section, we describe each of the components of our SFST system shown in Figure 1 . ", "mid_sen": "The SFST approach described here is similar to the one described in (Bangalore and Riccardi, 2000) which has subsequently been adopted by (Banchs et al., 2005 ).", "after_sen": ""}
{"citeStart": 256, "citeEnd": 283, "citeStartToken": 256, "citeEndToken": 283, "sectionName": "UNKNOWN SECTION NAME", "string": "to detransform the parse trees produced using the PCFG estimated from trees transformed by T . By calculating the labelled precision and recall scores for the detransformed trees in the usual manner, we can systematically compare the parsing accuracy of di erent kinds of stochastic generalized left-corner parsers. Table 5 presents the results of this comparison. As reported previously, the standard left-corner grammar embeds su cient non-local information in its productions to signi cantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransformed trees Manning and Carpenter, 1997; Roark and Johnson, 1999 . Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which t o d escribe this non-local information. There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy Johnson, 1998b . Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak 1997 and Collins 1997 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 5 presents the results of this comparison. ", "mid_sen": "As reported previously, the standard left-corner grammar embeds su cient non-local information in its productions to signi cantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransformed trees Manning and Carpenter, 1997; Roark and Johnson, 1999 . ", "after_sen": "Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which t o d escribe this non-local information. "}
{"citeStart": 242, "citeEnd": 254, "citeStartToken": 242, "citeEndToken": 254, "sectionName": "UNKNOWN SECTION NAME", "string": "It is interesting to note that a textbook method for conslructing decision trees for classification from attribute-value pairs is to minimize the (weighted average of the) remaining entropy 5 over all possible choices of root attribute, see [Quinlan 1986 ].", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Cut up the training examples by matching them against the and-or tree and cutting at the determined cutnodes.", "mid_sen": "It is interesting to note that a textbook method for conslructing decision trees for classification from attribute-value pairs is to minimize the (weighted average of the) remaining entropy 5 over all possible choices of root attribute, see [Quinlan 1986 ].", "after_sen": "4This can most easily be seen as follows: "}
{"citeStart": 136, "citeEnd": 164, "citeStartToken": 136, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of appraisal extraction is a generalization of problem formulations developed in earlier works. Mullen and Collier's (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions. In previous work (Whitelaw et al., 2005) , we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases. The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. Nigam and Hurst's (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni's (2005) opinion mining technique also fits well into our framework.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. ", "mid_sen": "Nigam and Hurst's (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni's (2005) opinion mining technique also fits well into our framework.", "after_sen": "In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense disambiguation. "}
{"citeStart": 176, "citeEnd": 199, "citeStartToken": 176, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . ", "after_sen": "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. "}
{"citeStart": 168, "citeEnd": 196, "citeStartToken": 168, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of using a bridge (i.e., full-form) to obtain translation entries for unseen words (i.e., abbreviation) is similar to the idea of using paraphrases in MT (see Callison-Burch et al. (2006) and references therein) as both are trying to introduce generalization into MT. At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To the best of our knowledge, our work is the first to systematically model Chinese abbreviation expansion to improve machine translation.", "mid_sen": "The idea of using a bridge (i.e., full-form) to obtain translation entries for unseen words (i.e., abbreviation) is similar to the idea of using paraphrases in MT (see Callison-Burch et al. (2006) and references therein) as both are trying to introduce generalization into MT. ", "after_sen": "At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein)."}
{"citeStart": 79, "citeEnd": 99, "citeStartToken": 79, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to learn with features generated using these definitions as input, it is important that features generated when applying the definitions on different ISs are given the same identification. In this presentation we assume that the composition operator along with the appropriate IS element (e.g., Ex. 2, Ex. 9) are written explicitly as the identification of the features. Some of the subtleties in defining the output representation are addressed in (Cumby and Roth, 2000) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this presentation we assume that the composition operator along with the appropriate IS element (e.g., Ex. 2, Ex. 9) are written explicitly as the identification of the features. ", "mid_sen": "Some of the subtleties in defining the output representation are addressed in (Cumby and Roth, 2000) .", "after_sen": ""}
{"citeStart": 185, "citeEnd": 210, "citeStartToken": 185, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "In this work, we use Racing to speed up direct search for SMT, but this requires two main adjustments compared to the LOOCV case. First, our models have real-valued parameters, so we cannot exhaustively evaluate the set of all models since it is infinite. Instead, we use direct search to select which models compete against each other during Racing. In the case of Powell's method, all points of a grid along the current search direction are evaluated in parallel using Racing, before we turn to the next line search. In the case of the downhill simplex optimizer and in the case of line searches other than grid search (e.g., golden section search), the use of Racing is more difficult because the function evaluations requested by these optimizers have dependencies that generally prevent concurrent function evaluations. Since functions in downhill simplex are evaluated in sequence and not in parallel, our solution is to race the current model against our current best model. 4 When the evaluation of a model M is interrupted because it is deemed significantly worse than the current best modelM , the error rate of M on the entire development set is extrapolated from its relative performance on the decoded subset. 5 The second main difference with the LOOCV case is that we do not use confidence intervals to determine which of two or models are best. In SMT, it is common to use either bootstrap resampling (Efron and Tibshirani, 1993; Och, 2003) or randomization tests (Noreen, 1989) . In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors 6 than bootstrap methods (Riezler and Maxwell, 2005) . Since both kinds of statistical tests involve a time-consuming sampling step, it 4 Since Racing only discards suboptimal models, the current best model M * is one for which we have decoded the entire development set. Once a new model M is evaluated, we perform at step j a significance test to determine whether M 's translation of sentences 1 . . . j is better or worse than M * translation for the same range of sentences. If M is significantly worse, we discard it. If M * is worse, we continue evaluating the performance of M , since we need M 's output for the full development set if M eventually becomes the new best model. 5 For example, if error rates ofM and M are respectively 10% and 11% on the subset decoded by both models andM 's error on the entire set is 20%, M 's extrapolated error is 22%.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In SMT, it is common to use either bootstrap resampling (Efron and Tibshirani, 1993; Och, 2003) or randomization tests (Noreen, 1989) . ", "mid_sen": "In this paper, we use the randomization test for discarding unpromising models, since this statistical test was shown to be less likely to cause type-I errors 6 than bootstrap methods (Riezler and Maxwell, 2005) . ", "after_sen": "Since both kinds of statistical tests involve a time-consuming sampling step, it 4 Since Racing only discards suboptimal models, the current best model M * is one for which we have decoded the entire development set. "}
{"citeStart": 4, "citeEnd": 25, "citeStartToken": 4, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "See Johnson et al. (1999) for details of the calculation of this quantity and its derivatives, and the conjugate gradient routine used to calculate the )~ which maximize the regularized log pseudo-likelihood of the training corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "m ~2 logPL~(~) -~ 2\"~2 (5) j=l vj", "mid_sen": "See Johnson et al. (1999) for details of the calculation of this quantity and its derivatives, and the conjugate gradient routine used to calculate the )~ which maximize the regularized log pseudo-likelihood of the training corpus.", "after_sen": ""}
{"citeStart": 127, "citeEnd": 149, "citeStartToken": 127, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "The committee membership information that we used in the experiments is from Stewart and Woon's committee assignment codebook (Stewart and Woon, 2005) . This provided us with a roster for each committee and rank and seniority information for each member. In our experiments we use the rank within party and committee seniority member attributes to test the output of our pipeline. The rank within party attribute orders the members of a committee based on the Resolution that appointed the members with the highest ranking members having the lowest number. The chair and ranking members always receive a rank of 1 within their party. A committee member's committee seniority attribute corresponds to the number of years that the member has served on the given committee.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The committee membership information that we used in the experiments is from Stewart and Woon's committee assignment codebook (Stewart and Woon, 2005) . ", "after_sen": "This provided us with a roster for each committee and rank and seniority information for each member. "}
{"citeStart": 64, "citeEnd": 76, "citeStartToken": 64, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we adopt the second-order sibling factorization (Eisner, 1996; , in which each sibling part consists of a tuple of indices (h, m, c) where (h, m) and (h, c) are a pair of adjacent edges to the same side of the head h. By adding labele information to this factorization, Score(x, y) can be rewritten as:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here Y(x) denotes the set of possible dependency trees for sentence x.", "mid_sen": "In this paper, we adopt the second-order sibling factorization (Eisner, 1996; , in which each sibling part consists of a tuple of indices (h, m, c) where (h, m) and (h, c) are a pair of adjacent edges to the same side of the head h. By adding labele information to this factorization, Score(x, y) can be rewritten as:", "after_sen": "EQUATION"}
{"citeStart": 55, "citeEnd": 79, "citeStartToken": 55, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Juan le dio pufialadas a Marla (ii) Juan le dio una pufialada a Marla Both of these sentences translate literally to \"John gave stab wound(s) to Mary.\" However, the first sentence is the repetitive version of the action (i.e., there were multiple stab wounds), whereas the second sentence is the non-repetitive version of the action (i. e., there was only one stab wound). This distinction is characterized by means of the atomicity feature. In (26)(i), the event is associated with the features [+d,+t,-a], whereas, in (26)(ii) the event is associated with the features [+d,+t,+a] . According to [Bennett et al., 1990] (in the spirit of [Moens and Steedman, 1988] ), predicates are allowed to undergo an atomicity \"coercion\" in which an inherently non-atomic predicate (such as dio) may become atomic under certain conditions. These conditions are language-specific in nature, i.e., they depend on the lexical-semantic structure of the predicate in question. Given the featural scheme that is imposed on top of the lexical-semantic framework, it is easy to specify coercion functions for each language. For example, the atomicity function for the stab example would specify that a singular NP verbal object maps a [+d,-a] predicate into a [+d,+a] predicate i.e., a non-atomic event becomes atomic if it is associated with a singular NP object. Thus, the notion of feature-based coercion is cross-linguistically applicable, providing a useful foundation for a model of interlingual machine translation.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (26)(i), the event is associated with the features [+d,+t,-a], whereas, in (26)(ii) the event is associated with the features [+d,+t,+a] . ", "mid_sen": "According to [Bennett et al., 1990] (in the spirit of [Moens and Steedman, 1988] ), predicates are allowed to undergo an atomicity \"coercion\" in which an inherently non-atomic predicate (such as dio) may become atomic under certain conditions. ", "after_sen": "These conditions are language-specific in nature, i.e., they depend on the lexical-semantic structure of the predicate in question. "}
{"citeStart": 1, "citeEnd": 14, "citeStartToken": 1, "citeEndToken": 14, "sectionName": "UNKNOWN SECTION NAME", "string": "The linguistic statements made by developers of current grammar checkers based on NLP ted> niques are often contradictory regarding the types of errors that grammar checkers must correct automatically. (Veronis, 1988) claims that native writers are unlikely to produce errors involving morphological features, while (Vosse, 1992) acce.t)ts such morpho-syntactic errors, in spite of tile fact that an examination of texts by the author revealed that their appearance in native writer's texts is not frequent. Both authors agree in characterizing morpho-syntactic errors as a sainple of lack of competence. On the other hand, an examination of real texts produced by Spanish writers revealed that they do produce morpho-syntactic errors I . Spanish is an inflectiolml language, which increases the possibilities of such exrors. Nevertheless, other errors related to structural configuration of the language ark: produced as well.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The linguistic statements made by developers of current grammar checkers based on NLP ted> niques are often contradictory regarding the types of errors that grammar checkers must correct automatically. ", "mid_sen": "(Veronis, 1988) claims that native writers are unlikely to produce errors involving morphological features, while (Vosse, 1992) acce.t)ts such morpho-syntactic errors, in spite of tile fact that an examination of texts by the author revealed that their appearance in native writer's texts is not frequent. ", "after_sen": "Both authors agree in characterizing morpho-syntactic errors as a sainple of lack of competence. "}
{"citeStart": 53, "citeEnd": 55, "citeStartToken": 53, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17, 18] or other non-CF grammars such as Tree Adjoining Grammars [19] . Hence we axe reusing and developing our theoretical [18] and experimental [38] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17, 18] or other non-CF grammars such as Tree Adjoining Grammars [19] . ", "mid_sen": "Hence we axe reusing and developing our theoretical [18] and experimental [38] approach in this much more general setting which is more likely to be effectively usable for natural language parsing.", "after_sen": "Furthermore, these extensions can also express, within the PDA model, non-left-to-fight behavior such as is used in island parsing [38] or in Shei]'s approach [26] • More generally they allow the formal analysis of agenda strategies, which we have not considered here. "}
{"citeStart": 214, "citeEnd": 234, "citeStartToken": 214, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data. This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features that have been shown to be optimal for word sense disambiguation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . ", "mid_sen": "A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). ", "after_sen": "In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. "}
{"citeStart": 36, "citeEnd": 54, "citeStartToken": 36, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "I have suggested that training corpus clustering can be used both to extend the effectiveness of a very general class of language models, and to provide evidence of whether a particular language model could benefit from extending it by hand to allow it to take better account of context. Clustering can be useful even when there is no reason to believe the training corpus naturally divides into any particular number of clusters on any extrinsic grounds. The experimental results presented show that clustering increases the (absolute) success rate of unigram and bigram language modeling for a particular ATIS task by up to about 12%, and that performance improves steadily as the number of clusters climbs towards 100 (probably a reasonable upper limit, given that there are only a few thousand training sentences). However, clusters do not improve trigram modeling at all. This is consistent with experience (Rayner et al, 1994) that, for the ATIS domain, trigrams model inter-word effects much better than bigrams do, but that extending the N-gram model beyond N = 3 is much less beneficial.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, clusters do not improve trigram modeling at all. ", "mid_sen": "This is consistent with experience (Rayner et al, 1994) that, for the ATIS domain, trigrams model inter-word effects much better than bigrams do, but that extending the N-gram model beyond N = 3 is much less beneficial.", "after_sen": "For N-rule modeling, clustering increases the success rate for both N = 1 and N = 2, although only by about half as much as for N-grams. "}
{"citeStart": 94, "citeEnd": 104, "citeStartToken": 94, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "The Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998) . Earlier versions of SNoW (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks. Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998) . Earlier versions of SNoW (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks. ", "after_sen": "Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction."}
{"citeStart": 197, "citeEnd": 217, "citeStartToken": 197, "citeEndToken": 217, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de ned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999) . Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sentences y f r o m a s e t Y, observed with empirical Input Reference model p 0 , property-functions vector with constant # , parses X(y) for each y in incomplete-data sample from Y. Output MLE model p on X. Procedure", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . ", "after_sen": "For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. "}
{"citeStart": 92, "citeEnd": 100, "citeStartToken": 92, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Bird & Klein 93 argue against the use of two-level morphology because of linguistic considerations. The linguistic background of twolevel rules--main stream segmental phonology-has widely been rejected as a valid linguistic model. Instead, they base their implementation on autosegmental phonology (of. Goldsmith 90). This is certainly linguistically appealing. But there are reasons for sticking to a more conservative approach. Finite-state morphology as a formalism is not necessarily tied to segmental phonology. There are various approaches to cope with non-concatenative phenomena--one of tlmm X2MoltF (Trost 90) . Also, for a nmnber of languages complete sets of two-level rules do exist and can immediately be brought to bear. Finally, finite-state morphology has proven to be eflicient while the method proposed by Bird ~: Klein 93 seems to be computationally costly.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finite-state morphology as a formalism is not necessarily tied to segmental phonology. ", "mid_sen": "There are various approaches to cope with non-concatenative phenomena--one of tlmm X2MoltF (Trost 90) . ", "after_sen": "Also, for a nmnber of languages complete sets of two-level rules do exist and can immediately be brought to bear. "}
{"citeStart": 106, "citeEnd": 125, "citeStartToken": 106, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996) . Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in (Zhang and Clark, 2007) and (Jiang et al., 2008a) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "where function segGEN maps character sequence c to the set of all possible segmentations of c. For example, w = (c 1 ..c l 1 )...(c n−l k +1 ...c n ) represents a segmentation of k words and the lengths of the first and last word are l 1 and l k respectively.", "mid_sen": "In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996) . ", "after_sen": "Exact search is possible with a Viterbi-style algorithm, but beamsearch decoding is more popular as used in (Zhang and Clark, 2007) and (Jiang et al., 2008a) ."}
{"citeStart": 115, "citeEnd": 127, "citeStartToken": 115, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "POS tagging. POS information for the source and the target languages was considered for both translation tasks that we have participated. The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "POS information for the source and the target languages was considered for both translation tasks that we have participated. ", "mid_sen": "The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. ", "after_sen": "The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags."}
{"citeStart": 36, "citeEnd": 49, "citeStartToken": 36, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "We adopted the Q learning paradigm (Watkins, 1989; Mitchell, 1997) to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state. While in state s ∈ S and performing action a ∈ A, the learner receives a reward r(s, a), and advances to state s = δ(s, a).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We adopted the Q learning paradigm (Watkins, 1989; Mitchell, 1997) to model our problem as a set of possible states, S, and a set of actions, A, which can be performed to alter the current state. ", "after_sen": "While in state s ∈ S and performing action a ∈ A, the learner receives a reward r(s, a), and advances to state s = δ(s, a)."}
{"citeStart": 105, "citeEnd": 117, "citeStartToken": 105, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "2We could just as easily use other symmetric \"association\" measures, such as ¢2 or the Dice coefficient (Smadja, 1992 co-occur is called a direct association. Now, suppose that uk and Uk+z often co-occur within their language. Then vk and uk+l will also co-occur more often than expected by chance. The arrow connecting vk and u~+l in Figure 1 represents an indirect association, since the association between vk and Uk+z arises only by virtue of the association between each of them and uk. Models of translational equivalence that are ignorant of indirect associations have \"a tendency ... to be confused by collocates\" (Dagan et al., 1993) . Fortunately, indirect associations are usually not difficult to identify, because they tend to be weaker than the direct associations on which they are based (Melamed, 1996c) . The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest. The competitive linking algorithm implements this heuristic:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, n(u) = ~-~v n(u.v), which is not the same as the frequency of u, because each token of u can co-occur with several differentv's.", "mid_sen": "2We could just as easily use other symmetric \"association\" measures, such as ¢2 or the Dice coefficient (Smadja, 1992 co-occur is called a direct association. ", "after_sen": "Now, suppose that uk and Uk+z often co-occur within their language. "}
{"citeStart": 164, "citeEnd": 173, "citeStartToken": 164, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "The ideas sketched out above have been realized as a prototype spoken language dialogue interface to a simulated version of the Personal Satellite Assistant (PSA; (PSA, 2000) ). This section gives an overview of the implementation; in the following section, we focus on the specific aspects of dialogue management which are facilitated by the output/meta-output architecture.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A Prototype Implementation", "mid_sen": "The ideas sketched out above have been realized as a prototype spoken language dialogue interface to a simulated version of the Personal Satellite Assistant (PSA; (PSA, 2000) ). ", "after_sen": "This section gives an overview of the implementation; in the following section, we focus on the specific aspects of dialogue management which are facilitated by the output/meta-output architecture."}
{"citeStart": 59, "citeEnd": 79, "citeStartToken": 59, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "We have generalised the method used in our previous study (Sharoff et al., 2006) for extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search space for each word and its dictionary translations with entries from automatically computed thesauri, and then checks which combinations are possible in target corpora. These potential translation equivalents are then ranked by their similarity to the original query and presented to the user. The range of retrievable equivalents is now extended from a relatively limited range of two-word constructions which mirror POS categories in SL and TL to a much wider set of co-occurring lexical content items, which may appear in a different order, at some distance from each other, and belong to different POS categories.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We have generalised the method used in our previous study (Sharoff et al., 2006) for extracting equivalents for continuous multiword expressions (MWEs). ", "after_sen": "Essentially, the method expands the search space for each word and its dictionary translations with entries from automatically computed thesauri, and then checks which combinations are possible in target corpora. "}
{"citeStart": 179, "citeEnd": 196, "citeStartToken": 179, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "If on the other hand we a.ssume that the random variables are independe'~l, we only need to calculate and compare t~,: (X~,iH, = telescope) and Pgi,'t(.\\'with = telescope) (c.f. (Li and Abe., 1995) ). The independence assumption can also be made in the case of a class-based model or a slot-based model. For slot-based models, with tile independence assumption, P.~(X,~,ith = 1) and", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "If on the other hand we a.ssume that the random variables are independe'~l, we only need to calculate and compare t~,: (X~,iH, = telescope) and Pgi,'t(.\\'with = telescope) (c.f. (Li and Abe., 1995) ). ", "after_sen": "The independence assumption can also be made in the case of a class-based model or a slot-based model. "}
{"citeStart": 49, "citeEnd": 59, "citeStartToken": 49, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The second column of Table 2 gives the results of the POS-based model, the third column gives the results of incorporating the detection and correction of speech repairs and detection of intonational phrase boundary tones, and the fourth column gives the results of adding in silence information. As can be seen, modeling the user's utterances improves POS tagging, identification of discourse markers, and word perplexity; with the POS error rate decreasing by 3.1% and perplexity by 5.3%. Furthermore, adding in silence information to help detect the boundary tones and speech repairs results in a further improvement, with the overall POS tagging error rate decreasing by 8.6% and reducing perplexity by 7.8%. In contrast, a word-based trigram backoff model (Katz, 1987) built with the CMU statistical language modeling toolkit (Rosenfeld, 1995) achieved a perplexity of 26.13. Thus our full language model results in 14.1% reduction in perplexity. Table 3 gives the results of detecting intonational boundaries. The second column gives the results of adding the boundary tone detection to the POS model, the third column adds silence information, Table 4 : Detecting and Correcting Speech Repairs and the fourth column adds speech repair detection and correction. We see that adding in silence information gives a noticeable improvement in detecting boundary tones. Furthermore, adding in the speech repair detection and correction further improves the results of identifying boundary tones. Hence to detect intonational phrase boundaries in spontaneous speech, one should also model speech repairs. Table-4 gives the results of detecting and correcting speech repairs. The detection results report the number of repairs that were detected, regardless of whether the type of repair (e.g. modification repair versus abridged repair) was properly determined. The second column gives the results of adding speech repair detection to the POS model. The third column adds in silence information. Unlike the case for boundary tones, adding silence does not have much of an effect. 4 The fourth column adds in speech repair correction, and shows that taking into account the correction, gives better detection rates (Heeman, Loken-Kim, and Allen, 1996) . The fifth column adds in boundary tone detection, which improves both the detection and correction of speech repairs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, adding in silence information to help detect the boundary tones and speech repairs results in a further improvement, with the overall POS tagging error rate decreasing by 8.6% and reducing perplexity by 7.8%. ", "mid_sen": "In contrast, a word-based trigram backoff model (Katz, 1987) built with the CMU statistical language modeling toolkit (Rosenfeld, 1995) achieved a perplexity of 26.13. ", "after_sen": "Thus our full language model results in 14.1% reduction in perplexity. "}
{"citeStart": 106, "citeEnd": 127, "citeStartToken": 106, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared to a strong Baseline SRL system that learns a logistic regression model using the features of Pradhan et al. (2005) . It has two stages. The first filters out nodes that are unlikely to be arguments. The second stage labels each remaining node either as a particular role (e.g. \"ARGO\") or as a non-argument. Note that the baseline feature set includes a feature corresponding to the subcategorization of the verb (specifically, the sequence of nonterminals which are children of the predicate's parent node). Thus, Baseline does have access to something similar to our model's role pattern feature, although the Baseline subcategorization feature only includes post-verbal modifiers and is generally much noisier because it operates on the original sentence.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used the Charniak parses provided by the Conll distribution.", "mid_sen": "We compared to a strong Baseline SRL system that learns a logistic regression model using the features of Pradhan et al. (2005) . ", "after_sen": "It has two stages. "}
{"citeStart": 47, "citeEnd": 58, "citeStartToken": 47, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous studies on AVC have focused on using SCFs. Our experiments reveal that SCFs, at least when used alone, compare poorly to the feature sets that mix syntactic and lexical information. One explanation for the poor performance could be that we use all the frames generated by the CCG parser in our experiment. A better way of doing this would be to use some expert-selected SCF set. Levin classifies English verbs on the basis of 78 SCFs, which should, at least in principle, be good at separating verb classes. To see if Levin-selected SCFs are more effective for AVC, we match each SCF generated by the C&C CCG parser (CCG-SCF) to one of 78 Levin-defined SCFs, and refer to the resulting SCF set as unfiltered-Levin-SCF. Following studies on automatic SCF extraction (Brent, 1993) , we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs, and denote the resulting SCF set as filtered-Levin-SCF. We then perform the 48-way task (one of Levin48) with these two different SCF sets. Recall that using CCG-SCF gives us a macro-averaged recall of 39.1% on the 48-way task. Our experiments show that using unfiltered-Levin-SCF and filtered-Levin-SCF raises the accuracy to 39.7% and 40.3% respectively. Although a little performance gain has been obtained by using expert-defined SCFs, the accuracy level is still far below that achieved by using a feature set that combines syntactic and semantic information. In fact, even the simple co-occurrence feature (CO) yields a better performance (42.4%) than these Levin-selected SCF sets.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To see if Levin-selected SCFs are more effective for AVC, we match each SCF generated by the C&C CCG parser (CCG-SCF) to one of 78 Levin-defined SCFs, and refer to the resulting SCF set as unfiltered-Levin-SCF. ", "mid_sen": "Following studies on automatic SCF extraction (Brent, 1993) , we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs, and denote the resulting SCF set as filtered-Levin-SCF. ", "after_sen": "We then perform the 48-way task (one of Levin48) with these two different SCF sets. "}
{"citeStart": 280, "citeEnd": 299, "citeStartToken": 280, "citeEndToken": 299, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ([Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes \"these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 83, "citeEnd": 88, "citeStartToken": 83, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that these transitions are more specific than focus movement as described in [Sid83] . The extension we propose makes them more specific still. Note also that the Cb of [GJW86] corresponds roughly to Sidner's discourse focus and the Cf to her potential foci.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The interaction of centering with global focusing mechanisms and with other factors such as intentional structure, semantic selectional restrictions, verb tense and aspect, modality, intonation and pitch accent are topics for further research.", "mid_sen": "Note that these transitions are more specific than focus movement as described in [Sid83] . ", "after_sen": "The extension we propose makes them more specific still. "}
{"citeStart": 56, "citeEnd": 70, "citeStartToken": 56, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "In the co-training algorithm, a basic classification algorithm is required to construct C en and C cn . Typical text classifiers include Support Vector Machine (SVM), Naïve Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), etc. In this study, we adopt the widely-used SVM classifier (Joachims, 2002) . Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets. The English or Chinese features used in this study include both unigrams and bigrams and the feature weight is simply set to term frequency 6 . Feature selection methods (e.g. Document Frequency (DF), Information Gain (IG), and Mutual Information (MI)) can be used for dimension reduction. But we use all the features in the experiments for comparative analysis, because there is no significant performance improvement after applying the feature selection techniques in our empirical study. The output value of the SVM classifier for a review indicates the confidence level of the review's classification. Usually, the sentiment polarity of a review is indicated by the sign of the prediction value.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the co-training algorithm, a basic classification algorithm is required to construct C en and C cn . Typical text classifiers include Support Vector Machine (SVM), Naïve Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN), etc. ", "mid_sen": "In this study, we adopt the widely-used SVM classifier (Joachims, 2002) . ", "after_sen": "Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets. "}
{"citeStart": 45, "citeEnd": 65, "citeStartToken": 45, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "We choose an adapted LexRank as the baseline, considering that LexRank outperforms both centroid-based methods and other systems participating in Document Understanding Conferences (DUC) in most of the cases, and proves quite insensitive to the noise in the data. Note that the one-line summarization system (Sharifi et al., 2010) , which requires a given topic and focuses on the selection of key phrases most related to the topic, works on a setting different from ours.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We choose an adapted LexRank as the baseline, considering that LexRank outperforms both centroid-based methods and other systems participating in Document Understanding Conferences (DUC) in most of the cases, and proves quite insensitive to the noise in the data. ", "mid_sen": "Note that the one-line summarization system (Sharifi et al., 2010) , which requires a given topic and focuses on the selection of key phrases most related to the topic, works on a setting different from ours.", "after_sen": "In general, LexRank is a graph-based method for computing relative importance of textual units. "}
{"citeStart": 75, "citeEnd": 98, "citeStartToken": 75, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the direct output of the 42-topic model of the 105th-108th Senates from (Quinn et al., 2006) to further divide the speech documents into topic clusters. In their paper, they use a model where the probabilities of a document belonging to a certain topic varies smoothly over time and the words within a given document have exactly the same probability of being drawn from a particular topic. These two properties make the model different than standard mixture models (McLachlan and Peel, 2000) and the latent Dirichlet allocation model of (Blei et al., 2003) . The model of (Quinn et al., 2006) is most closely related to the model of (Blei and Lafferty, 2006) , who present a generalization of the model used by (Quinn et al., 2006) . Table 1 lists the 42 topics and their related committees.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These two properties make the model different than standard mixture models (McLachlan and Peel, 2000) and the latent Dirichlet allocation model of (Blei et al., 2003) . ", "mid_sen": "The model of (Quinn et al., 2006) is most closely related to the model of (Blei and Lafferty, 2006) , who present a generalization of the model used by (Quinn et al., 2006) . ", "after_sen": "Table 1 lists the 42 topics and their related committees."}
{"citeStart": 225, "citeEnd": 246, "citeStartToken": 225, "citeEndToken": 246, "sectionName": "UNKNOWN SECTION NAME", "string": "In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006) . In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. Both constraints have been shown to be in very good fit with data from dependency treebanks (Kuhlmann and Nivre, 2006) . However, like all other such proposals, they are formulated on fully specified structures, which makes it hard to integrate them into a generative model, where dependency structures are composed from elementary units of lexicalized information. Consequently, little is known about the generative capacity and computational complexity of languages over restricted non-projective dependency structures.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005) , but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006) .", "mid_sen": "In search of a balance between structural flexibility and computational complexity, several authors have proposed constraints to identify classes of non-projective dependency structures that are computationally well-behaved (Bodirsky et al., 2005; Nivre, 2006) . ", "after_sen": "In this paper, we focus on two of these proposals: the gap-degree restriction, which puts a bound on the number of discontinuities in the region of a sentence covered by a word and its dependents, and the well-nestedness condition, which constrains the arrangement of dependency subtrees. "}
{"citeStart": 159, "citeEnd": 173, "citeStartToken": 159, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture= our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. We will have little to say about its relation to parsing, leaving such questions for later research.1", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. ", "mid_sen": "Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture= our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. ", "after_sen": "We will have little to say about its relation to parsing, leaving such questions for later research."}
{"citeStart": 127, "citeEnd": 146, "citeStartToken": 127, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "The implementation of support vector regression used for these experiments is SVM-Light (Joachims, 1999) . We performed all experiments using the 2004 NIST Chinese MT Eval dataset. It consists of 447 source sentences that were translated by four human translators as well as ten MT systems. Each machine translated sentence was evaluated by two human judges for their fluency and adequacy on a 5-point scale 2 . To remove the bias in the distributions of scores between different judges, we follow the normalization procedure described by Blatz et al. (2003) . The two judge's total scores (i.e., sum of the normalized fluency and adequacy scores) are then averaged.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each machine translated sentence was evaluated by two human judges for their fluency and adequacy on a 5-point scale 2 . ", "mid_sen": "To remove the bias in the distributions of scores between different judges, we follow the normalization procedure described by Blatz et al. (2003) . ", "after_sen": "The two judge's total scores (i.e., sum of the normalized fluency and adequacy scores) are then averaged."}
{"citeStart": 93, "citeEnd": 121, "citeStartToken": 93, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996) . Acoustic clues have been successfully employed for the reliable detection of the speaker's emotions, including frustration, annoyance, anger, happiness, sadness, and boredom (Liscombe et al. 2003) . Devillers et al. (2003) performed perceptual tests with and without speech in detecting the speaker's fear, anger, satisfaction and embarrassment. Though related, our work is not concerned with the speaker's emotions, but rather opinions toward the issues and topics addressed in the meeting.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "extend this basic annotation scheme to include different types of subjectivity, including Positive Sentiment, Negative Sentiment, Positive Arguing, and Negative Arguing.", "mid_sen": "Speech was found to improve inter-annotator agreement in discourse segmentation of monologs (Hirschberg and Nakatani 1996) . ", "after_sen": "Acoustic clues have been successfully employed for the reliable detection of the speaker's emotions, including frustration, annoyance, anger, happiness, sadness, and boredom (Liscombe et al. 2003) . "}
{"citeStart": 155, "citeEnd": 171, "citeStartToken": 155, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. ", "mid_sen": "These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003) .", "after_sen": "One of the most pressing problems is the restriction to a single, flat stream or sequence of primary data (called \"text\" in some approaches), or a single, flat timeline. "}
{"citeStart": 29, "citeEnd": 51, "citeStartToken": 29, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010) . We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008) . Each word is broken down into: stem, affixes, stem POS, and affixes POS. We also have features for the preceding and following noun and verb, thereby approximating relevant se-lectional properties. Although these are relatively shallow features, they provide enough lexical and grammatical context to help select better or worse training data (section 3) and to provide a basis for a preliminary system (section 4).", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In selecting features for Korean, we have to account for relatively free word order (Chung et al., 2010) . ", "mid_sen": "We follow our previous work (Dickinson et al., 2010) in our feature choices, using a fiveword window that includes the target stem and two words on either side for context (see also Tetreault and Chodorow, 2008) . ", "after_sen": "Each word is broken down into: stem, affixes, stem POS, and affixes POS. "}
{"citeStart": 30, "citeEnd": 45, "citeStartToken": 30, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents. These findings may not hold if the level of bracketing available does not adequately constrain the parses considered -see Hwa (1999) for a related investigation with EM.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, it suggests that it may be possible to usefully tune a parser to a new domain with less annotation effort.", "mid_sen": "Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). ", "after_sen": "The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents. "}
{"citeStart": 29, "citeEnd": 56, "citeStartToken": 29, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis, and Koller 2004) , the input to information ordering is an unordered set of informationbearing items represented as CF lists. A set of candidate orderings is produced by creating different permutations of these lists. A metric of coherence uses features from centering to compute a score for each candidate ordering and select the highest scoring ordering as the output. 9 A wide range of metrics of coherence can be defined in centering's terms, simply on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5 , is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE, SALIENCE, and CHEAPNESS, and the centering transitions for this ordering. 10 The candidate ordering contains two NOCBs in sentences (3e) and (3f). Its score according to M.NOCB, the metric used by Karamanis and Manurung (2002) and Althaus, Karamanis, and Koller (2004) , is 2. Another ordering with fewer NOCBs (should such an ordering exist) will be preferred over this candidate as the selected output of information ordering if M.NOCB is used to guide this process. M.NOCB relies only on CONTINUITY. Because satisfying this principle is a prerequisite for the computation of every other centering feature, M.NOCB is the simplest possible centering-based metric and will be used as the baseline in our experiments.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis, and Koller 2004) , the input to information ordering is an unordered set of informationbearing items represented as CF lists. ", "after_sen": "A set of candidate orderings is produced by creating different permutations of these lists. "}
{"citeStart": 62, "citeEnd": 84, "citeStartToken": 62, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Algorithm 1 is a version of the MST algorithm as presented by Camerini et al. (1980) ; subtleties of the algorithm have been omitted. Arguments Y (a branching 5 ) and Z (a set of edges) are constraints on the edges that can be part of the solution, A. Edges in Y are required to be in the solution and edges in A branching is a subgraph that contains no cycles and no more than one edge directed into each node.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The initial dependency graph in Figure 2 (column G) contains three regular nodes and a root node.", "mid_sen": "Algorithm 1 is a version of the MST algorithm as presented by Camerini et al. (1980) ; subtleties of the algorithm have been omitted. ", "after_sen": "Arguments Y (a branching 5 ) and Z (a set of edges) are constraints on the edges that can be part of the solution, A. Edges in Y are required to be in the solution and edges in A branching is a subgraph that contains no cycles and no more than one edge directed into each node."}
{"citeStart": 67, "citeEnd": 92, "citeStartToken": 67, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "where For instance, a particular feature/class function might fire if and only if the bigram \"still hate\" appears and the document's sentiment is hypothesized to be negative. 7 Importantly, unlike Naive Bayes, MaxEnt makes no assumptions about the relationships between features, and so might potentially perform better when conditional independence assumptions are not met. The λ i,c 's are feature-weight parameters; inspection of the definition of P ME shows that a large λ i,c means that f i is considered a strong indicator for class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier's name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The λ i,c 's are feature-weight parameters; inspection of the definition of P ME shows that a large λ i,c means that f i is considered a strong indicator for class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier's name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense. ", "mid_sen": "We use ten iterations of the improved iterative scaling algorithm (Della Pietra et al., 1997) for parameter training (this was a sufficient number of iterations for convergence of training-data accuracy), together with a Gaussian prior to prevent overfitting (Chen and Rosenfeld, 2000) .", "after_sen": "Z(d) is a normalization function. "}
{"citeStart": 183, "citeEnd": 194, "citeStartToken": 183, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992 ], Brill's [Brill 1995a] , and MaxEnt [Ratnaparkhi 1996 ]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history. A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This requires resolving sentence boundaries before tagging. ", "mid_sen": "We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992 ], Brill's [Brill 1995a] , and MaxEnt [Ratnaparkhi 1996 ]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. ", "after_sen": "The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. "}
{"citeStart": 33, "citeEnd": 56, "citeStartToken": 33, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "Words unknown to the lexicon present a substantial problem to part-of-speech (POS) tagging of realworld texts. Taggers assign a single POS-tag to a word-token, provided that it is known what partsof-speech this word can take on in principle. So, first words are looked up in the lexicon. However, 3 to 5% of word tokens are usually missing in the lexicon when tagging real-world texts. This is where word-Pos guessers take their place --they employ the analysis of word features, e.g. word leading and trailing characters, to figure out its possible POS categories. A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger (Kupiec, 1992) . A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g. (Brill, 1995; Weischedel et al., 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger (Kupiec, 1992) . ", "mid_sen": "A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. ", "after_sen": "In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. "}
{"citeStart": 329, "citeEnd": 346, "citeStartToken": 329, "citeEndToken": 346, "sectionName": "UNKNOWN SECTION NAME", "string": "Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011) , which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; Liben-Nowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a) . Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). In contrast, our work constructs heterogeneous information networks from unstructured, noisy multi-genre text without explicit entity attributes.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Yang et al., 2011; Huang et al., 2012) also showed the effectiveness of exploiting information from formal web documents to enhance tweet summarization and tweet ranking.", "mid_sen": "Other similar research lines are the TAC-KBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011) , which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; Liben-Nowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a) . ", "after_sen": "Most of the work focused on unstructured or structured data with clean and rich relations (e.g. DBLP). "}
{"citeStart": 76, "citeEnd": 108, "citeStartToken": 76, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002) 13 , we looked at the performance of using adjectives alone. Intuitively, we might expect that adjectives carry a great deal of information regarding a document's sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech. Yet, the results, shown in line (6) of Figure 3 , are relatively poor: the 2633 adjectives provide less useful information than unigram presence. Indeed, line (7) shows that simply using the 2633 most frequent unigrams is a better choice, yielding performance comparable to that of using (the presence of) all 16165 (line (2)). This may imply that applying explicit feature-selection algorithms on unigrams could improve performance.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the effect of this information seems to be a wash: as depicted in line (5) of Figure 3 , the accuracy improves slightly for Naive Bayes but declines for SVMs, and the performance of MaxEnt is unchanged.", "mid_sen": "Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002) 13 , we looked at the performance of using adjectives alone. ", "after_sen": "Intuitively, we might expect that adjectives carry a great deal of information regarding a document's sentiment; indeed, the human-produced lists from Section 4 contain almost no other parts of speech. "}
{"citeStart": 99, "citeEnd": 115, "citeStartToken": 99, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "(1 ((((he:l PPHS1)) (VSUBCAT NP_PP) ((attribute:6 VVD)) ((failure:8 NN1)) ((PSUBCAT SING) ((to:9 II)) ((no<blank>one:lO PN)) ((buy: The grammar consists of 455 phrase structure rule schemata in the format accepted by the parser (a syntactic variant of a Definite Clause Grammar with iterative (Kleene) operators). It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977) , by Chomsky-adjunction to maximal projections of adjuncts (XP --* XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within X1 projections; X1 ~ X0 Argl... ArgN). Furthermore, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. There are 29 distinct values for VSUBCAT and 10 for PSUBCAT; these are analysed in patterns along with specific closed-class head lemmas of arguments, such as it (dummy subjects), whether (wh-complements), and so forth, to classify patterns as evidence for one of the 160 subcategorization classes. Each of these classes can be parameterized for specific predicates by, for example, different prepositions or particles. Currently, the coverage of this grammar--the proportion of sentences for which at least one analysis is found--is 79% when applied to the Susanne corpus (Sampson, 1995) , a 138K word treebanked and balanced subset of the Brown corpus. Wide coverage is important since information is acquired only from successful parses. The combined throughput of the parsing components on a Sun UltraSparc 1/140 is around 50 words per CPU second.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is 'shallow' in that no atof which thetempt is made to fully analyse unbounded dependencies. ", "mid_sen": "However, the distinction between arguments and adjuncts is expressed, following X-bar theory (e.g. Jackendoff, 1977) , by Chomsky-adjunction to maximal projections of adjuncts (XP --* XP Adjunct) as opposed to 'government' of arguments (i.e. arguments are sisters within X1 projections; X1 ~ X0 Argl... ArgN). ", "after_sen": "Furthermore, all analyses are rooted (in S) so the grammar assigns global, shallow and often 'spurious' analyses to many sentences. "}
{"citeStart": 320, "citeEnd": 335, "citeStartToken": 320, "citeEndToken": 335, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003) . This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In order to obtain a linguistically plausible rightcorner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson (1998b) and Klein and Manning (2003) . ", "after_sen": "This binarization is done in in such a way as to preserve linguistic intuitions of head projection, so that the depth requirements of rightcorner transformed trees will be reasonable approximations to the working memory requirements of a human reader or listener."}
{"citeStart": 188, "citeEnd": 213, "citeStartToken": 188, "citeEndToken": 213, "sectionName": "UNKNOWN SECTION NAME", "string": "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991) , LFS (Iordanskaja et al., 1992) , and JOYCE (Rambow and Korelsky, 1992) . The framework was originally developed for the realization of deep-syntactic structures in NLG .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.", "mid_sen": "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991) , LFS (Iordanskaja et al., 1992) , and JOYCE (Rambow and Korelsky, 1992) . ", "after_sen": "The framework was originally developed for the realization of deep-syntactic structures in NLG ."}
{"citeStart": 119, "citeEnd": 133, "citeStartToken": 119, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications. Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ; Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b) . Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982; Byrd, 1983) ; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b) . ", "mid_sen": "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. ", "after_sen": "Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982; Byrd, 1983) ; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. "}
{"citeStart": 16, "citeEnd": 32, "citeStartToken": 16, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "The second optimization creates a table of the categories which have been used to make predictions from. As discussed in Gerdemann (1991) , such a table can be used to avoid redundant predictions without a full and expensive subsumption test. The third indexes lexical entries which is necessary to obtain constant-time lexical access.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second optimization creates a table of the categories which have been used to make predictions from. ", "mid_sen": "As discussed in Gerdemann (1991) , such a table can be used to avoid redundant predictions without a full and expensive subsumption test. ", "after_sen": "The third indexes lexical entries which is necessary to obtain constant-time lexical access."}
{"citeStart": 123, "citeEnd": 132, "citeStartToken": 123, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "The rule formalism and compiler described here work well for European languages with reasonably complex orthographic changes but a limited range of possible affix combinations. Development, compilation and run-time efficiency are quite acceptable, and the use of rules containing complex feature-augmented categories allows morphotactic behaviours and non-segmentM spelling constraints to be specified in a way that is perspicuous to linguists, leading to rapid development of descriptions adequate for full NLP. The kinds of non-linear effects common in Semitic languages, where vowel and consonant patterns are interpolated in words (Kay, 1987; Kiraz, 1994) could be treated efficiently by the mechanisms described here if it proved possible to define a representation that allowed the parts of an inflected word corresponding to the root to be separated fairly cleanly from the parts expressing the inflection. The latter could then be used by a modified version of the current system as the basis for efficient lookup of spelling patterns which, as in the current system, would allow possible lexical roots to be calculated.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Development, compilation and run-time efficiency are quite acceptable, and the use of rules containing complex feature-augmented categories allows morphotactic behaviours and non-segmentM spelling constraints to be specified in a way that is perspicuous to linguists, leading to rapid development of descriptions adequate for full NLP. ", "mid_sen": "The kinds of non-linear effects common in Semitic languages, where vowel and consonant patterns are interpolated in words (Kay, 1987; Kiraz, 1994) could be treated efficiently by the mechanisms described here if it proved possible to define a representation that allowed the parts of an inflected word corresponding to the root to be separated fairly cleanly from the parts expressing the inflection. ", "after_sen": "The latter could then be used by a modified version of the current system as the basis for efficient lookup of spelling patterns which, as in the current system, would allow possible lexical roots to be calculated."}
{"citeStart": 59, "citeEnd": 84, "citeStartToken": 59, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we built user simulations for older and younger adults and evaluated them using standard metrics. Our results suggest that SUs trained on older people may also cover the behaviour of younger users, but not vice versa. This finding supports the principle of \"inclusive design\" (Keates and Clarkson, 2004) : designers should consider a wide range of users when developing a product for general use. Furthermore, our results agree with predictions based on statistical analysis of our corpus. They are also in line with findings of tests of deployed Interactive Voice Response systems with younger and older users (Dulude, 2002) , which show the diversity of older people's behaviour. Therefore, we have shown that standard metrics for evaluating SUs are a good predictor of the behaviour of our two user types. Overall, the metrics we used yielded a clear and consistent picture. Although our result needs to be verified on similar corpora, it has an important implication for corpus design. In order to yield realistic models of user behaviour, we need to gather less data from students, and more data from older and middle-aged users.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our results suggest that SUs trained on older people may also cover the behaviour of younger users, but not vice versa. ", "mid_sen": "This finding supports the principle of \"inclusive design\" (Keates and Clarkson, 2004) : designers should consider a wide range of users when developing a product for general use. ", "after_sen": "Furthermore, our results agree with predictions based on statistical analysis of our corpus. "}
{"citeStart": 154, "citeEnd": 166, "citeStartToken": 154, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner's algorithm (Sidner, 1981) proposed in (Azzam, 1996) , with further refinements from development on real-world texts. The approach is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system (Gaizauskas et al., 1995) and , Sheffield University's entry in the MUC-6 and 7 evaluations.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature.", "mid_sen": "This paper describes an evaluation of a focusbased approach to pronoun resolution (not anaphora in general), based on an extension of Sidner's algorithm (Sidner, 1981) proposed in (Azzam, 1996) , with further refinements from development on real-world texts. ", "after_sen": "The approach is implemented within the general coreference mechanism provided by the LaSIE (Large Scale Information Extraction) system (Gaizauskas et al., 1995) and , Sheffield University's entry in the MUC-6 and 7 evaluations."}
{"citeStart": 138, "citeEnd": 152, "citeStartToken": 138, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowsky, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996) . But, if we use just the context surrounding a word, we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms, because of the massive volume of text that would be required. Now, observe that even though a word might not cooccur significantly with another given word, it might nevertheless predict the use of that word if the two words are mutually related to a third word. That is, we can treat lexical co-occurrence as though it were moderately transitive. For example, in (3), learn provides evidence for task because it co-occurs (in other contexts) with difficult, which in turn co-occurs with task (in other contexts), even though learn is not seen to co-occur significantly with task.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowsky, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996) . ", "after_sen": "But, if we use just the context surrounding a word, we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms, because of the massive volume of text that would be required. "}
{"citeStart": 70, "citeEnd": 87, "citeStartToken": 70, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "where S(i, j) = sim(s i , s j ). Equation 2shows that the vector of MavenRank scores p is the left eigenvector of B with eigenvalue 1. We can prove that the eigenvector p exists by using a techinque from (Page et al., 1999) . We can treat the matrix B as a Markov chain describing the transition probabilities of a random walk on the speech similarity graph. The vector p then represents the stationary distribution of the random walk. It is possible that some parts of the graph are disconnected or that the walk gets trapped in a component. These problems are solved by reserving a small escape probability at each node that represents a chance of jumping to any node in the graph, making the Markov chain irreducible and aperiodic, which guarantees the existence of the eigenvector. Assuming a uniform escape probability for each node on the graph, we can rewrite Equation 2as", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Equation 2shows that the vector of MavenRank scores p is the left eigenvector of B with eigenvalue 1. ", "mid_sen": "We can prove that the eigenvector p exists by using a techinque from (Page et al., 1999) . ", "after_sen": "We can treat the matrix B as a Markov chain describing the transition probabilities of a random walk on the speech similarity graph. "}
{"citeStart": 29, "citeEnd": 50, "citeStartToken": 29, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive Θ are not necessarily those problems we are aiming to solve. In fact, new problems can be invented for the sole purpose of obtaining a better Θ. Thus, we distinguish between two types of problems in ASO: auxiliary problems, which are used to obtain Θ, and target problems, which are the problems we are aiming to solve 1 . For instance, in the argument identification task, the only target problem is to identify arguments vs. non-arguments, whereas in the argument classification task, there are 20 binary target problems, one to identify each of the 20 labels (ARG0, ARG1, . . . ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive Θ are not necessarily those problems we are aiming to solve. ", "after_sen": "In fact, new problems can be invented for the sole purpose of obtaining a better Θ. "}
{"citeStart": 30, "citeEnd": 50, "citeStartToken": 30, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "We separated out opinion from non-opinion words by considering their relative frequency in the two collections, expressed as a probability, using SRILM, SRI's language modeling toolkit (http://www.speech.sri.com/projects/srilm/). For every word W occurring in either of the document sets, we computed the followings: We used Kneser-Ney smoothing (Kneser and Ney, 1995) to handle unknown/rare words. Having obtained the above probabilities we calculated the score of W as the following ratio:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We separated out opinion from non-opinion words by considering their relative frequency in the two collections, expressed as a probability, using SRILM, SRI's language modeling toolkit (http://www.speech.sri.com/projects/srilm/). For every word W occurring in either of the document sets, we computed the followings: ", "mid_sen": "We used Kneser-Ney smoothing (Kneser and Ney, 1995) to handle unknown/rare words. ", "after_sen": "Having obtained the above probabilities we calculated the score of W as the following ratio:"}
{"citeStart": 132, "citeEnd": 156, "citeStartToken": 132, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "The second problem we consider is extracting fields from advertisements (Grenager et al., 2005) . The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website. In the dataset, only 302 entries have been labeled with 12 fields, including size, rent, neighborhood, features, and so on. The data was preprocessed using regular expressions for phone numbers, email addresses and URLs. The list of the constraints for this domain is given in Table 1 . We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006) . We slightly modified the seedwords due to difference in preprocessing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The list of the constraints for this domain is given in Table 1 . ", "mid_sen": "We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006) . ", "after_sen": "We slightly modified the seedwords due to difference in preprocessing."}
{"citeStart": 214, "citeEnd": 237, "citeStartToken": 214, "citeEndToken": 237, "sectionName": "UNKNOWN SECTION NAME", "string": "The resource-based approach to semantic composition in Lexical-Functional Grammar (LFG) obtains the interpretation for a phrase via a logical deduction, beginning with the interpretations of its parts as premises (Dalrymple et al., 1993a) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The resource-based approach to semantic composition in Lexical-Functional Grammar (LFG) obtains the interpretation for a phrase via a logical deduction, beginning with the interpretations of its parts as premises (Dalrymple et al., 1993a) .", "after_sen": "The resource-sensitive system of linear logic is used to compute meanings in accordance with relationships manifest in LFG f-structures. "}
{"citeStart": 157, "citeEnd": 176, "citeStartToken": 157, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "Magic compilation is illustrated on the basis of the simple logic grammar extract in figure 1. This grammar has been optimized automatically for generation (Minnen et al., 1996) : The right-hand sides of the rules are reordered such that a simple left-to-right evaluation order constitutes the optimal evaluation order. With this grammar a simple top-down generation strategy does not terminate as a result of the head recursion in rule 3. It is necessary to use memoization extended with an abstraction function and a subsumption check. Strict bottom-up generation is not attractive either as it is extremely inefficient: One is forced to generate all possible natural language expressions licensed by the grammar and subsequently check them against the start category. It is possible to make the process more efficient through excluding specific lexical entries with a semantic filter. The use of such a semantic filter in bottom-up evaluation requires the grammar to obey the semantic monotonicity constraint in order to ensure completeness (Shieber, 1988 ) (see below).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Magic compilation is illustrated on the basis of the simple logic grammar extract in figure 1. This grammar has been optimized automatically for generation (Minnen et al., 1996) : ", "after_sen": "The right-hand sides of the rules are reordered such that a simple left-to-right evaluation order constitutes the optimal evaluation order. "}
{"citeStart": 114, "citeEnd": 136, "citeStartToken": 114, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "Both software and grammar development are similar processes: They result in a system transforming some input into some output, based on a functional specification (e.g., cf. (Ciravegna et al., 1998) for the application of a particular software design methodology to linguistic engineering). Although Grammar", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Both software and grammar development are similar processes: ", "mid_sen": "They result in a system transforming some input into some output, based on a functional specification (e.g., cf. (Ciravegna et al., 1998) for the application of a particular software design methodology to linguistic engineering). ", "after_sen": "Although Grammar"}
{"citeStart": 82, "citeEnd": 97, "citeStartToken": 82, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar extraction algorithm Systemic Functional Grammar (SFG) (Halliday, 1985) is based on the assumption that the differentiation of syntactic phenomena is always deter-mined by its function in the communicative context. This functional orientation has lead to the creation of detailed linguistic resources that are characterized by an integrated treatment of content-related, textual and pragmatic aspects. Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects--such as, for example, PENMAN (Mann, 1983) , COMMUNAL (Fawcett and Tucker, 1990) , TECHDOC (KSsner and Stede, 1994) , Drafter (Paris and Vander Linden , 1996) , and Gist (Not and Stock, 1994) . For our present purposes, however, it is the formal characteristics of systemic grammar and its implementations that are more important. Systemic grammar assumes multifunctional constituent structuresrepresentable as feature structures with coreferences. As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: ", "mid_sen": "Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. ", "after_sen": "The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):"}
{"citeStart": 42, "citeEnd": 53, "citeStartToken": 42, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "We sketch some key points in the proof of correctness of our reduction. The λ-term N obtained from the input λ-term by replacing occurrences of constants by free variables in the manner described above is the normal form of some almost linear λterm N . The leftmost reduction from an almost linear λ-term to its normal form must be non-deleting and almost non-duplicating in the sense that when a β-redex (λx.P)Q is contracted, Q is not deleted, and moreover it is not duplicated unless the type of x is atomic. We can show that the Subject Expansion Theorem holds for such β-reduction, so the principal typing of N is also the principal typing of N . By a slight generalization of a result by Aoto (1999) , this typing Γ N : α must be negatively non-duplicated in the sense that each atomic type has at most one negative occurrence in it. By Aoto and Ono's (1994) generalization of the Coherence Theorem (see Mints, 2000) , it follows that every λterm P such that Γ P : α for some Γ ⊆ Γ must be βη-equal to N (and consequently to N).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We can show that the Subject Expansion Theorem holds for such β-reduction, so the principal typing of N is also the principal typing of N . ", "mid_sen": "By a slight generalization of a result by Aoto (1999) , this typing Γ N : α must be negatively non-duplicated in the sense that each atomic type has at most one negative occurrence in it. ", "after_sen": "By Aoto and Ono's (1994) generalization of the Coherence Theorem (see Mints, 2000) , it follows that every λterm P such that Γ P : α for some Γ ⊆ Γ must be βη-equal to N (and consequently to N)."}
{"citeStart": 336, "citeEnd": 353, "citeStartToken": 336, "citeEndToken": 353, "sectionName": "UNKNOWN SECTION NAME", "string": "It is essentially ill addressing the issue of ovelgenerality that Mel'~:uk introduces sub-and superscripts to lexical functions, enhancing their precision and making them sensitive to meaning aspects of tile lcxical items over which they operate. Superscripts are illtended to make the nleaning of tile I,F nlore precise and he|me |nero likely to imply unary inappings between argu|nents and vahlcs, subscripts a|e used to reference a particular semautic COlllpOUellt of a keyword. The introduclion of such devices into tile account of l,Fs demtmstrates hoth the need tk)r precision and the fact lbat it does seeul necessary to address semantic aspects of lexemes stand| ng it| co-occurrence relatio|ls. Ill fact it has been asserted by sonm (e.g., (Anick and Pustciovsky, 1990) , (lteid and Raab, 1989) ) that collocational systems are systematically predictable from the lexical Selllantics Of nt)tUlS, it) till atteln]Jt to explore this notion furthel; we have investigated the appr(lach to nolninal semantics known as Qualia structure (Pustejovsky, 1991) and conside|ed how this lnay ct)tnple-u|ent the LF notion to inlprove its descriptive powe| r.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The introduclion of such devices into tile account of l,Fs demtmstrates hoth the need tk)r precision and the fact lbat it does seeul necessary to address semantic aspects of lexemes stand| ng it| co-occurrence relatio|ls. ", "mid_sen": "Ill fact it has been asserted by sonm (e.g., (Anick and Pustciovsky, 1990) , (lteid and Raab, 1989) ) that collocational systems are systematically predictable from the lexical Selllantics Of nt)tUlS, it) till atteln]Jt to explore this notion furthel; we have investigated the appr(lach to nolninal semantics known as Qualia structure (Pustejovsky, 1991) and conside|ed how this lnay ct)tnple-u|ent the LF notion to inlprove its descriptive powe| r.", "after_sen": "alnoDg tile prolnising avenues that occur to tlS are, firstly, tile postulation of I,F subscripts based on the four Qualia roles (assuming thal these are tim lexically hies) relevant aspects of noun selnantics) and, secondly, the application of l,Fs to senlaulic (Qualia) structures rather titan monolithic lexenles; cg: tile I ,l; Ibm is used in delivering evahlative qualitiers which are standard expressions of praise or approval. "}
{"citeStart": 188, "citeEnd": 205, "citeStartToken": 188, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "Empirical results presented in the literature show that bidirectional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998) . We leave this for future work.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). ", "mid_sen": "We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998) . ", "after_sen": "We leave this for future work."}
{"citeStart": 123, "citeEnd": 142, "citeStartToken": 123, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "There are two principal sources for the parameters of the model. If a tagged corpus prepared by a human annotator is available, the transition and lexical probabilities can be estimated from the frequencies of pairs of tags and of tags associated with words. Alternatively~ a procedure called Baum-Welch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial ruodel, and the resulting probabilities used to determine new values for the lexical and transition probabilities. By iterating the algorithm with the same corpus, the parameters of the model can be made to converge on values which are locally optimal for the given text. The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model. The algorithm is again described by Cutting et al. and by Sharman, and a mathematical justification for it can be tbund in Huang et al. (1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model. ", "mid_sen": "The algorithm is again described by Cutting et al. and by Sharman, and a mathematical justification for it can be tbund in Huang et al. (1990) .", "after_sen": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. "}
{"citeStart": 46, "citeEnd": 64, "citeStartToken": 46, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "The right column in Table 1 shows the scores if (using the product-of-ranks algorithm) four source languages are taken into account in parallel. As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. This contrasts with the findings described in Rapp & Zock (2010) where significant improvements could be achieved by increasing the number of source languages. So this casts some doubt on these. However, as English was not considered as a source language there, the performance levels were mostly between 10 and 20, leaving much room for improvement. This is not the case here, where we try to improve on a score of around 50 for English. Remember that this is a somewhat conservative score as we count correct but alternative translations, as errors. As this is already a performance much closer to the optimum, making further performance gains is more difficult. Therefore, perhaps we should take it as a success that the product-of-ranks algorithm could achieve a minimal performance gain despite the fact that the influence of the non-English languages was probably mostly detrimental.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As can be seen, with an average score of 51.8 the improvement over the English only variant (50.6) is minimal. ", "mid_sen": "This contrasts with the findings described in Rapp & Zock (2010) where significant improvements could be achieved by increasing the number of source languages. ", "after_sen": "So this casts some doubt on these. "}
{"citeStart": 32, "citeEnd": 44, "citeStartToken": 32, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "Although most prior studies performed 5-fold cross-validation on ACE 2004, it is often not clear whether the partitioning has been done on the instance or on the document level. Moreover, it is often not stated whether argument order is modeled explicitly, making it difficult to compare system performance. Citing Wang (2008) , \"We feel that there is a sense of increasing confusion down this line of research\". To ease comparison for future research we use the same 5-fold split on the document level as Sun et al. (2011) and make our system publicly available (see Section 5). Table 3 shows that our system (bottom) aligns well with the state of the art. Our best system (composite kernel with polynomial expansion) reaches an F1 of 70.1, which aligns well to the 70.4 of Sun et al. (2011) that use the same datasplit. This is slightly behind that of Zhang (2006) ; the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources ) (we have on average 9 features/instance, they use 40). Since we focus on evaluating the impact of semantic similarity in tree kernels, we think our system is very competitive. Removing gold entity and mention information results in a significant F1 drop from 66.3% to 54.2%. However, in a realistic setting we do not have gold entity info available, especially not in the case when we apply the system to any kind of text. Thus, in the domain adaptation setup we assume entity boundaries given but not their label. Clearly, evaluating the approach on predicted mentions, e.g. Giuliano et al. (2007) , is another important dimension, however, out of the scope of the current paper.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our best system (composite kernel with polynomial expansion) reaches an F1 of 70.1, which aligns well to the 70.4 of Sun et al. (2011) that use the same datasplit. ", "mid_sen": "This is slightly behind that of Zhang (2006) ; the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources ) (we have on average 9 features/instance, they use 40). ", "after_sen": "Since we focus on evaluating the impact of semantic similarity in tree kernels, we think our system is very competitive. "}
{"citeStart": 78, "citeEnd": 80, "citeStartToken": 78, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithm. Some of the published algorithms produce only a chart as described by Kay in [14] , which only associates nonterminal categories to segments of the analyzed sentence [11, 39, 13, 3, 9] , and which thus still requires non-trivial proceasing to extract parse-trees [26] . The worst size complexity of such a chart is only a square function of the size of the input 2.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithm. ", "mid_sen": "Some of the published algorithms produce only a chart as described by Kay in [14] , which only associates nonterminal categories to segments of the analyzed sentence [11, 39, 13, 3, 9] , and which thus still requires non-trivial proceasing to extract parse-trees [26] . ", "after_sen": "The worst size complexity of such a chart is only a square function of the size of the input 2."}
{"citeStart": 157, "citeEnd": 170, "citeStartToken": 157, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "our PSEs, we achieve positive results in opinion-piece classification using the basic knearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, Judge 1 classified only 53 (45%) of the control sentences as subjective, and Judge 2 classified only 47 (36%) of them as subjective.", "mid_sen": "our PSEs, we achieve positive results in opinion-piece classification using the basic knearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997) .", "after_sen": "Given a document, the basic KNN algorithm classifies the document according to the majority classification of the document's k closest neighbors. "}
{"citeStart": 63, "citeEnd": 74, "citeStartToken": 63, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "(1) a. flJai mSmSbhd -- (Hyman, 1985, 50) b. J~Jai't' SmSmbh6- (Stewart, 1993, 2(10) These two possibilities exist because of different F0 scaling parameters. These parameters deternfine the way in which the different tones are scaled relative to each other and to the speaker's pitch range. This is illustrated in (2), adapting Hyman's earlier notation (Hyman, 1979) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These parameters deternfine the way in which the different tones are scaled relative to each other and to the speaker's pitch range. ", "mid_sen": "This is illustrated in (2), adapting Hyman's earlier notation (Hyman, 1979) .", "after_sen": "(2) a. Hyman: flJli m~m,l.bhti 2  2  1  2  l  1  1  0 0  1  t  I  3  3  1  3  2 Example (2) displays a kind of phonetic interpretation function. "}
{"citeStart": 86, "citeEnd": 113, "citeStartToken": 86, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Exhaustive search for the global optimum is not an option when the search space is prohibitively large. In the present context, say for a sequence of 20 tones, the search space contains 6 ~° ~ 10 is possible tone transcriptions, and for each of these there are thousands of possible parameter settings, too large a search space for exhaustive search in a reasonable amount of compu- Non-deterministic search methods have been devised as a way of tackling large-scale combinatorial optimisation problems, problems that involve fin(ling optima of functions of discrete variables. 'I'hcse methods are only designed to yield an approximate solution, but they do so in a reasonable amount of computation time. The best known such methods are genetic search (Goldberg, 1989) and annealing search (van Laarhoven & Aarts, 1987) . Recently, annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata (Ellison, 1993 ). In the following sections I describe a genetic algorithm and an annealing algorithm for the tone transcription problem.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "'I'hcse methods are only designed to yield an approximate solution, but they do so in a reasonable amount of computation time. ", "mid_sen": "The best known such methods are genetic search (Goldberg, 1989) and annealing search (van Laarhoven & Aarts, 1987) . ", "after_sen": "Recently, annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata (Ellison, 1993 ). "}
{"citeStart": 115, "citeEnd": 133, "citeStartToken": 115, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "We made heuristic rules for demonstratives by consulting the papers (NLRI 81) (Hayashi 83) (Takahashi et al. 90 ) (Kinsui & Takubo 92) and by examining Japanese sentences by hand. Demonstratives have three categories: demonstrative pronouns, demonstrative adjectives, and demonstrative adverbs. In the following sections, we explain the rules for analyzing demonstratives. Rule in the case when the referent is a noun  phrase   Candidate enumerating rule 1 When a pronoun is a demonstrative pronoun or \"8ono (of it) / k0no (of this) ] an0 (of that)\", {(A topic which has weight W and distance D, W-D-2) (A focus which has weight W and distance D, W -D + 4)} This bracketed expression represents the lists of proposals in Figure 1 . The definition and weight W of the topic and focus are shown in Tables 1 and 2. The distance (D) is the number of topics and loci between the demonstrative and the possible referent. Since a demonstrative more often refers to loci than a zero pronoun does, we add the coefficient -2 or +4 as compared with the heuristic rules in zero pronoun resolution.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Heuristic Rules for Demonstratives", "mid_sen": "We made heuristic rules for demonstratives by consulting the papers (NLRI 81) (Hayashi 83) (Takahashi et al. 90 ) (Kinsui & Takubo 92) and by examining Japanese sentences by hand. ", "after_sen": "Demonstratives have three categories: demonstrative pronouns, demonstrative adjectives, and demonstrative adverbs. "}
{"citeStart": 49, "citeEnd": 73, "citeStartToken": 49, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the form (v,n1,p,n2) , where the attachment decision is either v or n1. The best published results over RRR are those of Stetina and Nagao (1997) , who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. Their work is particularly inspiring in that it significantly outperformed the plethora of lexicalised probabilistic models that had been proposed to that point, and has not been beaten in later attempts.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "RRR consists of 20,081 training and 3,097 test quadruples of the form (v,n1,p,n2) , where the attachment decision is either v or n1. ", "mid_sen": "The best published results over RRR are those of Stetina and Nagao (1997) , who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. ", "after_sen": "Their work is particularly inspiring in that it significantly outperformed the plethora of lexicalised probabilistic models that had been proposed to that point, and has not been beaten in later attempts."}
{"citeStart": 0, "citeEnd": 33, "citeStartToken": 0, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "One way of measuring how well a binary clustering technique works for dialect grouping is to compare for each site i its average dissimilarity from the other sites in the same dialect, a(i), with its average dissimilarity from the sites in the other dialect, b(i). Kaufman and Rousseeuw (1990:83-86 ) define the statistic s(i) to be 1 -a(i)/b(i) if a(i) is less than b(i), otherwise b(i)/a(i) -1. The statistic thus ranges from 1 (perfect fit) to -1 (site i would perfectly fit in the other group). Plotting this statistic gives a silhouelte by which the eye call judge how well classified each site is. Averaging this statistic across all sites gives an idea of how felicitous the overall clustering is, ~.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One way of measuring how well a binary clustering technique works for dialect grouping is to compare for each site i its average dissimilarity from the other sites in the same dialect, a(i), with its average dissimilarity from the sites in the other dialect, b(i). ", "mid_sen": "Kaufman and Rousseeuw (1990:83-86 ) define the statistic s(i) to be 1 -a(i)/b(i) if a(i) is less than b(i), otherwise b(i)/a(i) -1. ", "after_sen": "The statistic thus ranges from 1 (perfect fit) to -1 (site i would perfectly fit in the other group). "}
{"citeStart": 20, "citeEnd": 40, "citeStartToken": 20, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "The experiments were conducted using two taggers, one written in C at Cambridge University Computer Laboratory, and the other in C-t-+ at Sharp Laboratories. Both taggers implement the FB, Viterbi and BW algorithms. For training from a hand-tagged corpus, the model is estimated by counting the number of transitions from each tag i to each tag j, the total occurrence of each tag i, and the total occurrence of word w with tag i. Writing these as f(i,j), f(i) and f(i,w) respectively, the transition probability from tag i to tag j is estimated as f(i,j)/f(i) and the lexical probability as f(i, w)/f(i). Other estimation formulae have been used in the past. For example, CLAWS (Garside ct al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag. Consulting the Baum-Welch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy. Any transitions not seen in the training corpus are given a small, non-zero probability The lexicon lists, for each word, all of tags seen in the training corpus with their probabilities. For words not found in the lexicon, all open-class tags are hypothesised, with equal probabilities. These words are added to the lexicon at the end of first iteration when re-estimation is being used, so that the probabilities of their hypotheses subsequently diverge from being uniform.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other estimation formulae have been used in the past. ", "mid_sen": "For example, CLAWS (Garside ct al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag. ", "after_sen": "Consulting the Baum-Welch re-estimation formulae suggests that the approach described is more appropriate, and this is confirmed by slightly greater tagging accuracy. "}
{"citeStart": 515, "citeEnd": 533, "citeStartToken": 515, "citeEndToken": 533, "sectionName": "UNKNOWN SECTION NAME", "string": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992) , (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003) , (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Semi-supervised methods for WSD are characterized in terms of exploiting unlabeled data in learning procedure with the requirement of predefined sense inventory for target words. ", "mid_sen": "They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al., 2004; Seo et al., 2004; Yarowsky, 1992) , (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003) , (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al., 2000; Yarowsky, 1995) .", "after_sen": "As a commonly used semi-supervised learning method for WSD, bootstrapping algorithm works by iteratively classifying unlabeled examples and adding confidently classified examples into labeled dataset using a model learned from augmented labeled dataset in previous iteration. "}
{"citeStart": 60, "citeEnd": 84, "citeStartToken": 60, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "The treebank contains 999 MRS-nets, which we translate automatically into dominance graphs and further into RTGs; the median number of scope readings per sentence is 56. For our experiment, we consider all 950 MRS-nets with less than 650 000 configurations. We use a slightly weaker version of the rewrite system that Koller and Thater (2006) used in their evaluation.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For our experiment, we consider all 950 MRS-nets with less than 650 000 configurations. ", "mid_sen": "We use a slightly weaker version of the rewrite system that Koller and Thater (2006) used in their evaluation.", "after_sen": "It turns out that the median number of equivalence classes, computed by pairwise comparison of all configurations, is 8. "}
{"citeStart": 117, "citeEnd": 148, "citeStartToken": 117, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "The results suggest that neither version of Roget's is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget's 1987. Even on the largest set (Finkelstein et al., 2001) , however, the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4 . The difference between the 1911 Thesaurus and Vector would be statistically signifi- (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri, even at p < 0.1 for a two-tailed test. These data sets are too small for a meaningful comparison of systems with close correlation scores.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Even on the largest set (Finkelstein et al., 2001) , however, the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4 . ", "mid_sen": "The difference between the 1911 Thesaurus and Vector would be statistically signifi- (Miller and Charles, 1991) and (Rubenstein and Goodenough, 1965) data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri, even at p < 0.1 for a two-tailed test. ", "after_sen": "These data sets are too small for a meaningful comparison of systems with close correlation scores."}
{"citeStart": 125, "citeEnd": 137, "citeStartToken": 125, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognized left-corner in order to remove leftrecursion. A top-down parser using a grammar produced by the selective left-corner transform simulates a generalized left-corner parser Demers, 1977; Nijholt, 1980 which recognizes a user-speci ed subset of the original productions in a left-corner fashion, and the other productions top-down.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The selective left-corner transform reduces the transformed grammar size because only those productions which appear in a left-recursive cycle need be recognized left-corner in order to remove leftrecursion. ", "mid_sen": "A top-down parser using a grammar produced by the selective left-corner transform simulates a generalized left-corner parser Demers, 1977; Nijholt, 1980 which recognizes a user-speci ed subset of the original productions in a left-corner fashion, and the other productions top-down.", "after_sen": "Although we do not investigate it in this paper, the selective left-corner transform should usually have a smaller search space relative to the standard left-corner transform, all else being equal. "}
{"citeStart": 181, "citeEnd": 194, "citeStartToken": 181, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003) , Shen et al. (2003) . However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001) . UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS. For example, for the phrase immediate systemic anticoagulants, MetaMap identifies immediate as a TEMPORAL CONCEPT, systemic as a FUNCTIONAL CONCEPT, and anticoagulants as a PHARMACOLOGIC SUBSTANCE. More than one semantic category in UMLS may correspond to MED-ICATION or DISEASE. For example, either a PHAR-MACOLOGIC SUBSTANCE or a THERAPEUTIC OR PREVENTIVE PROCEDURE can be a MEDICATION; either a DISEASE OR SYNDROME or a PATHOLOGIC FUNCTION can be a DISEASE.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. ", "mid_sen": "To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001) . UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. ", "after_sen": "Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS. "}
{"citeStart": 180, "citeEnd": 196, "citeStartToken": 180, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . ", "mid_sen": "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . ", "after_sen": "That is, the current system learns procedures rather than data structures. "}
{"citeStart": 137, "citeEnd": 161, "citeStartToken": 137, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "While high-quality NLP corpora and tools are available in English, such resources are difficult to obtain in most other languages. Three challenges must be met when adapting results established in English to another language: (1) acquiring high quality annotated data; (2) adapting the English task definition to the nature of a different language, and (3) adapting the algorithm to the new language. This paper presents a case study in the adaptation of a well known task to a language with few NLP resources available. Specifically, we deal with SVM based Hebrew NP chunking. In (Goldberg et al., 2006) , we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify 'suspicious' data points in the corpus, and identify how robust the model is in the presence of noise;", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Specifically, we deal with SVM based Hebrew NP chunking. ", "mid_sen": "In (Goldberg et al., 2006) , we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. ", "after_sen": "We extend that work and study the problem from 3 angles: (1) how to deal with a corpus that is smaller and with a higher level of noise than is available in English; we propose techniques that help identify 'suspicious' data points in the corpus, and identify how robust the model is in the presence of noise;"}
{"citeStart": 133, "citeEnd": 162, "citeStartToken": 133, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "The segmentation algorithm presented in this paper focuses on one source of linguistic information for discourse analysis -lexical cohesion. Multiple studies of discourse structure, however, have shown that prosodic cues are highly predictive of changes in topic structure (Hirschberg and Nakatani, 1996; Shriberg et al., 2000) . In a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information. We can also explore an unsupervised fusion of these two sources of information; for instance, we can induce informative prosodic cues by using distributional evidence.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The segmentation algorithm presented in this paper focuses on one source of linguistic information for discourse analysis -lexical cohesion. ", "mid_sen": "Multiple studies of discourse structure, however, have shown that prosodic cues are highly predictive of changes in topic structure (Hirschberg and Nakatani, 1996; Shriberg et al., 2000) . ", "after_sen": "In a supervised framework, we can further enhance audio-based segmentation by combining features derived from pattern analysis with prosodic information. "}
{"citeStart": 122, "citeEnd": 145, "citeStartToken": 122, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008) ) or are (almost) knowledge-free (e.g., Koehn and Knight (2003) ). Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge en-coded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. ", "mid_sen": "Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008) ) or are (almost) knowledge-free (e.g., Koehn and Knight (2003) ). ", "after_sen": "Compound merging is less well studied. "}
{"citeStart": 0, "citeEnd": 19, "citeStartToken": 0, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "Several researchers have worked on learning grammatical properties of words. Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. Brill et al. (1990) try to infer grammatical category from bigram statistics. Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. Kneser and Ney (1993) present a probabilistic model for entropy maximization that also relies on the immediate neighbors of words in a corpus. Biber (1993) applies factor analysis to collocations of two target words (\"certain\" and \"right\") with their immediate neighbors.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Elman (1990) trains a connectionist net to predict words, a process that generates internal representations that reflect grammatical category. ", "mid_sen": "Brill et al. (1990) try to infer grammatical category from bigram statistics. ", "after_sen": "Finch and Chater (1992) and Finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in a corpus. "}
{"citeStart": 260, "citeEnd": 274, "citeStartToken": 260, "citeEndToken": 274, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section, we experimentally evaluate the effectiveness of query expansion with the term similarity functions discussed in Section 4 in the axiomatic framework. Experiment results show that the similarity function based on synset definitions is most effective. By incorporating this similarity function into the axiomatic retrieval models, we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance, which has not been shown in the previous studies (Voorhees, 1994; Stairmand, 1997) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Experiment results show that the similarity function based on synset definitions is most effective. ", "mid_sen": "By incorporating this similarity function into the axiomatic retrieval models, we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance, which has not been shown in the previous studies (Voorhees, 1994; Stairmand, 1997) .", "after_sen": ""}
{"citeStart": 82, "citeEnd": 100, "citeStartToken": 82, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent. A forward link (Flink) is the analog of a \"first pair-part\" of an adjacency pair (Sacks et al., 1974) , and is similarly restricted to specific speech act types. All Request-Information and Request-Action DFUs are assigned Flinks. The responses to such requests are assigned a backward link (Blink). In principle, a response can be any of the speech act types, thus it can be an answer to a question (Inform), a rejection of a Request-Action or a commitment to take the requested action (Commit), a request for clarification (Request-Information), and so on. In most but not all cases, requests are responded to, thus most Flinks and Blinks come in pairs. We refer to Flinks with no matching Blink as dangling links. If an utterance can be interpreted as a response to a preceding DFU, it will get a Blink even where the preceding DFU has no Flink. The preceding DFU taken to be the \"first pair-part\" of the Link will be assigned a secondary forward link (Sflink). All links except dangling links are annotated with the address of the DFU from which they originate. Figure 1 illustrates an email message (M2) containing a single sentence (\"That's fine\") that is a response to a DFU in a prior email (M1), where the prior email had no Flink because it only contains Inform DAs; thus M1 gets an Sflink.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent. ", "mid_sen": "A forward link (Flink) is the analog of a \"first pair-part\" of an adjacency pair (Sacks et al., 1974) , and is similarly restricted to specific speech act types. ", "after_sen": "All Request-Information and Request-Action DFUs are assigned Flinks. "}
{"citeStart": 124, "citeEnd": 145, "citeStartToken": 124, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002) . Following (Chiang, 2005) , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the \"diag-and\" method of . Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002) . ", "mid_sen": "Following (Chiang, 2005) , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. ", "after_sen": "First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. "}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section we present some applications of our analysis to related constructions. First, we consider the past perfect, as in sentence 2. De Swart (1991) gives this example to illustrate the inability to interpret temporal connectives without the use of the reference times. According to (de Swart, 1991) , the subordinate clause determines the reference time of the verb, which lies anteriorly to the event time. Trying to use the event times would give the wrong analysis. This would seem to be troublesome for our approach, which uses the location time of the event in the main clause, and not its reference time. However, this is not a problem, since our analysis of the perfect by the use of the operator perf, analyses the eventuality referred to by the main clause, as the result state of a previous event. The temporal relation in the sentence is inclusion between the event time of Anne's coming home, and the location time of the result state of Paul's already having prepared dinner.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, we consider the past perfect, as in sentence 2. ", "mid_sen": "De Swart (1991) gives this example to illustrate the inability to interpret temporal connectives without the use of the reference times. ", "after_sen": "According to (de Swart, 1991) , the subordinate clause determines the reference time of the verb, which lies anteriorly to the event time. "}
{"citeStart": 134, "citeEnd": 157, "citeStartToken": 134, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, the \"IR engine\" computes the similarity between T-query and each document in the surrogates based on the vector space model (Salton and McGill, 1983) , and sorts document according to the similarity, in descending order. We compute term weight based on the notion of TF.IDF. Note that T-query is decomposed into base words, as performed in the document preprocessing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "T-query can consist of more than one translation, because multiple translations are often appropriate for a single technical term.", "mid_sen": "Finally, the \"IR engine\" computes the similarity between T-query and each document in the surrogates based on the vector space model (Salton and McGill, 1983) , and sorts document according to the similarity, in descending order. ", "after_sen": "We compute term weight based on the notion of TF.IDF. "}
{"citeStart": 57, "citeEnd": 79, "citeStartToken": 57, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "We trained a language model of order 5 built on the entire EUROPARL corpus using the SRILM package. The method uses improved Kneser-Ney smoothing algorithm (Chen and Goodman, 1999) to compute sequence probabilities.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We trained a language model of order 5 built on the entire EUROPARL corpus using the SRILM package. ", "mid_sen": "The method uses improved Kneser-Ney smoothing algorithm (Chen and Goodman, 1999) to compute sequence probabilities.", "after_sen": ""}
{"citeStart": 213, "citeEnd": 233, "citeStartToken": 213, "citeEndToken": 233, "sectionName": "UNKNOWN SECTION NAME", "string": "Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996) . Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. ", "mid_sen": "At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "after_sen": "We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. "}
{"citeStart": 181, "citeEnd": 190, "citeStartToken": 181, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005) . However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998) . Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. An example of this is from CCGbank (Hockenmaier, 2003) , where all modifiers in noun-noun compound constructions modify the final noun (because the Penn Treebank, from which CCGbank is derived, does not contain the necessary information to obtain the correct bracketing). Thus there are nonnegligible, systematic errors in both the training and testing material, and the CCG parsers are being rewarded for following particular mistakes.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005) . ", "after_sen": "However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. "}
{"citeStart": 70, "citeEnd": 105, "citeStartToken": 70, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "(after Lakoff, 1971) is felicitously understood to mean that after a slanderous introduction, Bill retaliated in kind against John. What makes (1) felicitous is that the pitch accents on the pronominals contribute attentional information that cannot be gleaned from text alone. This suggests an attentional component to pitch accents, in addition to the propositional component explicated in Pierrehumbert and Hirschberg (1990) . In this paper, I combine their account of pitch accent semantics with Grosz, Joshi and Weinstein's (1989) account of centering to yield insights into the phenomenon of pitch accented pronominals, and the attentional consequences of pitch accents in general. The relevant claims in PH90 and GJW89 are reviewed in the next two sections.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This suggests an attentional component to pitch accents, in addition to the propositional component explicated in Pierrehumbert and Hirschberg (1990) . ", "mid_sen": "In this paper, I combine their account of pitch accent semantics with Grosz, Joshi and Weinstein's (1989) account of centering to yield insights into the phenomenon of pitch accented pronominals, and the attentional consequences of pitch accents in general. ", "after_sen": "The relevant claims in PH90 and GJW89 are reviewed in the next two sections."}
{"citeStart": 31, "citeEnd": 45, "citeStartToken": 31, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "For training, we use 'CRL data', which was prepared for IREX (Information Retrieval and Extraction Exercise 1 , Sekine and Eriguchi (2000) ). It has about 19,000 NEs in 1,174 articles. We also use additional data by Isozaki (2001) . Both datasets are based on Mainichi Newspaper's 1994 and 1995 CD-ROMs. We use IREX's formal test data called GENERAL that has 1,510 named entities in 71 articles from Mainichi Newspaper of 1999. Systems are compared in terms of GENERAL's F-measure Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has about 19,000 NEs in 1,174 articles. ", "mid_sen": "We also use additional data by Isozaki (2001) . ", "after_sen": "Both datasets are based on Mainichi Newspaper's 1994 and 1995 CD-ROMs. "}
{"citeStart": 110, "citeEnd": 132, "citeStartToken": 110, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Besides utilizing social signals, our system has two additional features. Firstly, the readability feature is introduced to the graph model to reduce the chance of tweets hard to read to appear in the summarization. Several factors are considered while computing a tweet's readability, including: 1) The number of out-of-vocabulary (OOV) words; 2) the number of words; and 3) the number of abnormal symbols, e.g., \"!,,),(,*\". Secondly, while selecting representative tweets using an alternative of the Maximal Marginal Relevance (MMR) (Goldstein et al., 1999 ) algorithm, our system penalizes tweets which are selected from a same twitter account, to achieve diversity among users.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several factors are considered while computing a tweet's readability, including: 1) The number of out-of-vocabulary (OOV) words; 2) the number of words; and 3) the number of abnormal symbols, e.g., \"!,,),(,*\". ", "mid_sen": "Secondly, while selecting representative tweets using an alternative of the Maximal Marginal Relevance (MMR) (Goldstein et al., 1999 ) algorithm, our system penalizes tweets which are selected from a same twitter account, to achieve diversity among users.", "after_sen": "We collect 100 sets of tweets, each of which is related to a trending topic. "}
{"citeStart": 210, "citeEnd": 226, "citeStartToken": 210, "citeEndToken": 226, "sectionName": "UNKNOWN SECTION NAME", "string": "tagger operates on text spans that form a sentence. This requires resolving sentence boundaries before tagging. We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992 ], Brill's [Brill 1995a] , and MaxEnt [Ratnaparkhi 1996 ]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. This issue can be also addressed by breaking the text into short text spans at positions where the previous tagging history does not affect current decisions. For instance, a bigram tagger operates within a window of two tokens, and thus a sequence of word tokens can be terminated at an unambiguous word token, since this unambiguous word token will be the only history used in tagging of the next token. At the same time since this token is unambiguous, it is not affected by the history. A trigram tagger operates within a window of three tokens, and thus a sequence of word tokens can be terminated when two unambiguous words follow each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This requires resolving sentence boundaries before tagging. ", "mid_sen": "We see no good reason, however, why such text spans should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden Markov Model [HMM] [Kupiec 1992 ], Brill's [Brill 1995a] , and MaxEnt [Ratnaparkhi 1996 ]) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens. ", "after_sen": "The only reason why taggers traditionally operate on the sentence level is that a sentence naturally represents a text span in which POS information does not depend on the previous and following history. "}
{"citeStart": 58, "citeEnd": 81, "citeStartToken": 58, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we have described how generation resources for restricted applications can be developed drawing on large-scale general generation grammars. This enables both re-use of those resources and progressive growth as new applications are met. The grammar extraction tool then makes it a simple task to extract from the large-scale resources specially tuned subgrammars for particular applications. Our approach shows some similarities to that proposed by (Rayner and Carter, 1996) for improving parsing performance by grammar pruning and specialization with respect to a training corpus. Rule components are 'chunked' and pruned when they are unlikely to contribute to a successful parse. Here we have shown how improvements in generation performance can be achieved for generation grammars by removing parts of the grammar specification that are not used in some particular sublanguage. The extracted grammar is generally known to cover the target sublanguage and so there is no loss of required coverage. Another motivation for this work is the need for smaller, but not toy-sized, systemic grammars for their experimental compilation into state-of-the-art feature logics. The ready access to consistent subgrammars of arbitrary size given with the automatic subgrammar extraction reported here allows us to investigate further the size to which feature logic representations of systemic grammar can grow while remaining practically usable. The compilation of the full grammar NIGEL has so far only proved possible for CUF (see (Henschel, 1995) ), and the resulting type deduction runs too slowly for practical applications.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The grammar extraction tool then makes it a simple task to extract from the large-scale resources specially tuned subgrammars for particular applications. ", "mid_sen": "Our approach shows some similarities to that proposed by (Rayner and Carter, 1996) for improving parsing performance by grammar pruning and specialization with respect to a training corpus. ", "after_sen": "Rule components are 'chunked' and pruned when they are unlikely to contribute to a successful parse. "}
{"citeStart": 15, "citeEnd": 30, "citeStartToken": 15, "citeEndToken": 30, "sectionName": "UNKNOWN SECTION NAME", "string": "1 Introduction Charniak (1995) and Carroll and Rooth (1998) present head-lexicalized probabilistic context free grammar formalisms, and show that they can effectively be applied in inside-outside estimation of syntactic language models for English, the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates. The present paper describes an experiment where a slightly modified version of Carroll and Rooth's model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order). We emphasize techniques which made it practical to apply inside-outside estimation of a lexicalized context free grammar to such a language. These techniques relate to the treatment of argument cancellation and scrambled phrase order; to the treatment of case features in category labels; to the category vocabulary for nouns, articles, adjectives and their projections; to lexicalization based on uninflected lemmata rather than word forms; and to exploitation of a parameter-tying feature.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "1 Introduction Charniak (1995) and Carroll and Rooth (1998) present head-lexicalized probabilistic context free grammar formalisms, and show that they can effectively be applied in inside-outside estimation of syntactic language models for English, the parameterization of which encodes lexicalized rule probabilities and syntactically conditioned word-word bigram collocates. ", "after_sen": "The present paper describes an experiment where a slightly modified version of Carroll and Rooth's model was applied in a systematic experiment on German, which is a language with rich inflectional morphology and free word order (or rather, compared to English, free-er phrase order). "}
{"citeStart": 88, "citeEnd": 108, "citeStartToken": 88, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that the unfolding of the magic_s literal leads to the instantiation of the argument VFORM to finite. As a result of the fact that there are no other magic_s literals in the remainder of the magic-compiled grammar the magic_s rule can be discarded. This filter optimization is reminiscent of computing the deterministic closure over the magic part of a compiled grammar (DSrre, 1993) at compile time. Performing this optimization throughout the magic part of the grammar in figure 2 not only leads to a more succinct grammar, but brings about a different processing behavior. Generation with the resulting grammar can be compared best with head corner generation (Shieber et al., 1990 ) (see next section).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Performing this optimization throughout the magic part of the grammar in figure 2 not only leads to a more succinct grammar, but brings about a different processing behavior. ", "mid_sen": "Generation with the resulting grammar can be compared best with head corner generation (Shieber et al., 1990 ) (see next section).", "after_sen": ""}
{"citeStart": 183, "citeEnd": 202, "citeStartToken": 183, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "Unless the desired set of GRs matches the set already annotated in some large training corpus (e.g., the Buchholz et al. (1999) GR finder used the GRs annotated in the Penn Treebank (Marcus et al., 1993) ), one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set. Manually writing rules is expensive, as is annotating a large corpus. We have performed experiments on learning to find GRs with just a small annotated training set. Our starting point is the work described in Ferro et al. (1999) , which used a fairly small training set.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999) .", "mid_sen": "Unless the desired set of GRs matches the set already annotated in some large training corpus (e.g., the Buchholz et al. (1999) GR finder used the GRs annotated in the Penn Treebank (Marcus et al., 1993) ), one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set. ", "after_sen": "Manually writing rules is expensive, as is annotating a large corpus. "}
{"citeStart": 103, "citeEnd": 115, "citeStartToken": 103, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "I\",v,'n if probability t|wory remains, as it currently is, th,, m~.l.llod of clloicc in making language processing qu.ntitative, this still h~aw:s the fieht wide open in terms .,f carving up languag~ processing into an appropriate set ,,f ,,wmts tbr probability theory to work with. For translation, a very direct apprgach using parameters based on surface positions of words in source and target sentences was adopted in the Candide system (Brown et at. 1990 ). However, this does not capture important structural properties of natural language. Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences. Such generalizations are, of course, central to qualitative structural approaches to translation (e.g. Isabelle and Macklovitch 1986, Alshawi et at. 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Nor does it take into account generalizations about translation that are independent of the exact word order in source and target sentences. ", "mid_sen": "Such generalizations are, of course, central to qualitative structural approaches to translation (e.g. Isabelle and Macklovitch 1986, Alshawi et at. 1992) .", "after_sen": "The aim of the quantitative language and translation models presented in sections 5 and 6 is to employ proba~ bilistic parameters that reflect linguistic structure without discarding rich lexical information or making the models too complex to train automatically. "}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. Their better performance is partly attributed to richer (speaker dependent) acoustic modeling, including phoneme duration, energy, and pitch. However, their model was trained and tested on professionally read speech, rather than spontaneous speech. Wang and Hirschberg (1992) did employ spontaneous speech, namely, the ATIS corpus. For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents with intonational features. One explanation for the difference in performance was that our model was trained on approximately ten times as much data. Secondly, their decision trees are used to classify each data point independently of the next, whereas we find the best interpretation over the entire turn, and incorporate speech repairs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, their model was trained and tested on professionally read speech, rather than spontaneous speech. ", "mid_sen": "Wang and Hirschberg (1992) did employ spontaneous speech, namely, the ATIS corpus. ", "after_sen": "For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents with intonational features. "}
{"citeStart": 153, "citeEnd": 163, "citeStartToken": 153, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "This work has principally been developed on text of technical manuals from Perkins Engines Ltd., which have been translated by a semi-automatic process (Pyre, 1993) . Now, a partial parse can support such a process. For instance, frequently occurring modal verbs such as \"must\" are not dis- tinguished by number in English, but they are in many other languages. It is necessary to locate the subject, then identify the head and determine its number in order to translate the main verb correctly in sentences like (1) below.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "This work has principally been developed on text of technical manuals from Perkins Engines Ltd., which have been translated by a semi-automatic process (Pyre, 1993) . ", "after_sen": "Now, a partial parse can support such a process. "}
{"citeStart": 107, "citeEnd": 126, "citeStartToken": 107, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal (WSJ) corpus (Marcus et al., 1993) . For all experiments, we used sections 00-19 as training material and 20-24 as test material. See Section 4 for results on other train/test set splittings.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the last stage of the cascade, we label several types of grammatical relations between pairs of words in the sentence.", "mid_sen": "The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal (WSJ) corpus (Marcus et al., 1993) . ", "after_sen": "For all experiments, we used sections 00-19 as training material and 20-24 as test material. "}
{"citeStart": 145, "citeEnd": 161, "citeStartToken": 145, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004) , where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001) , which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000) . However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. One of the reasons may be the deficiency of the dynamic programming-based systems, that the global information of sequences cannot be incorporated as features of the models. Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task. We had to wait until Tsai et al. (2006) , who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al. As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such a large variety of vocabulary naturally leads to long names with productive use of general words, making the task difficult to be solved by systems with naive Markov assumption of label sequences, because such systems must perform their prediction without seeing the entire string of the entities.", "mid_sen": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004) , where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. ", "after_sen": "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001) , which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000) . "}
{"citeStart": 128, "citeEnd": 129, "citeStartToken": 128, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "Achieving understanding is not unproblematic, it is a process that must be managed, just as other goal achieving processes are [3] . Inference of mutual understanding relies upon some evidence, e.g. the utterance that is made, and a number of underlying assumptions. The assumptions are given with the inference rule below.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I claim that a model of the achievement of mutual belief of understanding can he extended to the achievement of other goals in dialogue.", "mid_sen": "Achieving understanding is not unproblematic, it is a process that must be managed, just as other goal achieving processes are [3] . ", "after_sen": "Inference of mutual understanding relies upon some evidence, e.g. the utterance that is made, and a number of underlying assumptions. "}
{"citeStart": 273, "citeEnd": 301, "citeStartToken": 273, "citeEndToken": 301, "sectionName": "UNKNOWN SECTION NAME", "string": "If word A is used in the definition of word B, t.he,m words are expected to be strongly related. This is the basis of our hypothesis that the distances in the refi~rence network reflect the associative distances between words (Nitta 1933 Vdronis and Ide (1990) for word sense disainl)iguation) and as fields for artificial association, such its spreading activation (by Kojiina and l:urugori (1993) for context-coherence measurement). The distance vector of a word can be considered to be a list, of the activation strengths at the origin nodes when the word node is activated. Therefore, distance w~ctors can be expected to convey almost the santo information as the entire network, and clearly they are Ili~icli easier to handle.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If word A is used in the definition of word B, t.he,m words are expected to be strongly related. ", "mid_sen": "This is the basis of our hypothesis that the distances in the refi~rence network reflect the associative distances between words (Nitta 1933 Vdronis and Ide (1990) for word sense disainl)iguation) and as fields for artificial association, such its spreading activation (by Kojiina and l:urugori (1993) for context-coherence measurement). ", "after_sen": "The distance vector of a word can be considered to be a list, of the activation strengths at the origin nodes when the word node is activated. "}
{"citeStart": 9, "citeEnd": 23, "citeStartToken": 9, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "As with MT systems, existing CLIR systems still find it difficult to translate technical terms and proper nouns, which are often unlisted in general dictionaries. Since most CLIR systems target newspaper articles, which are comprised mainly of general words, the problem related to unlisted words has been less explored than other CLIR subtopics (such as resolution of translation ambiguity). However, Pirkola (1998) , for example, used a subset of the TREC collection related to health topics, and showed that combination of general and domain specific (i.e., medical) dictionaries improves the CLIR performance obtained with only a general dictionary. This result shows the potential contribution of technical term translation to CLIR. At the same time, note that even domain specific dictionaries lhttp ://www. rd. nacs is. ac. j p/-nt cadm/index-en, html do not exhaustively list possible technical terms. We classify problems associated with technical term translation as given below:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since most CLIR systems target newspaper articles, which are comprised mainly of general words, the problem related to unlisted words has been less explored than other CLIR subtopics (such as resolution of translation ambiguity). ", "mid_sen": "However, Pirkola (1998) , for example, used a subset of the TREC collection related to health topics, and showed that combination of general and domain specific (i.e., medical) dictionaries improves the CLIR performance obtained with only a general dictionary. ", "after_sen": "This result shows the potential contribution of technical term translation to CLIR. "}
{"citeStart": 84, "citeEnd": 105, "citeStartToken": 84, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995) , and support the conclusion that the dependency model is superior to the adjacency model. Lauer and Dras (1994) suggest two improvements to the method used above. These are:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It seems that additional instances admitted by the windowed schemes are too noisy to make an improvement.", "mid_sen": "Initial results from applying these methods to the EMA corpus have been obtained by Wilco ter Stal (1995) , and support the conclusion that the dependency model is superior to the adjacency model. ", "after_sen": "Lauer and Dras (1994) suggest two improvements to the method used above. "}
{"citeStart": 70, "citeEnd": 89, "citeStartToken": 70, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "CRFs share many of the advantageous properties of standard maximum entropy models, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. Traditional maximum entropy learning algorithms, such as GIS and IIS (Pietra et al., 1995) , can be used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much faster (Malouf, 2002; Sha and Pereira, 2003) . We use BFGS for optimization. In our experiments, we shall focus instead on two other aspects of CRF deployment, namely regularization and selection of different model structure and feature types.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "CRFs share many of the advantageous properties of standard maximum entropy models, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. ", "mid_sen": "Traditional maximum entropy learning algorithms, such as GIS and IIS (Pietra et al., 1995) , can be used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much faster (Malouf, 2002; Sha and Pereira, 2003) . ", "after_sen": "We use BFGS for optimization. "}
{"citeStart": 41, "citeEnd": 71, "citeStartToken": 41, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "We now compare our approach with existing methods. We used the same training-test splits as Zettlemoyer and Collins (2005) (600 training and 280 test examples for GEO, 500 training and 140 test examples for JOBS). For development, we created five random splits of the training data. For each split, we put 70% of the examples into a development training set and the remaining 30% into a development test set. The actual test set was only used for obtaining final numbers.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We now compare our approach with existing methods. ", "mid_sen": "We used the same training-test splits as Zettlemoyer and Collins (2005) (600 training and 280 test examples for GEO, 500 training and 140 test examples for JOBS). ", "after_sen": "For development, we created five random splits of the training data. "}
{"citeStart": 17, "citeEnd": 33, "citeStartToken": 17, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "Another question is why using transformation-based (rule) learning seems to be slightly better than memory-based learning for these type 1 GRs. Memory-based learning keeps all of the training instances and does not try to find generalizations such as rules (Daelemans et al., 1999, Ch. 4 ). However, with type 1 GRs, a few simple generalizations can account for many of the instances. In the manner of Stevenson (1998) , we wrote a set of six simple rules that when run on the test set type 1 GRs produces an F-score of 77%. This is better than what our reconstructed MB system originally achieved and is close to the TR system's original results (close enough not to be statistically significantly different). An example of these six rules: IF (1) the center chunk is a verb chunk and (2) is not considered as possibly passive and (3) its headword is not some form of to be and (4) the right neighbor is a noun or verb chunk, THEN consider that chunk to the right as being an object of the center chunk.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, with type 1 GRs, a few simple generalizations can account for many of the instances. ", "mid_sen": "In the manner of Stevenson (1998) , we wrote a set of six simple rules that when run on the test set type 1 GRs produces an F-score of 77%. ", "after_sen": "This is better than what our reconstructed MB system originally achieved and is close to the TR system's original results (close enough not to be statistically significantly different). "}
{"citeStart": 209, "citeEnd": 222, "citeStartToken": 209, "citeEndToken": 222, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 287, "citeEnd": 312, "citeStartToken": 287, "citeEndToken": 312, "sectionName": "UNKNOWN SECTION NAME", "string": "The previous studies regarded the task of identifying section names as a text-classification problem that determines a label (section name) for each sentence. Various classifiers for text categorization, Naïve Bayesian Model (NBM) (Teufel and Moens, 2002; Ruch et al., 2007) , Hidden Markov Model (HMM) (Wu et al., 2006; Lin et al., 2006) , and Support Vector Machines (SVM) (McKnight and Arinivasan, 2003; Shimbo et al., 2003; Ito et al., 2004; Yamamoto and Takagi, 2005) were applied. Table 1 summarizes these approaches and performances. All studies target scientific abstracts except for Teufel and Moens (2002) who target scientific full papers. Field classes show the set of section names that each study assumes: background (B), objective/aim/purpose (O), method (M), result (R), conclusion (C), and introduction (I) that combines the background and objective. Although we should not compare directly the performances of these studies, which use a different set of classification labels and evaluation corpora, SVM classifiers appear to yield better results for this task. The rest of this section elaborates on the previous studies with SVMs. Shimbo et al. (2003) presented an advanced text retrieval system for Medline that can focus on a specific section in abstracts specified by a user. The system classifies sentences in each Medline abstract into four sections, objective, method, results, and conclusion. Each sentence is represented by words, word bigrams, and contextual information of the sentence (e.g., class of the previous sentence, relative location of the current sentence). They reported 91.9% accuracy (per-sentence basis) and 51.2% accuracy (per-abstract basis 1 ) for the classification with the best feature set for quadratic SVM. Ito et al. (2004) extended the work with a semi-supervised learning technique using transductive SVM (TSVM). Yamamoto and Takagi (2005) developed a system to classify abstract sentences into five sections, background, purpose, method, result, and conclusion. They trained a linear-SVM classifier with features such as unigram, subject-verb, verb tense, relative sentence location, and sentence score (average TF*IDF score of constituent words). Their method achieved 68.9%, 63.0%, 83.6%, 87.2%, 89.8% Fscores for classifying background, purpose, method, result, and conclusion sentences respectively. They also reported the classification performance of introduction sentences, which combines background and purpose sentences, with 91.3% F-score.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The previous studies regarded the task of identifying section names as a text-classification problem that determines a label (section name) for each sentence. ", "mid_sen": "Various classifiers for text categorization, Naïve Bayesian Model (NBM) (Teufel and Moens, 2002; Ruch et al., 2007) , Hidden Markov Model (HMM) (Wu et al., 2006; Lin et al., 2006) , and Support Vector Machines (SVM) (McKnight and Arinivasan, 2003; Shimbo et al., 2003; Ito et al., 2004; Yamamoto and Takagi, 2005) were applied. ", "after_sen": "Table 1 summarizes these approaches and performances. "}
{"citeStart": 187, "citeEnd": 198, "citeStartToken": 187, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "There are various technical difliculties with Geedali's account (see e.g. van Oirsouw, 1987, and Moltnmnn, 11992) . There is also a f'undanmnl,al l)lroblem COl,cerning semantic interpretation of coordinated structures (see Moltmanu, 1992 which provides a revised a.nd more complex 3-1) account based on Muadz, 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are various technical difliculties with Geedali's account (see e.g. van Oirsouw, 1987, and Moltnmnn, 11992) . ", "mid_sen": "There is also a f'undanmnl,al l)lroblem COl,cerning semantic interpretation of coordinated structures (see Moltmanu, 1992 which provides a revised a.nd more complex 3-1) account based on Muadz, 1991) .", "after_sen": "[\"ot: coordinatioll el' unlike categories, as in the examples in (15), Goodatl proposes a treatment some what similar to Sag el; al. (1985) . I[owe.w~r there is still a problem in dealing with examples where there are clit[>rent nurnbers of modifiers, such as (19a) or the lbllowing: 20) a. We can meet at the office or in London outside the theaure b 'FNT deliver efficiently and after 5pro in Edinburgh (~onsider example (b). "}
{"citeStart": 235, "citeEnd": 256, "citeStartToken": 235, "citeEndToken": 256, "sectionName": "UNKNOWN SECTION NAME", "string": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004) , where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001) , which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000) . However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. One of the reasons may be the deficiency of the dynamic programming-based systems, that the global information of sequences cannot be incorporated as features of the models. Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task. We had to wait until Tsai et al. (2006) , who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al. As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004) , where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. ", "mid_sen": "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001) , which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000) . ", "after_sen": "However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. "}
{"citeStart": 0, "citeEnd": 19, "citeStartToken": 0, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "A recent study by also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006) , with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, we also evaluate the method on alternate extrinsic loss functions. ", "mid_sen": "Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. ", "after_sen": "Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Since combining the classifiers as a stacking ensemble did not work, we turned to reformulating our problem as a cascade of two-class classifiers. Cascade generalization is the process of sequentially using a set of small classifiers to perform an overall classification task. Gama and Brazdil (2000) showed that a cascade can outperform other ensemble methods like stacking or boosting. Kaynak and Alpaydin (2000) proposed a method to sequentially cascade classifiers and showed that this improves the accuracy without increasing the computational complexity and cost. Although the creation of our classifier cascades in this paper is not the same as any of the above mentioned research, their conclusion that cascading subsets of classifiers to build an overall classifier can possibly result in a better accuracy was the main motivation for this experiment. The SMO implementation in WEKA also considers multi-class classification as a combination of pairwise binary classifications. But, in our subsequent experiments, we combine our two-class classifiers as a multi-stage cascade rather than a multiexpert stacking ensemble. For these experiments, we first built the various binary classifiers that were later used to construct the cascades. We chose our combinations both by using a One vs All (OvA) as well as a One vs One (OvO) strategy. Thus, six binary classifiers were created, namely: In all the cases, our training data consisted of equal number of instances per class. In the cases of the last three classifiers, the training data for NotA, NotB and NotC categories consisted of instances from both the classes that were included in the respective \"Not-\" classes. The data from the heldout test set was not included in any of these binary classification experiments. The training data size for each classifier has a different size depending on the classes involved. In all cases, the number of training samples per category is equal to the number of documents belonging to the category with the least number of documents. Hence, in cases involving the C-class (ABC, AC, BC, CnotC), we trained the classifiers with 250 documents per category. In all the other cases (AB, AnotA, BnotB), we trained the classifiers with 750 documents per category. Table 5 summarizes the training data size and the classification accuracies using 10-fold cross validation. All the models were trained using the SMO algorithm. This binary classification shows that there is a clear trend among the features across the proficiency levels. In the case of a pair-wise classification between classes, the highest classification accuracy was achieved for the binary classifier that considered the A and C classes. Although the classification accuracies of the binary classifiers (A,B) and (B,C) are considerably higher than the overall three class classification accuracy (Table 3) , they are very low compared to that of the binary classifier (A,C). The confusion between the three classes is the highest when it involves the middle class, B. This confirmed the ordinal nature of proficiency classification. In the second set of binary classifiers, again, the classifier with a poor performance turned out to be (B,NotB).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Cascade generalization is the process of sequentially using a set of small classifiers to perform an overall classification task. ", "mid_sen": "Gama and Brazdil (2000) showed that a cascade can outperform other ensemble methods like stacking or boosting. ", "after_sen": "Kaynak and Alpaydin (2000) proposed a method to sequentially cascade classifiers and showed that this improves the accuracy without increasing the computational complexity and cost. "}
{"citeStart": 123, "citeEnd": 136, "citeStartToken": 123, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "The most basic feature is the =x licensor feature, which cancels out a corresponding x licensee feature and projects. A simple example is a determiner selecting a noun to form a determiner phrase (akin to the context free rule DP → det noun). This is shown below (underline indicates canceled features, and the node label < indicates that the left item projects): n Niyogi (2001) has developed an agenda-driven chart parser for the feature-driven formalism described above; please refer to his paper for a description of the parsing algorithm. I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework. As an example, a simplified derivation of the sentence \"The tire flattened.\" is shown in Figure 1 . The currently implemented system is still at the \"toy parser\" stage. Although the effectiveness and coverage of my parser remains to be seen, similar approaches have been successful at capturing complex linguistic phenomena. With a minimal set of features and a small number of lexical entries, Niyogi (2001) has successfully modeled many of the argument alternations described by Levin (1993) using a Hale and Keyser (1993) style analysis. I believe that with a suitable lexicon (either hand crafted or automatically induced), my framework can be elaborated into a system whose performance is comparable to that of current statistical parsers, but with the added advantage of simultaneously providing a richer lexical semantic representation of the input sentence than flat predicate argument structures based on semantic roles.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A simple example is a determiner selecting a noun to form a determiner phrase (akin to the context free rule DP → det noun). ", "mid_sen": "This is shown below (underline indicates canceled features, and the node label < indicates that the left item projects): n Niyogi (2001) has developed an agenda-driven chart parser for the feature-driven formalism described above; please refer to his paper for a description of the parsing algorithm. ", "after_sen": "I have adapted it for my needs and developed grammar fragments that reflect my non-lexicalist semantic framework. "}
{"citeStart": 97, "citeEnd": 123, "citeStartToken": 97, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "For each metric, after determining how well a new and current technique performs on some test set according to that metric, one takes the difference between those results and asks \"is that difference significant?\" A way to test this is to expect no difference in the results (the null hypothesis) and to ask, assuming this expectation, how unusual are these results? One way to answer this question is to assume that the difference has a normal or t distribution (Box et al., 1978, Sec. 2.4) . Then one calculates the following:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each metric, after determining how well a new and current technique performs on some test set according to that metric, one takes the difference between those results and asks \"is that difference significant?\" A way to test this is to expect no difference in the results (the null hypothesis) and to ask, assuming this expectation, how unusual are these results? ", "mid_sen": "One way to answer this question is to assume that the difference has a normal or t distribution (Box et al., 1978, Sec. 2.4) . ", "after_sen": "Then one calculates the following:"}
{"citeStart": 85, "citeEnd": 112, "citeStartToken": 85, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "The Wilcoxon Signed Ranks (Wilcoxon, henceforth) test is a non-parametric test for statistical significance that is appropriate when there is one data sample and several measures. For example, to compare the accuracy of two parsers over the same data set. As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test. We use a 0.05 level of significance, and provide z-value probabilities for significant results reported below. These results are computed over microaveraged F 1 scores for each sentence in DepBank.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, to compare the accuracy of two parsers over the same data set. ", "mid_sen": "As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test. ", "after_sen": "We use a 0.05 level of significance, and provide z-value probabilities for significant results reported below. "}
{"citeStart": 107, "citeEnd": 140, "citeStartToken": 107, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "This small experiment demonstrates a number of points. Firstly, it seems reasonable to conclude that the assignment of individual codes to verbs is on the whole relatively accurate in LDOCE. Of the 139 verbs tested, we only found code omissions in 10 cases. Secondly though, when we consider the interaction between the assignments of codes and word sense classification, LDOCE appears less reliable. This is the primary source of error in the case of the Object Raising rule. Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. Ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed (Perlmutter and Soames, 1979:460ff.) . However, only two of these criteria are explicit in the coding system.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thirdly, it seems clear that the Object Raising rule is straining the limits of what can be reliably extracted from the LDOCE coding system. ", "mid_sen": "Ideally, to distinguish between raising and equi verbs, a number of syntactic criteria should be employed (Perlmutter and Soames, 1979:460ff.) . ", "after_sen": "However, only two of these criteria are explicit in the coding system."}
{"citeStart": 130, "citeEnd": 151, "citeStartToken": 130, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993) , word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b ), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993) , and statistical translation (Brown et al. 1993 ). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it is less related to ours, addressing, for example, word sense disambiguation in the source language by statistically examining context (e.g., collocations) in the source language, thus allowing appropriate word selection in the target language. (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993) , word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b ), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993) , and statistical translation (Brown et al. 1993 ). ", "after_sen": "Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. "}
{"citeStart": 152, "citeEnd": 157, "citeStartToken": 152, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "• UTTERANCE TYPES 4The theory of centering, which is part of attentional state, depends on discourse participants' recognizing the beginning and end of a discourse segment [BFP87, Wal89]. 5The relationship between utterance level meaning and discourse intentions rests on a theory of joint commitment or shared plans[GSg0, CLNO90, LCN90] Note that prompts are in direct contrast to the other options that a participant has available at any point in the discourse. By indicating that the speaker does not want the floor, prompts function on a number of levels, including the expression of understanding or agreement [Sch82] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "5The relationship between utterance level meaning and discourse intentions rests on a theory of joint commitment or shared plans[GSg0, CLNO90, LCN90] Note that prompts are in direct contrast to the other options that a participant has available at any point in the discourse. ", "mid_sen": "By indicating that the speaker does not want the floor, prompts function on a number of levels, including the expression of understanding or agreement [Sch82] .", "after_sen": "The rules for the allocation of control are based on the utterance type classification and allow a dialogue to be divided into segments that correspond to which speaker is the controller of the segment. "}
{"citeStart": 151, "citeEnd": 171, "citeStartToken": 151, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991) , multidocument summarization (McKeown et al., 2002) , automatic evaluation of MT (Denkowski and Lavie, 2010) , and TE (Dinu and Wang, 2009) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. ", "mid_sen": "They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991) , multidocument summarization (McKeown et al., 2002) , automatic evaluation of MT (Denkowski and Lavie, 2010) , and TE (Dinu and Wang, 2009) .", "after_sen": "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005) . "}
{"citeStart": 29, "citeEnd": 46, "citeStartToken": 29, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; Löbner, 1998) . Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; Löbner, 1998) . Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). ", "mid_sen": "We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. ", "after_sen": "We also exclude comparative anaphora (Modjeska et al., 2003) ."}
{"citeStart": 77, "citeEnd": 89, "citeStartToken": 77, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "Computational morphology has received considerable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and ödevinin, ödevini, ödevleri; bitmez, bitirileceginden, bitmesiyle, ... Kay, 1994) . It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. ", "mid_sen": "Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.", "after_sen": "Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajič et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009) . "}
{"citeStart": 77, "citeEnd": 96, "citeStartToken": 77, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use Lib-SVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. ", "after_sen": "We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. "}
{"citeStart": 101, "citeEnd": 115, "citeStartToken": 101, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "The Compiled-Earley (CE) parser is based on a predictive chart-based CF parsing algorithm devised by Schabes (1991) which is driven by a table compiling out the predictive component of Earley's (1970) parser. The size of the table is related linearly to the size of the grammar (unlike the LR technique). Schabes demonstrates that this parser always takes fewer steps than Earley's, although its time complexity is the same: O(n3). The space complexity is also cubic, since the parser uses Earley's representation of parse forests.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A weaker kind of cache on partial analyses (and thus unification results) was found to be necessary in the implementation, though, to avoid duplication of unifications; this sped the parser up by a factor of about three, at little space cost.", "mid_sen": "The Compiled-Earley (CE) parser is based on a predictive chart-based CF parsing algorithm devised by Schabes (1991) which is driven by a table compiling out the predictive component of Earley's (1970) parser. ", "after_sen": "The size of the table is related linearly to the size of the grammar (unlike the LR technique). "}
{"citeStart": 197, "citeEnd": 220, "citeStartToken": 197, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . However none of these methods has considered the way of dealing both phenomena in the same concrete system. We propose in this paper an algorithm that deals with both phenomena, in the same analyser. The anaphora module pertains to the recent methods, uses a set of resolution rules based on the focusing approach, see (Sidner, 1983) . These rules are applied to the conceptual representation and their output is a set of candidate antecedents. Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1999) . The disambiguation procedure alms at filling the empty roles using attachment rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . ", "after_sen": "However none of these methods has considered the way of dealing both phenomena in the same concrete system. "}
{"citeStart": 286, "citeEnd": 302, "citeStartToken": 286, "citeEndToken": 302, "sectionName": "UNKNOWN SECTION NAME", "string": "Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990) . Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012) . For the referring expression generation task here, we also need a lexicon with grounded semantics.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Grounded semantics provides a bridge to connect symbolic labels or words with lower level visual features (Harnad, 1990) . ", "mid_sen": "Previous work has developed various approaches for grounded semantics mainly for the reference resolution task, i.e., identifying visual objects in the environment given language descriptions (Dhande, 2003; Gorniak and Roy, 2004; Tenbrink and Moratz, 2003; Siebert and Schlangen, 2008; Liu et al., 2012) . ", "after_sen": "For the referring expression generation task here, we also need a lexicon with grounded semantics."}
{"citeStart": 145, "citeEnd": 164, "citeStartToken": 145, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Different encodings were provided, and the UTF8 data for all four corpora were used in this experiment. Following the format of Table 5 , the results for this bakeoff are shown in Table 6 . We chose the three models that achieved at least one best score in the closed tests from Emerson (2005) , as well as the sub-word-based model of Zhang et al. (2006) for comparison. Row \"Zh-a\" and \"Zh-b\" represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively. Again, our model achieved better overall accuracy than the majority of the other models. One system to achieve comparable accuracy with our system is Zh-b, which improves upon the sub-word CRF model (Zh-a) by combining it with an independent dictionary-based submodel and improving the accuracy of known words. In comparison, our system is based on a single perceptron model. In summary, closed tests for both the first and the second bakeoff showed competitive results for our system compared with the best results in the literature. Our word-based system achieved the best Fmeasures over the AS (96.5%) and CU (94.6%) corpora in the first bakeoff, and the CU (95.1%) and MR (97.2%) corpora in the second bakeoff.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following the format of Table 5 , the results for this bakeoff are shown in Table 6 . ", "mid_sen": "We chose the three models that achieved at least one best score in the closed tests from Emerson (2005) , as well as the sub-word-based model of Zhang et al. (2006) for comparison. ", "after_sen": "Row \"Zh-a\" and \"Zh-b\" represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively. "}
{"citeStart": 210, "citeEnd": 228, "citeStartToken": 210, "citeEndToken": 228, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005) , which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order.", "mid_sen": "Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005) , which are obtained using a parser trained to determine linguistic constituency. ", "after_sen": "Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997) ."}
{"citeStart": 38, "citeEnd": 75, "citeStartToken": 38, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "I now show that the question whether the intersection of a FSA and an off-line parsable DCG is empty is undecidable. A yes-no problem is undecidable (cf. (Hopcroft and Ullman, 1979, pp.178-179) ) if there is no algorithm that takes as its input an instance of the problem and determines whether the answer to that instance is 'yes' or 'no'. An instance of a problem consists of a particular choice of the parameters of that problem. I use Post's Correspondence Problem (PCP) as a well-known undecidable problem. I show that if the above mentioned intersection problem were decidable, then we could solve the PCP too. The following definition and example of a PCP are taken from (Hopcroft and Ullman, 1979) The sequence il, • •., im is a solution to this instance of PCP. As an example, assume that :C = {0,1}. Furthermore, let A = (1, 10111, 10) and B = 011, 10, 0). A solution to this instance of PCP is the sequence 2,1,1,3 (obtaining the sequence 10111Ul0). For an illustration, cf. figure 3.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I now show that the question whether the intersection of a FSA and an off-line parsable DCG is empty is undecidable. ", "mid_sen": "A yes-no problem is undecidable (cf. (Hopcroft and Ullman, 1979, pp.178-179) ) if there is no algorithm that takes as its input an instance of the problem and determines whether the answer to that instance is 'yes' or 'no'. ", "after_sen": "An instance of a problem consists of a particular choice of the parameters of that problem. "}
{"citeStart": 70, "citeEnd": 91, "citeStartToken": 70, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "We tried two methods to obtain grammatical roles. First, we tried extracting grammatical roles from the parse trees which we obtained from the Berkeley parser, as this information is present in the edge labels that can be recovered from the parse. However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008) , which tags nouns with their morphological case. Morphological case is distinct from grammatical role, as noun phrases can function as adjuncts in possessive constructions and preposi- tional phrases. However, we can approximate the grammatical role of an entity using the morphological case. We follow the annotation conventions of TüBa-D/Z in not assigning a grammatical role when the noun phrase is a prepositional object. We also do not assign a grammatical role when the noun phrase is in the genitive case, as genitive objects are very rare in German and are far outnumbered by the possessive genitive construction. Table 2 shows the results of the sentence ordering permutation detection experiment. The top four performing entity representations are all topological field-based, and they outperform grammatical role-based and simple clausal order-based models. These results indicate that the information that topological fields provide about clause structure, appositives, right dislocation, etc. which is not captured by simple clausal order is important for coherence modelling. The representations incorporating linguistics-based heuristics do not outperform purely topological field-based models. Surprisingly, the VF-based models fare quite poorly, performing worse than not adding any annotations, despite the fact that topological fieldbased models in general perform well. This result may be a result of the heterogeneous uses of the VF.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, we tried extracting grammatical roles from the parse trees which we obtained from the Berkeley parser, as this information is present in the edge labels that can be recovered from the parse. ", "mid_sen": "However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008) , which tags nouns with their morphological case. ", "after_sen": "Morphological case is distinct from grammatical role, as noun phrases can function as adjuncts in possessive constructions and preposi- tional phrases. "}
{"citeStart": 39, "citeEnd": 56, "citeStartToken": 39, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "This result is different from that in (Wu and Wang, 2004) , where their method achieved an error rate reduction of 21.96% as compared with the method \"Gen+Spec\". The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004) . The training data and the testing data described in (Wu and Wang, 2004) are from a single manual. The data in our corpus are from several manuals describing how to use the diagnostic ultrasound systems.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "This result is different from that in (Wu and Wang, 2004) , where their method achieved an error rate reduction of 21.96% as compared with the method \"Gen+Spec\". ", "after_sen": "The main reason is that the in-domain training corpus and testing corpus in this paper are different from those in (Wu and Wang, 2004) . "}
{"citeStart": 114, "citeEnd": 135, "citeStartToken": 114, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "I The telalion between a WARRANT and the PR(II,OSI{ COlllmunicative act is similar to the MOTIVATION relation of [Moore and Paris, 1993; Mann and Thompson, 1987] . A WARRANT is always optional; this is consistent with the RST fl'anlewolk in which all satellites are optional information.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "~= Clearly lhe rule does not hold.", "mid_sen": "I The telalion between a WARRANT and the PR(II,OSI{ COlllmunicative act is similar to the MOTIVATION relation of [Moore and Paris, 1993; Mann and Thompson, 1987] . A WARRANT is always optional; this is consistent with the RST fl'anlewolk in which all satellites are optional information.", "after_sen": "2The WARRANT having lhe desired cflect of getting the hearer io listen to l{amesh depends till ltle hearel previously believing or coming Io believe that hldians know of good Indian restaurants [Webber arid ] A Warrant IR/J such a.S that in 2 suggests tllat B's cognilive limitations [nay be a factor in what A cllooses to say, so thai even if B knows a wlu'rant for adopting A's proposed, what is critical is whetlm,' the warrant is salient lot B, i.e. whether the warnmt is ah'eady accessible in B's working memory [Prince, 1981; Baddelcy, 1986] . 11 the w~u'rant is not already salient, then B must either infer or retrieve the wlurant information or obtain it from an external source in order to evaluate A's proposed. "}
{"citeStart": 65, "citeEnd": 77, "citeStartToken": 65, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "mid_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . ", "after_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) ."}
{"citeStart": 150, "citeEnd": 172, "citeStartToken": 150, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "To handle the contextual information, phrasebased models were introduced (Koehn et al., 2003) . The phrase-based models use the word alignment information from the IBM models and train source-target phrase pairs for lexical selection (phrase-table) and distortions of source phrases (reordering-table). These models are still relatively local, as the target phrases are tightly associated with their corresponding source phrases. In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model. Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast to a phrase-based model, a discriminative model has the power to integrate much richer contextual information into the training model. ", "mid_sen": "Contextual information is extremely useful in making lexical selections of higher quality, as illustrated by the models for Global Lexical Selection (Bangalore et al., 2007; Venkatapathy and Bangalore, 2009) .", "after_sen": "However, the limitation of global lexical selection models has been sentence construction. "}
{"citeStart": 21, "citeEnd": 34, "citeStartToken": 21, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "The distribution of VP-ellipsis has also been shown to be sensitive to the coherence relationship extant between the source and target clauses, but in a different respect. In a previous paper (Kehler, 1993b) , five contexts for VP-ellipsis were examined to determine whether the representations retrieved are syntactic or semantic in nature. Evidence was given that VP-ellipsis copies syntactic representations in what was termed parallelconstructions (predicting the unacceptability of the voice mismatch in example (6) and nominalized source in example (8)), but copies semantic representations in non-parallel constructions (predicting the acceptability of the voice mismatch in example (7) and the nominalized source in example (9)): 2 (6) # The decision was reversed by the FBI, and the ICC did too. [ reverse the decision ] (7) In March, four fireworks manufacturers asked that the decision be reversed, and on Monday the ICC did. [ reverse the decision ]", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The distribution of VP-ellipsis has also been shown to be sensitive to the coherence relationship extant between the source and target clauses, but in a different respect. ", "mid_sen": "In a previous paper (Kehler, 1993b) , five contexts for VP-ellipsis were examined to determine whether the representations retrieved are syntactic or semantic in nature. ", "after_sen": "Evidence was given that VP-ellipsis copies syntactic representations in what was termed parallelconstructions (predicting the unacceptability of the voice mismatch in example (6) and nominalized source in example (8)), but copies semantic representations in non-parallel constructions (predicting the acceptability of the voice mismatch in example (7) and the nominalized source in example (9)): "}
{"citeStart": 78, "citeEnd": 100, "citeStartToken": 78, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "paradigmatic relation: how the words are associated with each other. Similarity between words can be defined by either a syntagmatic or a paradigmatic relation. Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. This paper concentrates on paradigmatic similarity, because a paradigmatic relation can be established both inside a sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside a sentence --like syntax deals with sentence structure. The rest of this section focuses on two related works on measuring paradigmatic similarity --a psycholinguistic approach and a thesaurus-based approach.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similarity between words can be defined by either a syntagmatic or a paradigmatic relation. ", "mid_sen": "Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. ", "after_sen": "Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. "}
{"citeStart": 151, "citeEnd": 166, "citeStartToken": 151, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "We hypolhcsizcd lhal a warrant Inllsi be ,'qAIJENT for hoth agents (as shown by example 2). In l)esign-Wodd, salience is modeled by AWM model, adapted lronl [Landauer, 1975 I. While the AWM model is extremely sin> pie, [,andauer showed thai it could be pm'ameterized lo 1il many empirical resells on human memory and learning [Baddeley, 19861 . AWM consists of a three dimensional slsace in which propositions acquired Dora perceiving the world are stored in chronological sequence according 1o tile localion o1' a moving memory pointef. The sequence of memory loci iised lof slotage constitutes a fI.Itldolll walk lhrough memory wilh each loci a shorl dislance lfonl tile previous one. If items are encountefed illtllliple times, they me stored nmlliple limes [Itinlzmann and Block, 1971] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In l)esign-Wodd, salience is modeled by AWM model, adapted lronl [Landauer, 1975 I. ", "mid_sen": "While the AWM model is extremely sin> pie, [,andauer showed thai it could be pm'ameterized lo 1il many empirical resells on human memory and learning [Baddeley, 19861 . AWM consists of a three dimensional slsace in which propositions acquired Dora perceiving the world are stored in chronological sequence according 1o tile localion o1' a moving memory pointef. ", "after_sen": "The sequence of memory loci iised lof slotage constitutes a fI.Itldolll walk lhrough memory wilh each loci a shorl dislance lfonl tile previous one. "}
{"citeStart": 13, "citeEnd": 30, "citeStartToken": 13, "citeEndToken": 30, "sectionName": "UNKNOWN SECTION NAME", "string": "In (McCarthy et al., 2004) , a method was presented to determine the predominant sense of a word in a corpus. However, in (Chan and Ng, 2005) , we showed that in a supervised setting where one has access to some annotated training data, the EMbased method in section 5 estimates the sense priors more effectively than the method described in (Mc-Carthy et al., 2004) . Hence, we use the EM-based algorithm to estimate the sense priors in the WSJ evaluation data for each of the 21 nouns. The sense with the highest estimated sense prior is taken as the predominant sense of the noun.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (McCarthy et al., 2004) , a method was presented to determine the predominant sense of a word in a corpus. ", "mid_sen": "However, in (Chan and Ng, 2005) , we showed that in a supervised setting where one has access to some annotated training data, the EMbased method in section 5 estimates the sense priors more effectively than the method described in (Mc-Carthy et al., 2004) . ", "after_sen": "Hence, we use the EM-based algorithm to estimate the sense priors in the WSJ evaluation data for each of the 21 nouns. "}
{"citeStart": 155, "citeEnd": 167, "citeStartToken": 155, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a new space and a new metric for computing this distance. Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of questionanswer pairs (Q, S A ), we can train a probabilistic model for estimating the conditional probability P(Q | S A ). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR engine, one can find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a new space and a new metric for computing this distance. ", "mid_sen": "Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. ", "after_sen": "This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. "}
{"citeStart": 124, "citeEnd": 136, "citeStartToken": 124, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma 2 toolkit (Hulden, 2009) . After adapting the grammar and compiling it, the 55 separate transducers occupy 607 kB and operate at roughly 2,000 complete verb chains per second. 3 Passing the strings from one transducer to the next in the chain of 55 transducers in accomplished by the depth-first-search transducer chaining functionality available in the foma API.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This verbal chunk transfer module read and applied these regular expressions at a speed of 50 verbal chunks per second.", "mid_sen": "In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma 2 toolkit (Hulden, 2009) . ", "after_sen": "After adapting the grammar and compiling it, the 55 separate transducers occupy 607 kB and operate at roughly 2,000 complete verb chains per second. "}
{"citeStart": 114, "citeEnd": 125, "citeStartToken": 114, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. We refer to these as linguistic level representations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. ", "mid_sen": "Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. ", "after_sen": "We refer to these as linguistic level representations."}
{"citeStart": 93, "citeEnd": 117, "citeStartToken": 93, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "Reordering was applied on the source side prior to decoding through the generation of lattices encoding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus. For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007) . To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction. . When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus. ", "mid_sen": "For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007) . ", "after_sen": "To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction. . "}
{"citeStart": 3, "citeEnd": 15, "citeStartToken": 3, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "In Lauer (1994) , the degree of acceptability is again provided by statistical measures over a corpus. The metric used is a mutual information-like measure based on probabilities of modification relationships. This is derived from the idea that parse trees capture the structure of semantic relationships within a noun compound. 1", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figure 1 shows a graphical comparison of the two analysis models.", "mid_sen": "In Lauer (1994) , the degree of acceptability is again provided by statistical measures over a corpus. ", "after_sen": "The metric used is a mutual information-like measure based on probabilities of modification relationships. "}
{"citeStart": 211, "citeEnd": 224, "citeStartToken": 211, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, Kendall's % which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996) , is a nonparametric measure of the association between random variables (Gibbons, 1993) . In our context, it looks for correlation between the behavior of q and r on pairs of verbs. Three versions exist; we use the simplest, Ta, here:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that it incorporates unigram probabilities as well as the two distributions q and r.", "mid_sen": "Finally, Kendall's % which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996) , is a nonparametric measure of the association between random variables (Gibbons, 1993) . ", "after_sen": "In our context, it looks for correlation between the behavior of q and r on pairs of verbs. "}
{"citeStart": 274, "citeEnd": 291, "citeStartToken": 274, "citeEndToken": 291, "sectionName": "UNKNOWN SECTION NAME", "string": "Step 3. Finding translation pairs: A pair of words is treated as a translation pair when their context similarity is high. Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Déjean et al., 2002) , co-occurrence models generated from aligned documents (Prochasson and Fung, 2011) , and transliteration information (Shao and Ng, 2004) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Step 3. Finding translation pairs: A pair of words is treated as a translation pair when their context similarity is high. ", "mid_sen": "Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (Déjean et al., 2002) , co-occurrence models generated from aligned documents (Prochasson and Fung, 2011) , and transliteration information (Shao and Ng, 2004) .", "after_sen": ""}
{"citeStart": 190, "citeEnd": 207, "citeStartToken": 190, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "To verify the effectiveness of our term weighting schemes in experimental settings of the datadriven approach, we carry out a set of simple experiments with ML classifiers. Specifically, we explore the statistical term weighting features of the word generation model with Support Vector machine (SVM), faithfully reproducing previous work as closely as possible (Pang et al., 2002) . Each instance of train and test data is represented as a vector of features. We test various combinations of the term weighting schemes listed below.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To verify the effectiveness of our term weighting schemes in experimental settings of the datadriven approach, we carry out a set of simple experiments with ML classifiers. ", "mid_sen": "Specifically, we explore the statistical term weighting features of the word generation model with Support Vector machine (SVM), faithfully reproducing previous work as closely as possible (Pang et al., 2002) . ", "after_sen": "Each instance of train and test data is represented as a vector of features. "}
{"citeStart": 90, "citeEnd": 102, "citeStartToken": 90, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. ", "mid_sen": "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. ", "after_sen": "This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. "}
{"citeStart": 5, "citeEnd": 10, "citeStartToken": 5, "citeEndToken": 10, "sectionName": "UNKNOWN SECTION NAME", "string": "The [GJW86] centering model is based on the following assumptions. A discourse segment consists of a sequence of utterances U1 ..... U,~. With each utterance Ua is associated a list of forward.looking cen-~ers, Cf(U,) , consisting of those discourse entities that are directly realized or realized I by linguistic expressions in the utterance. Ranking of an entity on this list corresponds roughly to the likelihood that it will be the primary focus of subsequent discourse; the first entity on this list is the preferred cen~er, Cp (U, O. U,~ actually centers, or is \"about\", only one entity at a time, the backward-looking cen~er, Cb(U=). The backward center is a confirmation of an entity that has already been introduced into the discourse; more specifically, it must be realized in the immediately preceding utterance, Un-1. There are several distinct types of transitions from one utterance to the next. The typology of transitions is based on two factors: whether or not the center of attention, Cb, is the same from Un-1 to Un, and whether or not this entity coincides with the preferred center of U,~. Definitions of these transition types appear in figure 1.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For instance, it takes longer for hearers to process a pronominalized noun phrase that is no~ in focus than one that is, while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not [Gui85] .", "mid_sen": "The [GJW86] centering model is based on the following assumptions. ", "after_sen": "A discourse segment consists of a sequence of utterances U1 ..... U,~. With each utterance Ua is associated a list of forward."}
{"citeStart": 14, "citeEnd": 36, "citeStartToken": 14, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popović and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. More recently, (Goldwater and McClosky, 2005) In general, this line of work focused on translating from morphologically rich languages into English; there has been limited research in MT in the opposite direction. Koehn (2005) includes a survey of statistical MT systems in both directions for the Europarl corpus, and points out the challenges of this task. A recent work (El-Kahlout and Oflazer, 2006) experimented with English-to-Turkish translation with limited success, suggesting that inflection generation given morphological features may give positive results. In the current work, we suggest a probabilistic framework for morphology generation performed as post-processing. It can therefore be considered as complementary to the techniques described above. Our approach is general in that it is not specific to a particular language pair, and is novel in that it allows modelling of agreement on the target side. The framework suggested here is most closely related to (Suzuki and Toutanova, 2006) , which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MT. This work can be viewed as a generalization of (Suzuki and Toutanova, 2006) in that our model generates inflected forms of words, and is not limited to generating a small, closed set of case markers. In addition, the morphology generation problem is more challenging in that it requires handling of complex agreement phenomena along multiple morphological dimensions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. ", "mid_sen": "Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. ", "after_sen": "(Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. "}
{"citeStart": 160, "citeEnd": 184, "citeStartToken": 160, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al., 2003) , as described in Clark and Curran (2007a) . Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. Clark and Curran (2007a) report an upper bound on performance, using gold-standard CCGbank dependencies, of 84.76% F-score. This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. With our new version of CCGbank, the parser will be able to recover these GRs correctly, where before this was unlikely.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One problem with the evaluation in the previous section, is that the original CCGbank is not expected to recover internal NP structure, making its task easier and inflating its performance. ", "mid_sen": "To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) reannotation of Dep-Bank (King et al., 2003) , as described in Clark and Curran (2007a) . ", "after_sen": "Parser output is made similar to the grammatical relations (GRs) of the Briscoe and Carroll (2006) data, however, the conversion remains complex. "}
{"citeStart": 54, "citeEnd": 66, "citeStartToken": 54, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "The same methodology can be applied to modeling conversational impIicatures in indirect replies (Green, 1992 ). Green's algorithm makes use of discourse expectations, discourse plans, and discourse relations. The following dialog is considered (Green, 1992, p. Answer (17) conveys a \"yes\", but a reply consisting only of (17)a would implicate a \"no\". As Green notices, in previous models of implicatures (Gazdar, 1979; Hirschberg, 1985) , processing (17)a will block the implicature generated by (17)c. Green solves the problem by extending the boundaries of the analysis to discourse units. Our approach does not exhibit these constraints. As in the previous example, the one dealing with a sequence of utterances, we obtain a different interpretation after each step. When the question is asked, there is no conversational implicature. Answer (17)a makes the necessary conditions for implicating \"no\" true, and the implication is computed. Answer (17)b reinforces a previous condition. Answer (17)c makes the preconditions for implicating a \"no\" false, and the preconditions for implicating a \"yes\" true. Therefore, the implicature at the end of the dialogue is that the conversant who answered went shopping.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The following dialog is considered (Green, 1992, p. Answer (17) conveys a \"yes\", but a reply consisting only of (17)a would implicate a \"no\". ", "mid_sen": "As Green notices, in previous models of implicatures (Gazdar, 1979; Hirschberg, 1985) , processing (17)a will block the implicature generated by (17)c. ", "after_sen": "Green solves the problem by extending the boundaries of the analysis to discourse units. "}
{"citeStart": 84, "citeEnd": 97, "citeStartToken": 84, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "This is the rule that is derived from the headrecursive vp rule when the partially specified subcategorization list is considered as filtering information (cf., fn. 1). The rule builds up infinitely large subcategorization lists of which eventually only one is to be matched against the subcategorization list of, e.g., the lexical entry for \"buys\". Though this rule is not cyclic, it becomes cyclic upon off-line abstraction: Through trimming this magic rule, e.g., given a bounded term depth or a restrictor (Shieber, 1985) , constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule. This information can then be used to discard the culprit.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Though this rule is not cyclic, it becomes cyclic upon off-line abstraction: ", "mid_sen": "Through trimming this magic rule, e.g., given a bounded term depth or a restrictor (Shieber, 1985) , constructing an abstract unfolding tree reveals the fact that a cycle results from the magic rule. ", "after_sen": "This information can then be used to discard the culprit."}
{"citeStart": 76, "citeEnd": 89, "citeStartToken": 76, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "A second method is to structure the translated query, separating the translations for one term from translations for other terms. This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. There are several variations of such a method (Ballesteros and Croft, 1998; Pirkola, 1998; Hull 1997) . One such method is to treat different translations of the same term as synonyms. Ballesteros, for example, used the INQUERY (Callan et al, 1995) synonym operator to group translations of different query terms. However, if a term has two translations in the target language, it will treat them as equal even though one of them is more likely to be the correct translation than the other. By contrast, our HMM approach supports translation probabilities. The synonym approach is equivalent to changing all non-zero translation probabilities P(W~[ Wy)'s to 1 in our retrieyal function. Even estimating uniform translation probabilities gives higher weights to unambiguous translations and lower weights to highly ambiguous translations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This approach limits how much credit the retrieval algorithm can give to a single term in the original query and prevents the translations of one or a few terms from swamping the whole query. ", "mid_sen": "There are several variations of such a method (Ballesteros and Croft, 1998; Pirkola, 1998; Hull 1997) . ", "after_sen": "One such method is to treat different translations of the same term as synonyms. "}
{"citeStart": 10, "citeEnd": 36, "citeStartToken": 10, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "Following Atterer and Schütze (2007) , we wrote a script that, given a parse tree, identifies instances of PP attachment ambiguity and outputs the (v,n1,p,n2) quadruple involved and the attachment decision. This extraction system uses Collins' rules (based on TREEP (Chiang and Bikel, 2002) ) to locate the heads of phrases. Over the combined gold-standard parsing dataset, our script extracted a total of 2,541 PP attachment quadruples. As with the parsing data, we partitioned the data into 3 sets: 80% training, 10% development and 10% test data. Once again, this dataset and the script used to extract the quadruples are available on request to the research community.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following Atterer and Schütze (2007) , we wrote a script that, given a parse tree, identifies instances of PP attachment ambiguity and outputs the (v,n1,p,n2) quadruple involved and the attachment decision. ", "after_sen": "This extraction system uses Collins' rules (based on TREEP (Chiang and Bikel, 2002) ) to locate the heads of phrases. "}
{"citeStart": 184, "citeEnd": 196, "citeStartToken": 184, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006) . However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006) . ", "after_sen": "However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. "}
{"citeStart": 35, "citeEnd": 59, "citeStartToken": 35, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "In the demo of the system (available from http ://12r. cs. uiuc. edu/'cogcomp/eoh/index, html), an additional layer of chaining is used. Raw sentences are supplied as input and are processed using a SNoW based POS tagger (Roth and Ze!enko, 1998) first. 4There are other ways to define the B annotation, e.g., as always marking the beginning of a phrase. The indicating that the NPs are I, California and last May. This approach has been studied in (Ramshaw and Marcus, 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The indicating that the NPs are I, California and last May. ", "mid_sen": "This approach has been studied in (Ramshaw and Marcus, 1995) .", "after_sen": ""}
{"citeStart": 153, "citeEnd": 165, "citeStartToken": 153, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "Linguistic typology aims to distinguish between logically possible languages and actually observed languages. A fundamental building block for such an understanding is the universal implication (Greenberg, 1963) . These are short statements that restrict the space of languages in a concrete way (for instance \"object-verb ordering implies adjective-noun ordering\"); Croft (2003) , Hawkins (1983) and Song (2001) provide excellent introductions to linguistic typology. We present a statistical model for automatically discovering such implications from a large typological database (Haspelmath et al., 2005) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A fundamental building block for such an understanding is the universal implication (Greenberg, 1963) . ", "mid_sen": "These are short statements that restrict the space of languages in a concrete way (for instance \"object-verb ordering implies adjective-noun ordering\"); Croft (2003) , Hawkins (1983) and Song (2001) provide excellent introductions to linguistic typology. ", "after_sen": "We present a statistical model for automatically discovering such implications from a large typological database (Haspelmath et al., 2005) ."}
{"citeStart": 330, "citeEnd": 342, "citeStartToken": 330, "citeEndToken": 342, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . However none of these methods has considered the way of dealing both phenomena in the same concrete system. We propose in this paper an algorithm that deals with both phenomena, in the same analyser. The anaphora module pertains to the recent methods, uses a set of resolution rules based on the focusing approach, see (Sidner, 1983) . These rules are applied to the conceptual representation and their output is a set of candidate antecedents. Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1999) . The disambiguation procedure alms at filling the empty roles using attachment rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . ", "after_sen": "However none of these methods has considered the way of dealing both phenomena in the same concrete system. "}
{"citeStart": 135, "citeEnd": 158, "citeStartToken": 135, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. We did not use HIRAGANA expressions because they are also used in function words. Next, to evaluate inter annotator agreement, 207 randomly selected statement pairs were annotated by two human annotators. The annotators agreed in their judgment for 81.6% of the examples, which corresponds to a kappa level of 0.49. The annotation results are evaluated by calculating recall and precision in which one annotation result is treated as a gold standard and the other's as the output of the system, as shown in Talbe 2. ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We calculate the lexical similarity between the two sentences based on BOW. ", "mid_sen": "We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. ", "after_sen": "According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. "}
{"citeStart": 148, "citeEnd": 162, "citeStartToken": 148, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars (PCFGs) (Booth and Thomson, 1973; Baker, 1979) . However, early results on the utility of PCFGs for parse disambiguation and language modeling were somewhat disappointing. A conviction arose that lexicalized PCFGs (where head words annotate phrasal nodes) were the key tool for high performance PCFG parsing. This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (Ford et al., 1982; Hindle and Rooth, 1993) . In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001 ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such as PP attachments (Ford et al., 1982; Hindle and Rooth, 1993) . ", "mid_sen": "In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001 ).", "after_sen": "However, several results have brought into question how large a role lexicalization plays in such parsers. "}
{"citeStart": 75, "citeEnd": 94, "citeStartToken": 75, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "One cannot directly compare the two systems from the descriptions given in Ferro et al. (1999) and Buchholz et al. (1999) , as the results in the descriptions were based on different data sets and on different assumptions of what is known and what needs to be found.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "One cannot directly compare the two systems from the descriptions given in Ferro et al. (1999) and Buchholz et al. (1999) , as the results in the descriptions were based on different data sets and on different assumptions of what is known and what needs to be found.", "after_sen": "Here we test how well the systems perform using the same small annotated training set, the 3299 words of elementary school reading comprehension test bodies used in Ferro et al. (1999) . 2 We are mainly interested in comparing the parts of the system that takes in syntax (noun, verb, etc.) chunks (also known as groups) and finds the GRs between those chunks. "}
{"citeStart": 176, "citeEnd": 198, "citeStartToken": 176, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "To assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll & Briscoe, 1993) , a wide-coverage grammar of English. The grammar is defined in metagrammatical formalism which is compiled into a unification-based 'object gran~mar'--a syntactic variant of the Definite Clause Grammar formalism (Pereira & Warren, 1980 )--containing 84 features and 782 phrase structure rules. Parsing uses fixed-arity term unification. The grammar provides full coverage of the following constructions: declarative sentences, imperatives and questions (yes/no, tag and wh-questions); all unbounded dependency types (topicalisation, relativisation, wh-questions); a relatively exhaustive treatment of verb and adjective complement types; phrasal and prepositional verbs of many complement types; passivisation; verb phrase extraposition; sentence and verb phrase modification; noun phrase complements and pre-and post-modification; partitives; coordination of all major category types; and nominal and adjectival comparatives.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To assess the practical performance of the three unification-based parsers described above, a series of experiments were conducted using the ANLT grammar (Grover, Carroll & Briscoe, 1993) , a wide-coverage grammar of English. ", "mid_sen": "The grammar is defined in metagrammatical formalism which is compiled into a unification-based 'object gran~mar'--a syntactic variant of the Definite Clause Grammar formalism (Pereira & Warren, 1980 )--containing 84 features and 782 phrase structure rules. ", "after_sen": "Parsing uses fixed-arity term unification. "}
{"citeStart": 70, "citeEnd": 81, "citeStartToken": 70, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "The Web is being used to address data sparseness for language modeling. In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al. (2003) \"balance\" their corpus using Web documents.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Web is being used to address data sparseness for language modeling. ", "mid_sen": "In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al. (2003) \"balance\" their corpus using Web documents.", "after_sen": "The information retrieval community now has a Web track as a component of its TREC evaluation initiative. "}
{"citeStart": 235, "citeEnd": 247, "citeStartToken": 235, "citeEndToken": 247, "sectionName": "UNKNOWN SECTION NAME", "string": "Templates are the easiest way to implement surface NLG. A template for describing a flight noun phrase in the air travel domain might be flight departing from $city-fr at $time-dep and arriving in $city-to at $time-arr where the words starting with \"$\" are actually variables -representing the departure city, and departure time, the arrival city, and the arrival time, respectively-whose values will be extracted from the environment in which the template is used. The approach of writing individual templates is convenient, but may not scale to complex domains in which hundreds or thousands of templates would be necessary, and may have shortcomings in maintainability and text quality (e.g., see (Reiter, 1995) for a discussion).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A template for describing a flight noun phrase in the air travel domain might be flight departing from $city-fr at $time-dep and arriving in $city-to at $time-arr where the words starting with \"$\" are actually variables -representing the departure city, and departure time, the arrival city, and the arrival time, respectively-whose values will be extracted from the environment in which the template is used. ", "mid_sen": "The approach of writing individual templates is convenient, but may not scale to complex domains in which hundreds or thousands of templates would be necessary, and may have shortcomings in maintainability and text quality (e.g., see (Reiter, 1995) for a discussion).", "after_sen": "There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996) , KPML (Bateman, 1996) , MUMBLE (Meteer et al., 1987) , and RealPro (Lavoie and Rambow, 1997) , which produce natural language text from an abstract semantic representation. "}
{"citeStart": 117, "citeEnd": 134, "citeStartToken": 117, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "We also plan to explore finer emotional meaning distinctions, by using a hierarchical sequential model which better corresponds to different levels of cognitive difficulty in emotional categorization by humans, and to classify the full set of basic level emotional categories discussed in section 4.3. Sequential modeling of simple classifiers has been successfully employed to question classification, for example by (Li and Roth, 2002) . In addition, we are working on refining and improving the feature set, and given more data, tuning can be improved on a sufficiently large development set. The three subcorpora in the annotation project can reveal how authorship affects emotion perception and classification.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also plan to explore finer emotional meaning distinctions, by using a hierarchical sequential model which better corresponds to different levels of cognitive difficulty in emotional categorization by humans, and to classify the full set of basic level emotional categories discussed in section 4.3. ", "mid_sen": "Sequential modeling of simple classifiers has been successfully employed to question classification, for example by (Li and Roth, 2002) . ", "after_sen": "In addition, we are working on refining and improving the feature set, and given more data, tuning can be improved on a sufficiently large development set. "}
{"citeStart": 134, "citeEnd": 156, "citeStartToken": 134, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "(d) Distinguishing two kinds of adjectives: those which denote simple type (rouge (red), grand (big), etc.) and those like mental adjectives which denote dotted type. This distinc-tion is in accordance with the classical distinction drawn between stative adjectives and dynamic ones, which, following Quirk et al., 1994:434 , denote qualities that are thought to be subject to control by possessor. GL allows this distinction to be characterized and given a more formal representation, an adjective being dynamic if it refers to the cause or its further manifestation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(d) Distinguishing two kinds of adjectives: those which denote simple type (rouge (red), grand (big), etc.) and those like mental adjectives which denote dotted type. ", "mid_sen": "This distinc-tion is in accordance with the classical distinction drawn between stative adjectives and dynamic ones, which, following Quirk et al., 1994:434 , denote qualities that are thought to be subject to control by possessor. ", "after_sen": "GL allows this distinction to be characterized and given a more formal representation, an adjective being dynamic if it refers to the cause or its further manifestation."}
{"citeStart": 90, "citeEnd": 110, "citeStartToken": 90, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "We used a joint n-gram model for the graphemeto-phoneme conversion task. Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003) . Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. Examples of such approaches using Hidden Markov Models are (Rentzepopoulos and Kokkinakis, 1991 ) (who applied the HMM to the related task of phoneme-to-grapheme conversion), (Taylor, 2005) and (Minker, 1996) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used a joint n-gram model for the graphemeto-phoneme conversion task. ", "mid_sen": "Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003) . ", "after_sen": "Models that do not use joint letter-phoneme states, and therefore are not conditional on the preceding letters, but only on the actual letter and the preceding phonemes, achieved inferior results. "}
{"citeStart": 80, "citeEnd": 92, "citeStartToken": 80, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the TRIPS dialogue parser (Allen et al., 2007) to parse the utterances. The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. The contextual interpreter then uses a reference resolution approach similar to Byron (2002) , and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output. Utterance content is represented as a set of extracted objects and relations between them. Negation is supported, together with a heuristic scoping algorithm. The interpreter also performs basic ellipsis resolution. For example, it can determine that in the answer to the question \"Which bulbs will be on and which bulbs will be off in this diagram?\", \"off\" can be taken to mean \"all bulbs in the di-agram will be off.\" The resulting output is then passed on to the domain reasoning and diagnosis components.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The parser provides a domain-independent semantic representation including high-level word senses and semantic role labels. ", "mid_sen": "The contextual interpreter then uses a reference resolution approach similar to Byron (2002) , and an ontology mapping mechanism (Dzikovska et al., 2008a) to produce a domain-specific semantic representation of the student's output. ", "after_sen": "Utterance content is represented as a set of extracted objects and relations between them. "}
{"citeStart": 251, "citeEnd": 273, "citeStartToken": 251, "citeEndToken": 273, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based ma-chine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012) ). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, classification of sentences according to argumentative zones (AZ) -an information structure scheme that is applicable across scientific domains (Teufel et al., 2009 ) -can support information retrieval, information extraction and summarization (Teufel and Moens, 2002; Tbahriti et al., 2006; Ruch et al., 2007; Liakata et al., 2012; Contractor et al., 2012) .", "mid_sen": "Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based ma-chine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; Guo et al., 2010; Liakata et al., 2012) ). ", "after_sen": "Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76."}
{"citeStart": 88, "citeEnd": 109, "citeStartToken": 88, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "The system consists of a series of components which communicate through a facilitator component ( Figure 5 ). This develops and extends upon the multimodal architecture underlying the MATCH system (Johnston et al., 2002) . The underlying database of movie information is stored in XML format. When a new database is available, a Grammar Compiler component extracts and normalizes the relevant fields from the database. These are used in conjunction with a predefined multimodal grammar template and any available corpus training data to build a multimodal understanding model and speech recognition language model. The user interacts with the multimodal user interface client (Multimodal UI), which provides the graphical display. When the user presses 'CLICK TO SPEAK' a message is sent to the Speech Client, which activates the microphone and ships audio to a speech recognition server. Handwritten inputs are processed by a handwriting recognizer embedded within the multimodal user interface client. Speech recognition results, pointing gestures made on the display, and handwritten inputs, are all passed to a multimodal understanding server which uses finite-state multimodal language proc-essing techniques (Johnston and Bangalore, 2005) to interpret and integrate the speech and gesture. This model combines alignment of multimodal inputs, multimodal integration, and language understanding within a single mechanism. The resulting combined meaning representation (represented in XML) is passed back to the multimodal user interface client, which translates the understanding results into an XPATH query and runs it against the movie database to determine the new series of results. The graphical display is then updated to represent the latest query.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system consists of a series of components which communicate through a facilitator component ( Figure 5 ). ", "mid_sen": "This develops and extends upon the multimodal architecture underlying the MATCH system (Johnston et al., 2002) . ", "after_sen": "The underlying database of movie information is stored in XML format. "}
{"citeStart": 355, "citeEnd": 378, "citeStartToken": 355, "citeEndToken": 378, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ( [Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 7, "citeEnd": 19, "citeStartToken": 7, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "By extending the analysis of temporal subordinate clauses in (Kamp and Reyle, 1993) , to sentences which include quantification over eventualities, we can propose an alternative DRT solution to Partee's quantification problem. As in (Partee, 1984) , such sentences trigger box-splitting. But now, the location time of the eventuality in the subordinate clause serves as the antecedent for the location time of the eventuality in the main clause. In this approach, each of the relevant temporal markers resides in its appropriate box, yielding the correct quantificational structure. This quantification structure does not need to be stipulated as part of the Q-adverb's meaning, but arises directly from the temporal system. We illustrate this analysis by constructing a DRS in Figure lb for sentence 1.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By extending the analysis of temporal subordinate clauses in (Kamp and Reyle, 1993) , to sentences which include quantification over eventualities, we can propose an alternative DRT solution to Partee's quantification problem. ", "mid_sen": "As in (Partee, 1984) , such sentences trigger box-splitting. ", "after_sen": "But now, the location time of the eventuality in the subordinate clause serves as the antecedent for the location time of the eventuality in the main clause. "}
{"citeStart": 228, "citeEnd": 249, "citeStartToken": 228, "citeEndToken": 249, "sectionName": "UNKNOWN SECTION NAME", "string": "The morphologic analyzer (Marziali, 1992) derives from the work on a generative approach to the Italian morphology (Russo, 1987) , first used in DANTE, a NLP system for analysis of short narrative texts in the financial domain (Antonacci et al. 1989) . Tile analyzer includes over 7000 elementary lemmata (stems without affixes, e.g. flex is the elementary lemma for de-flex, in-flex, re-fiex) anti has been experimented since now on economic, financial, commercial and legal domains. Elementary lemmata cover much more than 70(}0 words, since many words have an affix.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The morphologic analyzer (Marziali, 1992) derives from the work on a generative approach to the Italian morphology (Russo, 1987) , first used in DANTE, a NLP system for analysis of short narrative texts in the financial domain (Antonacci et al. 1989) . ", "after_sen": "Tile analyzer includes over 7000 elementary lemmata (stems without affixes, e.g. flex is the elementary lemma for de-flex, in-flex, re-fiex) anti has been experimented since now on economic, financial, commercial and legal domains. "}
{"citeStart": 83, "citeEnd": 91, "citeStartToken": 83, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Cohen proposed a framework for analyzing the structure of argumentative discourse [Cohen 87 ], yet did not provkle a concrete identification procedure for 'evidence' relationships between sentences, where no linguistic clues indicate the relationships. Also, since only relationships between successive sentences were considered, the scope which the relationships cover cannot be analyzed, even if explicit connectives are detected.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "l,itman and Allen described a model in which a discourse structure of conversation was built by recognizing a participanUs plans [Litman et al. 87] . 'l'hese theories all depend on extra-linguistic knowledge, the accumulation of which presents a problem in the realization of a practical analyzer.", "mid_sen": "Cohen proposed a framework for analyzing the structure of argumentative discourse [Cohen 87 ], yet did not provkle a concrete identification procedure for 'evidence' relationships between sentences, where no linguistic clues indicate the relationships. ", "after_sen": "Also, since only relationships between successive sentences were considered, the scope which the relationships cover cannot be analyzed, even if explicit connectives are detected."}
{"citeStart": 294, "citeEnd": 305, "citeStartToken": 294, "citeEndToken": 305, "sectionName": "UNKNOWN SECTION NAME", "string": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. ", "mid_sen": "With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . ", "after_sen": "One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . "}
{"citeStart": 24, "citeEnd": 35, "citeStartToken": 24, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "When (2) is applied to the predicate, (15) will result after 13-reduction. However, under first-order unification, this needs to simulated by having the variable z in Az.run(z) unify both with Bill and John, and this is not possible. See (Jowsey, 1990) and (Moore, 1989 ) for a thorough discussion. (Moore, 1989) suggests that the way to overcome this problem is to use explicit A-terms and encode /~-reduction to perform the needed reduction. For example, the logical form in (3) would be produced, where X\\rtm(X) is the representation of Az.run (z).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, under first-order unification, this needs to simulated by having the variable z in Az.run(z) unify both with Bill and John, and this is not possible. ", "mid_sen": "See (Jowsey, 1990) and (Moore, 1989 ) for a thorough discussion. ", "after_sen": "(Moore, 1989) suggests that the way to overcome this problem is to use explicit A-terms and encode /~-reduction to perform the needed reduction. "}
{"citeStart": 11, "citeEnd": 24, "citeStartToken": 11, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "A more liberal alternative is the use of a cooccurrence window. Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation. Similarly, Smadja (1993) uses a six content word window to extract significant collocations. A range of windowed training schemes are employed below. Importantly, the use of a window provides a natural means of trading off the amount of data against its quality. When data sparseness undermines the system accuracy, a wider window may admit a sufficient volume of extra accurate data to outweigh the additional noise.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation. ", "mid_sen": "Similarly, Smadja (1993) uses a six content word window to extract significant collocations. ", "after_sen": "A range of windowed training schemes are employed below. "}
{"citeStart": 115, "citeEnd": 138, "citeStartToken": 115, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "Let n and m be two nouns whose distributional similarity is to be determined; for notational simplicity, we write q(v) for P(vln ) and r(v) for P(vlm), their respective conditional verb cooccurrence probabilities. Figure 1 lists several familiar functions. The cosine metric and Jaccard's coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983) . Note that Jaccard's coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q • r rather than the actual values of the distributions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figure 1 lists several familiar functions. ", "mid_sen": "The cosine metric and Jaccard's coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983) . ", "after_sen": "Note that Jaccard's coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q • r rather than the actual values of the distributions."}
{"citeStart": 187, "citeEnd": 199, "citeStartToken": 187, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and free text (Hearst, 1992; Caraballo, 1999) . That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, in this paper we focus on the problem of determining the categories of interest.", "mid_sen": "Another thread of work is on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and free text (Hearst, 1992; Caraballo, 1999) . ", "after_sen": "That work focuses on finding the right position for a word within a lexicon, rather than building up comprehensible and coherent faceted hierarchies."}
{"citeStart": 83, "citeEnd": 96, "citeStartToken": 83, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "Most work on confusible disambiguation using machine learning concentrates on hand-selected sets of notorious confusibles. The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; Even-Zohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The confusible sets are typically very small (two or three elements) and the machine learner will only see training examples of the members of the confusible set. ", "mid_sen": "This approach is similar to approaches used in accent restoration (Yarowsky, 1994; Golding, 1995; Mangu and Brill, 1997; Wu et al., 1999; Even-Zohar and Roth, 2000; Banko and Brill, 2001; Huang and Powers, 2001; Van den Bosch, 2006) .", "after_sen": "The task of the machine learner is to decide, using features describing information from the context, which word taken from the confusible set really belongs in the position of the confusible. "}
{"citeStart": 128, "citeEnd": 153, "citeStartToken": 128, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on Soon et al. (2001) . They built a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier (Denis and Baldridge, 2007) , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. ", "mid_sen": "Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier (Denis and Baldridge, 2007) , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . ", "after_sen": "This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. "}
{"citeStart": 162, "citeEnd": 173, "citeStartToken": 162, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "We collected the error and coverage measures for each of the fifteen subcorpora 8 of the Brown Corpus separately, and, using the bootstrap replicate technique (Efron and Tibshirani 1993) , we calculated the mean and the standard error for each combination of the taggers with the guessing components. For the fifteen accuracy means {al, d2 .... , a15} obtained upon tagging the fifteen subcorpora of the Brown Corpus, we generated a large number of bootstrap replicates of the form {bl, b2,..., b15} where each mean was randomly chosen with replacements such as, for instance, This way of calculating the estimated standard error for the mean does not assume the normal distribution and hence provides more accurate results. We noticed a certain inconsistency in the markup of proper nouns (NNP) in the Brown Corpus supplied with the Penn Treebank. Quite often obvious proper nouns as, for instance, Summerdale, Russia, or Rochester were marked as common nouns (NN) and sometimes lower-cased common nouns such as business or church were marked as proper nouns. Thus we decided not to count as an error the mismatch of the NN/NNP tags. Using the HMM tagger with the lexicon containing all the words from the Brown Corpus, we obtained the error rate (mean) 0* (.)=4.003093 with the standard error deB=0.155599. This agrees with the results on the closed dictionary (i.e., without unknown words) obtained by other researchers for this class of the model on the same corpus (Kupiec 1992; DeRose 1988) . The Brill tagger showed some better results: error rate (mean) 0* (.)=3.327366 with the standard error deB=O. 123903. Although our primary goal was not to compare the taggers themselves but rather their performance with the guessing components, we attribute the difference in their performance to the fact that Brill's tagger uses the information about the most likely tag for a word whereas the HMM tagger did not have this information and instead used the priors for a set of POS-tags (ambiguity class). When we removed from the lexicon all the hapax words and, following the recommendation of Church (1988) , all the capitalized words with frequency less than 20, we obtained some 51,522 unknown word-tokens (25,359 wordtypes) out of more than a million word-tokens in the Brown Corpus. We tagged the fifteen subcorpora of the Brown Corpus by the four combinations of the taggers and the guessers using the lexicon of 22,260 word-types. Table 4 displays the tagging results on the unknown words obtained by the four different combinations of taggers and guessers. It shows the overall error rate on unknown words and also displays the distribution of the error rate and the coverage between unknown proper nouns and the other unknown words. Indeed the error rate on the proper nouns was much smaller than on the rest of the unknown words, which means that they are much easier to guess. We can also see a difference in the distribution (coverage) of the unknown words using different taggers. This can be accounted for by the fact that the unguessed capitalized words were taken by default to be proper nouns and that the Brill tagger and the HMM tagger had slightly different strategies to apply to the first word of a sentence. The cascading guesser outperformed the other two guessers in general and most importantly in the non-proper noun category, where it had an advantage of 6.5% over Brill's guesser and about 8.7% over Xerox's guesser.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using the HMM tagger with the lexicon containing all the words from the Brown Corpus, we obtained the error rate (mean) 0* (.)=4.003093 with the standard error deB=0.155599. ", "mid_sen": "This agrees with the results on the closed dictionary (i.e., without unknown words) obtained by other researchers for this class of the model on the same corpus (Kupiec 1992; DeRose 1988) . ", "after_sen": "The Brill tagger showed some better results: error rate (mean) 0* (.)=3.327366 with the standard error deB=O. 123903. "}
{"citeStart": 109, "citeEnd": 126, "citeStartToken": 109, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared the classification accuracy of LEXAS against the default strategy of picking the most frequent sense. This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992) . There are two instantiations of this strategy in our current evaluation. Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment. This assignment method does not even need to look at the training sentences. We call this method \"Sense 1\" in Table 5 . Another assignment method is to determine the most frequently occurring sense in the training sentences, and to assign this sense to all test sentences. We call this method \"Most Frequent\" in Table 5 . The accuracy of LEXAS on these two test sets is given in Table 5 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compared the classification accuracy of LEXAS against the default strategy of picking the most frequent sense. ", "mid_sen": "This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992) . ", "after_sen": "There are two instantiations of this strategy in our current evaluation. "}
{"citeStart": 34, "citeEnd": 46, "citeStartToken": 34, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Rogers' regular form restriction (Rogers, 1994) , we can cite verb-raised complement auxiliary trees in Dutch as in Figure 5 (Kroch and Santorini, 1991) . Trees with this structure may adjoin into each others' internal spine nodes an unbounded number of times, in violation of Rogers' definition of regular form adjunction, but within our criteria of wrapping adjunction at only one node on the spine. ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "~Except in tile case of raising, discussed below.", "mid_sen": "Rogers' regular form restriction (Rogers, 1994) , we can cite verb-raised complement auxiliary trees in Dutch as in Figure 5 (Kroch and Santorini, 1991) . ", "after_sen": "Trees with this structure may adjoin into each others' internal spine nodes an unbounded number of times, in violation of Rogers' definition of regular form adjunction, but within our criteria of wrapping adjunction at only one node on the spine. "}
{"citeStart": 32, "citeEnd": 46, "citeStartToken": 32, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "Non-Linearity A Semitic stem consists of a root and a vowel melody, arranged according to a canonical pattern. For example, Arabic/kuttib/ 'caused to write -perfect passive' is composed from the root morpheme {ktb} 'notion of writing' and the vowel melody morpheme {ul} 'perfect passive'; the two are arranged according to the pattern morpheme {CVCCVC} 'causative'. This phenomenon is analysed by (McCarthy, 1981) along the fines of autosegmental phonology (Goldsmith, 1976) . The analysis appears in (1). 1", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Arabic/kuttib/ 'caused to write -perfect passive' is composed from the root morpheme {ktb} 'notion of writing' and the vowel melody morpheme {ul} 'perfect passive'; the two are arranged according to the pattern morpheme {CVCCVC} 'causative'. ", "mid_sen": "This phenomenon is analysed by (McCarthy, 1981) along the fines of autosegmental phonology (Goldsmith, 1976) . ", "after_sen": "The analysis appears in (1). "}
{"citeStart": 89, "citeEnd": 98, "citeStartToken": 89, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "because of a symmetric positive semi-definite matrix required by kernel methods. Consequently, such measures as the skew divergence were excluded from the consideration (Lee, 1999) . The Euclidean measure as defined in Table 1 does not necessarily vary from 0 to 1. It was therefore normalized by dividing an l2 score in Table 1 by a maximum score and retracting it from 1. ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "because of a symmetric positive semi-definite matrix required by kernel methods. ", "mid_sen": "Consequently, such measures as the skew divergence were excluded from the consideration (Lee, 1999) . ", "after_sen": "The Euclidean measure as defined in Table 1 does not necessarily vary from 0 to 1. "}
{"citeStart": 130, "citeEnd": 155, "citeStartToken": 130, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "In speech recognition and understanding systems, many kinds of language model may be used to choose between the word and sentence hypotheses for which there is evidence in the acoustic data. Some words, word sequences, syntactic constructions and semantic structures are more likely to occur than others, and the presence of more likely objects in a sentence hypothesis is evidence for the correctness of that hypothesis. Evidence from different knowledge sources can be combined in an attempt to optimize the selection of correct hypotheses; see e.g. Alshawi and Carter (1994) ; Rayner et al (1994) ; Rosenfeld (1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some words, word sequences, syntactic constructions and semantic structures are more likely to occur than others, and the presence of more likely objects in a sentence hypothesis is evidence for the correctness of that hypothesis. ", "mid_sen": "Evidence from different knowledge sources can be combined in an attempt to optimize the selection of correct hypotheses; see e.g. Alshawi and Carter (1994) ; Rayner et al (1994) ; Rosenfeld (1994) .", "after_sen": "Many of the knowledge sources used for this purpose score a sentence hypothesis by calculating a simple, typically linear, combination of scores associated with objects, such as N-grams and grammar rules, that characterize the hypothesis or its preferred linguistic analysis. "}
{"citeStart": 91, "citeEnd": 114, "citeStartToken": 91, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "A common computational treatment of lexical rules adopted, for example, in the ALE system (Carpenter and Penn 1994) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time. While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation. We mentioned in Section 2.2 that eliminating lexical rules in a precompilation step makes it impossible to process lexical rules or lexical entries that impose constraints that can only be properly executed once information from syntactic processing is available. A related problem is that for analyses resulting in infinite lexica, the number of lexical rule applications needs to be limited. In the ALE system, for example, a depth bound can be specified for this purpose. Finally, as shown in Section 6, using an expanded out lexicon can be less time and space efficient than using a lexicon encoding that makes computational use of generalizations over lexical information, as, for example, the covariation encoding.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A common computational treatment of lexical rules adopted, for example, in the ALE system (Carpenter and Penn 1994) consists of computing the transitive closure of the base lexical entries under lexical rule application at compile-time. ", "after_sen": "While this provides a front-end to include lexical rules in the grammars, it has the disadvantage that the generalizations captured by lexical rules are not used for computation. "}
{"citeStart": 96, "citeEnd": 115, "citeStartToken": 96, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "In our recent study (Carenini et al., 2007) , we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words. Our experiments showed that CWS had a higher accuracy than the email summarization approach in (Rambow et al., 2004) and the generic multidocument summarization approach MEAD . Though effective, the CWS method still suffers from the following four substantial limitations. First, we used a fragment quotation graph to represent the conversation, which has a coarser granularity than the sentence level. For email summarization by sentence extraction, the fragment granularity may be inadequate. Second, we only adopted one cohesion measure (clue words that are based on stemming), and did not consider more sophisticated ones such as semantically similar words. Third, we did not consider subjective opinions. Finally, we did not compared CWS to other possible graph-based approaches as we propose in this paper.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our recent study (Carenini et al., 2007) , we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words. ", "mid_sen": "Our experiments showed that CWS had a higher accuracy than the email summarization approach in (Rambow et al., 2004) and the generic multidocument summarization approach MEAD . ", "after_sen": "Though effective, the CWS method still suffers from the following four substantial limitations. "}
{"citeStart": 193, "citeEnd": 205, "citeStartToken": 193, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "We have presented a new approach to word order which preserves traditional notions (semantically motivated dependencies, topological fields) while being fully lexicalized and formally precise (BrSker, 1997) . Word order domains are sets of partially ordered words associated with words. A word is contained in an order domain of its head, or may float into an order domain of a transitive head, resulting in a discontinuous dependency tree while retaining a projective order domain structure. Restrictions on the floating are expressed in a lexicalized fashion in ,I ,i II i I:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We have presented a new approach to word order which preserves traditional notions (semantically motivated dependencies, topological fields) while being fully lexicalized and formally precise (BrSker, 1997) . ", "after_sen": "Word order domains are sets of partially ordered words associated with words. "}
{"citeStart": 65, "citeEnd": 82, "citeStartToken": 65, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. By combining word alignments in two directions using heuristics (Och and Ney, 2003) , a single set of static word alignments is then formed. Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. We measure translation performance by the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores with multiple translation references. ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Starting from the collection of parallel training sentences, we build word alignment models in two translation directions, from English to Chinese and from Chinese to English, and derive two sets of Viterbi alignments. ", "mid_sen": "By combining word alignments in two directions using heuristics (Och and Ney, 2003) , a single set of static word alignments is then formed. ", "after_sen": "Based on alignment models and word alignment matrices, we compare different approaches of building a phrase translation table and show the final translation results. "}
{"citeStart": 69, "citeEnd": 85, "citeStartToken": 69, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014) , (Malandrakis et al., 2014) , both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. ", "mid_sen": "Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014) , (Malandrakis et al., 2014) , both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons.", "after_sen": "In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015) . "}
{"citeStart": 154, "citeEnd": 175, "citeStartToken": 154, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "In Section 4 we will develop a general framework for semi-supervised learning with constraints. However, it is useful to illustrate the ideas on concrete problems. Therefore, in this section, we give a brief introduction to the two domains on which we tested our algorithms. We study two information extraction problems in each of which, given text, a set of pre-defined fields is to be identified. Since the fields are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours. 1 The first task is to identify fields from citations . The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005) . Given a citation, the task is to extract the (a) . While the predicted label assignment (b) is generally coherent, some constraints are violated. Most obviously, punctuation marks are ignored as cues for state transitions. The constraint \"Fields cannot end with stop words (such as \"the\")\" may be also good.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 The first task is to identify fields from citations . ", "mid_sen": "The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005) . ", "after_sen": "Given a citation, the task is to extract the (a) . "}
{"citeStart": 185, "citeEnd": 199, "citeStartToken": 185, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "Standard numerical methods for statistical inference of log-linear models from fully annotated data so-called complete data are the iterative scaling methods of Darroch and Ratcli (1972) and Della Pietra et al. (1997) . For data consisting of unannotated sentences so-called incomplete data the iterative method of the EM algorithm (Dempster et al., 1977) has to be employed. However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de ned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999) . Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: A training sample of unannotated sentences y f r o m a s e t Y, observed with empirical Input Reference model p 0 , property-functions vector with constant # , parses X(y) for each y in incomplete-data sample from Y. Output MLE model p on X. Procedure", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, since even complete-data estimation for log-linear models requires iterative methods, an application of EM to log-linear models results in an algorithm which is expensive since it is doubly-iterative. ", "mid_sen": "A singly-iterative algorithm interleaving EM and iterative scaling into a mathematically well-de ned estimation method for log-linear models from incomplete data is the IM algorithm of Riezler (1999) . ", "after_sen": "Applying this algorithm to stochastic constraint-based grammars, we assume the following to be given: "}
{"citeStart": 15, "citeEnd": 37, "citeStartToken": 15, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "As is shown in Prescher et al. (2000) in an evaluation on lexical ambiguity resolution, a gain of about 7% can be obtained by using the class-based estimated frequency f c (v n) as disambiguation criterion instead of classbased probabilities p(njv). In order to make the most direct use possible of this fact, we incorporated the decisions of the disambiguator directly into 45 additional properties for the grammatical relations of the subject, direct object, indirect object, in nitival object, oblique and adjunctival dative and accusative preposition, for active and passive forms of the rst three verbs in each parse. Let v r (x) be the verbal head of grammatical relation r in parse x, and n r (x) the nominal head of grammatical relation r in x. Then a lexicalized property r for grammatical relation r is de ned as", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(v n)-pairs, i.e., f c (v n) = max c2C p(cjv n)(f (v n) + 1).", "mid_sen": "As is shown in Prescher et al. (2000) in an evaluation on lexical ambiguity resolution, a gain of about 7% can be obtained by using the class-based estimated frequency f c (v n) as disambiguation criterion instead of classbased probabilities p(njv). ", "after_sen": "In order to make the most direct use possible of this fact, we incorporated the decisions of the disambiguator directly into 45 additional properties for the grammatical relations of the subject, direct object, indirect object, in nitival object, oblique and adjunctival dative and accusative preposition, for active and passive forms of the rst three verbs in each parse. "}
{"citeStart": 53, "citeEnd": 94, "citeStartToken": 53, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "Learn from Question-Answer Pairs. We first compare our system (henceforth, LJK11) with Clarke et al. (2010) (henceforth, CGCR10) , which is most similar to our work in that it also learns from question-answer pairs without using annotated logical forms. CGCR10 works with the FunQL language and casts semantic parsing as integer linear programming (ILP). In each iteration, the learning algorithm solves the Table 2 Results on GEO with 250 training and 250 test examples. Our system (LJK11 with base triggers and no logical forms) obtains higher test accuracy than CGCR10, even when CGCR10 is trained using logical forms.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Learn from Question-Answer Pairs. ", "mid_sen": "We first compare our system (henceforth, LJK11) with Clarke et al. (2010) (henceforth, CGCR10) , which is most similar to our work in that it also learns from question-answer pairs without using annotated logical forms. ", "after_sen": "CGCR10 works with the FunQL language and casts semantic parsing as integer linear programming (ILP). "}
{"citeStart": 111, "citeEnd": 133, "citeStartToken": 111, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Interactive spoken dialog provides many new challenges for natural language understanding systems. One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting the speaker's turn into utterances and determining the intended words in each utterance. Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases (Silverman et al., 1992) , which end with an acoustically signaled boundary lone. Even assuming perfect word recognition, the problem of determining the intended words is complicated due to the occurrence of speech repairs, which occur where the speaker goes back and changes (or repeats) something she just said. The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. The following example, from the Trains corpus (Heeman and Allen, 1995) , gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting the speaker's turn into utterances and determining the intended words in each utterance. ", "mid_sen": "Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases (Silverman et al., 1992) , which end with an acoustically signaled boundary lone. ", "after_sen": "Even assuming perfect word recognition, the problem of determining the intended words is complicated due to the occurrence of speech repairs, which occur where the speaker goes back and changes (or repeats) something she just said. "}
{"citeStart": 98, "citeEnd": 114, "citeStartToken": 98, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "This binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where what we will call the \"core parser\" performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver(to use Abney's terminology (1987 Abney's terminology ( , 1989 ), is required to solve the difficult cases, (such as on reaching melted in (1)). This \"core parser\" has been the subject of a number of computational implementations, including Marcus's deterministic parser (1980) , Description theory (henceforth, D-theory) (Marcus et al (1983) ), and Abney's licensing based model (1987, 1989) . It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This \"core parser\" has been the subject of a number of computational implementations, including Marcus's deterministic parser (1980) , Description theory (henceforth, D-theory) (Marcus et al (1983) ), and Abney's licensing based model (1987, 1989) . ", "mid_sen": "It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) ).", "after_sen": "The implementation described in this paper is based on the most recent model, that of (Gorrell (in press) ). "}
{"citeStart": 120, "citeEnd": 142, "citeStartToken": 120, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "Definite relations are a convenient way of encoding the interaction of lexical rules, as they readily support various program transformations to improve the encoding: We show that the definite relations produced by the compiler can be refined by program transformation techniques to increase efficiency. The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup. The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by G6tz and Meurers (1995 G6tz and Meurers ( , 1996 G6tz and Meurers ( , 1997b for encoding the main building block of HPSG grammars--the implicative constraints--as a logic program.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The resulting encoding allows the execution of lexical rules on-the-fly, i.e., coroutined with other constraints at some time after lexical lookup. ", "mid_sen": "The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by G6tz and Meurers (1995 G6tz and Meurers ( , 1996 G6tz and Meurers ( , 1997b for encoding the main building block of HPSG grammars--the implicative constraints--as a logic program.", "after_sen": "The structure of the paper is as follows: "}
{"citeStart": 334, "citeEnd": 354, "citeStartToken": 334, "citeEndToken": 354, "sectionName": "UNKNOWN SECTION NAME", "string": "WIT has been implemented in Common Lisp and C on UNIX, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al., 1999b) , a video-recording programming system, a schedule management system (Nakano et al., 1999a) , and a weather information system (Dohsaka et al., 2000) . The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. A sample dialogue between this system and a naive user is shown in Figure 2 . This system employs HTK as the speech recognition engine. The weather information system can answer the user's questions about weather forecasts in Japan. The vocabulary size is around 500, and the number of phrase structure rules is 31. The number of attributes in the semantic flame is 11, and the number of the files of the pre-recorded speech is about 13,000.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "WIT has been implemented in Common Lisp and C on UNIX, and we have built several experimental and demonstration dialogue systems using it, including a meeting room reservation system (Nakano et al., 1999b) , a video-recording programming system, a schedule management system (Nakano et al., 1999a) , and a weather information system (Dohsaka et al., 2000) . ", "after_sen": "The meeting room reservation system has vocabulary of about 140 words, around 40 phrase structure rules, nine attributes in the semantic frame, and around 100 speech files. "}
{"citeStart": 45, "citeEnd": 63, "citeStartToken": 45, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "pronouns usually do not refer to people Candidate judging rule 1 When a pronoun is a demonstrative pronoun and a candidate referent has a semantic marker HUM (human), it is given -10. We used the Noun Semantic Marker Dictionary (Watanabe et al. 92 ) as a semantic marker dictionary 4 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "pronouns usually do not refer to people Candidate judging rule 1 When a pronoun is a demonstrative pronoun and a candidate referent has a semantic marker HUM (human), it is given -10. ", "mid_sen": "We used the Noun Semantic Marker Dictionary (Watanabe et al. 92 ) as a semantic marker dictionary 4 .", "after_sen": ""}
{"citeStart": 9, "citeEnd": 31, "citeStartToken": 9, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition. However, Jakschik et al. (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As the given prefixes reduce the extent to which productive skills are required, Cohen (1984) considers the Ctest to be a test of reading ability examining only recognition. ", "mid_sen": "However, Jakschik et al. (2010) transform the C-test into a true recognition test by providing multiple choice options and find that this variant is significantly easier than open C-test gaps. ", "after_sen": "This indicates that C-test solving requires both, receptive and productive skills, and we reflect this in our feature choice."}
{"citeStart": 195, "citeEnd": 205, "citeStartToken": 195, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "Our work with distributional similarity is a generalisation of the approach taken by Lin and Pantel (2001) . These authors apply the distributional similarity principle to paths in a parse tree. A path exists between two words if there are grammatical relations connecting them in a sentence. For example, in the sentence \"John found a solution to the problem,\" there is a path between \"found\" and \"solution\" because solution is the direct object of found. Contexts of this path, in this sentence, are then the grammatical relations <ncsubj, John> and <iobj, problem> because these are grammatical relations associated with either end of the path. In their work on QA, Lin and Pantel restrict the grammatical relations considered to two \"slots\" at either end of the path where the word occupying the slot is a noun. Co-occurrence vectors for paths are then built up using evidence from multiple occurrences of the paths in corpus data, for which similarity can then be calculated using a standard metric (e.g., Lin (1998) ). In our work, we extend the notion of distributional similarity from linear paths to trees. This allows us to compute distributional similarity for any part of an expression, of arbitrary length and complexity (although, in practice, we are still limited by data sparseness). Further, we do not make any restrictions as to the number or types of the grammatical relation contexts associated with a tree.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In their work on QA, Lin and Pantel restrict the grammatical relations considered to two \"slots\" at either end of the path where the word occupying the slot is a noun. ", "mid_sen": "Co-occurrence vectors for paths are then built up using evidence from multiple occurrences of the paths in corpus data, for which similarity can then be calculated using a standard metric (e.g., Lin (1998) ). ", "after_sen": "In our work, we extend the notion of distributional similarity from linear paths to trees. "}
{"citeStart": 109, "citeEnd": 135, "citeStartToken": 109, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004) , and answer extraction in FAQs (Frequently Asked Questions) Jijkoun and de Rijke 2005; Soricut and Brill 2006) . An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. In fact, some e-mails do not contain any queries at all, and even if they do, it is not always straightforward to distinguish the queries from the text that provides background information. Therefore, the generation of a help-desk response needs to consider a request e-mail in its entirety, and ensure that there is sufficient evidence to match the request with a response or parts of responses.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With respect to these systems, the contribution of our work lies in the consideration of different kinds of corpus-based approaches (namely, retrieval and prediction) applied at different levels of granularity (namely, document and sentence).", "mid_sen": "Two applications that, like help-desk, deal with question-answer pairs are: summarization of e-mail threads (Dalli, Xia, and Wilks 2004; Shrestha and McKeown 2004) , and answer extraction in FAQs (Frequently Asked Questions) Jijkoun and de Rijke 2005; Soricut and Brill 2006) . ", "after_sen": "An important difference between these applications and help-desk is that help-desk request e-mails are not simple queries. "}
{"citeStart": 84, "citeEnd": 97, "citeStartToken": 84, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "Spelling correction is an important application for error-tolerant recognition. There has been substantial work on spelling correction (see the excellent review by Kukich [1992] ). All methods essentially enumerate plausible candidates that resemble the incorrect word, and use additional heuristics to rank the results. 8 Most techniques assume a word list of all words in the language. These approaches are suitable for languages like English, for which it is possible to enumerate such a list. They are not directly suitable or applicable to languages like German, which have very productive compounding, or agglutinative languages like Finnish, Hungarian, or Turkish, in which the concept of a word is much larger than what is normally found in a word list. For example, Finnish nouns have about 2,000 distinct forms, while Finnish verbs have about 12,000 forms (Gazdar and Mellish 1989, 59--60) . Turkish is similar: nouns, for instance, may have about 170 different forms, not counting the forms for adverbs, verbs, adjectives, or other nominal forms, generated (sometimes circularly) by derivational suffixes. Hankamer (1989) gives much higher figures (in the millions) for Turkish; presumably he took derivations into account in his calculations. Some recent approaches to spelling correction have used morphological analysis techniques. Veronis (1988) presents a method for handling quite complex combinations of typographical and phonographic errors (phonographic errors are the kind usually made by language learners using computer-aided instruction). This method takes into account phonetic similarity, in addition to standard errors. Aduriz et al. (1993) present a two-level morphology approach to spelling correction in Basque. They use twolevel rules to describe common insertion and deletion errors, in addition to the twolevel rules for the morphographemic component. Oflazer and G6zey (1994) present a two-level morphology approach to spelling correction in agglutinative languages using a coarser morpheme-based morphotactic description rather than the finer lexi-Recognizer for the word list abacus, abacuses, abalone, abandone, abandoned, abandoning access.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Spelling correction is an important application for error-tolerant recognition. ", "mid_sen": "There has been substantial work on spelling correction (see the excellent review by Kukich [1992] ). ", "after_sen": "All methods essentially enumerate plausible candidates that resemble the incorrect word, and use additional heuristics to rank the results. "}
{"citeStart": 40, "citeEnd": 62, "citeStartToken": 40, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "A class of technique that can handle all kinds of functions of random variables without the above problems is the computationally-intensive randomization tests (Noreen, 1989, Ch. 2) (Cohen, 1995, Sec. 5.3 ). These tests have previously used on such functions during the \"message understanding\" (MUC) evaluations (Chinchor et al., 1993) . The randomization test we use is like a randomization version of the paired sample (matched-pair) t test (Cohen, 1995, Sec. 5.3.2) . This is a type of stratified shuffling (Noreen, 1989, Sec. 2.7) . When comparing two techniques, we gather-up all the responses (whether actually of interest or not) produced by one of the two techniques when examining the test data, but not both techniques. Under the null hypothesis, the two techniques are not really different, so any response produced by one of the techniques could have just as likely come from the other. So we shuffle these responses, reassign each response to one of the two techniques (equally likely to either technique) and see how likely such a shuffle produces a difference (new technique minus old technique) in the metric(s) of interest (in our case, precision and F-score) that is at least as large as the difference observed when using the two techniques on the test data. n responses to shuffle and assign 4 leads to 2 n different ways to shuffle and assign those responses. So when n is small, one can try each of the different shuffles once and produce an exact randomization. When n gets large, the number of different shuffles gets too large to be exhaustively evaluated. Then one performs an approximate randomization where each shuffle is performed with random assignments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The randomization test we use is like a randomization version of the paired sample (matched-pair) t test (Cohen, 1995, Sec. 5.3.2) . ", "mid_sen": "This is a type of stratified shuffling (Noreen, 1989, Sec. 2.7) . ", "after_sen": "When comparing two techniques, we gather-up all the responses (whether actually of interest or not) produced by one of the two techniques when examining the test data, but not both techniques. "}
{"citeStart": 89, "citeEnd": 111, "citeStartToken": 89, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "Row C of Table 4 summarizes the highest results known to us (for all three tasks) produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997 ) with the Alembic system (Aberdeen et al. 1995) : a 0.5% error rate. The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989) , who trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of capitalized words, the most widespread method is POS tagging, which achieves about a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported in Mikheev (2000) . We are not aware of any studies devoted to the identification of abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "State-of-theart machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% measured on the Brown corpus and the WSJ corpus. ", "mid_sen": "The best performance on the WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 1997 ) with the Alembic system (Aberdeen et al. 1995) : a 0.5% error rate. ", "after_sen": "The best performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989) , who trained a decision tree classifier on a 25-million-word corpus. "}
{"citeStart": 58, "citeEnd": 70, "citeStartToken": 58, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "Since the SemEval dataset is of a very specific nature, we have also applied our classification framework to the (Nastase and Szpakowicz, 2003) dataset, which contains 600 pairs labeled with 5 main relationship types. We have used the exact evaluation procedure described in (Turney, 2006) , achieving a class f-score average of 60.1, as opposed to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al., 2006) . This shows that our method produces superior results for rather differing datasets.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the SemEval dataset is of a very specific nature, we have also applied our classification framework to the (Nastase and Szpakowicz, 2003) dataset, which contains 600 pairs labeled with 5 main relationship types. ", "mid_sen": "We have used the exact evaluation procedure described in (Turney, 2006) , achieving a class f-score average of 60.1, as opposed to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al., 2006) . ", "after_sen": "This shows that our method produces superior results for rather differing datasets."}
{"citeStart": 112, "citeEnd": 131, "citeStartToken": 112, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and subjectivity clues listed in (Wiebe, 1990) . Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al., 2003) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of the subjective clues are from manually developed resources, including entries from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and subjectivity clues listed in (Wiebe, 1990) . ", "mid_sen": "Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al., 2003) .", "after_sen": "The subjectivity clues are divided into those that are strongly subjective and those that are weakly subjective, using a combination of manual review and empirical results on a small training set of manually annotated data. "}
{"citeStart": 92, "citeEnd": 111, "citeStartToken": 92, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 . Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. While some of these systems did exhibit expectation capabilities at the sentence level, none acquired dialogues of the kind described here for the sake of dialogue level expectation and error correction. A detailed description of the kinds of expectation mechanisms appearing in these systems appears in Fink (1983) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A number of speech understanding systems have been developed during the past fifteen years (Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , Lowerre and Reddy 1980 , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 . ", "after_sen": "Most of these efforts concentrated on the interaction between low level information sources from a speech recognizer and a natural language processor to discover the meaning of an input sentence. "}
{"citeStart": 118, "citeEnd": 137, "citeStartToken": 118, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . ", "after_sen": "This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . "}
{"citeStart": 80, "citeEnd": 98, "citeStartToken": 80, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For our experiments, we use a phrase-based translation system similar to Moses (Koehn et al., 2007) .", "after_sen": "Our decoder uses many of the same features as Moses, including four phrasal and lexicalized translation scores, phrase penalty, word penalty, a language model score, linear distortion, and six lexicalized reordering scores. "}
{"citeStart": 130, "citeEnd": 143, "citeStartToken": 130, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "For comparison, a variety of other data has been collected. Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983) , using confusion matrices where appropriate (Kernighan, 1990) . Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). Suggestions have been made to look for low frequency words in corpora and news/mail archives, and to the Longmans learner corpus (not native speakers).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For comparison, a variety of other data has been collected. ", "mid_sen": "Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983) , using confusion matrices where appropriate (Kernighan, 1990) . ", "after_sen": "Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). "}
{"citeStart": 161, "citeEnd": 171, "citeStartToken": 161, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009) . However, there is little work that studies a collective system in which members individually write summaries.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This can eventually be extended to events and issues that are evolving either in time or scope such as elections, wars, or the economy.", "mid_sen": "In social sciences and the study of complex systems a lot of work has been done to study such collective systems, and their properties such as selforganization (Page, 2007) and diversity (Hong and Page, 2009; Fisher, 2009) . ", "after_sen": "However, there is little work that studies a collective system in which members individually write summaries."}
{"citeStart": 42, "citeEnd": 70, "citeStartToken": 42, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. The focus of our paper is on relations involving a specific concept.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The process can be bootstrapped as more words are added to the class.", "mid_sen": "Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. ", "after_sen": "The focus of our paper is on relations involving a specific concept."}
{"citeStart": 184, "citeEnd": 204, "citeStartToken": 184, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "Obligations (or at least beliefs that the agent has obligations) will thus form an important part of the reasoning process of a deliberative agent, e.g., the architecture proposed by [Bratman et al., 1988] . In addition to considering beliefs about the world, which will govern the possibility of performing actions and likelyhood of success, and desires or goals which will govern the utility or desirability of actions, a social agent will also have to consider obligations, which govern the permissibility of actions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Obligations (or at least beliefs that the agent has obligations) will thus form an important part of the reasoning process of a deliberative agent, e.g., the architecture proposed by [Bratman et al., 1988] . ", "after_sen": "In addition to considering beliefs about the world, which will govern the possibility of performing actions and likelyhood of success, and desires or goals which will govern the utility or desirability of actions, a social agent will also have to consider obligations, which govern the permissibility of actions."}
{"citeStart": 200, "citeEnd": 224, "citeStartToken": 200, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "Much of the Arabic parsing research to date uses the pipeline approach, either running a tokenizer prior to parsing or simply assuming the existence of gold tokenization (Bikel, 2004; Buchholz and Marsi, 2006; Kulick et al., 2006; Marton et al., 2010; Marton et al., 2011; Marton et al., 2013) . Of course, using gold tokenization results in optimistic evaluation figures. 13 Other methods exist however. For example, to parse Modern Hebrew, Cohen and Smith (2007) combine a morphological model with a syntactic model using a product of experts. Another alternative is lattice parsing, which can be used to jointly model both tokenization and parsing (Chappelier et al., 1999) . Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013) , Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1) . Why lattice parsing may help in some cases but not others is not clear.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another alternative is lattice parsing, which can be used to jointly model both tokenization and parsing (Chappelier et al., 1999) . ", "mid_sen": "Curiously, while researchers of Modern Hebrew parsing find lattice parsers outperforming their pipeline systems (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011; Goldberg and Elhadad, 2013) , Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1) . ", "after_sen": "Why lattice parsing may help in some cases but not others is not clear."}
{"citeStart": 26, "citeEnd": 40, "citeStartToken": 26, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "Unification Grammar (DUG, Hellwig (1986) ) defines a tree-like data structure for the representation of syntactic analyses. Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head. The approach requires that the parser interprets several features in a special way, and it cannot restrict the scope of discontinuities.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Unification Grammar (DUG, Hellwig (1986) ) defines a tree-like data structure for the representation of syntactic analyses. ", "after_sen": "Using morphosyntactic features with special interpretations, a word defines abstract positions into which modifiers are mapped. "}
{"citeStart": 81, "citeEnd": 106, "citeStartToken": 81, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001) . In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) , named entity recognition (Borthwick et al., 1998) , partial parsing (Inui and Inui, 2000) , word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003) . Brill and Hladká (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994 ). There have not been more sufficiently good parsers available. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999) . The authors have investigated two combination techniques (constituent voting and naïve Bayes), and two ways of their application to the (full) parsing: parser switching, and similarity switching. They were able to gain 1.6 constituent F-score, using their most successful technique.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While improving any single approach gets more and more difficult once some threshold has been touched, exploring the potential of approach combination should never be omitted, provided three or more approaches are available.", "mid_sen": "Combination techniques have been successfully applied to part of speech tagging (van Halteren et al., 1998; Brill and Wu, 1998; van Halteren et al., 2001) . ", "after_sen": "In both cases the investigators were able to achieve significant improvements over the previous best tagging results. "}
{"citeStart": 99, "citeEnd": 123, "citeStartToken": 99, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003) , table extraction (Pinto et al., 2003) and shallow parsing (Sha and Pereira, 2003) . The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase. Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000) , exponential (Goodman, 2003) , and hyperbolic-L 1 (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase. ", "mid_sen": "Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000) , exponential (Goodman, 2003) , and hyperbolic-L 1 (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties.", "after_sen": "We describe a large collection of experimental results on two traditional benchmark data sets. "}
{"citeStart": 320, "citeEnd": 337, "citeStartToken": 320, "citeEndToken": 337, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach is based on the assumption that each collocation is unambiguous in the source language and has a unique translation in the target language (at least in a clear majority of the cases). In this way, we can ignore the context of the collocations and their translations, and base our decisions only on the patterns of co-occurrence of each collocation and its candidate translations across the entire corpus. This approach is quite different from those adopted for the translation of single words (Klavans and Tzoukermann 1990; Dorr 1992; Klavans and Tzoukermann 1996) , since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) . The assumption of a single meaning per collocation was based on our previous experience with English collocations (Smadja 1993), is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation (Yarowsky 1993) , and was verified during our evaluation of Champollion (Section 7).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this way, we can ignore the context of the collocations and their translations, and base our decisions only on the patterns of co-occurrence of each collocation and its candidate translations across the entire corpus. ", "mid_sen": "This approach is quite different from those adopted for the translation of single words (Klavans and Tzoukermann 1990; Dorr 1992; Klavans and Tzoukermann 1996) , since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994) . ", "after_sen": "The assumption of a single meaning per collocation was based on our previous experience with English collocations (Smadja 1993), is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation (Yarowsky 1993) , and was verified during our evaluation of Champollion (Section 7)."}
{"citeStart": 127, "citeEnd": 145, "citeStartToken": 127, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983 ). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. ", "mid_sen": "For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983 ). ", "after_sen": "Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. "}
{"citeStart": 62, "citeEnd": 74, "citeStartToken": 62, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "Language reuse involves two components: a source text written by a human and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993) , the use of entire factual sentences extracted from corpora (e.g., \"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995) . In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. ", "mid_sen": "Some examples of language reuse include collocation analysis (Smadja, 1993) , the use of entire factual sentences extracted from corpora (e.g., \"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995) . ", "after_sen": "In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. "}
{"citeStart": 102, "citeEnd": 115, "citeStartToken": 102, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "where type1 to typen are exhaustive and disjoint subtypes of type entry, entry need not necessarily be a single type; it can be a logical expression over types formed with the connectors AND and oR. A systemic grammar therefore resembles more a type lattice than a type hierarchy in the HPSG tradition. In systemic grammar, these basic type axioms, the systems, are named; we will use entry(s) to denote the left-hand side of some named system s, and out(s) to denote the set of subtypes {type1, type2, ..., type,}- the output of the system. The following type axioms taken from the large systemic English grammar NXGI~L (Matthiessen, 1983) The meaning of these type axioms is fairly obvious: Nominal groups can be subcategorized in classnames and individual-names on the one hand, they can be subcategorized with respect to their WHcontainment into WH-containing nominal-groups and nominal-groups without WH-element on the other hand. Universal principles and rules are in systemic grammar not factored out. The lexicon contains stem forms and has a detailed word class type hierarchy at its top. Morphology is also organized as a monotonic type hierarchy. Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Morphology is also organized as a monotonic type hierarchy. ", "mid_sen": "Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "after_sen": "Our subgrammar extraction has been applied and tested in the context of the KPML environment. "}
{"citeStart": 90, "citeEnd": 115, "citeStartToken": 90, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": " We show that MINT's performance is significantly better than a state of the art method (Klementiev and Roth, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": " We demonstrate MINT's effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles).", "mid_sen": " We show that MINT's performance is significantly better than a state of the art method (Klementiev and Roth, 2006) .", "after_sen": "We discuss the motivation behind our approach in Section 2 and present the details in Section 3. "}
{"citeStart": 15, "citeEnd": 36, "citeStartToken": 15, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words. The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004) . Both identify product features from reviews, but OPINE significantly improves on both. (Hu and Liu, 2004) doesn't assess candidate features, so its precision is lower than OPINE's. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE's focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Neither model explicitly addresses composite (feature of feature) or implicit features. ", "mid_sen": "Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. ", "after_sen": "OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . "}
{"citeStart": 330, "citeEnd": 354, "citeStartToken": 330, "citeEndToken": 354, "sectionName": "UNKNOWN SECTION NAME", "string": "The seminal work of Brown et al. (1993b) introduced a series of probabilistic models (IBM Models 1-5) for statistical machine translation and the concept of \"word-byword\" alignment, the correspondence between words in source and target languages. Although no longer competitive as end-to-end translation models, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996) , are still widely used for word alignment. Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003 ] and rules [Galley et al. 2004; Chiang et al. 2005] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006) . But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009) ; discovery of paraphrases (Bannard and Callison-Burch 2005) ; and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word alignments are used primarily for extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003 ] and rules [Galley et al. 2004; Chiang et al. 2005] ) as well as for MT system combination (Matusov, Ueffing, and Ney 2006) . ", "mid_sen": "But their importance has grown far beyond machine translation: for instance, transferring annotations between languages (Yarowsky and Ngai 2001; Hwa et al. 2005; Ganchev, Gillenwater, and Taskar 2009) ; discovery of paraphrases (Bannard and Callison-Burch 2005) ; and joint unsupervised POS and parser induction across languages (Snyder and Barzilay 2008) .", "after_sen": "IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models, which produce the target sentence one target word at a time by choosing a source word and generating its translation. "}
{"citeStart": 108, "citeEnd": 131, "citeStartToken": 108, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Various statistical measures have been suggested for ranking expressions based on their compositionality. Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Schutze, 1998) . In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). Integrating these statistical measures should provide better evidence for ranking the expressions. We use a SVM based ranking function to integrate the features and rank the V-N collocations according to their compositionality. We then compare these ranks with the ranks provided by the human judge. A similar comparison between the ranks according to Latent-Semantic Analysis (LSA) based features and the ranks of human judges has been made by McCarthy, Keller and Caroll (McCarthy et al., 2003) for verb-particle constructions. (See Section for more details). Some preliminary work on recognition of V-N collocations was presented in (Venkatapathy and Joshi, 2004) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Various statistical measures have been suggested for ranking expressions based on their compositionality. ", "mid_sen": "Some of these are Frequency, Mutual Information (Church and Hanks, 1989) , distributed frequency of object (Tapanainen et al., 1998) and LSA model (Schutze, 1998) . ", "after_sen": "In this paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of MWEs of V-N type (see section 6 for details). "}
{"citeStart": 53, "citeEnd": 72, "citeStartToken": 53, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Figure 1: Class proach, our statistical inference method for clustering is formalized clearly as an EM-algorithm. Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998) . There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. For further applications of our clustering model see Rooth et al. (1998) . We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c E C, where the classes are given no prior interpretation. The semantically smoothed probability of a pair (v, n) is defined to be: n[c ) . Note that by construction, conditioning of v and n on each other is solely made through the classes c.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. ", "mid_sen": "For further applications of our clustering model see Rooth et al. (1998) . ", "after_sen": "We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. "}
{"citeStart": 104, "citeEnd": 118, "citeStartToken": 104, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005) . Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . Learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques, but incorporating various smoothing and back-off estimation tricks to cope with the sparse data problems (Collins, 1997; Bikel, 2004) . Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. \"maximum entropy\") to be applied (Ratnaparkhi, 1999; Charniak, 2000) . Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al., 2004; McDonald et al., 2005) , which demonstrates the state of the art performance in English dependency parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al., 2005; McDonald et al., 2005) . ", "after_sen": "Most of the early work in this area was based on postulating generative probability models of language that included parse structures (Magerman, 1995; Collins, 1997; Charniak, 1997) . "}
{"citeStart": 203, "citeEnd": 219, "citeStartToken": 203, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "In the following section, we initially concentrate on the simple Case in (1) and show how (1) may be compiled assuming left-to-right processing along with the overall longest match strategy described by Karttunen (1996) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such a left-most longest match concatenation operation is described in §3.", "mid_sen": "In the following section, we initially concentrate on the simple Case in (1) and show how (1) may be compiled assuming left-to-right processing along with the overall longest match strategy described by Karttunen (1996) .", "after_sen": "The major components of the algorithm are not new, but straightforward modifications of components presented in Karttunen (1996) and Mohri and Sproat (1996) . "}
{"citeStart": 46, "citeEnd": 69, "citeStartToken": 46, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "Beside IBI-IG, we have used IGTREE in the combination experiments. IGTREE is a decision tree variant of II31-IG (Daelemans et al., 1999b) . It uses the same feature weight method as IBI-IG. Data items are stored in a tree with the most important features close to the root node. A new item is classified by traveling down from the root node until a leaf node is reached or no branch is available for the current feature value. The most frequent classification of the current node will be chosen.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Beside IBI-IG, we have used IGTREE in the combination experiments. ", "mid_sen": "IGTREE is a decision tree variant of II31-IG (Daelemans et al., 1999b) . ", "after_sen": "It uses the same feature weight method as IBI-IG. "}
{"citeStart": 18, "citeEnd": 20, "citeStartToken": 18, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "Earley deduction [10] is based on grammars encoded as definite clauses. The instantiation (prediction) rule of top-down Earley deduction is not needed in bottom-up Earley deduction, because there is no prediction. There is only one inference rule, namely the reduction rule (1)3 In (1), X, G and G t are literals, ~ is a (possibly empty) sequence of literals, and a is the most general unifier of G and G'. The leftmost literal in the. lmdy of a non:unit clause is Mways the selected literal. In 1)riuciple, this rule can be applied to any pair of unit clanses and non:unit clauses of the program to derive any consequences of the pro: gram. In order to reduce this search space and achieve a more goal-directed behaviour, the rule is not applied to any pair of clauses, but clauses are on]y selected if they can contribute to a proof of the goal. The set of selected clauses is (;ailed the chart. 3 The selection of clauses is guided by a scanning step (section 2.1) an(l indexing of clauses (section 2.2).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Earley deduction [10] is based on grammars encoded as definite clauses. ", "after_sen": "The instantiation (prediction) rule of top-down Earley deduction is not needed in bottom-up Earley deduction, because there is no prediction. "}
{"citeStart": 159, "citeEnd": 170, "citeStartToken": 159, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990) . To understand con-versationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994) . Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990) . ", "mid_sen": "To understand con-versationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994) . ", "after_sen": "Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. "}
{"citeStart": 97, "citeEnd": 113, "citeStartToken": 97, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "The directional nature of the generative models used to recover word alignments conflicts with their interpretation as translations. In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1 ). The standard approach is to train two models independently and then intersect their predictions (Och and Ney 2003) . However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. Let the directional models be defined as: − → p ( − → z ) (source-target) and ← − p ( ← − z ) (target-source). We suppress dependence on x and y for brevity. Define z to range over the union of all possible", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In practice, we see that the choice of which language is source versus target matters and changes the mistakes made by the model (the first row of panels in Figure 1 ). ", "mid_sen": "The standard approach is to train two models independently and then intersect their predictions (Och and Ney 2003) . ", "after_sen": "However, we show that it is much better to train two directional models concurrently, coupling their posterior distributions over alignments to approximately agree. "}
{"citeStart": 97, "citeEnd": 123, "citeStartToken": 97, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. 5", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training. ", "mid_sen": "To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET. ", "after_sen": "5"}
{"citeStart": 327, "citeEnd": 349, "citeStartToken": 327, "citeEndToken": 349, "sectionName": "UNKNOWN SECTION NAME", "string": "The paper thus sheds light on two questions. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . J This result has been criticised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & BrSker, 1997) , and our use of a context-free backbone with further constraints imposed by dependency relations further supports the view that DG is not a notational ~riant of context-free grammar. The second question addressed is that of efficient processing of discontinuous DGs. By converting a native DG grammar into LFG rules, we are able to profit from the state of the art in context-free parsing technology. A context-free base (or skeleton) has often been cited as a prerequisite for practical applicability of a natural language grammar (Erbach & Uszkoreit, 1990 ), and we here show that a DG can meet this criterion with ease.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The paper thus sheds light on two questions. ", "mid_sen": "A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . J This result has been criticised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & BrSker, 1997) , and our use of a context-free backbone with further constraints imposed by dependency relations further supports the view that DG is not a notational ~riant of context-free grammar. ", "after_sen": "The second question addressed is that of efficient processing of discontinuous DGs. "}
{"citeStart": 93, "citeEnd": 119, "citeStartToken": 93, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984) , which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing. Figure 8 shows the phonological phrase tree that is built from the syntactic structure of Figure 7 . The rules for building this tree apply from left to right, following the analysis we described in the preceding section. Figures  9-11 show the prosodic phrase derivation. Numbered nodes refer to salience values, with higher numbers indicating greater salience. The index is assigned according to phonological word count, with one point added for the node itself. Figure 11 is the final prosodic phrase tree; in the notation we have been using, the phrasing represented by Figure 11 is He told me I last night I[ he was coming to London II for several days. Figure 9 shows the effect of two applications of verb balancing. Applying from left to right, the rule first looks at the phonological verb he + told + me. Since the material to the left of the verb is null, the rule must group this verb with the constituent on its right to form the node labeled ®. On its second application, the rule balances the prosodic phrase it has just formed against the single phonological word to + London, and groups the verb rightward to form node ®.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984) , which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. ", "mid_sen": "Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing. ", "after_sen": "Figure 8 shows the phonological phrase tree that is built from the syntactic structure of Figure 7 . "}
{"citeStart": 11, "citeEnd": 26, "citeStartToken": 11, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "The parameterization is as in C+R, with one significant modification. Parameters consist of (i) rule parameters, corresponding to right hand sides conditioned by parent category and parent head; (ii) lexical choice parameters for nonhead children, corresponding to child lemma conditioned by child category, parent category, and parent head lemma. See C+R or Charniak (1995) for an explanation of how such parame~ ters define a probabilistic weighting of trees. The change relative to C+R is that lexicalization is by uninflected lemma rather than word form. This reduces the number of lexical parameters, giving more acceptable model sizes and eliminating splitting of estimated frequencies among inflectional forms. Inflected forms are generated at the leaves of the tree, conditioned on terminal category and lemma. This results in a third family of parameters, though usually the choice of inflected form is deterministic.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Parameters consist of (i) rule parameters, corresponding to right hand sides conditioned by parent category and parent head; (ii) lexical choice parameters for nonhead children, corresponding to child lemma conditioned by child category, parent category, and parent head lemma. ", "mid_sen": "See C+R or Charniak (1995) for an explanation of how such parame~ ters define a probabilistic weighting of trees. ", "after_sen": "The change relative to C+R is that lexicalization is by uninflected lemma rather than word form. "}
{"citeStart": 84, "citeEnd": 106, "citeStartToken": 84, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Because of major data sparseness problems, smoothing is an important issue, in particular for the stress model which is based on syllable-stress-tag pairs. Performance varied by up to 20% in function of the smoothing algorithm chosen. Best results were obtained when using a variant of Modified Kneser-Ney Smoothing 2 (Chen and Goodman, 1996) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Performance varied by up to 20% in function of the smoothing algorithm chosen. ", "mid_sen": "Best results were obtained when using a variant of Modified Kneser-Ney Smoothing 2 (Chen and Goodman, 1996) .", "after_sen": ""}
{"citeStart": 110, "citeEnd": 122, "citeStartToken": 110, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Statistical machine translation (SMT) is complicated by the fact that words can move during translation. If one assumes arbitrary movement is possible, that alone is sufficient to show the problem to be NPcomplete (Knight, 1999) . Syntactic cohesion 1 is the notion that all movement occurring during translation can be explained by permuting children in a parse tree (Fox, 2002) . Equivalently, one can say that phrases in the source, defined by subtrees in its parse, remain contiguous after translation. Early methods for syntactic SMT held to this assumption in its entirety (Wu, 1997; Yamada and Knight, 2001 ). These approaches were eventually superseded by tree transducers and tree substitution grammars, which allow translation events to span subtree units, providing several advantages, including the ability to selectively produce uncohesive translations (Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005) . What may have been forgotten during this transition is that there is a reason it was once believed that a cohesive translation model would work: for some language pairs, cohesion explains nearly all translation movement. Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. We attempt to use this strong, but imperfect, characterization of movement to assist a non-syntactic translation method: phrase-based SMT.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Statistical machine translation (SMT) is complicated by the fact that words can move during translation. ", "mid_sen": "If one assumes arbitrary movement is possible, that alone is sufficient to show the problem to be NPcomplete (Knight, 1999) . ", "after_sen": "Syntactic cohesion 1 is the notion that all movement occurring during translation can be explained by permuting children in a parse tree (Fox, 2002) . "}
{"citeStart": 125, "citeEnd": 126, "citeStartToken": 125, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The reason that many models of belief transfer in dialogue would characterize them as redundant follows from a combination of facts: (1) The representation of belief in these models has been binary; (2) The effects of utterance actions are either assumed to always hold, or to hold as defaults unlcss the listener already believed otherwise. This means that these accounts cannot represent the fact that a belief must be supported by some kind of evidence and that the evidence may be stronger or weaker. It also follows from (2) that these models assume that agents are not autonomous, or at least do not have control over their own mental states. But belief revision is surely an autonomous process; agents can choose whether to accept a new belief or revise old beliefs [4, 8] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It also follows from (2) that these models assume that agents are not autonomous, or at least do not have control over their own mental states. ", "mid_sen": "But belief revision is surely an autonomous process; agents can choose whether to accept a new belief or revise old beliefs [4, 8] .", "after_sen": "The occurrence of IRU's in dialogue bas many ramifications for a model of dialogue. "}
{"citeStart": 47, "citeEnd": 58, "citeStartToken": 47, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hampie, 1985) . Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988) . On the other hand, Cn'ice's maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is required, s Thus, it is important that a collaborative agent selects suffmient and effective, but not excessive, evidence to justify an intended mutual belief.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988) . ", "mid_sen": "On the other hand, Cn'ice's maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is required, s Thus, it is important that a collaborative agent selects suffmient and effective, but not excessive, evidence to justify an intended mutual belief.", "after_sen": "To convince the user ofa belief,_bel, our system selects appropriate justification by identifying beliefs that could 7In collaborative dialogues, an agent should reject a proposal only ff she has strong evidence against it. "}
{"citeStart": 58, "citeEnd": 59, "citeStartToken": 58, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The correlation between the dist;ancc and the occurrence of structures tells us that a modifier tends to modify a closer modifiee. This tendency has been experimentally proven by Maruyama [7] . The tendency is expressed by the formula that follows:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The correlation between the dist;ancc and the occurrence of structures tells us that a modifier tends to modify a closer modifiee. ", "mid_sen": "This tendency has been experimentally proven by Maruyama [7] . ", "after_sen": "The tendency is expressed by the formula that follows:"}
{"citeStart": 151, "citeEnd": 173, "citeStartToken": 151, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. ", "mid_sen": "However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . ", "after_sen": "In this paper we adopt the latter theoretically more interesting perspective."}
{"citeStart": 134, "citeEnd": 156, "citeStartToken": 134, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002) , selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002) , manually or randomly selected (Teufel, 2010, p.60) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As the emphasis of this approach was the identification of the argumentative zones, less attention was given to the sentence selection criteria for the extractive summaries.", "mid_sen": "The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002) , selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002) , manually or randomly selected (Teufel, 2010, p.60) .", "after_sen": "More recently Contractor et al. (2012) have used automatically annotated argumentative zones (Guo et al., 2011) to guide the creation of extractive summaries of scientific articles. "}
{"citeStart": 58, "citeEnd": 76, "citeStartToken": 58, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "The delicacy of testsuite construction is acknowledged in EAGLES, 1996, p.37 . Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The delicacy of testsuite construction is acknowledged in EAGLES, 1996, p.37 . ", "after_sen": "Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. "}
{"citeStart": 47, "citeEnd": 61, "citeStartToken": 47, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "To ensure that our coding scheme leads to less biased annotation than some of the other resources available for building summarisation systems, and to ensure that other researchers besides ourselves can use it to replicate our results on different types of texts, we wanted to examine two properties of our scheme: stability and reproducibility (Krippendorff, 1980) . Stability is the extent to which an annotator will produce the same classifications at different times. Reproducibility is the extent to which different annotators will produce the same classification. We use the Kappa coefficient (Siegel and Castellan, 1988) to measure stability and reproducibility. The rationale for using Kappa is explained in (Carletta, 1996) . The studies used to evaluate stability and reproducibility we describe in more detail in (Teufel et al., To Appear) . In brief, 48 papers were annotated by three extensively trained annotators. The training period was four weeks consisting of 5 hours of annotation per week. There were written instructions (guidelines) of 17 pages. Skim-reading and annotation of an average length (3800 word) paper typically took 20-30 minutes. The studies show that the training material is reliable. In particular, the basic annotation scheme is stable (K=.82, .81, .76; N=1220; k=2 for all three annotators) and reproducible (K=.71, N=4261, k=3), where k denotes the number of annotators, N the number of sentences annotated, and K gives the Kappa value. The full annotation scheme is stable (K=.83, .79, .81; N=1248; k-2 for all three annotators) and reproducible (K=.78, N=4031, k=3). Overall, reproducibility and stability for trained annotators does not quite reach the levels found for, for instance, the best dialogue act coding schemes, which typically reach Kappa values of around K=.80 (Carletta et al., 1997; Jurafsky et al., 1997) . Our annotation requires more subjective judgements and is possibly more cognitively complex. Our reproducibility and stability results are in the range which Krippendorff (1980) describes as giving marginally significant results for reasonable size data sets when correlating two coded variables which would show a clear correlation if there were perfect agreement. As our requirements are less stringent than Krippendorff's, we find the level of agreement which we achieved acceptable. Figure 3 , which gives the overall distribution of categories, shows that OWN is by far the most frequent category. Figure 4 reports how well the four non-basic categories could be distinguished from all other categories, measured by Krippendorff's diagnostics for category distinctions (i.e. collapsing all other distinctions). When compared to the overall reproducibility of .71, we notice that the annotators were good at distinguishing AIM and TEX-TUAL, and less good at determining BASIS and CON-TRAST. This might have to do with the location of those types of sentences in the paper: AIM and TEX-TUAL are usually found at the beginning or end of the introduction section, whereas CONTRAST, and even more so BASIS, are usually interspersed within longer stretches of OWN. As a result, these categories are more exposed to lapses of attention during annotation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use the Kappa coefficient (Siegel and Castellan, 1988) to measure stability and reproducibility. ", "mid_sen": "The rationale for using Kappa is explained in (Carletta, 1996) . ", "after_sen": "The studies used to evaluate stability and reproducibility we describe in more detail in (Teufel et al., To Appear) . "}
{"citeStart": 237, "citeEnd": 262, "citeStartToken": 237, "citeEndToken": 262, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a new space and a new metric for computing this distance. Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of questionanswer pairs (Q, S A ), we can train a probabilistic model for estimating the conditional probability P(Q | S A ). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR engine, one can find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a new space and a new metric for computing this distance. ", "mid_sen": "Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. ", "after_sen": "This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. "}
{"citeStart": 48, "citeEnd": 73, "citeStartToken": 48, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we address both of these shortcomings by proposing regular tree grammars (RTGs) as a novel underspecification formalism. Regular tree grammars (Comon et al., 2007) are a standard approach for specifying sets of trees in theoretical computer science, and are closely related to regular tree transducers as used e.g. in recent work on statistical MT (Knight and Graehl, 2005) and grammar formalisms (Shieber, 2006) . We show that the \"dominance charts\" proposed by Koller and Thater (2005b) can be naturally seen as regular tree grammars; using their algorithm, classical underspecified descriptions (dominance graphs) can be translated into RTGs that describe the same sets of readings. However, RTGs are trivially expressively complete because every finite tree language is also regular. We exploit this increase in expressive power in presenting a novel redundancy elimination algorithm that is simpler and more powerful than the one by Koller and Thater (2006) ; in our algorithm, redundancy elimination amounts to intersection of regular tree languages. Furthermore, we show how to define a PCFG-style cost model on RTGs and compute best readings of deterministic RTGs efficiently, and illustrate this model on a machine learning based model of scope preferences (Higgins and Sadock, 2003) . To our knowledge, this is the first efficient algorithm for computing best readings of a scope ambiguity in the literature.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Regular tree grammars (Comon et al., 2007) are a standard approach for specifying sets of trees in theoretical computer science, and are closely related to regular tree transducers as used e.g. in recent work on statistical MT (Knight and Graehl, 2005) and grammar formalisms (Shieber, 2006) . ", "mid_sen": "We show that the \"dominance charts\" proposed by Koller and Thater (2005b) can be naturally seen as regular tree grammars; using their algorithm, classical underspecified descriptions (dominance graphs) can be translated into RTGs that describe the same sets of readings. ", "after_sen": "However, RTGs are trivially expressively complete because every finite tree language is also regular. "}
{"citeStart": 105, "citeEnd": 116, "citeStartToken": 105, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000) . While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993) ). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000) . ", "after_sen": "While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. "}
{"citeStart": 51, "citeEnd": 69, "citeStartToken": 51, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "We used a Chinese word segmentation tool, Achilles, to implement word segmentation. Part of the work using this tool was described by (Zhang et al., 2006) . The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff. Moreover, this tool meets our need to test the effect of the two kinds of CWS approaches for SMT. We can easily train a dictionary-based and a CRF-based CWS by using this tool. By turning the program's option for the CRF model on and off, we can use the Achilles as a dictionary-based approach and as a CRF-based CWS. In fact, the dictionary-based approach is the default approach for Achilles.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used a Chinese word segmentation tool, Achilles, to implement word segmentation. ", "mid_sen": "Part of the work using this tool was described by (Zhang et al., 2006) . ", "after_sen": "The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff. "}
{"citeStart": 361, "citeEnd": 384, "citeStartToken": 361, "citeEndToken": 384, "sectionName": "UNKNOWN SECTION NAME", "string": "The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003) . Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The volume of information available to users on the World Wide Web is growing at an exponential rate (Lyman and Varian, 2003) . ", "mid_sen": "Current keyword-matching information retrieval (IR) systems suffer from several limitations, most notably an inability to accurately model the ambiguities in natural language, such as synonymy (different words having the same meaning) and polysemy (one word having multiple different meanings), which is largely governed by the context in which a word appears (Metzler and Croft, 2006) .", "after_sen": "In recent years, much research attention has therefore been given to semantic techniques of information retrieval. "}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "There are very few reported attempts at corpus-based automation of help-desk responses (Carmel, Shtalhaim, and Soffer 2000; Lapalme and Kosseim 2003; Bickel and Scheffer 2004; Malik, Subramaniam, and Kaushik 2007) . eResponder, the system developed by Carmel, Shtalhaim, and Soffer (2000) , retrieves a list of request-response pairs and presents a ranked list of responses to the user. If the user is unsatisfied with this list, an operator is asked to generate a new response. The operator is assisted in this task by the retrieval results: The system highlights the request-relevant sentences in the ranked responses. However, there is no attempt to automatically generate a single response. Bickel and Scheffer (2004) compared the performance of document retrieval and document prediction for generating help-desk responses. Their retrieval technique, which is similar to our request-to-request Doc-Ret method, matches user questions to the questions in a database of question-answer pairs. Their prediction method, which is similar to Doc-Pred, is based on clustering the responses in the corpus into semantically equivalent answers, and then training a classifier to match a query with one of these classes. The generated response is the answer that is closest to the centroid of the cluster. Bickel and Scheffer's results are consistent with ours, in the sense that the performance of the Doc-Ret method is significantly worse than that of Doc-Pred. However, it is worth noting that their corpus is significantly smaller than ours (805 question-answer pairs), their questions seem to be much simpler and shorter than those in our corpus, and the replies shorter and more homogeneous. Malik, Subramaniam, and Kaushik (2007) developed a system that builds questionanswer pairs from help-center e-mails, and then maps new questions to existing questions in order to retrieve an answer. This part of their approach resembles our Doc-Ret method, but instead of retrieving entire response documents, they retrieve individual sentences. In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering. Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. The question-answering approach and the retrieval component of the case-based reasoning approach were data driven, using word-level matches. However, the personalization component of the case-based reasoning approach was rule-based (e.g., rules were applied to substitute names of individuals and companies in texts).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, rather than including actual response sentences in a reply, their system matches response sentences to pre-existing templates and returns the templates. ", "mid_sen": "Lapalme and Kosseim (2003) investigated three approaches to the automatic generation of response e-mails: text classification, case-based reasoning, and question answering. ", "after_sen": "Text classification was used to group request e-mails into broad categories, some of which, such as requests for financial reports, can be automatically addressed. "}
{"citeStart": 110, "citeEnd": 122, "citeStartToken": 110, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions (Friedl, 1997) . For example, Emacs uses the special brackets \\( and \\) to capture strings along with the notation \\n to recall the nth such string. The expression \\(a*\\)b\\l matches strings of the form anba n. Unrestricted use of backreferencing thus can introduce non-regular languages. For NLP finite state calculi van Noord, 1997) this is unacceptable. The form of backreferences introduced in this paper will therefore be restricted.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The explicit use of backreferencing leads to more elegant and general solutions.", "mid_sen": "Backreferencing is widely used in editors, scripting languages and other tools employing regular expressions (Friedl, 1997) . ", "after_sen": "For example, Emacs uses the special brackets \\( and \\) to capture strings along with the notation \\n to recall the nth such string. "}
{"citeStart": 11, "citeEnd": 38, "citeStartToken": 11, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "Annotators were both native English speakers who speak French as a second language. Each has a strong comprehension of written French. consider them to be equal. To avoid bias, the competing systems were presented anonymously and in random order. Following (Collins et al., 2005) , we provide the annotators with only short sentences: those with source sentences between 10 and 25 tokens long. Following (Callison-Burch et al., 2006) , we conduct a targeted evaluation; we only draw our evaluation pairs from the uncohesive subset targeted by our constraint. All 75 sentences that meet these two criteria are included in the evaluation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following (Collins et al., 2005) , we provide the annotators with only short sentences: those with source sentences between 10 and 25 tokens long. ", "mid_sen": "Following (Callison-Burch et al., 2006) , we conduct a targeted evaluation; we only draw our evaluation pairs from the uncohesive subset targeted by our constraint. ", "after_sen": "All 75 sentences that meet these two criteria are included in the evaluation."}
{"citeStart": 209, "citeEnd": 226, "citeStartToken": 209, "citeEndToken": 226, "sectionName": "UNKNOWN SECTION NAME", "string": "Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996) . Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006) . The baseline translation results (BLEU and TER) on the dev and test set are presented in the line \"HMM\" of Table 1 . We also compare with results of IBM Model-4 word alignments implemented in GIZA++ toolkit (Och and Ney, 2003) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our baseline word alignment model is the word-toword Hidden Markov Model (Vogel et al., 1996) . ", "mid_sen": "Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by (Zens et al., 2004) and (Liang et al., 2006) . ", "after_sen": "The baseline translation results (BLEU and TER) on the dev and test set are presented in the line \"HMM\" of Table 1 . "}
{"citeStart": 377, "citeEnd": 389, "citeStartToken": 377, "citeEndToken": 389, "sectionName": "UNKNOWN SECTION NAME", "string": "Additionally, we require for a dependency structure four more conditions: (1) Each word w is contained in exactly one of the domains from V~(w), (2) all domains in V~(w) are pairwise disjoint, (3) each word (except w~) is contained in at least two domains, one of which is associated with a (transitive) head, and (4) the (partial) ordering of domains (as described by VM) is consistent with the precedence of the words contained in the domains (see (Brhker, 1997) for more details). Fig.3 defines the logical language /:~ used to describe dependency structures. Although they have been presented differently, they can easily be rewritten as (multimodal) Kripke models:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Additionally, we require for a dependency structure four more conditions: ", "mid_sen": "(1) Each word w is contained in exactly one of the domains from V~(w), (2) all domains in V~(w) are pairwise disjoint, (3) each word (except w~) is contained in at least two domains, one of which is associated with a (transitive) head, and (4) the (partial) ordering of domains (as described by VM) is consistent with the precedence of the words contained in the domains (see (Brhker, 1997) for more details). ", "after_sen": "Fig.3 defines the logical language /:~ used to describe dependency structures. "}
{"citeStart": 205, "citeEnd": 227, "citeStartToken": 205, "citeEndToken": 227, "sectionName": "UNKNOWN SECTION NAME", "string": "IIead-driven generation methods combine both, topclown search and bottom-np combination, in an ideal way. [Shieber el al., 1990] proposed to define the 'head' constituent It of phrase with category a~ on semantic grounds: the semantic representations of h and z are identical. This puts a strong restriction on the shape of semantic analysis rules: one of the leaves must share its semantic form with the. root node. IIowever, there are composition rules for semantic representations which violate this restriction, e.g. tim schemata for the construction of Underspecified Discourse It,epresentatim, Structures (UI)ILSs) [l\"rank and Reyle, 1992] where, in general, the root of a tree is associated with a strictly larger semantic structure (,hal, a,ly of I, he leaves. Ill order to make at generation method available for grammars wl,ich do not follow the striet notion of a semantic head, a syntactic-headdriven generation algorithm is presented, which can be specialized to generate from UDRSs. In a second step, the method will be extended in order to handle the movement of (syntactic) heads in a logically well-defined manner.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "root node. ", "mid_sen": "IIowever, there are composition rules for semantic representations which violate this restriction, e.g. tim schemata for the construction of Underspecified Discourse It,epresentatim, Structures (UI)ILSs) [l\"rank and Reyle, 1992] where, in general, the root of a tree is associated with a strictly larger semantic structure (,hal, a,ly of I, he leaves. ", "after_sen": "Ill order to make at generation method available for grammars wl,ich do not follow the striet notion of a semantic head, a syntactic-headdriven generation algorithm is presented, which can be specialized to generate from UDRSs. "}
{"citeStart": 65, "citeEnd": 77, "citeStartToken": 65, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy f r o m W ordNet. In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Using the tree-cut technique described above, our previous work (Tomuro, 2000) extracted systematic polysemy f r o m W ordNet. ", "after_sen": "In this section, we g i v e a summary of this method, and describe the cluster pairs obtained by the method."}
{"citeStart": 36, "citeEnd": 49, "citeStartToken": 36, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "An Indexed Grammar (IG) can be viewed as a CFG in which each nonterminal is associated with a stack of indices. Productions specify not only how nonterminals can be rewritten but also how their associated stacks are modified. 1_16, which were first described by Gazdar (1988) , are constrained such that stacks are passed from the mother to at most a single daughter.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Productions specify not only how nonterminals can be rewritten but also how their associated stacks are modified. ", "mid_sen": "1_16, which were first described by Gazdar (1988) , are constrained such that stacks are passed from the mother to at most a single daughter.", "after_sen": "For I_IG, the size of the domain of nonterminals and associated stacks (the analogue of the nonterminals in CFG) is not bound by the grammar. "}
{"citeStart": 43, "citeEnd": 52, "citeStartToken": 43, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been many approachs to automatic detection of similar words from text. Our method is similar to (Hindle, 1990) , (Lin, 1998) , and (Gasperin, 2001) in the use of dependency relationships as the word features. Another approach used the words' distribution to cluster the words (Pereira, 1993) , and Inoue (Inoue, 1991) also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There have been many approachs to automatic detection of similar words from text. ", "mid_sen": "Our method is similar to (Hindle, 1990) , (Lin, 1998) , and (Gasperin, 2001) in the use of dependency relationships as the word features. ", "after_sen": "Another approach used the words' distribution to cluster the words (Pereira, 1993) , and Inoue (Inoue, 1991) also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem."}
{"citeStart": 148, "citeEnd": 165, "citeStartToken": 148, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "We next hypothesize that query-dependent text summarization algorithms will improve the performance of the QA system by focusing the system on the most relevant portions of the retrieved documents. The goal for query-dependent summarization algorithms is to provide a short summary of a document with respect to a specific query. Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings (Mani et al., 1999) , we again propose the use of vector space methods from IR, which can be easily extended to the summarization task (Salton et al., 1994) :", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The goal for query-dependent summarization algorithms is to provide a short summary of a document with respect to a specific query. ", "mid_sen": "Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings (Mani et al., 1999) , we again propose the use of vector space methods from IR, which can be easily extended to the summarization task (Salton et al., 1994) :", "after_sen": "1. Given a question and a document, divide the document into chunks (e.g. sentences, paragraphs, 200-word passages)."}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "At one time the Gaelic languag~ group was spoken throughout Ireland, from where it spread to the Isle of Man and to much of Scotland. Currently fully native use of Gaelic is limited to a few discontiguous areas in the westernmost reaches of Ireland and Scotland. In the case of Ireland, everyone agrees that Gaelic is nowadays found in three main dialects: that of Ulster, that of Connacht, and that of Munster (0 Siadhail, 1989) . But several questions are raised that are less easily answered. Do the three provinces separate out so neatly for intrinsic linguistic reasons, or simply because their speakers have become so widely separated from each other geographically as speakers in intervening areas have adopted English? Does the language of Connacht naturally group with that of Ulster or with that of Munster? And looking beyond Ireland, many have commented that the language of Ulster in general is similar to that of Scotland. Are Irish, Manx, and Scottish Gaelic considered three separate languages for intrinsic linguistic reasons, or because they are spoken in different countries? To a large extent, dialectologists have found these questions difficult to answer because they accepted Paris's conundrum. For 6 Siadhail, the ultimate scientific justification ill adopting the three-dialect account is the fact that the Gaeltacht (Irish-speaking territory) is so fragmented nowadays that it no longer forms a continuum. O Cuiv (1951:4-49) felt that there can be no dialect boundaries because transitions are gradual. Elsie (1986:240 ) considers a dialect to be an area where all communities are linguistically more similar to each other than any community is to any site outside the dialect. Such notions provide a very firm, absolute notion of dialecthood: a set of communities either constitutes a dialect area, or it does not. But as the dialectometrists have shown, other notions of clustering are equally scientific and may more accurately correspond to intuitive notions of what it means to be a dialect.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "O Cuiv (1951:4-49) felt that there can be no dialect boundaries because transitions are gradual. ", "mid_sen": "Elsie (1986:240 ) considers a dialect to be an area where all communities are linguistically more similar to each other than any community is to any site outside the dialect. ", "after_sen": "Such notions provide a very firm, absolute notion of dialecthood: a set of communities either constitutes a dialect area, or it does not. "}
{"citeStart": 156, "citeEnd": 183, "citeStartToken": 156, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "Regardless of the criterion for success, the algorithm does need further evaluation. Immediate plans include a larger scale version of the experiment presented here, involving thesaurus classes, as well as a similarly designed evaluation of how the algorithm fares when presented with noun groups produced by distributional clustering. In addition, I plan to explore alternative measures of semantic similarity, for example an improved variant on simple path length that has been proposed by Leacock and Chodorow (1994) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Immediate plans include a larger scale version of the experiment presented here, involving thesaurus classes, as well as a similarly designed evaluation of how the algorithm fares when presented with noun groups produced by distributional clustering. ", "mid_sen": "In addition, I plan to explore alternative measures of semantic similarity, for example an improved variant on simple path length that has been proposed by Leacock and Chodorow (1994) .", "after_sen": "Ultimately, this algorithm is intended to be part of a suite of techniques used for disambiguating words in running text with respect to WordNet senses. "}
{"citeStart": 155, "citeEnd": 182, "citeStartToken": 155, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "Moreover, arousal appears to be an important dimension for emotional prosody (Scherer, 2003) , especially in storytelling (Alm and Sproat, 2005) . Thus, we are planning on exploring degrees of emotional intensity in a learning scenario, i.e. a problem similar to measuring strength of opinion clauses (Wilson, Wiebe and Hwa, 2004) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, arousal appears to be an important dimension for emotional prosody (Scherer, 2003) , especially in storytelling (Alm and Sproat, 2005) . ", "mid_sen": "Thus, we are planning on exploring degrees of emotional intensity in a learning scenario, i.e. a problem similar to measuring strength of opinion clauses (Wilson, Wiebe and Hwa, 2004) .", "after_sen": "Finally, emotions are not discrete objects; rather they have transitional nature, and blend and overlap along the temporal dimension. "}
{"citeStart": 3, "citeEnd": 20, "citeStartToken": 3, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988) , who identifies and quantifies the linguistic features associated with different spoken and written text types. Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000) . Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation. A practical discussion of a central technical concern is Vossen (2001) , which tailors a general-language resource for a domain. Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures. His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. ", "mid_sen": "In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. ", "after_sen": "As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000) . "}
{"citeStart": 198, "citeEnd": 220, "citeStartToken": 198, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The biggest difference between this work and theirs is in what the links represent linguistically. ", "mid_sen": "Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). ", "after_sen": "Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . "}
{"citeStart": 72, "citeEnd": 90, "citeStartToken": 72, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "To assess the DRI effort, clearly more experiments are needed. However, we believe our results show that the goal of an adaptable core coding scheme is reasonable. We think we achieved good results on Forward Functions because, as the DRI enterprise intended, we adapted the high level definitions to our domain. However, we have not yet done so for Agreement since our initial trial codings did not reveal strong disagreements; now given our K results, refinement is clearly needed. Another possible contributing factor for the low K on Agreement is that these tags are much rarer than the Forward Function tags. The highest possible value for K may be smaller for low frequency tags (Grove et al., 1981) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another possible contributing factor for the low K on Agreement is that these tags are much rarer than the Forward Function tags. ", "mid_sen": "The highest possible value for K may be smaller for low frequency tags (Grove et al., 1981) .", "after_sen": "Our assessment is supported by comparing our results to those of Core and Allen (1997) who used the unadapted DRI manual --see Table 2 . "}
{"citeStart": 113, "citeEnd": 131, "citeStartToken": 113, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "The sequence of evaluation is the normal order, which corresponds to reducing the leftmostoutermost redex first (Peyton Jones, 1987) . In treetheoretic terms, this is depth-first reduction of the combinator tree in which the rearrangement is controlled by the reduction rule of the leftmost combinator, e.g., Tin' X>_Xm' where X is the parenthesized subexpression in (13b). Reduction by T yields: ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "PAS is the semantic normal form of a derivation.", "mid_sen": "The sequence of evaluation is the normal order, which corresponds to reducing the leftmostoutermost redex first (Peyton Jones, 1987) . ", "after_sen": "In treetheoretic terms, this is depth-first reduction of the combinator tree in which the rearrangement is controlled by the reduction rule of the leftmost combinator, e.g., Tin' X>_Xm' where X is the parenthesized subexpression in (13b). "}
{"citeStart": 24, "citeEnd": 33, "citeStartToken": 24, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "However, D(p||q) = ∞ if there are any contexts c for which p(c) > 0 and q(c) = 0. Thus, this measure cannot be used directly on maximum likelihood estimate (MLE) probabilities. One possible solution is to use the JS divergence measure, which measures the cost of using the average distribution in place of each individual distribution. Another is the α-skew divergence measure, which uses the p distribution to smooth the q distribution. The value of the parameter α controls the extent to which the KL divergence is approximated. We use α = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001 ). The confusion probability (Sugawara et al., 1985) is an estimate of the probability that one word can be substituted for another. Words w 1 and w 2 are completely confusable if we are equally as likely to see w 2 in a given context as we are to see w 1 in that context. Jaccard's coefficient (Salton and McGill, 1983) calculates the proportion of features belonging to either word that are shared by both words. In the simplest case, the features of a word are defined as the contexts in which it has been seen to occur. sim ja+mi is a variant (Lin, 1998) in which the features of a word are those contexts for which the pointwise mutual information (MI) between the word and the context is positive, where MI can be calculated using I(c, w) = log P (c|w) P (c) . The related Dice Coefficient (Frakes and Baeza-Yates, 1992 ) is omitted here since it has been shown (van Rijsbergen, 1979) that Dice and Jaccard's Coefficients are monotonic in each other.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the simplest case, the features of a word are defined as the contexts in which it has been seen to occur. ", "mid_sen": "sim ja+mi is a variant (Lin, 1998) in which the features of a word are those contexts for which the pointwise mutual information (MI) between the word and the context is positive, where MI can be calculated using I(c, w) = log P (c|w) P (c) . ", "after_sen": "The related Dice Coefficient (Frakes and Baeza-Yates, 1992 ) is omitted here since it has been shown (van Rijsbergen, 1979) that Dice and Jaccard's Coefficients are monotonic in each other."}
{"citeStart": 69, "citeEnd": 71, "citeStartToken": 69, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "• operations on strings that go beyond concatenation (head wrapping [11] , tree adjoining", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• principle-based approaches to grammar that state general well-formedness conditions instead of describing particular constructions (e.g. IIPSG)", "mid_sen": "• operations on strings that go beyond concatenation (head wrapping [11] , tree adjoining", "after_sen": ""}
{"citeStart": 38, "citeEnd": 51, "citeStartToken": 38, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "In Section 2, we review previous approaches to the semantics of coordination and argument shar-ing. and make note of some of their drawbacks. We describe the revised sere.antic framework in Section 3. and work through several examples of non-constituent coordination (specifically, rightnode raising) in Section 4. We discnss examples involving intensioual verbs in Section 5, 2 Previous Work Steedman (198.5; 1989; , working in the framework of Combinatory Categorial Grammar (CCG), presents what is probably the most adequate analysis of non-constituent coordination to date. As noted by Steedman and discussed by Oehrle (1990) , the addition of the rule of function composition to the inventory of syntactic rules in Categorial Grammar enables the formation of constituents with right-peripheral gaps, providing a basis for a clean treatment of cases of right node raising as exemplified by sentence (1). Such examples are handled by a coordination schema which allows like categories to be conjoined, shown in (2).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We discnss examples involving intensioual verbs in Section 5, 2 Previous Work Steedman (198.5; 1989; , working in the framework of Combinatory Categorial Grammar (CCG), presents what is probably the most adequate analysis of non-constituent coordination to date. ", "mid_sen": "As noted by Steedman and discussed by Oehrle (1990) , the addition of the rule of function composition to the inventory of syntactic rules in Categorial Grammar enables the formation of constituents with right-peripheral gaps, providing a basis for a clean treatment of cases of right node raising as exemplified by sentence (1). ", "after_sen": "Such examples are handled by a coordination schema which allows like categories to be conjoined, shown in (2)."}
{"citeStart": 71, "citeEnd": 91, "citeStartToken": 71, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Considering the desiderata discussed in the last section, we present two perceptron-like algorithms for MT reranking. The first one is a splitting algorithm specially designed for MT reranking, which has similarities to a classification algorithm. We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004) . For the sake of completeness, we will briefly describe the algorithm here.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first one is a splitting algorithm specially designed for MT reranking, which has similarities to a classification algorithm. ", "mid_sen": "We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004) . ", "after_sen": "For the sake of completeness, we will briefly describe the algorithm here."}
{"citeStart": 28, "citeEnd": 40, "citeStartToken": 28, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to using a stopword list to remove words from consideration, we also leverage POS information to filter unlikely keywords. Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only. We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our hypothesis is that verb, noun and adjective words are more likely to be keywords, so we restrict our selection to words with these POS tags only. ", "mid_sen": "We used the TnT POS tagger (Brants, 2000) trained from the Switchboard data to tag the meeting transcripts.", "after_sen": ""}
{"citeStart": 29, "citeEnd": 55, "citeStartToken": 29, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "We chose three different NLP tasks to evaluate our instance weighting method for domain adaptation. The first task is POS tagging, for which we used 6166 WSJ sentences from Sections 00 and 01 of Penn Treebank as the source domain data, and 2730 PubMed sentences from the Oncology section of the PennBioIE corpus as the target domain data. The second task is entity type classification. The setup is very similar to Daumé III and Marcu (2006) . We assume that the entity boundaries have been correctly identified, and we want to classify the types of the entities. We used ACE 2005 training data for this task. For the source domain, we used the newswire collection, which contains 11256 examples, and for the target domains, we used the weblog (WL) collection (5164 examples) and the conversational telephone speech (CTS) collection (4868 examples). The third task is personalized spam filtering. We used the ECML/PKDD discovery challenge data set. The source domain contains 4000 spam and ham emails from publicly available sources, and the target domains are three individual users' inboxes, each containing 2500 emails.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second task is entity type classification. ", "mid_sen": "The setup is very similar to Daumé III and Marcu (2006) . ", "after_sen": "We assume that the entity boundaries have been correctly identified, and we want to classify the types of the entities. "}
{"citeStart": 65, "citeEnd": 87, "citeStartToken": 65, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "ing. Ferro et al. (1999) and Buchholz et al. (1999) both describe learning systems to find GRs. The former (TR) uses transformation-based error-driven learning (Brill and Resnik, 1994) and the latter (MB) uses memory-based learning . In addition, there are other differences. The TR system includes several types of information not used in the MB system (some because memory-based systems have a harder time handling set-valued attributes): possible syntactic (Comlex) and semantic (Wordnet) classes of a chunk headword, the stem(s) and named-entity category (e.g., person, location), if any, of a chunk headword, lexemes in a chunk besides the headword, pp-attachment estimate and certain verb chunk properties (e.g., passive, infinitive).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Ferro et al. (1999) and Buchholz et al. (1999) both describe learning systems to find GRs. ", "mid_sen": "The former (TR) uses transformation-based error-driven learning (Brill and Resnik, 1994) and the latter (MB) uses memory-based learning . ", "after_sen": "In addition, there are other differences. "}
{"citeStart": 198, "citeEnd": 218, "citeStartToken": 198, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003) . The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005) . In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003) , to learning translation lexicon from monolingual and/or comparable corpora Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996) . While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006) , and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007) . However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent. NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006) , but the need for language specific knowledge restricts its applicability across languages. We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008) . In this work, we extend the approach and provide a detailed description of the empirical studies.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman & Oard, 2002; Mandl & Womser-hacker, 2005; Xu &Weischedel, 2005) . ", "mid_sen": "In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003) , to learning translation lexicon from monolingual and/or comparable corpora Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996) . ", "after_sen": "While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. "}
{"citeStart": 274, "citeEnd": 298, "citeStartToken": 274, "citeEndToken": 298, "sectionName": "UNKNOWN SECTION NAME", "string": "Especially in recent work on parsing, there is a particular interest in non-projective dependency structures, in which a word and its dependents may be spread out over a discontinuous region of the sentence. These structures naturally arise in the syntactic analysis of languages with flexible word order, such as Czech (Veselá et al., 2004) . Unfortunately, most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999) , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997) . Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005) , but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfortunately, most formal results on non-projectivity are discouraging: ", "mid_sen": "While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar (Eisner and Satta, 1999) , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted (Neuhaus and Bröker, 1997) . ", "after_sen": "Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another (McDonald et al., 2005) , but becomes intractable when this assumption is abandoned (McDonald and Pereira, 2006) ."}
{"citeStart": 68, "citeEnd": 90, "citeStartToken": 68, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Left-corner transforms are particularly useful because they can preserve annotations on productions more on this below and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a . Because the emission probability o f a PCFG production can be regarded as an annotation o n a C F G production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar Abney et al., 1999 . However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Left-corner transforms are particularly useful because they can preserve annotations on productions more on this below and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. ", "mid_sen": "For example, they apply to left-recursive uni cation-based grammars Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a . ", "after_sen": "Because the emission probability o f a PCFG production can be regarded as an annotation o n a C F G production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar Abney et al., 1999 . "}
{"citeStart": 249, "citeEnd": 263, "citeStartToken": 249, "citeEndToken": 263, "sectionName": "UNKNOWN SECTION NAME", "string": "held at HLT-NAACL 2003 , pp. 25-32 Proceeings of the Seventh CoNLL conference The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982) . The scheme is more detailed and comprehensive than previous ones. We mention only those aspects of the annotation scheme relevant to this paper.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "held at HLT-NAACL 2003 , pp. 25-32 Proceeings of the Seventh CoNLL conference The scheme was inspired by work in linguistics and literary theory on subjectivity, which focuses on how opinions, emotions, etc. are expressed linguistically in context (Banfield, 1982) . ", "after_sen": "The scheme is more detailed and comprehensive than previous ones. "}
{"citeStart": 98, "citeEnd": 117, "citeStartToken": 98, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional techniques become increasingly widespread (e.g. (Bensch and Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1994; McKcown and Hatzivassiloglou, 1993; Pereira et al., 1993; Schtltze, 1993) ). However, for many tasks, one is interested in relationships among word senses, not words. Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown et al. (1992) to illustrate a \"semantically sticky\" group of words. As is often the case where sense ambiguity is involved, we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so. Yet a computational system has no choice but to consider other, more awkward possibilities --for example, this cluster might be capturing a distributional relationship between advice (as one sense of counsel) and royalty (as one sense of court). This would be a mistake for many applications, such as query expansion in information retrieval, where a surfeit of false connections can outweigh the benefits obtained by using lexical knowledge.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, for many tasks, one is interested in relationships among word senses, not words. ", "mid_sen": "Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown et al. (1992) to illustrate a \"semantically sticky\" group of words. ", "after_sen": "As is often the case where sense ambiguity is involved, we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so. "}
{"citeStart": 210, "citeEnd": 222, "citeStartToken": 210, "citeEndToken": 222, "sectionName": "UNKNOWN SECTION NAME", "string": "6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives. Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994) . This is not necessary in our approach, which drastically reduces the search space for parsing.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "6 Comparison to PSG Approaches One feature of word order domains is that they factor ordering alternatives from the syntactic tree, much like feature annotations do for morphological alternatives. ", "mid_sen": "Other lexicalized grammars collapse syntactic and ordering information and are forced to represent ordering alternatives by lexical ambiguity, most notable L-TAG (Schabes et al., 1988) and some versions of CG (Hepple, 1994) . ", "after_sen": "This is not necessary in our approach, which drastically reduces the search space for parsing."}
{"citeStart": 197, "citeEnd": 222, "citeStartToken": 197, "citeEndToken": 222, "sectionName": "UNKNOWN SECTION NAME", "string": "While the above interpretation of trajectory-ofmotion events forces one to abstract away from *The spatial trace function r~ maps eventualities to their trajectories (cf. White 1993) . 5Much as in Moens and Steedman (1988) and Jackendoff (1991), the introduction of gr is necessary to avoid having an ill-sorted formula. the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the normal speed as well as the meanings of the prepositions 10, lowards, etc. By making two additional restrictive assumptions, namely that these events be of constant velocity and in one dimension, I have been able to construct and implement an algorithm which determines whether a specified sequence of such events is or is not possible under certain situationally supplied constraints. These constraints include the locations of various landmarks (assumed to remain stationary) and the minimum, maximum, and normal rates associated with various manners of motion (e.g. running, jogging) for a given individual.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "While the above interpretation of trajectory-ofmotion events forces one to abstract away from *The spatial trace function r~ maps eventualities to their trajectories (cf. White 1993) . 5Much as in Moens and Steedman (1988) and Jackendoff (1991), the introduction of gr is necessary to avoid having an ill-sorted formula. ", "after_sen": "the manner of motion supplied by a verb, it does nevertheless permit one to consider factors such as the normal speed as well as the meanings of the prepositions 10, lowards, etc. "}
{"citeStart": 0, "citeEnd": 25, "citeStartToken": 0, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "The summarized inter-subject correlation between 21 subjects was r=.478 (cf. Resnik (1995) reported a correlation of r=.9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. ", "mid_sen": "Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. ", "after_sen": "Gurevych (2006) reported a correlation of r=.69. "}
{"citeStart": 71, "citeEnd": 89, "citeStartToken": 71, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to test the scalability of the global lexical selection approach, we also performed lexical selection experiments on the United Nations (Arabic-English) corpus and the Hansard (French-English) corpus using the SFST model and the BOW Maxent model. We used 1,000,000 training sentence pairs and tested on 994 test sentences for the UN corpus. For the Hansard corpus we used the same training and test split as in (Zens and Ney, 2004) : 1.4 million training sentence pairs and 5432 test sentences. The vocabulary sizes for the two corpora are mentioned in Table 4 . Also in Table 4 , are the results in terms of F-measure between the words in the reference sentence and the decoded sentences. We can see that the BOW model outperforms the SFST model on both corpora significantly. This is due to a systematic 10% relative improvement for open class words, as they benefit from a much wider context. BOW performance on close class words is higher for the UN corpus but lower for the Hansard corpus. ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used 1,000,000 training sentence pairs and tested on 994 test sentences for the UN corpus. ", "mid_sen": "For the Hansard corpus we used the same training and test split as in (Zens and Ney, 2004) : ", "after_sen": "1.4 million training sentence pairs and 5432 test sentences. "}
{"citeStart": 104, "citeEnd": 126, "citeStartToken": 104, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "Once we had assembled the final training corpus, we annotated it with statistical word alignments and constituent parse trees on both sides. Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008) , then symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2005) . For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008) , then symmetrized with the grow-diag-final-and heuristic (Koehn et al., 2005) . ", "mid_sen": "For generating parse trees, we used the French and English grammars of the Berkeley statistical parser (Petrov and Klein, 2007) .", "after_sen": "Except for minor bug fixes, our method for extracting and scoring a translation grammar remains the same as in our WMT 2010 submission. "}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "The summarized inter-subject correlation between 21 subjects was r=.478 (cf. Resnik (1995) reported a correlation of r=.9026. 10 The results are not directly comparable, because he only used noun-noun pairs, words instead of concepts, a much smaller dataset, and measured semantic similarity instead of semantic relatedness. Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. Gurevych (2006) reported a correlation of r=.69. Test subjects were trained students of computational linguistics, and word pairs were selected analytically.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Finkelstein et al. (2002) did not report inter-subject correlation for their larger dataset. ", "mid_sen": "Gurevych (2006) reported a correlation of r=.69. ", "after_sen": "Test subjects were trained students of computational linguistics, and word pairs were selected analytically."}
{"citeStart": 274, "citeEnd": 293, "citeStartToken": 274, "citeEndToken": 293, "sectionName": "UNKNOWN SECTION NAME", "string": "ing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot & Lang, 1989) , or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma & van Noord, 1993; Maxwell & Kaplan, 1993) . It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard & Sag, 1987) . However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard & Sag, 1987) . ", "mid_sen": "However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed.", "after_sen": "In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967) , although the latter is often augmented with top-down predic-tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975) . "}
{"citeStart": 92, "citeEnd": 106, "citeStartToken": 92, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Our analysis was carried out within the framework of Systemic-Functional Linguistics (SFL) (Halliday, 1978; Halliday, 1985) which views language as a resource for the creation of meaning. SFL stratifies meaning into context and language. The strata of the linguistic resources are organised into networks of choices, each choice resulting in a different meaning realised (i.e., expressed) by appropriate structures. The emphasis is on paradigmatic choices, as opposed to syntagma~ic structures. Choices made in each stratum constrain the choices available in the stratum beneath. Context thus constrains language. This framework was chosen for several reasons. First, the organisation of linguistic resources according to this principle is well-suited to natural language generation, where the starting point is necessarily a communicative goal, and the task is to find the most appropriate expression for the intended meaning (Matthiessen and Bateman, 1991) . Second, a functional perspective offers an advantage for multilingual text generation, because of its ability to achieve a level of linguistic description which holds across languages more effectively than do structurally-based accounts. The approach has been shown capable of supporting the sharing of linguistic resources between languages as structurally distinct as English and Japanese (Bateman et al., 1991a; Bateman et at., 1991b) . It is therefore reasonable to expect that at least the same degree of commonality of description is achievable between English and French within this framework. Finally, KPML (Bateman, 1994) , the tactical generator we employ, is based on SFL, and it is thus appropriate for us to characterise the corpus in terms immediately applicable to our generator.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our analysis was carried out within the framework of Systemic-Functional Linguistics (SFL) (Halliday, 1978; Halliday, 1985) which views language as a resource for the creation of meaning. ", "after_sen": "SFL stratifies meaning into context and language. "}
{"citeStart": 277, "citeEnd": 291, "citeStartToken": 277, "citeEndToken": 291, "sectionName": "UNKNOWN SECTION NAME", "string": "It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of PCFG parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs (Charniak, 1997) , or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997) . Clearly further research is warranted. Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C&C", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of PCFG parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability. ", "mid_sen": "One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs (Charniak, 1997) , or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997) . ", "after_sen": "Clearly further research is warranted. "}
{"citeStart": 174, "citeEnd": 191, "citeStartToken": 174, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "Determining a representative vector. Each review is represented as a vector of boolean attributes, where each attribute indicates the presence or absence of a word or punctuation mark in the text. We elect to use boolean attributes since they have been shown to be advantageous over term-frequency approaches for sentiment detection, particularly when SVMs are employed (Pang et al., 2002) . We considered two ways of determining a representative vector: centroid and sample selection.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each review is represented as a vector of boolean attributes, where each attribute indicates the presence or absence of a word or punctuation mark in the text. ", "mid_sen": "We elect to use boolean attributes since they have been shown to be advantageous over term-frequency approaches for sentiment detection, particularly when SVMs are employed (Pang et al., 2002) . ", "after_sen": "We considered two ways of determining a representative vector: centroid and sample selection."}
{"citeStart": 173, "citeEnd": 194, "citeStartToken": 173, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997) . For a broader review of WSD in NLP applications, see Resnik (2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank.", "mid_sen": "Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997) . ", "after_sen": "For a broader review of WSD in NLP applications, see Resnik (2006) ."}
{"citeStart": 77, "citeEnd": 98, "citeStartToken": 77, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "This approach is somehow similar to those proposed by (Qi et al., 2004) and (Xu y Schuurmans, 2005) . Nonetheless, the 2-steps-SVM approach uses the same method for both the first and second steps. A supervised multiclass SVM is used to increase the labeled set and, after that, to classify the test set.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the second step, now with a fully labeled training set, the usual supervised classification process is done, learning with the training documents and predicting the documents in the test set.", "mid_sen": "This approach is somehow similar to those proposed by (Qi et al., 2004) and (Xu y Schuurmans, 2005) . ", "after_sen": "Nonetheless, the 2-steps-SVM approach uses the same method for both the first and second steps. "}
{"citeStart": 118, "citeEnd": 139, "citeStartToken": 118, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Two other tests for comparing how two techniques perform by comparing how well they perform on each test sample are the sign and Wilcoxon tests (Harnett, 1982, Sec. 15.5) . Unlike the matched-pair t test, neither of these two tests assume that the sum of the differences has a normal (Gaussian) distribution. The two tests are so-called nonparametric tests, which do not make assumptions about how the results are distributed (Harnett, 1982, Ch. 15) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unlike the matched-pair t test, neither of these two tests assume that the sum of the differences has a normal (Gaussian) distribution. ", "mid_sen": "The two tests are so-called nonparametric tests, which do not make assumptions about how the results are distributed (Harnett, 1982, Ch. 15) .", "after_sen": "The sign test is the simplier of the two. "}
{"citeStart": 0, "citeEnd": 20, "citeStartToken": 0, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. ", "mid_sen": "Blunsom et al. (2008 utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. ", "after_sen": "The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007) . Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. "}
{"citeStart": 121, "citeEnd": 133, "citeStartToken": 121, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\" Rather, we stress the possibility that one can axiomatize and productively use such a rule. We shall see this in the next example: two sentences, regarded as a fragment of paragraph, are a variation on a theme by Hobbs (1979) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do not claim that Gla is the best or unique way of expressing the rule \"assume that the writer did not say too much.\" Rather, we stress the possibility that one can axiomatize and productively use such a rule. ", "mid_sen": "We shall see this in the next example: two sentences, regarded as a fragment of paragraph, are a variation on a theme by Hobbs (1979) .", "after_sen": ""}
{"citeStart": 31, "citeEnd": 49, "citeStartToken": 31, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to produce many-to-many alignments we combine the outputs of two models, one for each translation direction. We use the refined method from Och and Ney (2003) which starts from the intersection of the two models' predictions and 'grows' the predicted alignments to neighbouring alignments which only appear in the output of one of the models.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to produce many-to-many alignments we combine the outputs of two models, one for each translation direction. ", "mid_sen": "We use the refined method from Och and Ney (2003) which starts from the intersection of the two models' predictions and 'grows' the predicted alignments to neighbouring alignments which only appear in the output of one of the models.", "after_sen": ""}
{"citeStart": 11, "citeEnd": 31, "citeStartToken": 11, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Many natural language processing (NLP) problems such as part-of-speech (POS) tagging, named entity (NE) recognition, relation extraction, and semantic role labeling, are currently solved by supervised learning from manually labeled data. A bottleneck problem with this supervised learning approach is the lack of annotated data. As a special case, we often face the situation where we have a sufficient amount of labeled data in one domain, but have little or no labeled data in another related domain which we are interested in. We thus face the domain adaptation problem. Following (Blitzer et al., 2006) , we call the first the source domain, and the second the target domain.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We thus face the domain adaptation problem. ", "mid_sen": "Following (Blitzer et al., 2006) , we call the first the source domain, and the second the target domain.", "after_sen": "The domain adaptation problem is commonly encountered in NLP. "}
{"citeStart": 19, "citeEnd": 20, "citeStartToken": 19, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "We remarked in [4] that this transformation :'is closely re(ated to le('t<.orner pa.rsing\", but did not give details. In a recent paper [7] , Mark Johnson introduces \"a left-corner program transR)rmation for natural (anguage parsing\", which has some similarity to the abow~ transformation, but whic.h is applied to definite clause programs, rather than to ()CGs. lie proves that this transformation respects deelarative equivalcnee, and also shows, using a mode(q;heoretic approach, the close connection of his transformation with (eft-corner parsing [12, 9, 1()]. r (t 1TlUSt be noted that the left-reeursion elimination procedure can 1)e a*pplied to any ])C(~, whether OP or not. Even in the case where the grammar is OP, how ever, it wil( not (ead to a terminating parsing algorithm unless empty l)roductions have been prea(ably eliminated from the grammar, a l)roblem wlfirh is shared by the usual left-corner parser-interpreter. 4'Fhe fact that the standard (','FG emptyq)roduction elinfio nation transformation is always possible is relal.ed to the fact that this transformation does not preserve degrees of ambiguity. Dae to the space available, we do not give here col rectncss proof~ Jbr the algorithms presented, but ez'peet to publish them in a tidier version of this paper. These algorithms have actually been implemented in a slightly extended version, where the*,/ are also used to decide whether the grammar proposed for\" transformation is in fact oJfline-parsable or not.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We remarked in [4] that this transformation :'is closely re(ated to le('t<.orner pa.rsing\", but did not give details. ", "mid_sen": "In a recent paper [7] , Mark Johnson introduces \"a left-corner program transR)rmation for natural (anguage parsing\", which has some similarity to the abow~ transformation, but whic.h is applied to definite clause programs, rather than to ()CGs. ", "after_sen": "lie proves that this transformation respects deelarative equivalcnee, and also shows, using a mode(q;heoretic approach, the close connection of his transformation with (eft-corner parsing [12, 9, 1()]. "}
{"citeStart": 213, "citeEnd": 239, "citeStartToken": 213, "citeEndToken": 239, "sectionName": "UNKNOWN SECTION NAME", "string": "our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about 'attributes'-only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network-and this information is still very sparse. On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in Word-Net: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002) . 2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of 'attribute' and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically (e.g., to supplement WordNet).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about 'attributes'-only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network-and this information is still very sparse. ", "mid_sen": "On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in Word-Net: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002) . 2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. ", "after_sen": "Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of 'attribute' and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically (e.g., to supplement WordNet)."}
{"citeStart": 218, "citeEnd": 237, "citeStartToken": 218, "citeEndToken": 237, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense in VP-ellipsis illustrates how categories can be put to work. In (24) I enjoyed it. And so will you the ellipsis is contained within a form expression whose category is vp_ellipsis It ense=inf ,modalffivill ,perf ectffi_, progressive=_,pol=pos .... ] This states the syntactic tense, aspect and polarity marked on the ellipsis (underscores indicate lack of specification). The category constrains resolution to look for verb phrase/sentence sources, which come wrapped in forms with categories like [t ease=past, modalffino, pexf ectffino, Heuristics simi]ar to those described by Hardt (1992) may be used for this. The category also says that, for this kind of VP match 9, the term in the antecedent whose category identifies it as being the subject should be treated as parallel to the explicit term in the ellipsis. As this example illustrates, tense and aspect on ellipsis and antecedent do not have to agree. When Sforns axe described in . 9Not all VP ellipses have VP antecedents. this is so, the antecedent and ellipsis categories are both used to determine what fozm should be substituted for the antecedent form. This comprises the restriction of the antecedent form and a new category constructed by taking the features of the antecedent category, unless overridden by those on the ellipsis--a kind of (monotonic) priority union (Grover et ai., 1994) except using skeptical as opposed to credulous default unification (Carpenter, 1993) . When a new category is constructed for the antecedent, any tense resolutions also need to be undone, since the original ones may no longer be appropriate for the revised category. One thus merges the category information from source and antecedent to determine what verb phrase form should be substituted for the original. In this case, it will have a category", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "this is so, the antecedent and ellipsis categories are both used to determine what fozm should be substituted for the antecedent form. ", "mid_sen": "This comprises the restriction of the antecedent form and a new category constructed by taking the features of the antecedent category, unless overridden by those on the ellipsis--a kind of (monotonic) priority union (Grover et ai., 1994) except using skeptical as opposed to credulous default unification (Carpenter, 1993) . ", "after_sen": "When a new category is constructed for the antecedent, any tense resolutions also need to be undone, since the original ones may no longer be appropriate for the revised category. "}
{"citeStart": 275, "citeEnd": 295, "citeStartToken": 275, "citeEndToken": 295, "sectionName": "UNKNOWN SECTION NAME", "string": "Weighting against each other the contributions of different measures taken for improving log-linear models for parse selection, we can conclude that property design is at least as important as property selection and/or regularization, since even a completely unregularized model based on all properties performs significantly better than the bestadjusted model among the ones that are based on the template-based properties only. Moreover, property design can be carried out in a targeted way, i.e. properties can be designed in order to improve the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of special interest for the task that the system's output is used for. By demonstrating that property design is the key to good log-linear models for 'deep' syntactic disambiguation, our work confirms that \"specifying the features of a SUBG [stochastic unification-based grammar] is as much an empirical matter as specifying the grammar itself\" (Johnson et al., 1999 ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, property design can be carried out in a targeted way, i.e. properties can be designed in order to improve the disambiguation of grammatical relations that, so far, are disambiguated particularly poorly or that are of special interest for the task that the system's output is used for. ", "mid_sen": "By demonstrating that property design is the key to good log-linear models for 'deep' syntactic disambiguation, our work confirms that \"specifying the features of a SUBG [stochastic unification-based grammar] is as much an empirical matter as specifying the grammar itself\" (Johnson et al., 1999 ", "after_sen": "Since we have a list of title nouns available, we might also introduce a more general property that would count the number of occurrences of title nouns in general that govern a proper name via the dependency APP. "}
{"citeStart": 252, "citeEnd": 266, "citeStartToken": 252, "citeEndToken": 266, "sectionName": "UNKNOWN SECTION NAME", "string": "Projective Bilexical Dependency Grammars (PB-DGs) have attracted attention recently for two reasons. First, because they capture bilexical head-tohead dependencies they are capable of producing extremely high-quality parses: state-of-the-art discriminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today (McDonald, 2006) . Second, Eisner-Satta O(n 3 ) PBDG parsing algorithms are extremely fast (Eisner, 1996; Eisner and Satta, 1999; Eisner, 2000) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Projective Bilexical Dependency Grammars (PB-DGs) have attracted attention recently for two reasons. ", "mid_sen": "First, because they capture bilexical head-tohead dependencies they are capable of producing extremely high-quality parses: state-of-the-art discriminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today (McDonald, 2006) . ", "after_sen": "Second, Eisner-Satta O(n 3 ) PBDG parsing algorithms are extremely fast (Eisner, 1996; Eisner and Satta, 1999; Eisner, 2000) ."}
{"citeStart": 312, "citeEnd": 325, "citeStartToken": 312, "citeEndToken": 325, "sectionName": "UNKNOWN SECTION NAME", "string": "We divided the data into 20 contiguous and equally-sized sections, and used the first 18 sections for training, and the last 2 sections for testing while development (henceforth the training and development sets, respectively). The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger's 'unrealistically good' performance on the training set (Collins, 2000) ). Among the n-best sequences output by the MEMM tagger, the sequence with the highest F-score is used as the 'correct' sequence for training the reranker.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We divided the data into 20 contiguous and equally-sized sections, and used the first 18 sections for training, and the last 2 sections for testing while development (henceforth the training and development sets, respectively). ", "mid_sen": "The training data of the reranker is created by the n-best tagger, and every set of 17 sections from the training set is used to train the n-best tagger for the remaining section (The same technique is used by previous studies to avoid the n-best tagger's 'unrealistically good' performance on the training set (Collins, 2000) ). ", "after_sen": "Among the n-best sequences output by the MEMM tagger, the sequence with the highest F-score is used as the 'correct' sequence for training the reranker."}
{"citeStart": 114, "citeEnd": 137, "citeStartToken": 114, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996) . For the thirdperson pronouns and reflexives, the performance was (1) 86% of 560 cases in five computer manuals and (2) 75% of 306 cases in twenty-seven Web page texts. The present FASTUS system correctly resolved 71% of 34 cases in five newspaper arti-cles. This progressive decline in performance corresponds to the progressive decline in the amount of syntactic information in the input to reference resolution. To summarize the latter decline, Lap-pin and Leass (1994) had the following components in their algorithm. 8. Decision procedure for choosing among equally preferred candidate antecedents Kennedy and Boguraev (1996) approximated the above components with a poorer syntactic input, which is an output of a part-of-speech tagger with grammatical function information, plus NPs recognized by finite-state patterns and NPs' adjunct and subordination contexts recognized by heuristics. With this input, grammatical functions and precedence relations were used to approximate 2 and 5. Finite-state patterns approximated 4. Three additional salience factors were used in 7, and a preference for intraclausal antecedents was added in 6; 3 and 8 were the same. The present algorithm works with an even poorer syntactic input, as summarized here.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system did quite well (78%) with third-person pronouns with intrasentential antecedents, the largest class of such pronouns.", "mid_sen": "Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996) . ", "after_sen": "For the thirdperson pronouns and reflexives, the performance was (1) 86% of 560 cases in five computer manuals and (2) 75% of 306 cases in twenty-seven Web page texts. "}
{"citeStart": 660, "citeEnd": 673, "citeStartToken": 660, "citeEndToken": 673, "sectionName": "UNKNOWN SECTION NAME", "string": "The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: 1Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996) , (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus-but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997) , and (4) no figures about computational effort -space/time complexity-are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The above mentioned factors can affect either the evaluation or the comparison process. ", "mid_sen": "Factors affecting the evaluation process are: 1Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996) , (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus-but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997) , and (4) no figures about computational effort -space/time complexity-are usually reported, even from an empirical perspective. ", "after_sen": "A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance."}
{"citeStart": 58, "citeEnd": 78, "citeStartToken": 58, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of the Good-Tufing method in natural language technology is far from new. It is commonly applied in speech recognition and part-of-speech tagging for adjusting the frequencies of (un)seen word sequences (e.g. Jelinek, 1985; Katz, 1987; Church & Gale, 1991) . In stochastic parsing, Good-Turing has to our knowledge never been tried out. Designers of stochastic parsers seem to have given up on the problem of creating a statistically adequate theory concerning parsing unknown events. Stochastic parsing systems either use a closed lexicon, or use a two step approach where first the words are tagged by a stochastic tagger, after which the p-o-s tags (with or without the words) are parsed by a stochastic parser. The latter approach has become increasingly popular (e.g. Schabes et al., 1993; Weischedel et al., 1993; Briscoe, 1994; Magerman, 1995; Collins, 1996) . Notice, however, that the tagger used in this two step approach often uses Good-Turing (or a similar smoothing method) to adjust the observed frequencies of n-grams. So why not apply Good-Turing directly to the structural units of a stochastic grammar?", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Stochastic parsing systems either use a closed lexicon, or use a two step approach where first the words are tagged by a stochastic tagger, after which the p-o-s tags (with or without the words) are parsed by a stochastic parser. ", "mid_sen": "The latter approach has become increasingly popular (e.g. Schabes et al., 1993; Weischedel et al., 1993; Briscoe, 1994; Magerman, 1995; Collins, 1996) . ", "after_sen": "Notice, however, that the tagger used in this two step approach often uses Good-Turing (or a similar smoothing method) to adjust the observed frequencies of n-grams. "}
{"citeStart": 343, "citeEnd": 355, "citeStartToken": 343, "citeEndToken": 355, "sectionName": "UNKNOWN SECTION NAME", "string": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. ", "mid_sen": "With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . ", "after_sen": "One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . "}
{"citeStart": 107, "citeEnd": 119, "citeStartToken": 107, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993) , robust parsing (Abney, 1991) , and general parsing (deMarcken, 1990; Charniak et al., 1994) . The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. While the proposed algorithm is not successful for all grammatical categories, it does show that fully automatic tagging is possible when demands on accuracy are modest.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we describe an experiment on fully automatic derivation of the knowledge necessary for part-of-speech tagging. ", "mid_sen": "Part-of-speech tagging is of interest for a number of applications, for example access to text data bases (Kupiec, 1993) , robust parsing (Abney, 1991) , and general parsing (deMarcken, 1990; Charniak et al., 1994) . ", "after_sen": "The goal is to find an unsupervised method for tagging that relies on general distributional properties of text, properties that are invariant across languages and sublanguages. "}
{"citeStart": 83, "citeEnd": 92, "citeStartToken": 83, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "The performance of the RFF-based similarity measure was evaluated for a sample of nouns and compared with that of Lin98. The experiment was conducted using an 18 million tokens subset of the Reuters RCV1 corpus, parsed by Lin's Minipar dependency parser (Lin, 1993) . We considered first an evaluation based on WordNet data as a gold standard, as in (Lin, 1998; Weeds and Weir, 2003) . However, we found that many word pairs from the Reuters Corpus that are clearly substitutable are not linked appropriately in WordNet.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The experiment was conducted using an 18 million tokens subset of the Reuters RCV1 corpus, parsed by Lin's Minipar dependency parser (Lin, 1993) . ", "mid_sen": "We considered first an evaluation based on WordNet data as a gold standard, as in (Lin, 1998; Weeds and Weir, 2003) . ", "after_sen": "However, we found that many word pairs from the Reuters Corpus that are clearly substitutable are not linked appropriately in WordNet."}
{"citeStart": 160, "citeEnd": 183, "citeStartToken": 160, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "Several groups have continued working with the Ramshaw and Marcus data sets for base noun phrases. (Argamon et al., 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. This method records POS tag sequences which contain chunk boundaries and uses these sequences to classify the test data. Its performance is somewhat worse than that of Ramshaw and Marcus (F~=1=91.6 vs. 92.0) but it is the best result obtained without using lexical information 6. (Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data. This approach performs worse than the method of Argamon et al. (F~=1=90.9).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This method records POS tag sequences which contain chunk boundaries and uses these sequences to classify the test data. ", "mid_sen": "Its performance is somewhat worse than that of Ramshaw and Marcus (F~=1=91.6 vs. 92.0) but it is the best result obtained without using lexical information 6. (Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data. ", "after_sen": "This approach performs worse than the method of Argamon et al. (F~=1=90.9)."}
{"citeStart": 110, "citeEnd": 131, "citeStartToken": 110, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the lack of an accepted formalism for the specification of prindple-based grammars, Crocker and Lewi, [Crocker and Lewin1992] define the declarative 'Proper Branch' formalism, which can be used with a number of different parsing methods. A proper branch is a set of three nodes --a mother and two daughters --which are constructed by the parser, using a simple mechanism such as a shift-reduce interpreter, and then 'licensed' by the principles of grammar. A complete phrase marker of the input string can then be constructed by following the manner in which the mother node from one proper branch is used as a daughter node in a dominating proper branch.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although attempts have been made to modify PS grammars/parsers to cope with extragrammatical input, e.g. [Carbonell and Hayes1983, Douglas and Dale1992, Jensen et al.1983 , this is a feature which has to be 'added on' and tends to affect the statement of the grammar.", "mid_sen": "Due to the lack of an accepted formalism for the specification of prindple-based grammars, Crocker and Lewi, [Crocker and Lewin1992] define the declarative 'Proper Branch' formalism, which can be used with a number of different parsing methods. ", "after_sen": "A proper branch is a set of three nodes --a mother and two daughters --which are constructed by the parser, using a simple mechanism such as a shift-reduce interpreter, and then 'licensed' by the principles of grammar. "}
{"citeStart": 132, "citeEnd": 156, "citeStartToken": 132, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "A statistically significant (according to a two-tailed t-test) improvement in g2p conversion accuracy (from 13.7% WER to 13.2% WER) was obtained with the manually annotated morphological boundaries from CELEX. The segmentation from both of the rule-based systems (ETI and SMOR) also resulted in an accuracy increase with respect to the baseline (13.6% WER), which is not annotated with morphological boundaries. Among the unsupervised systems, best results 7 on the g2p task with morphological annotation were obtained with the RePortS system (Keshava and Pitler, 2006) . But none of the segmentations led to an error reduction when compared to a baseline that used no morphological information (see Table 3 ). Word error rate even increased when the quality of the For all results refer to (Demberg, 2006 Table 3 : Systems evaluation on German CELEX manual annotation and on the g2p task using a joint n-gram model. WERs refer to implementation v2. morphological segmentation was too low (the unsupervised algorithms achieved 52%-62% F-measure with respect to CELEX manual annotation). Table 4 shows that high-quality morphological information can also significantly improve performance on a syllabification task for German. We used the syllabifier described in (Schmid et al., 2005) , which works similar to the joint n-gram model used for g2p conversion. Just as for g2p conversion, we found a significant accuracy improvement when using the manually annotated data, a smaller improvement for using data from the rule-based morphological system, and no improvement when using segmentations from an unsupervised algorithm. Syllabification works best when performed on phonemes, because syllables are phonological units and therefore can be determined most easily in terms of phonological entities such as phonemes.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The segmentation from both of the rule-based systems (ETI and SMOR) also resulted in an accuracy increase with respect to the baseline (13.6% WER), which is not annotated with morphological boundaries. ", "mid_sen": "Among the unsupervised systems, best results 7 on the g2p task with morphological annotation were obtained with the RePortS system (Keshava and Pitler, 2006) . ", "after_sen": "But none of the segmentations led to an error reduction when compared to a baseline that used no morphological information (see Table 3 ). "}
{"citeStart": 160, "citeEnd": 174, "citeStartToken": 160, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "Our research builds on earlier work defining illocutionary points of speech acts (Searle, 1975) , and relating such speech acts to email and workflow tracking (Winograd, 1987 , Flores & Ludlow, 1980 , Weigant et al, 2003 . Winograd suggested that research explicating the speech-act based \"language-action perspective\" on human communication could be used to build more useful tools for coordinating joint activities. The Coordinator (Winograd, 1987) was one such system, in which users augmented email messages with additional annotations indicating intent.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our research builds on earlier work defining illocutionary points of speech acts (Searle, 1975) , and relating such speech acts to email and workflow tracking (Winograd, 1987 , Flores & Ludlow, 1980 , Weigant et al, 2003 . ", "after_sen": "Winograd suggested that research explicating the speech-act based \"language-action perspective\" on human communication could be used to build more useful tools for coordinating joint activities. "}
{"citeStart": 102, "citeEnd": 125, "citeStartToken": 102, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "To make an efficient use of the declarative knowledge we build a taxonomy of information structure categories centered around the distinction between categories that describe the authors' OWN work and those that describe OTHER work (see Section 5). In practice, our model labels every sentence with an AZ category augmented by one of the two categories, OWN or OTHER. In evaluation we consider only the standard AZ categories which are part of the annotation scheme of (Contractor et al., 2012) . ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In practice, our model labels every sentence with an AZ category augmented by one of the two categories, OWN or OTHER. ", "mid_sen": "In evaluation we consider only the standard AZ categories which are part of the annotation scheme of (Contractor et al., 2012) . ", "after_sen": "1 Accordingly, E p λ (y|x) [f k (x, y)] = p λ (y k |x k )"}
{"citeStart": 80, "citeEnd": 102, "citeStartToken": 80, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996) , a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998) . The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? What is the effect of cascading? Will errors at a lower level percolate to higher modules?", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. ", "mid_sen": "Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996) , a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998) . ", "after_sen": "The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers? "}
{"citeStart": 226, "citeEnd": 255, "citeStartToken": 226, "citeEndToken": 255, "sectionName": "UNKNOWN SECTION NAME", "string": "A prot nising way of generating contours from tone sequences is to specify one or more pitch targets per tone and then to interpolate between the targets; the task then becomes one of providing a suitable sequence of targets (Pierrehumbert & Beckman, 1988) . It is perhaps less clear how we should go about recognising tone sequences from pitch contours. Hidden Markov Models (HMMs) (Huang et al., 1990 ) offer a powerful statistical approach to this problem, though it is unch:ar how they could be used to rccognise the units of interest to phonologists, ttMMs do not encode timing information in a way that would allow them to output, say, one tone per syllable (or vowel) . Moreover, the same section of a pitch contour may correspond to either H or L tones. For example, a H between two Hs looks just like an L between two Ls. There is no principled upper bound on the amount of context that needs to be inspected in order to resolve the ambiguity, lea(ling to a multiplication of state information required by the HMM and problems for training it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A prot nising way of generating contours from tone sequences is to specify one or more pitch targets per tone and then to interpolate between the targets; the task then becomes one of providing a suitable sequence of targets (Pierrehumbert & Beckman, 1988) . ", "after_sen": "It is perhaps less clear how we should go about recognising tone sequences from pitch contours. "}
{"citeStart": 12, "citeEnd": 24, "citeStartToken": 12, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "The argument structure (AltC~STIQ specifies that mentM adjectives select; for two arguments, one for human (argl) and a second for event (see Croft, 11984, for a similar view). The second is a default argument (D_argJ) as it need not to be present at the syntactic level (as shown in exampies (1)). As agent-oriented adjectives refer to the manifestation of the state (examples (2)), the second argmnent is e3, the event which follows the state. It is subtyped as an intellectual act. For emotion adjeetlves, the second argument is c2 or e3, as they can refer either to the manifestation of the. state (example (3e)) or its cause (examples (3a,b)). e2 is subtyped as an experiencing event, as we consider that the cause of an emotion corresponds to the experiencing of sonlething. I,'ollowing Croft (1990) , we think that there are two processes implied in a causal emotional state: an cxperiencer must direct his or her attention to a stimulus and this causes the experiencer to enter ill a mental state.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "e2 is subtyped as an experiencing event, as we consider that the cause of an emotion corresponds to the experiencing of sonlething. ", "mid_sen": "I,'ollowing Croft (1990) , we think that there are two processes implied in a causal emotional state: an cxperiencer must direct his or her attention to a stimulus and this causes the experiencer to enter ill a mental state.", "after_sen": "The qualla structure (QUALIA) encodes the basic semantic type of a word (its Lexical Coilceptual Paradigm, or LCP) and specifics how it is linked to other events and arguments of the event and argument structures (see Pustejovsky, 1995, chapter 6 ). "}
{"citeStart": 243, "citeEnd": 257, "citeStartToken": 243, "citeEndToken": 257, "sectionName": "UNKNOWN SECTION NAME", "string": "To sum up, this work has been carried out to automatically classify Arabic documents using the NB algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in (Yahyaoui, 2001) . In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments. The corresponding performances in (Yahyaoui, 2001) are 75.6% and 50%, respectively. Thus, the overall performance (including cross validation and evaluation set experiments) in this work is comparable to that in (Yahyaoui, 2001) . This offers some indication that the performance of NB algorithm in classifying Arabic documents is not sensitive to the Arabic root extraction algorithm. Future work will be directed at experimenting with other root extraction algorithms. Further improvement of NB's performance may be effected by using unlabeled documents; e.g., EM has been used successfully for this purpose in (Nigam et al., 200) , where EM has increased the classification accuracy by 30% for classifying English documents.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To sum up, this work has been carried out to automatically classify Arabic documents using the NB algorithm, with the use of a different data set, a different number of categories, and a different root extraction algorithm from those used in (Yahyaoui, 2001) . ", "after_sen": "In this work, the average accuracy over all categories is: 68.78% in cross validation and 62% in evaluation set experiments. "}
{"citeStart": 9, "citeEnd": 38, "citeStartToken": 9, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "For I_IG, the size of the domain of nonterminals and associated stacks (the analogue of the nonterminals in CFG) is not bound by the grammar. However, Vijay-Shanker and Weir (1993) demonstrate that polynomial time performance can be achieved through the use of structuresharing made possible by constraints in the way that LI6 use stacks.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For I_IG, the size of the domain of nonterminals and associated stacks (the analogue of the nonterminals in CFG) is not bound by the grammar. ", "mid_sen": "However, Vijay-Shanker and Weir (1993) demonstrate that polynomial time performance can be achieved through the use of structuresharing made possible by constraints in the way that LI6 use stacks.", "after_sen": "Although stacks of unbounded size can arise during a derivation, it is not possible for a LIG to specify that two dependent, unbounded stacks must appear at distinct places in the derivation tree. "}
{"citeStart": 192, "citeEnd": 212, "citeStartToken": 192, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS). There are four sound gender-number suffixes in Arabic: 5 +φ (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural. Form-based GENDER and NUMBER feature values are set only according to these four morphemes (and a few others, ignored for simplicity). There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes. A sound plural example is the word pair / Hafiyd+a /Hafiyd+At ('granddaughter/granddaughters.) On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+φ ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix. This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data). A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Âzraq+φ ('blue') is zarqA'+φ not * *Âzraq+a . To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007) , we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features. 6 Most available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012) . The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012) . See Section 5.2 for more details.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007) , we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features. ", "mid_sen": "6 Most available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012) . ", "after_sen": "The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. "}
{"citeStart": 99, "citeEnd": 111, "citeStartToken": 99, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "We have shown how higher-order logic programming can be used to elegantly implement the semantic theory of CCG, including the previously difficult case of its handling of coordination constructs. The techniques used here should allow similar advantages for a variety of such theories. An argument can be made that the approach taken here relies on a formalism that entails implementation issues that are more difficult than for the other solutions and inherently not as efficient. However, the implementation issues, although more complex, are also well-understood and it can be expected that future work will bring further improvements. For example, it is a straightforward matter to transform the ,XProlog code into a logic called L~ (Miller, 1990) which requires only a restricted form of unification that is decidable in linear time and space. Also, the declarative nature of ~Prolog programs opens up the possibility for applications of program transformations such as partial evaluation. 6", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the implementation issues, although more complex, are also well-understood and it can be expected that future work will bring further improvements. ", "mid_sen": "For example, it is a straightforward matter to transform the ,XProlog code into a logic called L~ (Miller, 1990) which requires only a restricted form of unification that is decidable in linear time and space. ", "after_sen": "Also, the declarative nature of ~Prolog programs opens up the possibility for applications of program transformations such as partial evaluation. "}
{"citeStart": 272, "citeEnd": 293, "citeStartToken": 272, "citeEndToken": 293, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to surface word language models, we did experiments with language models based on part-of-speech for English-German. We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and #pairs(G) Moses * 10 3 (s) KIT The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008) , which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RF-Tagger produces 756 different fine-grained tags on the same corpus.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition to surface word language models, we did experiments with language models based on part-of-speech for English-German. ", "mid_sen": "We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and #pairs(G) Moses * 10 3 (s) KIT The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008) , which produces more fine-grained tags that include also person, gender and case information. ", "after_sen": "While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RF-Tagger produces 756 different fine-grained tags on the same corpus."}
{"citeStart": 81, "citeEnd": 94, "citeStartToken": 81, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "This year, our constrained-track system made use of part of the English Gigaword data, along with other provided text, in its target-side language model. From among the data released directly for WMT 2011, we used the English side of the Europarl, news commentary, French-English UN document, and English monolingual news corpora. From the English Gigaword corpus, we included the entire Xinhua portion and the most recent 13 million sentences of the AP Wire portion. Some of these corpora contain many lines that are repeated a disproportionate number of times -the monolingual news corpus in particular, when filtered to only one occurrence of each sentence, reaches only 27% of its original line count. As part of preparing our language modeling data, we deduplicated both the English news and the UN documents, the corpora with the highest percentages of repeated sentences. We also removed lines containing more than 750 characters (about 125 average English words) before tokenization. The final prepared corpus was made up of approximately 1.8 billion words of running text. We built a 5-gram language model from it with the SRI language modeling toolkit (Stolcke, 2002) . To match the treatment given to the training data, the language model was also built in mixed case.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The final prepared corpus was made up of approximately 1.8 billion words of running text. ", "mid_sen": "We built a 5-gram language model from it with the SRI language modeling toolkit (Stolcke, 2002) . ", "after_sen": "To match the treatment given to the training data, the language model was also built in mixed case."}
{"citeStart": 127, "citeEnd": 151, "citeStartToken": 127, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "4.1 Procedure Before referents are determined, sentences are transformed into a case structure by the case structure analyzer (Kurohashi and Nagao 1994) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "4.1 Procedure Before referents are determined, sentences are transformed into a case structure by the case structure analyzer (Kurohashi and Nagao 1994) .", "after_sen": "Referents of noun phrases are determined by using heuristic rules which are made from information such as the three constraints mentioned in Section 3. "}
{"citeStart": 100, "citeEnd": 126, "citeStartToken": 100, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "There are several reasons for the prevailing emphasis on linear annotation and the lack of work on automatic methods for unifying hierarchical discourse annotations. First, initial attempts to create annotated hierarchical corpora of discourse structure using naive annotators have met with difficulties. Rotondo (1984) reported that \"hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.\" Passonneau and Litman (1993) conducted a pilot study in which subjects found it \"difficult and time-consuming\" to identify hierarchical relations in discourse. Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996) . Second, hierarchical segmentation of discourse is subjective. While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997) , with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995) . Moreover, the precise definition of \"agreement\" with respect to hierarchical segmentation is unclear, complicating evaluation. It is natural to consider two segments in separate annotations to agree if they both span precisely the same utterances and agree on the level of embeddedness. However, it is less clear how to handle segments that share the same utterances but differ with respect to the level of embeddedness.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Rotondo (1984) reported that \"hierarchical segmentation is impractical for naive subjects in discourses longer than 200 words.\" Passonneau and Litman (1993) conducted a pilot study in which subjects found it \"difficult and time-consuming\" to identify hierarchical relations in discourse. ", "mid_sen": "Other attempts have had more success using improved annotation tools and more precise instructions (Grosz and Hirschberg, 1992; Hirschberg and Nakatani, 1996) . Second, hierarchical segmentation of discourse is subjective. ", "after_sen": "While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997) , with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995) . "}
{"citeStart": 52, "citeEnd": 69, "citeStartToken": 52, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "We generate a training corpus of utterances paired with representations of the non-linguistic context in which they were uttered. The first step in generating this corpus is to generate the low level features described in Section 2.1 for each video in our training set. We then segment each video into a set of independent events based on the visual context features we have extracted. We follow previous work in sports video processing (Gong et al., 2004) and define an event in a baseball video as any sequence of shots starting with a pitching-scene and continuing for four subsequent shots. This definition follows from the fact that the vast majority of events in baseball start with a pitch and do not last longer than four shots. For each of these events in our corpus, a temporal pattern feature vector is generated as described in section 2.2. These events are then paired with all the words from the closed captioning transcription that occur during each event (plus or minus 10 seconds). Because these transcriptions are not necessarily time synched with the audio, we use the method described in Hauptmann and Witbrock (1998) to align the closed captioning to the announcers' speech.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We then segment each video into a set of independent events based on the visual context features we have extracted. ", "mid_sen": "We follow previous work in sports video processing (Gong et al., 2004) and define an event in a baseball video as any sequence of shots starting with a pitching-scene and continuing for four subsequent shots. ", "after_sen": "This definition follows from the fact that the vast majority of events in baseball start with a pitch and do not last longer than four shots. "}
{"citeStart": 42, "citeEnd": 64, "citeStartToken": 42, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "A variant of the problem is considered by Sibun and Spitz (1994) , and Sibun and Reynar (1996) , who look at it from the point of view of Optical Character Recognition (OCR).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Versions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994) , while an interesting practical implementation is described by Adams and Resnik (1997) .", "mid_sen": "A variant of the problem is considered by Sibun and Spitz (1994) , and Sibun and Reynar (1996) , who look at it from the point of view of Optical Character Recognition (OCR).", "after_sen": "Here, the language model for the OCR system cannot be selected until the language has been identified. "}
{"citeStart": 276, "citeEnd": 294, "citeStartToken": 276, "citeEndToken": 294, "sectionName": "UNKNOWN SECTION NAME", "string": "Left-corner transforms are particularly useful because they can preserve annotations on productions more on this below and are therefore applicable to more complex grammar formalisms as well as CFGs; a property which other approaches to left-recursion elimination typically lack. For example, they apply to left-recursive uni cation-based grammars Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a . Because the emission probability o f a PCFG production can be regarded as an annotation o n a C F G production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar Abney et al., 1999 . However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, they apply to left-recursive uni cation-based grammars Matsumoto et al., 1983; Pereira and Shieber, 1987; Johnson, 1998a . ", "mid_sen": "Because the emission probability o f a PCFG production can be regarded as an annotation o n a C F G production, the left-corner transform can produce a CFG with weighted productions which assigns the same probabilities to strings and transformed trees as the original grammar Abney et al., 1999 . ", "after_sen": "However, the transformed grammars can be much larger than the original, which is unacceptable for many applications involving large grammars."}
{"citeStart": 108, "citeEnd": 120, "citeStartToken": 108, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "Hellinger distance In Section 2.2 we have seen how to derive psd kernels (similarities) from nsd kernels (distances). It seems likely that distance measures that are known to work well for comparing co-occurrence distributions will also give us suitable psd similarity measures. Negative semi-definite kernels are by definition symmetric, which rules the Kullback-Leibler divergence and Lee's (1999) α-skew divergence out of consideration. The nsd condition 2is met if the distance function is a squared metric in a Hilbert space. In this paper we use a parametric family of squared Hilbertian metrics on probability distributions that has been discussed by Hein and Bousquet (2005) . This family contains many familiar distances including the L 1 distance, Jensen-Shannon divergence (JSD) and the Hellinger distance used in statistics, though not the squared L 2 distance. Positive semi-definite distributional kernels can be derived from these distances through equations (3a) and (3b). We interpret the distributional kernels produced by (3a) and (3b) as analogues of the linear and Gaussian kernels respectively, given by a different norm or concept of distance in the feature space F. Hence the linear distributional kernels produced by (3a) correspond to inner products in the input space X , and the rbf distributional kernels produced by (3b) are radial basis functions corresponding to inner products in a high-dimensional Hilbert space of Gaussianlike functions. In this paper we use the unmodified term \"linear kernel\" in the standard sense of the linear kernel derived from the L 2 distance and make explicit the related distance when referring to other linear kernels, e.g., the \"JSD linear kernel\". Likewise, we use the standard term \"Gaussian\" to refer to the L 2 rbf kernel, and denote other rbf kernels as, for example, the \"JSD rbf kernel\". Table 1 lists relevant squared metric distances and their derived linear kernels. The linear kernel derived from the L 1 distance is the same as the difference-weighted token-based similarity measure of . The JSD linear kernel can be rewritten as (2 -JSD), where JSD is the value of the Jensen-Shannon divergence. This formulation is used as a similarity measure by Lin (1999) . Dagan et al. (1999) use a similarity measure 10 −αJSD , though they acknowledge that this transformation is heuristically motivated. The rbf kernel exp(−αJSD) provides a theoretically sound alternative when the psd property is required. It follows from the above discussion that these previously known distributional similarity measures are valid kernel functions and can be used directly for SVM classification.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It seems likely that distance measures that are known to work well for comparing co-occurrence distributions will also give us suitable psd similarity measures. ", "mid_sen": "Negative semi-definite kernels are by definition symmetric, which rules the Kullback-Leibler divergence and Lee's (1999) α-skew divergence out of consideration. ", "after_sen": "The nsd condition 2is met if the distance function is a squared metric in a Hilbert space. "}
{"citeStart": 141, "citeEnd": 156, "citeStartToken": 141, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "The RenTAL system is implemented in LiL-FeS (Makino et al., 1998) 2 . LiLFeS is one of the fastest inference engines for processing feature structure logic, and efficient HPSG parsers have already been built on this system (Nishida et al., 1999; . We applied our system to the XTAG English grammar (The XTAG Research Group, 2001) 3 , which is a large-scale FB-LTAG grammar for English. The original and the obtained grammar generated exactly the same number of derivation trees in the parsing experiment with 457 sentences from the ATIS corpus (Marcus et al., 1994) 6 (the average length is 6.32 words). This result empirically attested the strong equivalence of our algorithm. Table 2 shows the average parsing time with the LTAG and HPSG parsers. In Table 2 , lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). Table 2 clearly shows that the HPSG parser is significantly faster than the LTAG parser. This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Another paper ) describes the detailed analysis on the factor of the difference of parsing performance.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 2 shows the average parsing time with the LTAG and HPSG parsers. ", "mid_sen": "In Table 2 , lem refers to the LTAG parser , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing (van Noord, 1994) without features (phase 1), and then executes feature unification (phase 2). ", "after_sen": "TNT refers to the HPSG parser , C++ implementation of the two-phase parsing algorithm that performs filtering with a compiled CFG (phase 1) and then executes feature unification (phase 2). "}
{"citeStart": 5, "citeEnd": 28, "citeStartToken": 5, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "ITG is slow mainly because it considers every pair of spans in two sentences as a possible chart element. In reality, the set of useful chart elements is much smaller than the possible scriptO(n 4 ), where n is the average sentence length. Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space. Given a bitext cell defined by the four boundary indices (i, j, l, m) as shown in Figure 1a , we prune based on a figure of merit V (i, j, l, m) approximating the utility of that cell in a full ITG parse. The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005) . Like Zhang and Gildea (2005) , it is used to prune bitext cells rather than score phrases. The total score is the product of the Model probabilities for each column; \"inside\" columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and \"outside\" columns use the sum (or maximum) of all probabilities not in the range [i, j] .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005) . ", "mid_sen": "Like Zhang and Gildea (2005) , it is used to prune bitext cells rather than score phrases. ", "after_sen": "The total score is the product of the Model probabilities for each column; \"inside\" columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and \"outside\" columns use the sum (or maximum) of all probabilities not in the range [i, j] ."}
{"citeStart": 88, "citeEnd": 106, "citeStartToken": 88, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "We model the clause boundary identification problem through sequence labeling and use Conditional Random Fields (CRFs) to identify clause boundaries. Words and part-of-speech (POS) tags are used as feature sets. Since we do not allow embedded segments, the performance of our method is promising, which achieves the F1 score of 92.8%. The result is comparable with the best results obtained during the CoNLL-2001 campaign (Tjong et al., 2001 ).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since we do not allow embedded segments, the performance of our method is promising, which achieves the F1 score of 92.8%. ", "mid_sen": "The result is comparable with the best results obtained during the CoNLL-2001 campaign (Tjong et al., 2001 ).", "after_sen": ""}
{"citeStart": 317, "citeEnd": 336, "citeStartToken": 317, "citeEndToken": 336, "sectionName": "UNKNOWN SECTION NAME", "string": "Before explaining our CLIR system, we classify existing CLIR into three approaches in terms of the implementation of the translation phase. The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . We prefer the first approach, the \"query translation\", to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. At the same time, we concede that other CLIR approaches are worth further exploration. Figure 1 depicts the overall design of our CLIR system, where most components are the same as those for monolingual IR, excluding \"translator\".", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Before explaining our CLIR system, we classify existing CLIR into three approaches in terms of the implementation of the translation phase. ", "mid_sen": "The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . ", "after_sen": "The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . "}
{"citeStart": 117, "citeEnd": 141, "citeStartToken": 117, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "Several approaches have been suggested to account for this behavior. [Litman and Allen, 1987] introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991 ] or Shared Plans [Grosz and Sidner, 1990] . While these accounts do help explain some discourse phenomena more satisfactorily, they still require a strong degree of cooperativity to account for dialogue coherence, and do not provide easy explanations of why an agent might act in cases which do not support high-level mutual goals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "[Litman and Allen, 1987] introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. ", "mid_sen": "Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991 ] or Shared Plans [Grosz and Sidner, 1990] . ", "after_sen": "While these accounts do help explain some discourse phenomena more satisfactorily, they still require a strong degree of cooperativity to account for dialogue coherence, and do not provide easy explanations of why an agent might act in cases which do not support high-level mutual goals."}
{"citeStart": 140, "citeEnd": 154, "citeStartToken": 140, "citeEndToken": 154, "sectionName": "UNKNOWN SECTION NAME", "string": "During the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems. Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models Magerman, 1995) , training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993) , or other techniques (Bod, 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "During the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems. ", "mid_sen": "Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models Magerman, 1995) , training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993) , or other techniques (Bod, 1993) .", "after_sen": "Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). "}
{"citeStart": 14, "citeEnd": 36, "citeStartToken": 14, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999) , a memory-based learner (MBL), for both phases. We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001) ; allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space. We mostly use the default settings of TiMBL-the IB1 learning algorithm and overlap comparison metric between instances-and experiment with different values of k.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999) , a memory-based learner (MBL), for both phases. ", "after_sen": "We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001) ; allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space. "}
{"citeStart": 128, "citeEnd": 145, "citeStartToken": 128, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "This section describes a modification to standard phrase-based decoding, so that the system is constrained to produce only cohesive output. This will take the form of a check performed each time a hypothesis is extended, similar to the ITG constraint for phrasal SMT (Zens et al., 2004) . To create a such a check, we need to detect a cohesion violation inside a partial translation hypothesis. We cannot directly apply our span-based cohesion definition, because our word-to-phrase alignment is not yet complete. However, we can still detect violations, and we can do so before the spans involved are completely translated.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This section describes a modification to standard phrase-based decoding, so that the system is constrained to produce only cohesive output. ", "mid_sen": "This will take the form of a check performed each time a hypothesis is extended, similar to the ITG constraint for phrasal SMT (Zens et al., 2004) . ", "after_sen": "To create a such a check, we need to detect a cohesion violation inside a partial translation hypothesis. "}
{"citeStart": 40, "citeEnd": 59, "citeStartToken": 40, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "We can make several additional observations: 1). Stemming Vs Expansion. UMASS uses document and query stemming while Naive Exp uses expansion by word alteration. We stated that both approaches are equivalent. The equivalence is confirmed by our experiment results: for all Gov2 collections, these approaches perform equivalently. 2). The Similarity model performs very well. Compared with the Naïve Expansion model, it produces quite similar retrieval effectiveness, while the query traffic is dramatically reduced. This approach is similar to the work of Xu and Croft (1998) , and can be considered as another state-ofthe-art result. 3). In comparison, the Bigram Expansion model performs better than the Similarity model. This shows that it is useful to consider query context in selecting word alterations. 4). The Regression model performs the best of all the models. Compared with the Original query, it adds fewer than 2 alterations for each query on average (since each group has 50 queries); nevertheless we obtained improvements on all the six collections. Moreover, the improvements on five collections are statistically significant. It also performs slightly better than the Similarity and Bigram Expansion methods, but with fewer alterations. This shows that the supervised learning approach, if used in the correct way, is superior to an unsupervised approach. Another advantage over the two other models is that the Regression model can reduce the number of alterations further. Because the Regression model selects alterations according to their expected improvement, the improvement of the alterations to one query term can be compared with that of the alterations to other query terms. Therefore, we can select at most one optimal alteration for the whole query. However, with the Similarity or Bigram Expansion models, the selection value, either similarity or query likelihood, cannot be compared across the query terms. As a consequence, more alterations need to be selected, leading to heavier query traffic.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Compared with the Naïve Expansion model, it produces quite similar retrieval effectiveness, while the query traffic is dramatically reduced. ", "mid_sen": "This approach is similar to the work of Xu and Croft (1998) , and can be considered as another state-ofthe-art result. ", "after_sen": "3). In comparison, the Bigram Expansion model performs better than the Similarity model. "}
{"citeStart": 87, "citeEnd": 106, "citeStartToken": 87, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "We recruited three Computer Science undergraduate students to annotate keywords for each topic segment, using 27 selected ICSI meetings. 1 Up to five indicative key words or phrases were annotated for each topic. In total, we have 208 topics annotated with keywords. The average length of the topics (measured using the number of dialog acts) among all the meetings is 172.5, with a high standard deviation of 236.8. We used six meetings as our development set (the same six meetings as the test set in (Murray et al., 2005) ) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The average length of the topics (measured using the number of dialog acts) among all the meetings is 172.5, with a high standard deviation of 236.8. ", "mid_sen": "We used six meetings as our development set (the same six meetings as the test set in (Murray et al., 2005) ) to optimize our keyword extraction methods, and the remaining 21 meetings for final testing in Section 5.", "after_sen": "One example of the annotated keywords for a topic segment is:"}
{"citeStart": 125, "citeEnd": 140, "citeStartToken": 125, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "The most direct points of comparison of our method are the approaches of Johnson et al. (1999) and Johnson and Riezler (2000) . In the rst approach, log-linear models on LFG grammars using about 200 con gurational properties were trained on treebanks of about 400 sentences by maximum pseudo-likelihood estimation. Precision was evaluated on an exact match task in a 10-way cross validation paradigm for an ambiguity rate of 10, and achieved 59% for the rst approach. Johnson and Riezler (2000) achieved a gain of 1% over this result by including a classbased lexicalization. Our bestmodels clearly outperform these results, both in terms of precision relative to ambiguity and in terms of relative gain due to lexicalization. A comparison of performance is more di cult for the lexicalized PCFG of Beil et al. (1999) which was trained by EM on 450,000 sentences of German newspaper text. There, a 70.4% precision is reported on a verb frame recognition task on 584 examples. However, the gain achieved by Beil et al. (1999) due to grammar lexicalizaton is only 2%, compared to about 10% in our case. A comparison is di cult also for most other state-of-theart PCFG-based statistical parsers, since di erent training and test data, and most importantly, di erent e v aluation criteria were used. A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in Charniak (1997) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A comparison is di cult also for most other state-of-theart PCFG-based statistical parsers, since di erent training and test data, and most importantly, di erent e v aluation criteria were used. ", "mid_sen": "A comparison of the performance gain due to grammar lexicalization shows that our results are on a par with that reported in Charniak (1997) .", "after_sen": ""}
{"citeStart": 80, "citeEnd": 97, "citeStartToken": 80, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "where type1 to typen are exhaustive and disjoint subtypes of type entry, entry need not necessarily be a single type; it can be a logical expression over types formed with the connectors AND and oR. A systemic grammar therefore resembles more a type lattice than a type hierarchy in the HPSG tradition. In systemic grammar, these basic type axioms, the systems, are named; we will use entry(s) to denote the left-hand side of some named system s, and out(s) to denote the set of subtypes {type1, type2, ..., type,}- the output of the system. The following type axioms taken from the large systemic English grammar NXGI~L (Matthiessen, 1983) The meaning of these type axioms is fairly obvious: Nominal groups can be subcategorized in classnames and individual-names on the one hand, they can be subcategorized with respect to their WHcontainment into WH-containing nominal-groups and nominal-groups without WH-element on the other hand. Universal principles and rules are in systemic grammar not factored out. The lexicon contains stem forms and has a detailed word class type hierarchy at its top. Morphology is also organized as a monotonic type hierarchy. Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In systemic grammar, these basic type axioms, the systems, are named; we will use entry(s) to denote the left-hand side of some named system s, and out(s) to denote the set of subtypes {type1, type2, ..., type,}- the output of the system. ", "mid_sen": "The following type axioms taken from the large systemic English grammar NXGI~L (Matthiessen, 1983) The meaning of these type axioms is fairly obvious: Nominal groups can be subcategorized in classnames and individual-names on the one hand, they can be subcategorized with respect to their WHcontainment into WH-containing nominal-groups and nominal-groups without WH-element on the other hand. ", "after_sen": "Universal principles and rules are in systemic grammar not factored out. "}
{"citeStart": 242, "citeEnd": 253, "citeStartToken": 242, "citeEndToken": 253, "sectionName": "UNKNOWN SECTION NAME", "string": "An automatic VSD system usually has at its disposal a diverse set of features among which the semantic features play an important role: verb sense distinctions often depend on the distinctions in the semantics of the target verb's arguments (Hanks, 1996) . Therefore, some method of capturing the semantic knowledge about the verb's arguments is crucial to the success of a VSD system. The approaches to obtaining this kind of knowledge can be based on extracting it from ele ctronic dictionaries such as WordNet (Fellbaum, 1998) , using Named Entity (NE) tags, or a combi-nation of both (Chen, 2005) . In this paper, we propose a novel method for obtaining semantic knowledge about words and show how it can be applied to VSD. We contrast this method with the other two approaches and compare their performances in a series of experiments.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In a supervised setting, a VSD system is usually trained on a set of pre-labeled examples; the goal of this system is to tag unseen examples with a sense from some sense inventory.", "mid_sen": "An automatic VSD system usually has at its disposal a diverse set of features among which the semantic features play an important role: verb sense distinctions often depend on the distinctions in the semantics of the target verb's arguments (Hanks, 1996) . ", "after_sen": "Therefore, some method of capturing the semantic knowledge about the verb's arguments is crucial to the success of a VSD system. "}
{"citeStart": 49, "citeEnd": 76, "citeStartToken": 49, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "As with genetic algorithms, simulated annealing (van Laarhoven & Aarts, 1987 ) is a combinatorial optimisation technique based on an analogy with a natural process. Annealing is the heating and slow cooling of a solid which allows the formati,m of regular crystalline structure having a mininu,n of excess energy. In its early stages when the temperature is high, annealing search rcsembles random search. There is so much free euergy in the system that a transition to a higher energy state is highly probable. As the temperature decreases the search begins to resemble hill-climbing. Now there is much less free energy and so transitions to higher energy states are h'ss and loss likely. In what follows, I explain some of the I)arameters of annealing search as used in the curreut implementation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As with genetic algorithms, simulated annealing (van Laarhoven & Aarts, 1987 ) is a combinatorial optimisation technique based on an analogy with a natural process. ", "after_sen": "Annealing is the heating and slow cooling of a solid which allows the formati,m of regular crystalline structure having a mininu,n of excess energy. "}
{"citeStart": 173, "citeEnd": 194, "citeStartToken": 173, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "The knowledge base is used to check the factual correctness of the answers first, and then a diagnoser checks the explanation correctness. The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer. At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The diagnoser, based on Dzikovska et al. (2008b), outputs a diagnosis which consists of lists of correct, contradictory and non-mentioned objects and relations from the student's answer. ", "mid_sen": "At present, the system uses a heuristic matching algorithm to classify relations into the appropriate category, though in the future we may consider a classifier similar to Nielsen et al. (2008) .", "after_sen": ""}
{"citeStart": 71, "citeEnd": 90, "citeStartToken": 71, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Stochastic parsing models capturing contextual constraints beyond the dependencies of probabilistic context-free grammars (PCFGs) are currently the subject of intensive research. An interesting feature common to most such models is the incorporation of contextual dependencies on individual head words into rulebased probability models. Such word-based lexicalizations of probability models are used successfully in the statistical parsing models of, e.g., Collins (1997) , Charniak (1997) , or Ratnaparkhi (1997) . However, it is still an open question which kind of lexicalization, e.g., statistics on individual words or statistics based upon word classes, is the best choice. Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which o b viates the standard e ort required for treebank training handannotating large corpora of speci c domains of speci c languages with speci c parse types. Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. Experimental results con rming this wisdom have beenpresented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Secondly, these approaches have in common the fact that the probability models are trained on treebanks, i.e., corpora of manually disambiguated sentences, and not from corpora of unannotated sentences. ", "mid_sen": "In all of the cited approaches, the Penn Wall Street Journal Treebank (Marcus et al., 1993) is used, the availability of which o b viates the standard e ort required for treebank training handannotating large corpora of speci c domains of speci c languages with speci c parse types. ", "after_sen": "Moreover, common wisdom is that training from unannotated data via the expectationmaximization (EM) algorithm (Dempster et al., 1977) yields poor results unless at least partial annotation is applied. "}
{"citeStart": 157, "citeEnd": 171, "citeStartToken": 157, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "For each of the nodes listed above, the decision tree could also ask about the number of children and span of the node. For the tagging model, the values of the previous two words and their tags are also asked, since they might differ from the head words of the previous two constituents. The training algorithm proceeds as follows. The training corpus is divided into two sets, approximately 90% for tree growing and 10% for tree smoothing. For each parsed sentence in the tree growing corpus, the correct state sequence is traversed. Each state transition from si to 8i+1 is an event; the history is made up of the answers to all of the questions at state sl and the future is the value of the action taken from state si to state Si+l. Each event is used as a training example for the decisiontree growing process for the appropriate feature's tree (e.g. each tagging event is used for growing the tagging tree, etc.). After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in (Magerman, 1994) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each event is used as a training example for the decisiontree growing process for the appropriate feature's tree (e.g. each tagging event is used for growing the tagging tree, etc.). ", "mid_sen": "After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in (Magerman, 1994) .", "after_sen": ""}
{"citeStart": 78, "citeEnd": 112, "citeStartToken": 78, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "There are various technical difliculties with Geedali's account (see e.g. van Oirsouw, 1987, and Moltnmnn, 11992) . There is also a f'undanmnl,al l)lroblem COl,cerning semantic interpretation of coordinated structures (see Moltmanu, 1992 which provides a revised a.nd more complex 3-1) account based on Muadz, 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(k)odall's aeco,mt does not deal with examples such as (171)), which he argues to I)e examples of'a different phenomcaon, llowew;r flmse can be incorporated into a ad) aeco,nt (e.g. Moltmamh 1992).", "mid_sen": "There are various technical difliculties with Geedali's account (see e.g. van Oirsouw, 1987, and Moltnmnn, 11992) . ", "after_sen": "There is also a f'undanmnl,al l)lroblem COl,cerning semantic interpretation of coordinated structures (see Moltmanu, 1992 which provides a revised a.nd more complex 3-1) account based on Muadz, 1991) ."}
{"citeStart": 129, "citeEnd": 150, "citeStartToken": 129, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "The technical key to these results is a procedure to encode arbitrary, even non-projective dependency structures into trees (terms) over a signature of local order-annotations. The constructors of these trees can be read as lexical entries, and both the gap-degree restriction and the well-nestedness condition can be couched as syntactic properties of these entries. Sets of gap-restricted dependency structures can be described using regular tree grammars. This gives rise to a notion of regular dependency languages, and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms (Joshi, 1985) : We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987) , and that the gap-degree measure is the structural correspondent of the concept of 'fan-out' in this formalism (Satta, 1992) . We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996) , and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al., 1987) , and that the gap-degree measure is the structural correspondent of the concept of 'fan-out' in this formalism (Satta, 1992) . ", "mid_sen": "We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars (Hotz and Pitsch, 1996) , and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag). ", "after_sen": "This result generalizes previous work on the relation between ltag and dependency representations (Rambow and Joshi, 1997; Bodirsky et al., 2005) ."}
{"citeStart": 291, "citeEnd": 306, "citeStartToken": 291, "citeEndToken": 306, "sectionName": "UNKNOWN SECTION NAME", "string": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. ", "mid_sen": "In some case.s, these have be.en incorporated to traditional parsing techniques, as it is the case with feature relaxation in the context of unification-based formalisms (Bolioli et al., 1992) , or the addition of a set of catching error rules si)ecially handling the deviant constructions (Thurlnair, 1990) . ", "after_sen": "In other eases, the relaxation component has heen included as a new add-in feature to the parsing algoril,hm, as in the IBM's PLNLI' aI)proach (Heidorn et al., 1982) , or in the work developed for tim Tra.nslator's Workbench t)roject using the METAL MTsystem (TWB, 1992) ."}
{"citeStart": 54, "citeEnd": 65, "citeStartToken": 54, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, our procedure induces a \"hard\" part-ofspeech classification of occurrences in context, i.e., each occurrence is assigned to only one category. It is by no means generally accepted that such a classification is linguistically adequate. There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994 ) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. For example, \"fun\" SBecause of phrases like \"I had sweet potatoes\", forms of \"have\" cannot serve as a reliable discriminator either.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is by no means generally accepted that such a classification is linguistically adequate. ", "mid_sen": "There is both synchronic (Ross, 1972) and diachronic (Tabor, 1994 ) evidence suggesting that words and their uses can inherit properties from several prototypical syntactic categories. ", "after_sen": "For example, \"fun\" SBecause of phrases like \"I had sweet potatoes\", forms of \"have\" cannot serve as a reliable discriminator either."}
{"citeStart": 44, "citeEnd": 56, "citeStartToken": 44, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "In determining whether to accept a proposed befief or evidential relationship, the evaluator first constructs an evidence set containing the system's evidence thin supports or attacks _bcl and the evidence accepted by the system that was proposed by the user as support for -bel. Each piece of evidence contains a belief _beli, and an evidential relationship supports (.beli,-bel) . Following Walker's weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. The evaluator then employs a simplified version of Galliers' belief revision mechanism 2 (Galliers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of evidence strongly outweighs that of the other, the decision to accept or reject.bel is easily made. However, if the difference in their strengths does not exceed a pre-determined 2For details on how our model determines the acceptance of a belief using the ranking of endorsements proposed by GaUiers, see (Chu-Carroll, 1995) . ..... : ............................................................... \". \"d \"\" \"[ lnf~J,S,~Teache~ 2and 3threshold, the evaluator has insufficient information to determine whether to adopt _bel and therefore will initiate an information-sharing subdialogue (Cho-Carmll and Carberry, 1995) to share information with the user so that each of them can knowiedgably re-evaluate the user's original proposal. If, during infommtion-sharing, the user provides convincing support for a belief whose negation is held by the system, the system may adopt the belief after the re-evaluation process, thus resolving the conflict without negotiation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each piece of evidence contains a belief _beli, and an evidential relationship supports (.beli,-bel) . ", "mid_sen": "Following Walker's weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. ", "after_sen": "The evaluator then employs a simplified version of Galliers' belief revision mechanism 2 (Galliers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. "}
{"citeStart": 300, "citeEnd": 314, "citeStartToken": 300, "citeEndToken": 314, "sectionName": "UNKNOWN SECTION NAME", "string": "A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. ", "mid_sen": "Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991) .", "after_sen": "In this article, we present the AT'R/Lancaster 7'reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University (UK)'s Unit for Computer Research on the English Language, according to specifications provided by ATR (Japan)'s Statistical Parsing Group. "}
{"citeStart": 50, "citeEnd": 79, "citeStartToken": 50, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Meaning-Text Theory (Melc'flk, 1988) assumes seven strata of representation. The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Melc'~k & Pertsov, 1987p.187f ) (but see the proposal by Rambow & Joshi (in print)).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. ", "mid_sen": "These rules have not yet been formally specified (Melc'~k & Pertsov, 1987p.187f ) (but see the proposal by Rambow & Joshi (in print)).", "after_sen": "Word Grammar (WG, Hudson (1990) ) is based on general graphs instead of trees. "}
{"citeStart": 101, "citeEnd": 118, "citeStartToken": 101, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al. 1993) for training translation systems automatically. One advantage is that our method attempts to model the natural decomposition of sentences into phrases. Another is that the compilation of this decomposition into lexically anchored finite-state head transducers produces implementations that are much more efficient than those for the IBM model. In particular, our search algorithm finds optimal transductions of test sentences in less than \"real time\" on a 300MHz processor, that is, the time to translate an utterance is less than the time taken to speak it, an important consideration for our speech translation application.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Compared with left-to-right transduction, middle-out transduction also aids robustness because, when complete derivations are not available, partial derivations tend to have meaningful headwords.", "mid_sen": "At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al. 1993) for training translation systems automatically. ", "after_sen": "One advantage is that our method attempts to model the natural decomposition of sentences into phrases. "}
{"citeStart": 134, "citeEnd": 144, "citeStartToken": 134, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "The Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998) . Earlier versions of SNoW (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks. Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Learning Approach Our experimental investigation is done using the SNo W learning system (Roth, 1998) . Earlier versions of SNoW (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998; Munoz et al., 1999) have been applied successfully to several natural language related tasks. ", "after_sen": "Here we use SNo W for the task of word prediction; a representation is learned for each word of interest, and these compete at evaluation time to determine the prediction."}
{"citeStart": 4, "citeEnd": 15, "citeStartToken": 4, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "Words unknown to the lexicon present a substantial problem to part-of-speech (POS) tagging of realworld texts. Taggers assign a single POS-tag to a word-token, provided that it is known what partsof-speech this word can take on in principle. So, first words are looked up in the lexicon. However, 3 to 5% of word tokens are usually missing in the lexicon when tagging real-world texts. This is where word-Pos guessers take their place --they employ the analysis of word features, e.g. word leading and trailing characters, to figure out its possible POS categories. A set of rules which on the basis of ending characters of unknown words, assign them with sets of possible POS-tags is supplied with the Xerox tagger (Kupiec, 1992) . A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g. (Brill, 1995; Weischedel et al., 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A similar approach was taken in (Weischedel et al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular POS, its capitalisation feature and its ending. ", "mid_sen": "In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. ", "after_sen": "The best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g. (Brill, 1995; Weischedel et al., 1993) ."}
{"citeStart": 106, "citeEnd": 119, "citeStartToken": 106, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexicalist approaches to MT, particularly those incorporating the technique of Shake-and-Bake generation (Beaven, 1992a; Beaven, 1992b; Whitelock, 1994) , combine the linguistic advantages of transfer (Arnold et al., 1988; Allegranza et al., 1991) and interlingual (Nirenburg et al., 1992; Dorr, 1993) approaches. Unfortunately, the generation algorithms described to date have been intractable. In this paper, we describe an alternative generation component which has polynomial time complexity.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Lexicalist approaches to MT, particularly those incorporating the technique of Shake-and-Bake generation (Beaven, 1992a; Beaven, 1992b; Whitelock, 1994) , combine the linguistic advantages of transfer (Arnold et al., 1988; Allegranza et al., 1991) and interlingual (Nirenburg et al., 1992; Dorr, 1993) approaches. ", "after_sen": "Unfortunately, the generation algorithms described to date have been intractable. "}
{"citeStart": 45, "citeEnd": 62, "citeStartToken": 45, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work in information extraction from research papers has been based on two major machine learning techniques. The first is hidden Markov models (HMM) (Seymore et al., 1999; Takasu, 2003) . An HMM learns a generative model over input sequence and labeled sequence pairs. While enjoying wide historical success, standard HMM models have difficulty modeling multiple non-independent features of the observation sequence. The second technique is based on discriminatively-trained SVM classifiers (Han et al., 2003) . These SVM classifiers can handle many nonindependent features. However, for this sequence labeling problem, Han et al. (2003) work in a two stages process: first classifying each line independently to assign it label, then adjusting these labels based on an additional classifier that examines larger windows of labels. Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These SVM classifiers can handle many nonindependent features. ", "mid_sen": "However, for this sequence labeling problem, Han et al. (2003) work in a two stages process: first classifying each line independently to assign it label, then adjusting these labels based on an additional classifier that examines larger windows of labels. ", "after_sen": "Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations."}
{"citeStart": 98, "citeEnd": 117, "citeStartToken": 98, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "To address the above problems, we build a decision tree of SVMs that reduces the set of possible classes at each decision node, and takes relative class similarity into account during the tree construction process. We construct the decision tree as a Minimum Cost Spanning Tree (MCST), denoted MCST-SVM, based on inter-class similarity measured from feature values (Lorena and de Carvalho, 2005) . Each of the decision tree leaves corresponds to a target class, and the interior nodes group classes into disjoint sets. For each internal node in the MCST, an SVM is trained to separate all the samples belonging to classes in its left subtree from those in its right subtree. We use linear SVMs, which have been shown to be effective text classifiers (Pang et al., 2002; Pang and Lee, 2005) , and set the SVM parameters to match those used in (Pang and Lee, 2005) . 1 Figure 1 contrasts SVMs are implemented using the C/C++ library liblinear, a variant of libsvm (Chang and Lin, 2001 ). The MCST is constructed using Kruskal's algorithm (1956) , which works in polynomial time (Algorithm 1). This algorithm requires a measure of the similarity between every pair of classes, which is calculated using the distance between a representative vector for each class (Section 3.2). The MCST is iteratively built in a bottom-up fashion, beginning with all classes as singleton nodes. In each iteration, the algorithm constructs a node comprising the most similar sets of classes from two previously generated nodes. The similarity between two sets of classes is the shortest distance between the representative vectors of the classes in each set. For instance, the shortest distance between the sets of classes {*/**} and {***/****} is min{dist(*,***), dist(*,****), dist(**,***), dist(**,****)}. An SVM is then trained to discriminate between the children of the constructed nodes.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use linear SVMs, which have been shown to be effective text classifiers (Pang et al., 2002; Pang and Lee, 2005) , and set the SVM parameters to match those used in (Pang and Lee, 2005) . ", "mid_sen": "1 Figure 1 contrasts SVMs are implemented using the C/C++ library liblinear, a variant of libsvm (Chang and Lin, 2001 ). ", "after_sen": "The MCST is constructed using Kruskal's algorithm (1956) , which works in polynomial time (Algorithm 1). "}
{"citeStart": 47, "citeEnd": 49, "citeStartToken": 47, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "To discus• the above issue• in a uniform way, we need a genera] framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take a• a l~sk a formalism developed by the second author in previous papers [15, 16] . The idea of this approach is to separate the dynamic programming construct• needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Gritfith & Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers 5. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not alway• terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. However ity. This approach may thus be used as a uniform framework for comparing chart parsers s.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The idea of this approach is to separate the dynamic programming construct• needed for efficient chart parsing from the chosen parsing schema. ", "mid_sen": "Comparison between the classifications of Kay [14] and Gritfith & Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers 5. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. ", "after_sen": "Their backtrack simulation does not alway• terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. "}
{"citeStart": 86, "citeEnd": 96, "citeStartToken": 86, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "In these experiments we have proposed new measure and weight functions that, as our evaluation has shown, significantly outperform existing similarity functions. The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation. We would also like to expand our evaluation to include direct methods used by others (Lin, 1998a) and using the extracted thesaurus in NLP tasks.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation. ", "mid_sen": "We would also like to expand our evaluation to include direct methods used by others (Lin, 1998a) and using the extracted thesaurus in NLP tasks.", "after_sen": "We have also investigated the speed/performance trade-off using frequency cutoffs. "}
{"citeStart": 39, "citeEnd": 51, "citeStartToken": 39, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "We adopt a suggestion by Chierchia in (Partee, 1984) , that the whole implication be rendered as a state. This state is no longer an atomic eventuality. It is a complex state denoting John's habit. This state holds during the present, and so its location time is n. This solution is not prone to de Swart's (1991) criticism against the naive solution of moving the reference time to the right DRS. The temporal clause may be processed before the main clause, since t', the location time of e', which 'replaces' rl, the reference time of Partee's analysis, as the temporal index of the eventuality in the the main clause, arises from processing the main clause (not updating the reference time of the subordinate clause).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the temporal connec-tire in this sentence is before, the relation between these two markers is one of precedence.", "mid_sen": "We adopt a suggestion by Chierchia in (Partee, 1984) , that the whole implication be rendered as a state. ", "after_sen": "This state is no longer an atomic eventuality. "}
{"citeStart": 63, "citeEnd": 76, "citeStartToken": 63, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "Ellipsis interpretations are represented as simple sets of substitutions on semantic representations of the antecedent. The substitutions can be built up in an order-independent way (i.e. before, after or during scoping), and without recourse to higherorder unification. The treatment is similar to the discourse copying analysis of (Kehler, 1993a) , and to the substitutional treatment suggested by Kamp within Discourse Representation Theory, described in (Gawron and Peters, 1990) . However, we extend the notion of strict and sloppy identity to deal with more than just pronouns. In doing so, we readily deal with phenomena like scope parallelism.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The substitutions can be built up in an order-independent way (i.e. before, after or during scoping), and without recourse to higherorder unification. ", "mid_sen": "The treatment is similar to the discourse copying analysis of (Kehler, 1993a) , and to the substitutional treatment suggested by Kamp within Discourse Representation Theory, described in (Gawron and Peters, 1990) . ", "after_sen": "However, we extend the notion of strict and sloppy identity to deal with more than just pronouns. "}
{"citeStart": 43, "citeEnd": 63, "citeStartToken": 43, "citeEndToken": 63, "sectionName": "UNKNOWN SECTION NAME", "string": "Like the SENNA model, the Skip-gram model (Mikolov et al., 2013) is trained to differentiate between the correct central word of a phrase and a random replacement, which they refer to as negative sampling. Unlike SENNA, however, the Skipgram model tries to make this prediction using only a single one of the surrounding words at a time and ignores the ordering of those words, i.e. taking a bag-of-words approach to context. The published 300-dimensional vectors 2 were trained on 100B words of Google News text using stochastic gradient ascent, and cover a vocabulary of 3M words. We also retrained the same 300dimensional model on our 1.2 billion word unlabelled biomedical corpus, giving a vocabulary of around 1M words. In both cases, we measured similarity using the cosine metric, Equation 3.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "dist(u, v) = u • v |u||v| (3) 2.3 Skip-gram", "mid_sen": "Like the SENNA model, the Skip-gram model (Mikolov et al., 2013) is trained to differentiate between the correct central word of a phrase and a random replacement, which they refer to as negative sampling. ", "after_sen": "Unlike SENNA, however, the Skipgram model tries to make this prediction using only a single one of the surrounding words at a time and ignores the ordering of those words, i.e. taking a bag-of-words approach to context. "}
{"citeStart": 25, "citeEnd": 46, "citeStartToken": 25, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a Zadrozny , 1987b ) that natural language programs, such as on-line grammars and dictionaries, can be used as referential levels for commonsense reasoning--for example, to disambiguate PP attachment. This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Gricean maxims belong there; Section 6 will be devoted to a presentation of the metalevel rules corresponding to them.", "mid_sen": "We have shown elsewhere (Jensen and Binot 1988; Zadrozny 1987a Zadrozny , 1987b ) that natural language programs, such as on-line grammars and dictionaries, can be used as referential levels for commonsense reasoning--for example, to disambiguate PP attachment. ", "after_sen": "This means that information contained in grammars and dictionaries can be used to constrain possible interpretations of the logical predicates of an object-level theory."}
{"citeStart": 86, "citeEnd": 113, "citeStartToken": 86, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "For each nominal pair (w 1 , w 2 ) in a given sentence S, we use a method similar to (Davidov and Rappoport, 2006) to extract words that have a shared meaning with w 1 or w 2 . We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006) ) that contain w 1 or w 2 . To avoid getting instances of w 1,2 with a different meaning, we also require that the second word will appear in the same text paragraph or the same web page. For example, if we are given a pair <loans, students> and we see a sentence '... loans and scholarships for students and professionals ...', we use the symmetric pattern 'X and Y' to add the word scholarships to the group of loans and to add the word professionals to the group of students. We do not take words from the sentence 'In European soccer there are transfers and loans...' since its context does not contain the word students. In cases where there are only several or zero instances where the two nominals co-appear, we dismiss the latter rule and scan for each nominal separately. Note that 'loans' can also be a verb, so usage of a part-of-speech tagger might reduce noise.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For each nominal pair (w 1 , w 2 ) in a given sentence S, we use a method similar to (Davidov and Rappoport, 2006) to extract words that have a shared meaning with w 1 or w 2 . ", "after_sen": "We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006) ) that contain w 1 or w 2 . "}
{"citeStart": 207, "citeEnd": 219, "citeStartToken": 207, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "The input to reference resolution in the theoretical literature is assumed to be fully parsed sentences, often with syntactic attributes such as grammatical functions and thematic roles on the constituents (Webber, 1978; Sidner, 1979; Hobbs, 1978; Grosz, Joshi, and Weinstein, 1995) . In implemented reference resolution systems, for pronoun resolution in particular, there seems to be a trade-off between the completeness of syntactic input and the robustness with real-world sentences. In short, more robust and partial parsing gives us wider coverage, but less syntactic information also leads to less accuyate reference resolution. For instance, Lappin and Leass (1994) report an 86% accuracy for a resolution algorithm for third-person pronouns using fully parsed sentences as input. Kennedy and Boguraev (1996) then report a 75% accuracy for an algorithm that approximates Lappin and Leass's with more robust and coarse-grained syntactic input. After describing the algorithm in the next section, I will briefly compare the present approach with these pronoun resolution approaches.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We must approximate fine-grained theoretical proposals about referential dependencies, and adapt them to the context of sparse and incomplete syntactic input.", "mid_sen": "The input to reference resolution in the theoretical literature is assumed to be fully parsed sentences, often with syntactic attributes such as grammatical functions and thematic roles on the constituents (Webber, 1978; Sidner, 1979; Hobbs, 1978; Grosz, Joshi, and Weinstein, 1995) . ", "after_sen": "In implemented reference resolution systems, for pronoun resolution in particular, there seems to be a trade-off between the completeness of syntactic input and the robustness with real-world sentences. "}
{"citeStart": 242, "citeEnd": 264, "citeStartToken": 242, "citeEndToken": 264, "sectionName": "UNKNOWN SECTION NAME", "string": "Besides, an increasing concern in current projects is that of linguistic relevance of the analysis t)erformed by the grammar correction system. In this sense, the adequate integration of error detection and correction techniques within mainstream grammm\" formalisms has l)een addressed by a nunl|)er of these projects ([Iolioli eta/., 1992) , (Vosse, 1992) , ((]enthia.l ctal., t992), (O(~uthial et al., 1994) . l~bllowing this concern, this paper presents resuits fl'om the project GramCheck (A Grammar and Style Checker, MLAP93-11), flmded by the CEC. GramCheck has developed a grammar checker demonstrator for Spanish and Greek native writers using ALEP (ET6/1, 1991), (Simpkins, 1994) as the NLP development platform, a client-server architeeUlre as implenmnted in the X Windows system, Motif as the 'look ~md fe, el' interface and Xminfo as the kllowh!dge t)ase, storage format. Generalized use of extensions to the highly typed and unifi(:ation based formalism imi)Iemented in ALEP has been 1)erformed. These extensions (called Constraint Solvers, CSs) are nothing but pieces of PR()I,OG code l)erforlning different l)oolean and relational operations over feature wdues. Besides, GramCheck has used ongoing results Dora LS-GRAM (LRE61029), a project alining at the implementation of middle coverage ALEP grammars for a number of European languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Besides, an increasing concern in current projects is that of linguistic relevance of the analysis t)erformed by the grammar correction system. ", "mid_sen": "In this sense, the adequate integration of error detection and correction techniques within mainstream grammm\" formalisms has l)een addressed by a nunl|)er of these projects ([Iolioli eta/., 1992) , (Vosse, 1992) , ((]enthia.l ctal., t992), (O(~uthial et al., 1994) . l~bllowing this concern, this paper presents resuits fl'om the project GramCheck (A Grammar and Style Checker, MLAP93-11), flmded by the CEC. ", "after_sen": "GramCheck has developed a grammar checker demonstrator for Spanish and Greek native writers using ALEP (ET6/1, 1991), (Simpkins, 1994) as the NLP development platform, a client-server architeeUlre as implenmnted in the X Windows system, Motif as the 'look ~md fe, el' interface and Xminfo as the kllowh!"}
{"citeStart": 99, "citeEnd": 114, "citeStartToken": 99, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section I show how to implement the Koasati case from (3) using the FSA Utilities toolbox (van Noord, 1997) . FSA Utilities is a Prolog-based finite-state toolkit and extendible regular expression compiler. It is freely available and encourages rapid prototyping. Figure 2 displays the regular expression operators that will be used (italicized operators are modifications or extensions). The grammar will be pre- Starting with the definition of stems (line 1), we add the three enrichments to the bare phonological string (2). However, the innermost producer-type string constructed by stringToAutomaton (3) is intersected with phonological constraints (5,6) that need to see the string only, minus its enrichments. This is akin to lexical rule application. ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In this section I show how to implement the Koasati case from (3) using the FSA Utilities toolbox (van Noord, 1997) . FSA Utilities is a Prolog-based finite-state toolkit and extendible regular expression compiler. ", "after_sen": "It is freely available and encourages rapid prototyping. "}
{"citeStart": 122, "citeEnd": 135, "citeStartToken": 122, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "The implementation described in this paper is based on the most recent model, that of (Gorrell (in press) ). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980) , Marcus et al (1983) ), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987 (Abney ( , 1989 , Pritehett (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that \"the parser attempts incrementally to satisfy the principles of grammar\". For the purposes of this implementation, I have interpreted this to mean that each word must be attached into a fullyconnected phrase marker as it is found in the input. 1 The psychological desirability of such a 1In fact, GorreU conjectures that, where there is insufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue & Fodor (1991) , Frazier (1987) ). Such models have also been explored computationally (Milward (1995) , Crocker (1991) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The implementation described in this paper is based on the most recent model, that of (Gorrell (in press) ). ", "mid_sen": "This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980) , Marcus et al (1983) ), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987 (Abney ( , 1989 , Pritehett (1992)). ", "after_sen": "Instead, processing is guided by the principle of Incremental Licensing, which states that \"the parser attempts incrementally to satisfy the principles of grammar\". "}
{"citeStart": 231, "citeEnd": 244, "citeStartToken": 231, "citeEndToken": 244, "sectionName": "UNKNOWN SECTION NAME", "string": "Both of the methods we have evaluated involve learning a set of string-transformation rules to convert words, morphemes, or individual letters (graphemes) in the dialectal forms to the standard variant. The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a) . The reason for the ultimate conversion of the rule set to finite-state transducers is twofold: first, the transducers are easy to apply rapidly to input data using available tools, and secondly, the transducers can further be modified and combined with the standard morphology already available to us as a finite transducer.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Both of the methods we have evaluated involve learning a set of string-transformation rules to convert words, morphemes, or individual letters (graphemes) in the dialectal forms to the standard variant. ", "mid_sen": "The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a) . ", "after_sen": "The reason for the ultimate conversion of the rule set to finite-state transducers is twofold: first, the transducers are easy to apply rapidly to input data using available tools, and secondly, the transducers can further be modified and combined with the standard morphology already available to us as a finite transducer."}
{"citeStart": 82, "citeEnd": 103, "citeStartToken": 82, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Multi-document summarization has been well studied, and a couple of systems have been developed. We test LexRank (Erkan and Radev, 2004) , a state-of-the-art summarization system and find a sharp drop of ROUGE-2, from 0.3894 on news to 0.2871 on tweets. This can be largely attributed to the short and noise prone nature of tweets, which causes a single tweet to be insufficient to provide reliable information to compute its salience score. We develop a graph-based summarization system that aggregates social signals, i.e., re-tweeted times and follower numbers to handle this challenge. More specifically, the translation probability from one tweet to the other depends on both the similarity between the two tweets and the social network features associated with the second tweet. This largely differentiates our system from existing studies, such as the work of Sharifi et al. (2010) , which uses only tweet-level content features (e.g., keywords) to select representative sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More specifically, the translation probability from one tweet to the other depends on both the similarity between the two tweets and the social network features associated with the second tweet. ", "mid_sen": "This largely differentiates our system from existing studies, such as the work of Sharifi et al. (2010) , which uses only tweet-level content features (e.g., keywords) to select representative sentences.", "after_sen": "Besides utilizing social signals, our system has two additional features. "}
{"citeStart": 42, "citeEnd": 65, "citeStartToken": 42, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section we describe in detail the baseline NER system we use. It is inspired by the system described in Ratinov and Roth (2009) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section we describe in detail the baseline NER system we use. ", "mid_sen": "It is inspired by the system described in Ratinov and Roth (2009) .", "after_sen": "Because NER annotations are commonly not nested (for example, in the text \"the US Army\", \"US Army\" is treated as a single entity, instead of the location \"US\" and the organization \"US Army\") it is possible to treat NER as a sequence labeling problem, where each token in the sentence receives a label which depends on which entity type it belongs to and its position in the entity. "}
{"citeStart": 159, "citeEnd": 173, "citeStartToken": 159, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "The data for BWI was obtained using the TIES implementation [tcc.itc.it/research/textec/toolsresources/ties.html]. The data for the LP 2 learning curve was obtained from (Ciravegna, 2003) . The results for ELIE were generated by the current implementation [http://smi.ucd.ie/aidan/Software.html]. For the CRF results, we used MALLET's SimpleTagger (McCallum, 2002) , with each token encoded with a set of binary features (one for each observed literal, as well as the eight token generalizations).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The data for the LP 2 learning curve was obtained from (Ciravegna, 2003) . ", "mid_sen": "The results for ELIE were generated by the current implementation [http://smi.ucd.ie/aidan/Software.html]. For the CRF results, we used MALLET's SimpleTagger (McCallum, 2002) , with each token encoded with a set of binary features (one for each observed literal, as well as the eight token generalizations).", "after_sen": "Our results in Fig. 2 indicate that in Acquisitions dataset, our algorithm substantially outperforms the competitors at all points on the learning curve. "}
{"citeStart": 18, "citeEnd": 38, "citeStartToken": 18, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "To preprocess a document, we first tokenize and downcase it, and then represent it as a vector of unigrams, using frequency as presence. In addition, we remove from the vector punctuation, numbers, words of length one, and words that occur in only a single review. Following the common practice in the information retrieval community, we also exclude words with high document frequency, many of which are stopwords or domainspecific general-purpose words (e.g., \"movies\" in the movie domain). A preliminary examination of our evaluation datasets reveals that these words typically comprise 1-2% of a vocabulary. The decision of exactly how many terms to remove from each dataset is subjective: a large corpus typically requires more removals than a small corpus. To be consistent, we simply sort the vocabulary by document frequency and remove the top 1.5%. Evaluation metrics. We employ two evaluation metrics. First, we report results in terms of the accuracy achieved on the 2000 labeled reviews for each dataset. Second, following Kamvar et al. (2003) , we evaluate the clusters produced by our approach against the gold-standard clusters using the Adjusted Rand Index (ARI). ARI ranges from -1 to 1; better clusterings have higher ARI values.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, we report results in terms of the accuracy achieved on the 2000 labeled reviews for each dataset. ", "mid_sen": "Second, following Kamvar et al. (2003) , we evaluate the clusters produced by our approach against the gold-standard clusters using the Adjusted Rand Index (ARI). ", "after_sen": "ARI ranges from -1 to 1; better clusterings have higher ARI values."}
{"citeStart": 156, "citeEnd": 168, "citeStartToken": 156, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories (Tugwell 1995) 15. Unlike a simple Markov process, there are a potentially infinite number of states, so there is inevitably a problem of sparse data. It is therefore necessary to make various generalisations over the states, for example by ignoring the R2 lists.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "14The usage of the term language tuning is perhaps broader here than its use in the psycholinguistic literature to refer to different structural preferences between languages e.g. for high versus low attachment (Mitchell et al. 1992 ).", "mid_sen": "There has already been some early work done on providing statistically based parsing using transitions between recursively structured syntactic categories (Tugwell 1995) 15. ", "after_sen": "Unlike a simple Markov process, there are a potentially infinite number of states, so there is inevitably a problem of sparse data. "}
{"citeStart": 57, "citeEnd": 69, "citeStartToken": 57, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to account for this phenomenon, we test whether the solution is in lemma form or carries any inflection markers using a lemmatizer. We also check whether the word occurs elsewhere in the text in full form appropriate, skeletons, tempting, extract, ancient, private, design, concentrations, state-of-the-art, scientists, modern, examined, constant, essential, stable, entering, basis, synthetic, cost, demands 10 longer, coffee, coffee, in, water, very, give, you, for, people, living, other, number, water, water, from, over, you, over DE: Skelett, FR: squelette, ES: esqueleto, NL: skelet 12 http://www.academicvocabulary.info/ 13 http://en.wikipedia.org/wiki/List of Latin words with English derivatives (i.e. not as a gap) because it facilitates the correct production for the student. This feature is comparable to the semantic cache used by Brown (1989) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also check whether the word occurs elsewhere in the text in full form appropriate, skeletons, tempting, extract, ancient, private, design, concentrations, state-of-the-art, scientists, modern, examined, constant, essential, stable, entering, basis, synthetic, cost, demands 10 longer, coffee, coffee, in, water, very, give, you, for, people, living, other, number, water, water, from, over, you, over DE: Skelett, FR: squelette, ES: esqueleto, NL: skelet 12 http://www.academicvocabulary.info/ 13 http://en.wikipedia.org/wiki/List of Latin words with English derivatives (i.e. not as a gap) because it facilitates the correct production for the student. ", "mid_sen": "This feature is comparable to the semantic cache used by Brown (1989) .", "after_sen": "Phonetic complexity Wrong answer variants for C-test gaps are often rooted in phonetic problems. "}
{"citeStart": 40, "citeEnd": 75, "citeStartToken": 40, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, I argue for a general method for extending the context-sensitivity of any knowledge source that calculates sentence hypothesis scores as linear combinations of scores for objects. The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994) , involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects. An utterance hypothesis encountered at run time is then treated as if it had been selected from the subpopulation of sentences represented by one of these subcorpora. This technique addresses as follows the three drawbacks just alluded to. Firstly, it is able to capture the most important sentence-internal contextual effects regardless of the complexity of the probabilistic dependencies between the objects involved. Secondly, it makes only modest additional demands on training data. Thirdly, it can be applied in a standard way across knowledge sources for very different kinds of object, and if it does improve on the unclustered model this constitutes proof that additional, as yet unexploited relationships exist between linguistic objects of the type the model is based on, and that therefore it is worth looking for a more specific, more powerful way to model them.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, I argue for a general method for extending the context-sensitivity of any knowledge source that calculates sentence hypothesis scores as linear combinations of scores for objects. ", "mid_sen": "The method, which is related to that of Iyer, Ostendorf and Rohlicek (1994) , involves clustering the sentences in the training corpus into a number of subcorpora, each predicting a different probability distribution for linguistic objects. ", "after_sen": "An utterance hypothesis encountered at run time is then treated as if it had been selected from the subpopulation of sentences represented by one of these subcorpora. "}
{"citeStart": 144, "citeEnd": 161, "citeStartToken": 144, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "This topic-based approach is in contrast to Kameyama's ,Japanese version (Kameyama 1985 , Kameyama 1986 of\" tbcus-based spproach to anaphora by Grosz et al. 1983 . In her framewock, subjecthood and predicate deixis play the principal role, and the fact that topic provides the most important clue to anaphora identification in actual spoken Japanese discourse is not utilized explicitly.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Its identification needs ~peech act cal;egorization of sentences.)", "mid_sen": "This topic-based approach is in contrast to Kameyama's ,Japanese version (Kameyama 1985 , Kameyama 1986 of\" tbcus-based spproach to anaphora by Grosz et al. 1983 . ", "after_sen": "In her framewock, subjecthood and predicate deixis play the principal role, and the fact that topic provides the most important clue to anaphora identification in actual spoken Japanese discourse is not utilized explicitly."}
{"citeStart": 192, "citeEnd": 210, "citeStartToken": 192, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a new space and a new metric for computing this distance. Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of questionanswer pairs (Q, S A ), we can train a probabilistic model for estimating the conditional probability P(Q | S A ). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR engine, one can find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a new space and a new metric for computing this distance. ", "mid_sen": "Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. ", "after_sen": "This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. "}
{"citeStart": 71, "citeEnd": 90, "citeStartToken": 71, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "As our third corpus, we use the Alex portion of the Providence corpus (Demuth et al., 2006; Börschinger et al., 2012) . A major benefit of the Providence corpus is that the video-recordings from which the transcripts were produced are available through CHILDES alongside the transcripts. This will allow future work to rely on even more realistic stress cues that can be derived directly from the acoustic signal. While beyond the scope of this paper, we believe choosing a corpus that makes richer information available will be important for future work on stress (and other acoustic) cues. Another major benefit of the Alex corpus is that it provides longitudinal data for a single infant, rather than being a concatenation of transcripts collected from multiple children, such as the Korman and the Brent-Bernstein-Ratner corpus. In total, the Alex corpus comprises 17948 utterances.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ing models of Bayesian word segmentation (Brent, 1999; Goldwater, 2007; , comprising in total 9790 utterances.", "mid_sen": "As our third corpus, we use the Alex portion of the Providence corpus (Demuth et al., 2006; Börschinger et al., 2012) . ", "after_sen": "A major benefit of the Providence corpus is that the video-recordings from which the transcripts were produced are available through CHILDES alongside the transcripts. "}
{"citeStart": 44, "citeEnd": 57, "citeStartToken": 44, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "We use notation from the PATR-II formalism (Shieber, 1986) and (Shieber, 1992) . The extraction function D/pl extracts the subdag under path pl for a given D, and the embedding function D \\ pl injects D into the enclosing dag D' such that D'/pl = D. The filtering function p is similar to (Shieber, 1992) : p(D) returns a copy of D in which some features may be removed. Note that in this paper .restrictor. specifies the features to be removed by p, whereas in (Shieber, 1985 (Shieber, , 1992 restrictor specifies the features to be retained by restriction which is equivalent to p.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use notation from the PATR-II formalism (Shieber, 1986) and (Shieber, 1992) . ", "after_sen": "The extraction function D/pl extracts the subdag under path pl for a given D, and the embedding function D \\ pl injects D into the enclosing dag D' such that D'/pl = D. "}
{"citeStart": 125, "citeEnd": 134, "citeStartToken": 125, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Some studies have tried to incorporate syntactic information into vector-based models. In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of interest. This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998) . Although syntactic information is now used to select a word's appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. ", "mid_sen": "However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998) . ", "after_sen": "Although syntactic information is now used to select a word's appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing."}
{"citeStart": 217, "citeEnd": 232, "citeStartToken": 217, "citeEndToken": 232, "sectionName": "UNKNOWN SECTION NAME", "string": "However, there are at least three arguments against iterating PT. First of all, iteration would increase the complexity of building a model of a paragraph; infinite iteration would almost certainly make impossible such a construction in real time. Secondly, the cooperative principle of Grice (1975 Grice ( , 1978 , under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Secondly, the cooperative principle of Grice (1975 Grice ( , 1978 , under the assumption that referential levels of a writer and a reader are quite similar, implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader; and this seems to imply that he should appeal only to the most direct knowledge of the reader. ", "mid_sen": "Finally, it has been shown by Groesser (1981) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1; furthermore, our reading of the analysis of five paragraphs by Crothers (1979) strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph. ", "after_sen": "Thus, for example, we can expect that in the worst case only one or two steps of such an iteration would be needed to find answers to wh-questions."}
{"citeStart": 185, "citeEnd": 195, "citeStartToken": 185, "citeEndToken": 195, "sectionName": "UNKNOWN SECTION NAME", "string": "The incremental parser learns while parsing, and it could, in principle, simply be evaluated for a single pass of the data. But, because the quality of the parses of the first sentences would be low, I first trained on the full corpus and then measured parsing accuracy on the corpus subset. By training on the full corpus, the procedure differs from that of Klein, Manning and Bod who only train on the subset of bounded length sentences. However, this excludes the induction of parts-of-speech for parsing from plain text. When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire (Klein and Manning, 2002) . The comparison between the algorithms remains, therefore, valid. Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (Klein and Manning, 2004) , U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a) . The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text. Results for the incremental parser are given for learning and parsing from left to right and from right to left.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The comparison between the algorithms remains, therefore, valid. ", "mid_sen": "Table 1 gives two baselines and the parsing results for WSJ10, WSJ40, Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM (Klein and Manning, 2004) , U-DOP (Bod, 2006b) and UML-DOP (Bod, 2006a) . ", "after_sen": "The middle part of the table gives results for parsing from part-of-speech sequences extracted from the treebank while the bottom part of the table given results for parsing from plain text. "}
{"citeStart": 355, "citeEnd": 378, "citeStartToken": 355, "citeEndToken": 378, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ([Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes \"these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 52, "citeEnd": 78, "citeStartToken": 52, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "A different version of our own SENSELEARNER system (Mihalcea and Faruque, 2004) , using three of the semantic models described in this paper, combined with semantic generalizations based on syntactic dependencies, achieved a performance of 64.6%.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The statistical models are built based on SemCor and WordNet, for an overall disambiguation accuracy of 64.1%.", "mid_sen": "A different version of our own SENSELEARNER system (Mihalcea and Faruque, 2004) , using three of the semantic models described in this paper, combined with semantic generalizations based on syntactic dependencies, achieved a performance of 64.6%.", "after_sen": ""}
{"citeStart": 71, "citeEnd": 89, "citeStartToken": 71, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "With the growing wdume of text available in electronic lorm, a number of methods have been proposed tor extracting word correspondences from bilingual corpora automatically. These methods can be divided into those taking a statistical approach (Gale & Church 1991a; Kupiec 1993; Dagan et al. 1993; Inoue & Nogaito 1993; Fung 1995) and those taking a linguistic approach (Yamamoto & Sakamoto 1993; Kum~mo & Hirakawa 1994; Ishimoto & Nagao 1994) . The statistical approach utilizes the occurrence frequencies and locations of words in a parallel corpus to calculate the pairwise correlations between the words in the two languages. The linguistic approach primarily extracts correspondences between compound words by consulting a bilingual dictionary of simple words.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With the growing wdume of text available in electronic lorm, a number of methods have been proposed tor extracting word correspondences from bilingual corpora automatically. ", "mid_sen": "These methods can be divided into those taking a statistical approach (Gale & Church 1991a; Kupiec 1993; Dagan et al. 1993; Inoue & Nogaito 1993; Fung 1995) and those taking a linguistic approach (Yamamoto & Sakamoto 1993; Kum~mo & Hirakawa 1994; Ishimoto & Nagao 1994) . ", "after_sen": "The statistical approach utilizes the occurrence frequencies and locations of words in a parallel corpus to calculate the pairwise correlations between the words in the two languages. "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "An assumption made by all the methods mentioned in this paper is that the members of the test set are all independent of one another. That is, knowing how a method performs on one test set sample should not give any information on how that method performs on other test set samples. This assumption is not always true. Church and Mercer (1993) give some examples of dependence between test set instances in natural language. One type of dependence is that of a lexeme's part of speech on the parts of speech of neighboring lexemes (their section 2.1). Similar is the concept of collocation, where the probability of a lexeme's appearance is influenced by the lexemes appearing in nearby positions (their section 3). A type of dependence that is less local is that often, a content word's appearance in a piece of text greatly increases the chances of that same word appearing later in that piece of text (their section 2.3).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This assumption is not always true. ", "mid_sen": "Church and Mercer (1993) give some examples of dependence between test set instances in natural language. ", "after_sen": "One type of dependence is that of a lexeme's part of speech on the parts of speech of neighboring lexemes (their section 2.1). "}
{"citeStart": 68, "citeEnd": 87, "citeStartToken": 68, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. ", "mid_sen": "The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. ", "after_sen": "The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). "}
{"citeStart": 97, "citeEnd": 117, "citeStartToken": 97, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011) . However, this work does not handle citation context. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources. A common approach for sentiment detection is to use a labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003) . However, such approaches have been found to be highly topic dependent (Engström, 2004; Gamon and Aue, 2005; Blitzer et al., 2007) . Teufel et al. (2006) worked on a 2,829 sentence citation corpus using a 12-class classification scheme. Although they used context in their annotation, their focus was on determining the author's reason for citing a given paper. This task differs from citation sentiment, which is in a sense a \"lower level\" of analysis.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011) . ", "after_sen": "However, this work does not handle citation context. "}
{"citeStart": 3, "citeEnd": 16, "citeStartToken": 3, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "As Kehler (1994) points out, if forward movement of time is considered a default with consecutive event sentences, then the use of \"because\" in (5) should cause a temporal clash-whereas it is perfectly felicitous. Temporal expressions such as at noon and the previous Thursday can have a similar effect: they too can override the default temporal relations and place constraints on tense. In (6), for example, the default interpretation would be that John's being in Detroit overlaps with his being in Boston, but the phrase the previous Thursday overrides this, giving the interpretation that John's being in Detroit precedes his being in Boston:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(5) John entered the room because Mary stood up.", "mid_sen": "As Kehler (1994) points out, if forward movement of time is considered a default with consecutive event sentences, then the use of \"because\" in (5) should cause a temporal clash-whereas it is perfectly felicitous. ", "after_sen": "Temporal expressions such as at noon and the previous Thursday can have a similar effect: they too can override the default temporal relations and place constraints on tense. "}
{"citeStart": 160, "citeEnd": 178, "citeStartToken": 160, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "We chose the verb prediction task which is similar to other word prediction tasks (e.g., (Golding and Roth, 1999) ) and, in particular, follows the paradigm in Dagan et al., 1999; Lee, 1999) . There, a list of the confusion sets is constructed first, each consists of two different verbs. The verb vl is coupled with v2 provided that they occur equally likely in the corpus. In the test set, every occurrence of vl or v2 was replaced by a set {vl, v2} and the classification task was to predict the correct verb. For example, if a confusion set is created for the verbs \"make\" and \"sell\", then the data is altered as follows:", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To evaluate word prediction as a simple language model.", "mid_sen": "We chose the verb prediction task which is similar to other word prediction tasks (e.g., (Golding and Roth, 1999) ) and, in particular, follows the paradigm in Dagan et al., 1999; Lee, 1999) . ", "after_sen": "There, a list of the confusion sets is constructed first, each consists of two different verbs. "}
{"citeStart": 28, "citeEnd": 47, "citeStartToken": 28, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "(1) Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The following are examples of citing sentences that dispute previous work.", "mid_sen": "(1) Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role.", "after_sen": "(2) Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data."}
{"citeStart": 64, "citeEnd": 88, "citeStartToken": 64, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "The main impetus to change Applicative CG came from the work of Ades and Steedman (1982) . Ades and Steedman noted that the use of function composition allows CGs to deal with unbounded dependency constructions. Function composition enables a function to be applied to its argument, even if that argument is incomplete e.g. s/pp + pp/np --+ s/np This allows peripheral extraction, where the 'gap' is at the start or the end of e.g. a relative clause. Variants of the composition rule were proposed in order to deal with non-peripheral extraction, 4The reformulation is not entirely faithful here to Bar-Hillel, who used a slightly problematic 'double slash' notation for functions of functions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most CGs either choose the third of these (to give a vp structure), or include a rule of Associativity which means that the types are interchangeable (in the Lambek Calculus, Associativity is a consequence of the calculus, rather than being specified separately).", "mid_sen": "The main impetus to change Applicative CG came from the work of Ades and Steedman (1982) . ", "after_sen": "Ades and Steedman noted that the use of function composition allows CGs to deal with unbounded dependency constructions. "}
{"citeStart": 200, "citeEnd": 212, "citeStartToken": 200, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "Several substantial machine-readable subcategorization dictionaries exist for English, either built largely automatically from machine-readable versions of conventional learners' dictionaries, or manually by (computational) linguists (e.g. the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) ; the COMLEX Syntax dictionary, Grishman et al. (1994) ). Unfortunately, neither approach can yield a genuinely accurate or comprehensive computational lexicon, because both rest ultimately on the manual efforts of lexicographers / linguists and are, therefore, prone to errors of omission and commission which are hard or impossible to detect automatically (e.g. Boguraev & Briscoe, 1989 ; see also section 3.1 below for an example). Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, manual encoding is labour intensive and, therefore, it is costly to extend it to neologisms, information not currently encoded (such as relative frequency of different subcategorizations), or other (sub)languages. ", "mid_sen": "These problems are compounded by the fact that predicate subcategorization is closely associated to lexical sense and the senses of a word change between corpora, sublanguages and/or subject domains (Jensen, 1991) .", "after_sen": "In a recent experiment with a wide-coverage parsing system utilizing a lexicalist grammatical framework, Briscoe & Carroll (1993) observed that half of parse failures on unseen test data were caused by inaccurate subcategorization information in the ANLT dictionary. "}
{"citeStart": 207, "citeEnd": 220, "citeStartToken": 207, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "The third area of research that influenced our work is computational modeling of language acquisition and grounding. Recent studies have shown that multisensory information (e.g., through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004) . Especially in (Yu and Ballard, 2004) , an unsupervised approach based on a generative correspondence model was developed to capture the mapping between spoken words and the occurring perceptual features of objects. This approach is most similar to the translation model used in our work. However, compared to this work where multisensory information comes from vision and language processing, our work focuses on a different aspect. Here, instead of applying vision processing on objects, we are interested in eye gaze behavior when users interact with a graphic display. Eye gaze is an implicit and subconscious input modality during human machine interaction. Eye gaze data inevitably contain a significant amount of noise. Therefore, it is the goal of this paper to examine whether this modality can be utilized for vocabulary acquisition for conversational systems.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The third area of research that influenced our work is computational modeling of language acquisition and grounding. ", "mid_sen": "Recent studies have shown that multisensory information (e.g., through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004) . ", "after_sen": "Especially in (Yu and Ballard, 2004) , an unsupervised approach based on a generative correspondence model was developed to capture the mapping between spoken words and the occurring perceptual features of objects. "}
{"citeStart": 26, "citeEnd": 38, "citeStartToken": 26, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been many approachs to automatic detection of similar words from text. Our method is similar to (Hindle, 1990) , (Lin, 1998) , and (Gasperin, 2001) in the use of dependency relationships as the word features. Another approach used the words' distribution to cluster the words (Pereira, 1993) , and Inoue (Inoue, 1991) also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There have been many approachs to automatic detection of similar words from text. ", "mid_sen": "Our method is similar to (Hindle, 1990) , (Lin, 1998) , and (Gasperin, 2001) in the use of dependency relationships as the word features. ", "after_sen": "Another approach used the words' distribution to cluster the words (Pereira, 1993) , and Inoue (Inoue, 1991) also used the word distributional information in the Japanese-English word pairs to resolve the polysemous word problem."}
{"citeStart": 236, "citeEnd": 240, "citeStartToken": 236, "citeEndToken": 240, "sectionName": "UNKNOWN SECTION NAME", "string": "This binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where what we will call the \"core parser\" performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver(to use Abney's terminology (1987 Abney's terminology ( , 1989 ), is required to solve the difficult cases, (such as on reaching melted in (1)). This \"core parser\" has been the subject of a number of computational implementations, including Marcus's deterministic parser (1980) , Description theory (henceforth, D-theory) (Marcus et al (1983) ), and Abney's licensing based model (1987, 1989) . It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where what we will call the \"core parser\" performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver(to use Abney's terminology (1987 Abney's terminology ( , 1989 ), is required to solve the difficult cases, (such as on reaching melted in (1)). ", "mid_sen": "This \"core parser\" has been the subject of a number of computational implementations, including Marcus's deterministic parser (1980) , Description theory (henceforth, D-theory) (Marcus et al (1983) ), and Abney's licensing based model (1987, 1989) . ", "after_sen": "It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) )."}
{"citeStart": 175, "citeEnd": 195, "citeStartToken": 175, "citeEndToken": 195, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996) ), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. ", "mid_sen": "Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). ", "after_sen": "These studies represent the context in which an ambiguous word occurs with a wide variety of features. "}
{"citeStart": 140, "citeEnd": 152, "citeStartToken": 140, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we successfully integrate a stateof-the-art WSD system into the state-of-the-art hierarchical phrase-based MT system, Hiero (Chiang, 2005) . The integration is accomplished by introducing two additional features into the MT model which operate on the existing rules of the grammar, without introducing competing rules. These features are treated, both in feature-weight tuning and in decoding, on the same footing as the rest of the model, allowing it to weigh the WSD model predictions against other pieces of evidence so as to optimize translation accuracy (as measured by BLEU). The contribution of our work lies in showing for the first time that integrating a WSD system significantly improves the performance of a state-of-the-art statistical MT system on an actual translation task.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Considering the conflicting results reported by prior work, it is not clear whether a WSD system can help to improve the performance of a state-of-the-art statistical MT system.", "mid_sen": "In this paper, we successfully integrate a stateof-the-art WSD system into the state-of-the-art hierarchical phrase-based MT system, Hiero (Chiang, 2005) . ", "after_sen": "The integration is accomplished by introducing two additional features into the MT model which operate on the existing rules of the grammar, without introducing competing rules. "}
{"citeStart": 118, "citeEnd": 133, "citeStartToken": 118, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Although we do not investigate it in this paper, the selective left-corner transform should usually have a smaller search space relative to the standard left-corner transform, all else being equal. The partial parses produced during a top-down parse consist of a single connected tree fragment, while the partial parses produced produced during a left-corner parse generally consist of several disconnected tree fragments. Since these fragments are only weakly related via the link\" constraint described below, the search for each fragment is relatively independent. This may be responsible for the observation that exhaustive left-corner parsing is less e cient than top-down parsing Covington, 1994 . Informally, b ecause the selective left-corner transform recognizes only a subset of productions in a left-corner fashion, its partial parses contain fewer tree discontiguous fragments and the search m a y be more e cient.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since these fragments are only weakly related via the link\" constraint described below, the search for each fragment is relatively independent. ", "mid_sen": "This may be responsible for the observation that exhaustive left-corner parsing is less e cient than top-down parsing Covington, 1994 . ", "after_sen": "Informally, b ecause the selective left-corner transform recognizes only a subset of productions in a left-corner fashion, its partial parses contain fewer tree discontiguous fragments and the search m a y be more e cient."}
{"citeStart": 135, "citeEnd": 155, "citeStartToken": 135, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "To create our summaries, we employed automatically generated CoreSC annotations, which are the output of the classifiers described in (Liakata et al., 2012) . These classifiers assign CoreSC categories to sentences on the basis of features local to a sentence, such as significant n-grams, verbs and word triples, as well as global features such as the position of the sentence within the document and within a paragraph and section headers. The following subsections give details about the creation of extractive summaries from CoreSC categories.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While we do not consider abstracts to be adequate summaries, we at least consider them to be coherent summaries, which is why the content model reflects the distribution of CoreSCs in the abstracts.", "mid_sen": "To create our summaries, we employed automatically generated CoreSC annotations, which are the output of the classifiers described in (Liakata et al., 2012) . ", "after_sen": "These classifiers assign CoreSC categories to sentences on the basis of features local to a sentence, such as significant n-grams, verbs and word triples, as well as global features such as the position of the sentence within the document and within a paragraph and section headers. "}
{"citeStart": 5, "citeEnd": 18, "citeStartToken": 5, "citeEndToken": 18, "sectionName": "UNKNOWN SECTION NAME", "string": "We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002) 's NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999) , we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%. Teufel (1999) introduced a new rhetorical analysis for scientific texts called Argumentative Zoning. Each sentence of an article from the scientific literature is classified into one of seven basic rhetorical structures shown in Table 1 . The first three: Background, Other, and Own, are part of the basic schema and represent attribution of intellectual ownership. The four additional categories: aim, textual, contrast, and basis, are based upon Swales (1990) 's Creating A Research Space (CARS) model, and provide pointed information about the author's stance and the paper itself. Teufel assumes that each sentence only requires a single classification and that all sentences clearly fit into the above structure. The assumption is clearly not always correct, but is a useful approximation nevertheless.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare our zone classifier to a reimplementation of Teufel and Moens (2002) 's NB classifier and features on their original Computational Linguistics corpus. ", "mid_sen": "Like Teufel (1999) , we model zone classification as a sequence tagging task. ", "after_sen": "Our zone classifier achieves an F-score of 96.88%, a 20% improvement. "}
{"citeStart": 159, "citeEnd": 170, "citeStartToken": 159, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996) ). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The task of identifying sentence boundaries in text has not received as much attention as it deserves. ", "mid_sen": "Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996) ). ", "after_sen": "Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992) )."}
{"citeStart": 97, "citeEnd": 119, "citeStartToken": 97, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "The theoretical ground for incorporating negative examples in a language learning process originates with Gold's work (Gold, 1967; Angluin, 1980) . He examined the process of learning the grammar of a formal language from examples. He showed that, for languages at least as high in the Chomsky hierarchy as CFGs, inference from positive data alone is strictly less powerful than inference from both positive and negative data together. To illustrate this informally consider a case of inference from a number of examples: as they are presented to the inference machine, pos- Table 3 : Performance on text from Perkins manuals, using improved representation and larger training set, after 2% sentences have been excluded sible grammars are postulated. However, with positive data alone a problem of over generalization arises: the postulated grammar may be a superset of the real grammar, and sentences that are outside the real grammar could be accepted. If both positive and negative data is used, counter examples will reduce the postulated grammar so that it is nearer the real grammar. Gold developed his theory for formal languages: it is argued that similar considerations apply here. A grammar may be inferred from positive examples alone for certain subsets of regular languages (Garcia and Vidal, 1990) , or an inference process may degenerate into a look up procedure if every possible positive example is stored. In these cases negative information is not required, but they are not plausible models for unbounded natural language. In our method the required parse is found by inferring the grammar from both positive and negative information, which is effectively modelled by the neural net. ~-hture work will investigate the effect of training the networks on the positive examples alone. With our current size corpus there is not enough data.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Gold developed his theory for formal languages: it is argued that similar considerations apply here. ", "mid_sen": "A grammar may be inferred from positive examples alone for certain subsets of regular languages (Garcia and Vidal, 1990) , or an inference process may degenerate into a look up procedure if every possible positive example is stored. ", "after_sen": "In these cases negative information is not required, but they are not plausible models for unbounded natural language. "}
{"citeStart": 323, "citeEnd": 346, "citeStartToken": 323, "citeEndToken": 346, "sectionName": "UNKNOWN SECTION NAME", "string": "We focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions. Identifying opinion sources will be especially critical for opinion-oriented questionanswering systems (e.g., systems that answer questions of the form \"How does [X] feel about [Y] ?\") and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another. 1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text. To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003) , Wiebe and Riloff (2005) , Wilson et al. (2005) ) and the nesting structure of sources (e.g., Breck and Cardie (2004) ). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text. ", "mid_sen": "To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003) , Wiebe and Riloff (2005) , Wilson et al. (2005) ) and the nesting structure of sources (e.g., Breck and Cardie (2004) ). ", "after_sen": "The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus."}
{"citeStart": 113, "citeEnd": 132, "citeStartToken": 113, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. We refer to these as linguistic level representations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . ", "mid_sen": "The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). ", "after_sen": "Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . "}
{"citeStart": 123, "citeEnd": 147, "citeStartToken": 123, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "The advantages of the AWM model is that it was shown to reproduce, in simulation, mlmy results on human memory ~md lem'ning. Because search st~n'ts from the current pointer location, items that have been stored most recently are more likely to be retrieved, predicting recency effects [BMdeley, 19861. Because items that are stored in multiple locations are more likely to be retrieved, tbe model predicts fiequency effects [Hiutzmmm and Block, 1971 I. Because items are stored in chrono-IogicN sequence, the model produces natural associativity effects [Landaner, 19751 . Because deliberation and means-end re~tsoning can only operate on salient belietls, limited attention produces a concomitlmt inl)rential limitation, i.e. if a belief is nol salient it cannot be used in deliberation or mcans-end-reltsoniug. This means that mistakes that agents make in their planning process have a plansible cognitive basis. Agents can both fail to access a belief that would idlow them to produce ,an optim~d plan, its well as make a mistake in pl~mning if a belief about bow the world has changed its a result of pllmning is not sldicnt. I)epending on the preceding discourse, and the agent's attentionld capacity, the propositions that im agent knows may or may not be salient when a proposed is made.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because search st~n'ts from the current pointer location, items that have been stored most recently are more likely to be retrieved, predicting recency effects [BMdeley, 19861. ", "mid_sen": "Because items that are stored in multiple locations are more likely to be retrieved, tbe model predicts fiequency effects [Hiutzmmm and Block, 1971 I. ", "after_sen": "Because items are stored in chrono-IogicN sequence, the model produces natural associativity effects [Landaner, 19751 . "}
{"citeStart": 0, "citeEnd": 27, "citeStartToken": 0, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "ing could be used. However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. Budanitsky and Hirst (2006) pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs. This empty band is not observed here. However, Figure 4 shows the distribution of averaged judgments with the highest agreement between annotators (standard deviation < 0.8). The plot clearly shows an empty horizontal band with no judgments. The connection between averaged judgments and standard deviation is plotted in Figure 5 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, even with the present setup, automatic extraction of concept pairs performs remarkably well and can be used to quickly create balanced test datasets. ", "mid_sen": "Budanitsky and Hirst (2006) pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs. ", "after_sen": "This empty band is not observed here. "}
{"citeStart": 140, "citeEnd": 152, "citeStartToken": 140, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section we outline the main ideas underlying a polynomial time recognition algorithm for PlPATR that generalizes the CKY algorithm (Kasami, 1965; Younger, 1967) . The key to this algorithm is the use of structure sharing techniques similar to those used to process I_lG efficiently (Vijay-Shanker and Weir, 1993). To understand how these techniques are applied in the case of PLPATR, it is therefore helpful to consider first the somewhat simpler case of I_lG.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In this section we outline the main ideas underlying a polynomial time recognition algorithm for PlPATR that generalizes the CKY algorithm (Kasami, 1965; Younger, 1967) . ", "after_sen": "The key to this algorithm is the use of structure sharing techniques similar to those used to process I_lG efficiently (Vijay-Shanker and Weir, 1993). "}
{"citeStart": 201, "citeEnd": 225, "citeStartToken": 201, "citeEndToken": 225, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002) . The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If nouns co-occurring in coordination patterns are often semantically similar, and if a simi-larity measure could be defined so that, for example: sim(executives, spouses) > sim(busloads, spouses) then it is potentially useful for coordination disambiguation.", "mid_sen": "The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002) . ", "after_sen": "The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase."}
{"citeStart": 105, "citeEnd": 123, "citeStartToken": 105, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing. The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of Quirk et al. (1972 Quirk et al. ( , 1985 . The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in. Most of the subcategorisation frames are specified by syntactic category, but some are very ill-specified; for instance, 9 is defined as \"needs a descriptive word or phrase\". In practice many adverbial and predicative complements will satisfy this code, when attached to a verb; for example, put [xg] where the code marks a locative adverbial prepositional phrase vs. make under sense 14 (hereafter written make(14)) is coded IX9] where it marks a predicative noun phrase or prepositional phrase.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Once the grammar codes have been restructured, it still remains to be shown that the information they encode is going to be of some utility for natural language processing. ", "mid_sen": "The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of Quirk et al. (1972 Quirk et al. ( , 1985 . ", "after_sen": "The codes are doubly articulated; capital letters represent the grammatical relations which hold between a verb and its arguments and numbers represent subcategorisation frames which a verb can appear in. "}
{"citeStart": 10, "citeEnd": 34, "citeStartToken": 10, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to converting the ATB's constituent parses to dependency trees, we make a handful of other changes. Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous. Following , Treebank sentences headed by TOP elements containing multiple S daughters are split into separate sentences. 8 Additionally, if the dependency converter concludes that an S node without treebank functional tags is dependent upon another S node and is separated from it via sentence-final punctuation (e.g., an exclamation point), these S nodes are separated into distinct sentences as well. For the broadcast news data, we remove all subtrees headed by EDITED tags to make it more closely resemble newswire text. 9 Since we adhere to the tokenization scheme used by the ATB, and we do not split off the determiner Al as its own tree token. Instead, we treat it as a prefix.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition to converting the ATB's constituent parses to dependency trees, we make a handful of other changes. ", "mid_sen": "Following Green and Manning (2010) and others, sentences headed by X nodes are deleted because the treebank annotators considered them unbracketable or somehow erroneous. ", "after_sen": "Following , Treebank sentences headed by TOP elements containing multiple S daughters are split into separate sentences. "}
{"citeStart": 251, "citeEnd": 260, "citeStartToken": 251, "citeEndToken": 260, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to the ideas mentioned earlier, our future plans include looking into optimal ways of acquiring SPs for verb classification. Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. Brockmann and Lapata (2003) have showed that WordNet-based approaches do not always outperform simple frequency-based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach (Erk, 2007; Bergsma et al., 2008) . The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Considerable research has been done on SP acquisition most of which has involved collecting argument headwords from data and generalizing to Word-Net classes. ", "mid_sen": "Brockmann and Lapata (2003) have showed that WordNet-based approaches do not always outperform simple frequency-based models, and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach (Erk, 2007; Bergsma et al., 2008) . ", "after_sen": "The number and type (and combination) of GRs for which SPs can be reliably acquired, especially when the data is sparse, requires also further investigation."}
{"citeStart": 59, "citeEnd": 77, "citeStartToken": 59, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "Many length-based alignment algorithms have been proposed (Brown et al., 1991; Gale and Church, 1991a) . The correct rates are good. However, the languages they processed belong to occidental family. When these algorithms are applied to other rtmning texts from different families, will the performance keep on tile same level? Other translation-based alignments (Kay, 199l; Chen, 1993) show the difficulty in determining the word correspondence and are very complex.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To do such kinds of researches, the most impmlant task is to align the bilingual texts.", "mid_sen": "Many length-based alignment algorithms have been proposed (Brown et al., 1991; Gale and Church, 1991a) . ", "after_sen": "The correct rates are good. "}
{"citeStart": 122, "citeEnd": 147, "citeStartToken": 122, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper has presented a novel finite-state method for reduplication that is applicable for both unbounded total cases, truncated or otherwise phonologically modified types and infixing instances. The key ingredients of the proposal are suitably enriched automaton representations, the identification of reduplicative copying with automaton intersection and a resource-conscious interpretation that differentiates between two types of arc symbols, namely producers and consumers of information. After demonstrating the existence of efficient ondemand algorithms to reduplication's central operations, a case study from Koasati has shown that all of the above ingredients may be necessary in the analysis of a single complex piece of prosodic morphology. It is worth mentioning that our method can be transferred into a two-level transducer setting without major difficulties (Walther, 1999, appendix B) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After demonstrating the existence of efficient ondemand algorithms to reduplication's central operations, a case study from Koasati has shown that all of the above ingredients may be necessary in the analysis of a single complex piece of prosodic morphology. ", "mid_sen": "It is worth mentioning that our method can be transferred into a two-level transducer setting without major difficulties (Walther, 1999, appendix B) .", "after_sen": "I conclude that the one-level approach to reduplicative prosodic morphology presents an attractive way of extending finite-state techniques to difficult phenomena that hitherto resisted elegant computational analyses."}
{"citeStart": 135, "citeEnd": 167, "citeStartToken": 135, "citeEndToken": 167, "sectionName": "UNKNOWN SECTION NAME", "string": "There are other problematic aspects of the annotation. Some concern the fact that authors do not always state their purpose clearly. For instance, several earlier studies found that negational citations are rare (Moravcsik and Murugesan, 1975; Spiegel-Rüsing, 1977) ; MacRoberts and MacRoberts (1984) argue that the reason for this is that they are potentially politically dangerous, and that the authors go through lengths to diffuse the impact of negative references, hiding a negative point behind insincere praise, or diffusing the thrust of criticism with perfunctory remarks. In our data we found ample evidence of this effect, illustrated by the following example:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some concern the fact that authors do not always state their purpose clearly. ", "mid_sen": "For instance, several earlier studies found that negational citations are rare (Moravcsik and Murugesan, 1975; Spiegel-Rüsing, 1977) ; MacRoberts and MacRoberts (1984) argue that the reason for this is that they are potentially politically dangerous, and that the authors go through lengths to diffuse the impact of negative references, hiding a negative point behind insincere praise, or diffusing the thrust of criticism with perfunctory remarks. ", "after_sen": "In our data we found ample evidence of this effect, illustrated by the following example:"}
{"citeStart": 162, "citeEnd": 181, "citeStartToken": 162, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. Note that although our current system uses MeSH headings assigned by human indexers, manually assigned terms can be replaced with automatic processing if needed (Aronson et al. 2004) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The function α(t) maps a MeSH term to a positive score if the term is a positive indicator for that particular task type, or a negative score if the term is a negative indicator for the clinical task. ", "mid_sen": "Note that although our current system uses MeSH headings assigned by human indexers, manually assigned terms can be replaced with automatic processing if needed (Aronson et al. 2004) .", "after_sen": "Below, we enumerate the relevant indicator terms by clinical task. "}
{"citeStart": 67, "citeEnd": 90, "citeStartToken": 67, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Up to now, we only looked at the overall results. Table 4 also shows individual FZ=i values for four selected common grammatical relations: subject NP, (in)direct object NP, locative PP adjunct and temporal PP adjunct. Note that the steps have different effects on the different relations: Adding NPs increases FZ=i by 11.3% for subjects resp. 16.2% for objects, but only 3.9% resp. 3.7% for locatives and temporals. Adverbial functions are more important for the two adjuncts (+6.3% resp. +15%) than for the two complements (+0.2% resp. +0.7%). Argamon et al. (1998) report FZ=i for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper. Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier. For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a) . That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier. ", "mid_sen": "For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a) . ", "after_sen": "That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%."}
{"citeStart": 312, "citeEnd": 332, "citeStartToken": 312, "citeEndToken": 332, "sectionName": "UNKNOWN SECTION NAME", "string": "Statistical parsing using combined systems of handcoded linguistically fine-grained grammars and stochastic disambiguation components has seen considerable progress in recent years. However, such attempts have so far been confined to a relatively small scale for various reasons. Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems. Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000) , or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999) . Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text. The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems. ", "mid_sen": "Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000) , or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999) . ", "after_sen": "Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text. "}
{"citeStart": 80, "citeEnd": 98, "citeStartToken": 80, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At the lexical level, a relation instance can be seen as a sequence of tokens which form a five tuple <Before, M1, Between, M2, After>. Tokens of the five members and the interaction between the heads of the two mentions can be extracted as features as shown in Table 3 .", "mid_sen": "In addition, we cherry-picked the following features which were not included in Zhou et al. (2005) but were shown to be quite effective for relation extraction.", "after_sen": "Bigram of the words between the two mentions: "}
{"citeStart": 97, "citeEnd": 109, "citeStartToken": 97, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "mid_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . ", "after_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) ."}
{"citeStart": 229, "citeEnd": 247, "citeStartToken": 229, "citeEndToken": 247, "sectionName": "UNKNOWN SECTION NAME", "string": "focus from the candidate foci tree using the heuristic \"attack the belief(s) that will most likely resolve the conflict about the top-level belief.\" A candidate loci tree contains the pieces of evidence in a proposed belief tree which, if disbelieved by the user, might change the user's view of the unaccepted top-level proposed belief (the root node of that belief tree). It is identified by performing a depthfirst search on the proposed belief tree. When a node is visited, both the belief and the evidential relationship between it and its parent are examined. If both the belief and relationship were accepted by the evaluator, the search on the current branch will terminate, since once the system accepts a belief, it is irrelevant whether it accepts the user's support for that belief (Young et al., 1994) . Otherwise, this piece of evidence will be included in the candidate loci tree and the system will continue to search through the evidence in the belief tree proposed as support for the unaccepted belief and/or evidential relationship.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When a node is visited, both the belief and the evidential relationship between it and its parent are examined. ", "mid_sen": "If both the belief and relationship were accepted by the evaluator, the search on the current branch will terminate, since once the system accepts a belief, it is irrelevant whether it accepts the user's support for that belief (Young et al., 1994) . ", "after_sen": "Otherwise, this piece of evidence will be included in the candidate loci tree and the system will continue to search through the evidence in the belief tree proposed as support for the unaccepted belief and/or evidential relationship."}
{"citeStart": 30, "citeEnd": 50, "citeStartToken": 30, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "The probability distribution trans (.1, ~) is a word-to-word translation model. Unlike the models proposed by Brown et al. (1993b) , this model is symmetric, because both word bags are generated together from a joint probability distribution. Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions. A sequenceto-sequence translation model can be obtained from a word-to-word translation model by combining Equation 11 with order information as in Equation 8.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The probability distribution trans (.1, ~) is a word-to-word translation model. ", "mid_sen": "Unlike the models proposed by Brown et al. (1993b) , this model is symmetric, because both word bags are generated together from a joint probability distribution. ", "after_sen": "Brown and his colleagues' models, reviewed in Section 4.3, generate one half of the bitext given the other hall so they are represented by conditional probability distributions. "}
{"citeStart": 159, "citeEnd": 182, "citeStartToken": 159, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "Language identification is an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible. It can be solved using many techniques, including knowledge-poor statistical approaches. Typically, the distribution of n-grams of characters or other objects is used to form a model. A comparison of the input against the model determines the language which matches best. Versions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994) , while an interesting practical implementation is described by Adams and Resnik (1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A comparison of the input against the model determines the language which matches best. ", "mid_sen": "Versions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994) , while an interesting practical implementation is described by Adams and Resnik (1997) .", "after_sen": "A variant of the problem is considered by Sibun and Spitz (1994) , and Sibun and Reynar (1996) , who look at it from the point of view of Optical Character Recognition (OCR)."}
{"citeStart": 16, "citeEnd": 40, "citeStartToken": 16, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiments we re-created co-occurrence frequencies for unseen verb-subject and verb-object pairs using three maximally different approaches: back-off smoothing, class-based smoothing using a predefined taxonomy, and distance-weighted averaging. We preferred taxonomic class-based methods over distributional clustering mainly because we wanted to compare directly methods that use distributional information inherent in the corpus without making external assumptions with regard to how concepts and their similarity are represented with methods that quantify similarity relationships based on information present in a hand-crafted taxonomy. Furthermore, as Lee and Pereira's (1999) results indicate that distributional clustering and distance-weighted averaging obtain similar levels of performance, we restricted ourselves to the latter.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We preferred taxonomic class-based methods over distributional clustering mainly because we wanted to compare directly methods that use distributional information inherent in the corpus without making external assumptions with regard to how concepts and their similarity are represented with methods that quantify similarity relationships based on information present in a hand-crafted taxonomy. ", "mid_sen": "Furthermore, as Lee and Pereira's (1999) results indicate that distributional clustering and distance-weighted averaging obtain similar levels of performance, we restricted ourselves to the latter.", "after_sen": "We evaluated the contribution of the different smoothing methods on the nominalization task by exploring how each method and their combination influences disambiguation performance. "}
{"citeStart": 83, "citeEnd": 99, "citeStartToken": 83, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "For evaluation of our results we use the precision and recall measures. Precision is the percentage of predicted chunks/relations that are actually correct, recall is the percentage of correct chunks/relations that are actually found. For convenient comparisons of only one value, we also list the FZ=i value (C.J.van Rijsbergen, 1979) : (Z2+l)'prec'rec /~2.prec+rec , with/~ = 1", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Precision is the percentage of predicted chunks/relations that are actually correct, recall is the percentage of correct chunks/relations that are actually found. ", "mid_sen": "For convenient comparisons of only one value, we also list the FZ=i value (C.J.van Rijsbergen, 1979) : (Z2+l)'prec'rec /~2.prec+rec , with/~ = 1", "after_sen": ""}
{"citeStart": 99, "citeEnd": 116, "citeStartToken": 99, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering Rose et al., 1990) , in which the number of clusters s determined through a sequence of phase transitions by continuously increasing the parameter/? following an annealing schedule. The higher is fl, the more local is the influence of each noun on the definition of centroids. Distributional similarity plays here the role of distortion. When the scale parameter fl is close to zero, the similarity is almost irrelevant. All words contribute about equally to each centroid, and so the lowest average distortion solution involves just one cluster whose centroid is the average of all word distributions. As fl is slowly increased, a critical point is eventually reached for which the lowest F solution involves two distinct centroids. We say then that the original cluster has split into the two new clusters.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering Rose et al., 1990) , in which the number of clusters s determined through a sequence of phase transitions by continuously increasing the parameter/? ", "after_sen": "following an annealing schedule. "}
{"citeStart": 154, "citeEnd": 171, "citeStartToken": 154, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "We also intend to experiment with a full scale subcategorization dictionary acquired from the BNC. We believe this will address issues such as: (a) relations between frames and classes (what are the frames for which the semantic class is predicted most accurately) and (b) relations between verbs and classes (what are the verbs for which the semantic class is predicted most accurately). We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We believe this will address issues such as: (a) relations between frames and classes (what are the frames for which the semantic class is predicted most accurately) and (b) relations between verbs and classes (what are the verbs for which the semantic class is predicted most accurately). ", "mid_sen": "We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998) .", "after_sen": ""}
{"citeStart": 50, "citeEnd": 70, "citeStartToken": 50, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "The proof is almost identical to the one given by Chi and Geman (1998) . Let qa = p (derivation tree rooted in A fails to terminate). We will show that qs = 0 (i.e., derivation trees rooted in S always terminate). For each A E V, letf(A;T) be the number of nonroot instances of A in T. Given a E (V U T)*, let ai be the ith symbol of the sentential form a. For any A E V []", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The proof is almost identical to the one given by Chi and Geman (1998) . ", "after_sen": "Let qa = p (derivation tree rooted in A fails to terminate). "}
{"citeStart": 80, "citeEnd": 94, "citeStartToken": 80, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998) and Klein and Manning (2003) . In retrospect, however, there are perhaps even greater similarities to that of (Magerman, 1995; Henderson, 2003; Matsuzaki et al., 2005) . Consider the approach of Matsuzaki et al. (2005) . They posit a series of latent annotations for each nonterminal, and learn a grammar using an EM algorithm similar to the inside-outside algorithm. Their approach, however, requires the number of annotations to be specified ahead of time, and assigns the same number of annotations to each treebank nonterminal. We would like to infer the number of annotations for each nonterminal automatically.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998) and Klein and Manning (2003) . ", "mid_sen": "In retrospect, however, there are perhaps even greater similarities to that of (Magerman, 1995; Henderson, 2003; Matsuzaki et al., 2005) . ", "after_sen": "Consider the approach of Matsuzaki et al. (2005) . "}
{"citeStart": 29, "citeEnd": 43, "citeStartToken": 29, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized. We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each document in each corpus was preprocessed into individual sentences, lower-cased, and tokenized. ", "mid_sen": "We used an implementation of Brill's (1992) part-of-speech tagger to find adjectives and modifiers; for parsing, we used the Stanford dependency parser (Klein and Manning, 2003) .", "after_sen": ""}
{"citeStart": 54, "citeEnd": 55, "citeStartToken": 54, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "1 I)CGs on I, he empty string might be dismissed as extreme, computationM properties for DCGs, it is then necessary to impose certain restrictions on their form such as o[fline-parsability (OP), a nomenclature introduced by Pereira and Warren [11] , who define an OP DCG as a grammar whose context-free skeleton CFG is not infinitely ambiguous, and show that OP DCGs lead to a decidable parsing problem. 2 Our aim in this paper is to propose a simple transformation lbr an arbitrary OP DCG putting it into a form which leads to the completeness of the direct top-down interpretation by the standard Prolog interpreter: parsing is guaranteed to enumerate all solutions to the parsing problem and terminate. The e.xistence of such a transformation is kuown: in [1, 2] , we have recently introduced a \"Generalized Greibach Normal Form\" (GGNF) for DCGs, which leads to termination of top-down interpretation in the OP case. lIowever, the awdlable presentation of the GGNF transformation is rather complex (it involves an algebraic study of the fixpoints of certain equational systems representing grammars.). Our aim here is to present a related, but much simpler, transformation, which from a theoretical viewpoint performs somewhat less than the GGNF transformation (it; involves some encoding of the initial DCG, which the (~GNF does not, and it only handles oflline-parsable grammars, while the GGNF is defined for arbitrary DCGs), a but in practice is extremely easy to implement and displays a comparable behavior when parsing with an OP grammar.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2 Our aim in this paper is to propose a simple transformation lbr an arbitrary OP DCG putting it into a form which leads to the completeness of the direct top-down interpretation by the standard Prolog interpreter: parsing is guaranteed to enumerate all solutions to the parsing problem and terminate. ", "mid_sen": "The e.xistence of such a transformation is kuown: in [1, 2] , we have recently introduced a \"Generalized Greibach Normal Form\" (GGNF) for DCGs, which leads to termination of top-down interpretation in the OP case. ", "after_sen": "lIowever, the awdlable presentation of the GGNF transformation is rather complex (it involves an algebraic study of the fixpoints of certain equational systems representing grammars.). "}
{"citeStart": 148, "citeEnd": 161, "citeStartToken": 148, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993) , decision lists (Yarowsky, 1994) , and Bayesian hybrids (Golding, 1995) , have had varying degrees of success for the problem of context-sensitive spelling correction. However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic. Train Test  Most freq. Base  their, there, they're  3265  850  than, then  2096  514  its, it's  1364  366  your, you're  750  187  begin, being  559  146  passed, past  307  74  quiet, quite  264  66  weather, whether  239  61  accept, except  173  50  lead, led  173 Table 1 : Performance of the baseline method for 18 confusion sets. \"Train\" and \"Test\" give the number of occurrences of any word in the confusion set in the training and test corpora. \"Most freq.\" is the word in the confusion set that occurred most often in the training corpus. \"Base\" is the percentage of correct predictions of the baseline system on the test corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense.", "mid_sen": "Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993) , decision lists (Yarowsky, 1994) , and Bayesian hybrids (Golding, 1995) , have had varying degrees of success for the problem of context-sensitive spelling correction. ", "after_sen": "However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic. "}
{"citeStart": 46, "citeEnd": 71, "citeStartToken": 46, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "The first processing complement (el) of the head (H) has been displaced. This is problematic in case cl provides essential bindings for the successful evaluation of the complement c2. cl can not be evaluated prior to the head and once H is evaluated it is no longer possible to evaluate cl prior to c2. An example of problematic complement displacement taken from our test-grammar is given in figure 2 (see next page). The topicalized partial vP \"Anna lichen\" receives its restricting semantic information from the auxiliary verb and upon its evaluation provides essential bindings not only for the direct object, but also for the subject that stayed behind in the Mittelfeld together with the auxiliary verb. These mutual dependencies between the subconstituents of two different local trees lead either to the unrestricted generation of the partial vP, or to the unrestricted generation of the subject in the Mittelfeld. We handled this problem by partial execution (Pereira and Shieber, 1987) of the filler-head rule. This allows the evaluation of the filler right after the evaluation of the auxiliary verb, but prior to the subject. A head-driven generator has to rely on a similar solution, as it will not be able to find a successful ordering for the local trees either, simply because it does not exist.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These mutual dependencies between the subconstituents of two different local trees lead either to the unrestricted generation of the partial vP, or to the unrestricted generation of the subject in the Mittelfeld. ", "mid_sen": "We handled this problem by partial execution (Pereira and Shieber, 1987) of the filler-head rule. ", "after_sen": "This allows the evaluation of the filler right after the evaluation of the auxiliary verb, but prior to the subject. "}
{"citeStart": 204, "citeEnd": 216, "citeStartToken": 204, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "Collocations have been studied by computational linguists in different contexts. For instance, there is a substantial body of papers on the extraction of \"frequently co-occurring words\" from corpora using statistical methods (e.g., (Choueka et al., 1983) , (Church and Hanks, 1989) , (Smadja, 1993) to list only a few). These authors focus on techniques for providing material that can be used in other processing tasks such as", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Collocations have been studied by computational linguists in different contexts. ", "mid_sen": "For instance, there is a substantial body of papers on the extraction of \"frequently co-occurring words\" from corpora using statistical methods (e.g., (Choueka et al., 1983) , (Church and Hanks, 1989) , (Smadja, 1993) to list only a few). ", "after_sen": "These authors focus on techniques for providing material that can be used in other processing tasks such as"}
{"citeStart": 186, "citeEnd": 199, "citeStartToken": 186, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "The possibility of strict definition of each sense of a polysemous word, and the possibility of unambiguous assignment of a given sense in a given situation are, in themselves, nontrivial issues in philosophy (Quine 1960) and linguistics (Weinreich 1980; Cruse 1986) . Different dictionaries often disagree on the definitions; the split into senses may also depend on the task at hand. Thus, it is important to maintain the possibility of flexible distinction of the different senses, e.g., by letting this distinction be determined by an external knowledge source such as a thesaurus or a dictionary. Although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility. For example, defining the senses by the possible translations of the word (Dagan, Itai and Schwall 1991; Brown et al. 1991; Gale, Church, and Yarowsky 1992) , by the Roget's categories (Yarowsky 1992) , or by clustering (Schi~tze 1992) , yields a grouping that does not always conform to the desired sense distinctions. In comparison to these approaches, our reliance on the MRD for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions. Specifically, the user of our system may choose a certain dictionary definition, a combination of definitions from several dictionaries, or manually listed seed words for every sense that needs to be defined. Whereas pure MRD-based methods allow the same flexibility, their potential so far has not been fully tapped, because definitions alone do not contain enough information for disambiguation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility. ", "mid_sen": "For example, defining the senses by the possible translations of the word (Dagan, Itai and Schwall 1991; Brown et al. 1991; Gale, Church, and Yarowsky 1992) , by the Roget's categories (Yarowsky 1992) , or by clustering (Schi~tze 1992) , yields a grouping that does not always conform to the desired sense distinctions. ", "after_sen": "In comparison to these approaches, our reliance on the MRD for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions. "}
{"citeStart": 24, "citeEnd": 44, "citeStartToken": 24, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "The perfect is analyzed by using the notion of a nucleus (Moens and Steedman, 1988) to account for the inner structure of an eventuality. A nucleus is defined as a structure containing a preparatory process, culmination and consequent state. The categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to. The perfect is seen in (Kamp and Reyle, 1993) as an aspectual operator. The eventualities described by the perfect of a verb refer to the consequent state of its nucleus. For example, the following sentence 14 denotes the state, s, holding at the present, that Mary has met the president. This state is a result of the event e, in which Mary met the president.\" Temporally, the state s starts just when e ends, or as it is put in (Kamp and Reyle, 1993 ):e and s abut, (represented as e DCs). 14Mary has met the president.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The categorization of verb phrases into different aspectual classes can be phrased in terms of which part of the nucleus they refer to. ", "mid_sen": "The perfect is seen in (Kamp and Reyle, 1993) as an aspectual operator. ", "after_sen": "The eventualities described by the perfect of a verb refer to the consequent state of its nucleus. "}
{"citeStart": 213, "citeEnd": 236, "citeStartToken": 213, "citeEndToken": 236, "sectionName": "UNKNOWN SECTION NAME", "string": "When constructing a l,exieal Knowledge Ilase (1,KB) useful for Natural l,anguage Processing, the source of information from which knowledge is acquired and the structuring of this information within the LKB are two key issues. Machine Readable Dictionaries (MIH)s) are a good sour(:e of lexical information and have been shown to be al)plical)le to the task of I,KII COllStruction (l)ola.n ct al., 1993; Calzolari, t992; Copestake, [990; Wilks et al., 1989; Byrd et al., 1987) . Often though, a localist approaeh is adopted whereby the words are kept in alphabetical order with some representation of their definitions in the form of a template or feature structure. F, flbrt in findlug cormections between words is seen in work on automatic extraction of sem~mtic relations Dora MRI)s (Ahlswede and Evens, 1988; Alshawi, 1989; Montemagrfi and Vandorwende, 19!32) . Additionally, effort in finding words that are close semantically is seen by the current interest in statistical techniques for word clustering, looking at (-ooccurrences of words in text corpora or dictionaries (Church and IIanks, 1989; Wilks et al., 1989; Brown et al., 11992; l'ereira et al., 11995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "F, flbrt in findlug cormections between words is seen in work on automatic extraction of sem~mtic relations Dora MRI)s (Ahlswede and Evens, 1988; Alshawi, 1989; Montemagrfi and Vandorwende, 19!32) . ", "mid_sen": "Additionally, effort in finding words that are close semantically is seen by the current interest in statistical techniques for word clustering, looking at (-ooccurrences of words in text corpora or dictionaries (Church and IIanks, 1989; Wilks et al., 1989; Brown et al., 11992; l'ereira et al., 11995) .", "after_sen": "Inspired by research in the. areas of semantic relations, semantic distance, concept clustering, and using (,once I tual (Ji a l hs (Sowa, 1984) as our knowledge representation, we introduce (;oncept (?lustering I{nowledge Graphs (CCKGs). "}
{"citeStart": 48, "citeEnd": 67, "citeStartToken": 48, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, \"bank\" often occurs in the sentences related to the economy topic when translated into \"yínháng\", and occurs in the sentences related to the geography topic when translated to \"héàn\". Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if \"bank\" occurs more often in the sentences related to the economy topic than the ones related to the geography topic, it is more likely that \"bank\" is translated to \"yínháng\" than to \"héàn\". With the out-of-domain bilingual corpus, we first incorporate the topic information into translation probability estimation, aiming to quantify the effect of the topical context information on translation selection. Then, we rescore all phrase pairs according to the phrasetopic and the word-topic posterior distributions of the additional in-domain monolingual corpora. As compared to the previous works, our method takes advantage of both the in-domain monolingual corpora and the out-of-domain bilingual corpus to incorporate the topic information into our translation model, thus breaking down the corpus barrier for translation quality improvement. The experimental results on the NIST data set demonstrate the effectiveness of our method.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain monolingual corpora. ", "mid_sen": "Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. ", "after_sen": "For example, \"bank\" often occurs in the sentences related to the economy topic when translated into \"yínháng\", and occurs in the sentences related to the geography topic when translated to \"héàn\". "}
{"citeStart": 28, "citeEnd": 54, "citeStartToken": 28, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "Self-supervised boosting was presented as a general method for density estimation, and was not tested in the context of language modeling. Rather, Welling at al. demonstrated its effectiveness in modeling hand-written digits and on synthetic data. Đn both cases essentially linear classifiers were used as features. As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. Unfortunately, as shown in (Okanohara and Tsujii, 2007) , with the represetation of sentences that we use, linear classifiers cannot discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non-linear large-margin classifier (see section 3 for details). While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. This means that MCMC techniques like Gibbs sampling quickly become intractable, even for small corpora, as they require performing very large numbers of classifications. For this reason we use a different sampling scheme which we refer to as rejection sampling. This allows us to sample from the true model distribution while requiring a drastically smaller number of classifications, as long as the current model isn't too far removed from the baseline.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As these are computationally very efficient, the authors could use a variant of Gibbs sampling for generating negative samples. ", "mid_sen": "Unfortunately, as shown in (Okanohara and Tsujii, 2007) , with the represetation of sentences that we use, linear classifiers cannot discriminate real sentences from sentences sampled from a trigram, which is the model we use as a baseline, so here we resort to a non-linear large-margin classifier (see section 3 for details). ", "after_sen": "While large-margin classifiers consistently out-perform other learning algorithms in many NLP tasks, their non-linear variations are also notoriously slow when it comes to computing their decision function -taking time that can be linear in the size of their training data. "}
{"citeStart": 177, "citeEnd": 197, "citeStartToken": 177, "citeEndToken": 197, "sectionName": "UNKNOWN SECTION NAME", "string": "The last point may be seen better if we look at some differences between our system and KRYPTON, which also distinguishes between an object theory and background knowledge (cf. Brachman et al. 1985) . KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL. However, the distinction between the two parts is purely functional--that is, characterized in terms of the system's behavior. From the logical point of view, the knowledge base is the union of the two boxes, i.e. a theory, and the entailment is standard. In our system, we also distinguish between the \"definitional\" and factual information, but the \"definitional\" part contains collections of mutually excluding theories, not just of formulas describing a semantic network. Moreover, in addition to proposing this structure of R, we have described the two mechanisms for exploiting it, \"coherence\" and \"dominance,\" which are not variants of the standard first order entailment, but abduction.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Rather, R must be treated as a separate logical level for these syntactic reasons, and because of its function--being a pool of possibly conflicting semantic constraints.", "mid_sen": "The last point may be seen better if we look at some differences between our system and KRYPTON, which also distinguishes between an object theory and background knowledge (cf. Brachman et al. 1985) . ", "after_sen": "KRYPTON's A-box, encoding the object theory as a set of assertions, uses standard first order logic; the T-box contains information expressed in a frame-based language equivalent to a fragment of FOL. "}
{"citeStart": 8, "citeEnd": 26, "citeStartToken": 8, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we made an attempt to solve the difficult problem of identifying word translations on the basis of a single monolingual cor-pus, whereby the same corpus is used for several language pairs. The basic idea underlying our work is to look at foreign words, to compute their co-occurrence-based associations, and to consider these as translations of the respective words. Whereas Rapp & Zock (2010) dealt only with an English corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora. We were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair. For example, it is more promising to look at occurrences of English words in a German corpus rather than the other way around. Because of the special status of English it is also advisable to use it as a pivot wherever possible.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The basic idea underlying our work is to look at foreign words, to compute their co-occurrence-based associations, and to consider these as translations of the respective words. ", "mid_sen": "Whereas Rapp & Zock (2010) dealt only with an English corpus, the current work shows that this methodology is applicable to a wide range of languages and corpora. ", "after_sen": "We were able to shed some light on criteria influencing performance, such as the selection of text type and the direction of a language pair. "}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "Not all dialectometrists agree on the wisdom of delineating dialect areas. Sdguy (1973:18) insisted that the concept of dialect boundaries was meaningless, and his emphasis on the gradience of language similarity has been widely maintained. Bu't those who do look for firna dialect affiliations (such as Babitch and Ebobisse) use bottom-up agglomerative techniques. The two linguistically closest sites are grouped into one dialect, and thenceforth treated as a unit. The process continues recursively until all sites are grouped into one superdialect embracing the entire language area under consideration. This yields a binary tree. But Kaufman and Rousseeuw (1990:44) suggest that when the emphasis in a clustering problem is on the top-level clusters--here, finding the two main dialects--then such bottom-up methods, which can potentially introduce error at each of several steps, are less reliable than top-down partitioning methods. Perhaps past researchers have used inferior bottom-up techniques simply because the necessary algorithms are computationally more tractable. Comparing all possible pairs of sites is a O(N 2) problem, 1 whereas considering all possible two-way partitions of tile dialect area is 0(2 N).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Not all dialectometrists agree on the wisdom of delineating dialect areas. ", "mid_sen": "Sdguy (1973:18) insisted that the concept of dialect boundaries was meaningless, and his emphasis on the gradience of language similarity has been widely maintained. ", "after_sen": "Bu't those who do look for firna dialect affiliations (such as Babitch and Ebobisse) use bottom-up agglomerative techniques. "}
{"citeStart": 46, "citeEnd": 59, "citeStartToken": 46, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "Since ensemble models are known to obtain a better performance than their constituent models, we compared the performance of a stacking ensemble against its individual constituent models. We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers. We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier. Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set. The ensemble did not result in any significant improvement (<1%) compared to the best model amongst the three of its individual components (SMO). The ensemble's performance on the test set was poor compared to the best classification model. ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We trained three classification models on the entire feature set, using the same train-test sets as explained before and trained an ensemble model with three classifiers. ", "mid_sen": "We used the StackingC implementation of WEKA (Seewald, 2002) to combine the models, with a linear regression model as our meta classifier. ", "after_sen": "Table 4 shows the classification accuracies for the individual classifiers as well as the ensemble on a 10-fold CV of the training set and on the held out test set. "}
{"citeStart": 0, "citeEnd": 25, "citeStartToken": 0, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "In the first experiment described in this section, the task is to segment the sentence into chunks and to assign labels to these chunks. This process of chunking and labeling is carried out by assigning a tag to each word in a sentence leftto-right. Ramshaw and Marcus (1995) first assigned a chunk tag to each word in the sentence: I for inside a chunk, O for outside a chunk, and type precision B tbr inside a chunk, but tile preceding word is in another chunk. As we want to find more than one kind of chunk, we have to [hrther differentiate tile IOB tags as to which kind of chunk (NP, VP, Prep, ADJP or ADVP) the word is ill. With the extended IOB tag set at hand we can tag the sentence: chlmks, we collapse Preps and NPs to PPs in a second step. While the GR assigner finds relations between VPs and other chunks (cf. Section 3.2), the PP chunker finds relations between prepositions and NPs 2 in a way similar to GR assignment (see Section 3.2). In the last chunking/labeling step, we assign adverbial functions to chunks. The classes are the adverbial function labels from the treebank: LOC (locative), TMP (temporal), DIR (directional), PRP (purpose and reason), MNR (manner), EXT (extension) or \"-\" for none of the fornmr. Table 1 gives an overview of the results of the chunking-labeling experiments, using the following algorithms, determined by validation on the train set: IBi-IG for XP-chunking and IGTree for PP-chunking and ADVFUNCs assignment.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This process of chunking and labeling is carried out by assigning a tag to each word in a sentence leftto-right. ", "mid_sen": "Ramshaw and Marcus (1995) first assigned a chunk tag to each word in the sentence: I for inside a chunk, O for outside a chunk, and type precision B tbr inside a chunk, but tile preceding word is in another chunk. ", "after_sen": "As we want to find more than one kind of chunk, we have to [hrther differentiate tile IOB tags as to which kind of chunk (NP, VP, Prep, ADJP or ADVP) the word is ill. "}
{"citeStart": 78, "citeEnd": 103, "citeStartToken": 78, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "The crucial advantage of PCFGs over CFGs is that they can be trained and/or learned from corpora. Readers for whom this fact is unfamiliar are referred to Charniak's textbook (Charniak, 1993, Chapter 7) . We do not have space to recapitulate the discussion of training which can be found there. We do however illustrate the outcome of training.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The crucial advantage of PCFGs over CFGs is that they can be trained and/or learned from corpora. ", "mid_sen": "Readers for whom this fact is unfamiliar are referred to Charniak's textbook (Charniak, 1993, Chapter 7) . ", "after_sen": "We do not have space to recapitulate the discussion of training which can be found there. "}
{"citeStart": 49, "citeEnd": 72, "citeStartToken": 49, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "one of two ways. The syntactic VP could be copied down with its corresponding semantics, from which the semantics for the complete sentence can be derived. In this case, the anaphoric expression is constrained to have the same semantics as the copied constituent. Alternatively, the anaphoric expression could be resolved purely semantically, resulting in the discharge of the anaphoric assumption P. The higher-order unification method developed by Dalrymple et al. (1991) could be used for this purpose; in this case the sentence-level semantics is recovered without copying any syntactic representations.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Alternatively, the anaphoric expression could be resolved purely semantically, resulting in the discharge of the anaphoric assumption P. ", "mid_sen": "The higher-order unification method developed by Dalrymple et al. (1991) could be used for this purpose; in this case the sentence-level semantics is recovered without copying any syntactic representations.", "after_sen": "Event referential forms such as do it, do tha~, and do so constitute full verb phrases in the syntax. "}
{"citeStart": 110, "citeEnd": 126, "citeStartToken": 110, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "We take a model-based approach to measure to what degree, if any, two document collections are different. A document is represented as a point in a V -dimensional space, where V is vocabulary size. Each coordinate is the frequency of a word in a document, i.e., term frequency. Although vector representation, commonly known as a bag of words, is oversimplified and ignores rich syntactic and semantic structures, more sophisticated representation requires more data to obtain reliable models. Practically, bag-of-word representation has been very effective in many tasks, including text categorization (Sebastiani, 2002) and information retrieval (Lewis, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although vector representation, commonly known as a bag of words, is oversimplified and ignores rich syntactic and semantic structures, more sophisticated representation requires more data to obtain reliable models. ", "mid_sen": "Practically, bag-of-word representation has been very effective in many tasks, including text categorization (Sebastiani, 2002) and information retrieval (Lewis, 1998) .", "after_sen": "We assume that a collection of N documents, y 1 , y 2 , . . . , y N are sampled from the following process,"}
{"citeStart": 62, "citeEnd": 91, "citeStartToken": 62, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Provided with the candidate fragment elements, we previously (Wang and Callison-Burch, 2011) used a chunker 3 to finalize the output fragments, in order to follow the linguistic definition of a (para-) phrase. We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec. 5.3). Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All the word alignments (excluding stop-words) with positive scores are selected as candidate fragment elements.", "mid_sen": "Provided with the candidate fragment elements, we previously (Wang and Callison-Burch, 2011) used a chunker 3 to finalize the output fragments, in order to follow the linguistic definition of a (para-) phrase. ", "after_sen": "We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec. 5.3). "}
{"citeStart": 34, "citeEnd": 56, "citeStartToken": 34, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001 ), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ). This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Figure 1 depicts a brief sketch of the RenTAL system. The system consists of the following four modules: Tree converter, Type hierarchy extractor, Lexicon converter and Derivation translator. The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse (Tateisi et al., 1998) . However, their method depended on translator's intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. ", "mid_sen": "Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. ", "after_sen": "However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. "}
{"citeStart": 100, "citeEnd": 123, "citeStartToken": 100, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "For the generation of word features, we only consider one dominant analysis for any surface word for simplicity. In case of ambiguity, we considered only the first (arbitrary) analysis for Russian. For Arabic, we apply the following heuristic: use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank (Maamouri et al., 2005) ; if a word does not appear in the treebank, we choose the first analysis returned by the Buckwalter analyzer. Ideally, the best word analysis should be provided as a result of contextual disambiguation (e.g., (Habash and Rambow, 2005) ); we leave this for future work.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For Arabic, we apply the following heuristic: use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank (Maamouri et al., 2005) ; if a word does not appear in the treebank, we choose the first analysis returned by the Buckwalter analyzer. ", "mid_sen": "Ideally, the best word analysis should be provided as a result of contextual disambiguation (e.g., (Habash and Rambow, 2005) ); we leave this for future work.", "after_sen": ""}
{"citeStart": 290, "citeEnd": 312, "citeStartToken": 290, "citeEndToken": 312, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we propose a new space and a new metric for computing this distance. Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. Given a corpus of questionanswer pairs (Q, S A ), we can train a probabilistic model for estimating the conditional probability P(Q | S A ). Once the parameters of this model are learned, given a question Q and the set of sentences Σ returned by an IR engine, one can find the sentence S i ∈ Σ and an answer in it A i,j by searching for the S i,A i,j that maximizes the conditional probability P(Q | S i,A i,j ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper, we propose a new space and a new metric for computing this distance. ", "mid_sen": "Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997) , part of speech tagging (Church, 1988) , machine translation (Brown et al., 1993) , information retrieval (Berger and Lafferty, 1999) , and text summarization (Knight and Marcu, 2002) , we develop a noisy channel model for QA. ", "after_sen": "This model explains how a given sentence S A that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations. "}
{"citeStart": 191, "citeEnd": 211, "citeStartToken": 191, "citeEndToken": 211, "sectionName": "UNKNOWN SECTION NAME", "string": "The HPSG grammar developed with MicroCUF models a fragment of German. Since our focus is on the lexicon, the range of syntactic variation treated is currently limited to simplex sentences with canonical word order. We have incorporated some recent developments of HPSG, esp. the revisions of Pollard & Sag (1994, ch. 9) , Manning & Sag (1995) 's proposal for an independent level of argument structure and Bouma (1997) 's use of argument structure to eliminate procedural lexical rules in favour of relational constraints. Our elaborate ontology of semantic types -useful for non-trivial acquisition of selectional restrictions and nominal sorts -was derived from a systematic corpus study of a biological domain (Knodel 1980, 154-188) . The grammar also covers all valence classes encountered in the corpus. As for the lexicon format, we currently list full forms only. Clearly, a morphology component would supply more contextual information from known affixes but would still require the processing of unknown stems.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have incorporated some recent developments of HPSG, esp. the revisions of Pollard & Sag (1994, ch. 9) , Manning & Sag (1995) 's proposal for an independent level of argument structure and Bouma (1997) 's use of argument structure to eliminate procedural lexical rules in favour of relational constraints. ", "mid_sen": "Our elaborate ontology of semantic types -useful for non-trivial acquisition of selectional restrictions and nominal sorts -was derived from a systematic corpus study of a biological domain (Knodel 1980, 154-188) . ", "after_sen": "The grammar also covers all valence classes encountered in the corpus. "}
{"citeStart": 115, "citeEnd": 139, "citeStartToken": 115, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar extraction algorithm Systemic Functional Grammar (SFG) (Halliday, 1985) is based on the assumption that the differentiation of syntactic phenomena is always deter-mined by its function in the communicative context. This functional orientation has lead to the creation of detailed linguistic resources that are characterized by an integrated treatment of content-related, textual and pragmatic aspects. Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects--such as, for example, PENMAN (Mann, 1983) , COMMUNAL (Fawcett and Tucker, 1990) , TECHDOC (KSsner and Stede, 1994) , Drafter (Paris and Vander Linden , 1996) , and Gist (Not and Stock, 1994) . For our present purposes, however, it is the formal characteristics of systemic grammar and its implementations that are more important. Systemic grammar assumes multifunctional constituent structuresrepresentable as feature structures with coreferences. As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. ", "mid_sen": "The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):", "after_sen": "entry = type_l I type_2 I ... I type_n."}
{"citeStart": 65, "citeEnd": 77, "citeStartToken": 65, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "In this context, we characterize an agreement as accepting a partner's suggestion to include a specific furniture item in the solution. In this paper we will focus on the issue of recognizing that a suggestion has been made (i.e. a proposal). The problem is not easy, since, as speech act theory points out (Austin, 1962; Searle, 1975) [35] is the first mention of a sofa in the conversa-", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we will focus on the issue of recognizing that a suggestion has been made (i.e. a proposal). ", "mid_sen": "The problem is not easy, since, as speech act theory points out (Austin, 1962; Searle, 1975) [35] is the first mention of a sofa in the conversa-", "after_sen": "x Participants work in separate rooms and communicate via the computer interface. "}
{"citeStart": 135, "citeEnd": 149, "citeStartToken": 135, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "Some of the problelns with eategorial granlmar accounts of coordination do reoccur with a dynamic acconnt based on the parser used ill Milward (1992a Fred won the seholarship This second batch of examples is particularly dillicult to exclude withont malting changes to the eharacterisation of the states. A feature I>lus or lninns tensed vel% on each conjlmet does block them, but is difl-icnlt to motivate. I)ynanlie grammars can be regarded Imrely as forreal systems, as direct representations of proeessing, or as something inbetween (for example, ill the packed l°l)ependency grammar does not have VP modifiers", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This lack of distinction as to whethel: one or lnore modilier is expected is actually a necessary prereqnisite ['or performing decidable fl,lly word-by-word incremental interpretation (see Milward and Cooper, 1994 , in these proceedings).", "mid_sen": "Some of the problelns with eategorial granlmar accounts of coordination do reoccur with a dynamic acconnt based on the parser used ill Milward (1992a Fred won the seholarship This second batch of examples is particularly dillicult to exclude withont malting changes to the eharacterisation of the states. ", "after_sen": "A feature I>lus or lninns tensed vel% on each conjlmet does block them, but is difl-icnlt to motivate. "}
{"citeStart": 1, "citeEnd": 24, "citeStartToken": 1, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Three papers mention having used the memorybased learning method IBI-IG. (Veenstra, 1998) introduced cascaded chunking, a two-stage process in which the first stage classifications are used to improve the performance in a second processing stage. This approach reaches the same performance level as Argamon et al. but it requires lexical information. (Daelemans et al., 1999a ) report a good performance for baseNP recognition but they use a different data set and do not mention precision and recall rates. (Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. Their baseNP results are slightly better than those of Ramshaw and Marcus (F~=1=92.37).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "but it requires lexical information. ", "mid_sen": "(Daelemans et al., 1999a ) report a good performance for baseNP recognition but they use a different data set and do not mention precision and recall rates. ", "after_sen": "(Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. "}
{"citeStart": 188, "citeEnd": 205, "citeStartToken": 188, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "Mention detection. Many ACE participants have also adopted a corpus-based approach to SC determination that is investigated as part of the mention detection (MD) task (e.g., Florian et al. (2006) ). Briefly, the goal of MD is to identify the boundary of a mention, its mention type (e.g., pronoun, name), and its semantic type (e.g., person, location). Unlike them, (1) we do not perform the full MD task, as our goal is to investigate the role of SC knowledge in coreference resolution; and (2) we do not use the ACE training data for acquiring our SC classifier; instead, we use the BBN Entity Type Corpus (Weischedel and Brunstein, 2005) , which consists of all the Penn Treebank Wall Street Journal articles with the ACE mentions manually identified and annotated with their SCs. This provides us with a training set that is approximately five times bigger than that of ACE. More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004) ); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This provides us with a training set that is approximately five times bigger than that of ACE. ", "mid_sen": "More importantly, the ACE participants do not evaluate the role of induced SC knowledge in coreference resolution: many of them evaluate coreference performance on perfect mentions (e.g., Luo et al. (2004) ); and for those that do report performance on automatically extracted mentions, they do not explain whether or how the induced SC information is used in their coreference algorithms.", "after_sen": "Joint probabilistic models of coreference. "}
{"citeStart": 7, "citeEnd": 31, "citeStartToken": 7, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use Lib-SVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our SVM model uses a linear kernel. ", "mid_sen": "We use Weka (Hall et al., 2009) for logistic regression classification. ", "after_sen": "We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. "}
{"citeStart": 81, "citeEnd": 104, "citeStartToken": 81, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "The task is illustrated in example 1: Example 1 The parses assigned to these two compounds differ, even though the sequence of parts of speech are identical. The problem is analogous to the prepositional phrase attachment task explored in Hindle and Rooth (1993) . The approach they propose involves computing lexical associations from a corpus and using these to select the correct parse. A similar architecture may be applied to noun compounds.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The task is illustrated in example 1: Example 1 The parses assigned to these two compounds differ, even though the sequence of parts of speech are identical. ", "mid_sen": "The problem is analogous to the prepositional phrase attachment task explored in Hindle and Rooth (1993) . ", "after_sen": "The approach they propose involves computing lexical associations from a corpus and using these to select the correct parse. "}
{"citeStart": 44, "citeEnd": 65, "citeStartToken": 44, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments-those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004) -inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006) , which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For some restricted combinatorial spaces of alignments-those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004) -inference can be accomplished using polynomial time dynamic programs. ", "mid_sen": "However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006) , which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. ", "after_sen": "Indeed, Marcu and Wong (2002) conjectures that none exist. "}
{"citeStart": 100, "citeEnd": 113, "citeStartToken": 100, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach to lexicalized stochastic modeling is based on the parametric family of loglinear probability models, which i s u s e d t o d ene a probability distribution on the parses of a Lexical-Functional Grammar (LFG) for German. In previous work on log-linear models for LFG b y Johnson et al. (1999) , pseudo-likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999) . We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. In our largest experiment, we used 250,000 parses which were produced by parsing 36,000 newspaper sentences with the German LFG. Experimental evaluation of our models on an exact-match task (i.e. percentage of exact match of most probable parse with correct parse) on 550 manually examined examples with on average 5.4 analyses gave 86% precision. Another evaluation on a verb frame recognition task (i.e. percentage of agreement between subcategorization frames of main verb of most probable parse and correct parse) gave 90% precision on 375 manually disambiguated examples with an average ambiguity of 25. Clearly, a direct comparison of these results to stateof-the-art statistical parsers cannot bemade because of di erent training and test data and other evaluation measures. However, we w ould like to draw the following conclusions from our experiments:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, to our knowledge, to date no large LFG annotated corpora of unrestricted German text are available. ", "mid_sen": "Fortunately, algorithms exist for statistical inference of log-linear models from unannotated data (Riezler, 1999) . ", "after_sen": "We apply this algorithm to estimate log-linear LFG models from large corpora of newspaper text. "}
{"citeStart": 91, "citeEnd": 115, "citeStartToken": 91, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "This section briefly explains the compound word translation method we previously proposed (Fujii and Ishikawa, 1999) . This method translates input compound words on a word-byword basis, maintaining the word order in the source language 2. The formula for the source compound word and one translation candidate are represented as below. 2A preliminary study showed that approximately 95% of compound technical terms defined in a bilingual dictionary maintain the same word order in both source and target languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "This section briefly explains the compound word translation method we previously proposed (Fujii and Ishikawa, 1999) . ", "after_sen": "This method translates input compound words on a word-byword basis, maintaining the word order in the source language 2. "}
{"citeStart": 164, "citeEnd": 176, "citeStartToken": 164, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "We regard Paradigme as a field for the interaction between text and episodes in memory --the interaction between what one is hearing or reading and what one knows [Schank, 1990] . The meaning of words, sentences, or even texts can be projected in a uniform way on Paradigme, as we have seen in Section 4 and 5. Similarly, we can project text and episodes, and recall the most relevant episode for interpretation of the text.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This enables us to measure the similarity between texts as a syntactic process, not as word lists.", "mid_sen": "We regard Paradigme as a field for the interaction between text and episodes in memory --the interaction between what one is hearing or reading and what one knows [Schank, 1990] . ", "after_sen": "The meaning of words, sentences, or even texts can be projected in a uniform way on Paradigme, as we have seen in Section 4 and 5. "}
{"citeStart": 76, "citeEnd": 100, "citeStartToken": 76, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2 . Using these weights, we run Hiero's decoder to perform the actual translation of the MT 2003 test sentences and obtained a BLEU score of 29.73, as shown in the row Hiero of Table 1. This is higher than the score of 28.77 reported in (Chiang, 2005) , perhaps due to differences in word segmentation, etc. Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005 ), the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is higher than the score of 28.77 reported in (Chiang, 2005) , perhaps due to differences in word segmentation, etc. ", "mid_sen": "Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005 ), the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve.", "after_sen": ""}
{"citeStart": 131, "citeEnd": 132, "citeStartToken": 131, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "In th.e Japanese language, the causative ~nd the change of voice a.re realized by agglutinations of those auxiliary verbs at the tail of current verbs. These auxiliary verbs as well a.s ordinary verbs can dominate some cases so that these agglutinations may change the whole syntax [9] . Namely the scope of the operation of these auxiliary verbs is not the operated verb but the whole sentence. In order to illustrate these role changes, we show the ~dteruation of the agent of tlhe main verb in Table l with a short tip to Japanese lexicon.As an example, we will take the sentence:Kcn-wa Naomi-hi h.on-wo yom-ascru. (Ken makes Naomi read the book.)First, we give I)AG's for each lexical items in l)'ig 2. The last DAG in Fig. 2 represents that the verb 'yomu (rex(l)' requires two roles 'the re~der' and 'the object to be rend', and one optional ro]e %he counter-agent' who hears what the reader reads. In that tigure, 'W I =' means that each word is recognized in the general world however a verb 'yomu' in trod uced a special membrane sl as a subworld of W. Each DAG means a polymorphic type of the lexical item.Assume that there is a. parser that constructs partial tree. st:ructures, as recognizing each word from the head sequentially. Then, when the first :fbur words are recognized, they can form a con> plete sentence of (3).I= {read (lClo~, NIo, 2, Vlo~) Because all th.e three nouns are adequately coneateuated by 'read.', a sentential representatio:n is m.ade in the subworld of .st. in (3), Oi's a.re the records of unification, thaJ; contain the costs and the original types; they becom.e necessary when they are backtracked, and in that meaning, those bindings are transitive. Now, let us recapitulate what ihas occurred in the membrane, sl. \"['here were four lexical items in the set, a:nd they are duly organized to a sentence and sl becomes a singM;on.. sl = {K:N, N:N, B:N, ,\\zyz.rcad(x, y, z) Then, the problematic final word '-aseru (causative)' arrives; its ])AG representation is as in Fig. 3 . The DAG in Pig. 3 requires a sententiaJ lbrm (category ,S') as an argulnent;, and in addition, it subcategorizes an item of category N as an agent of l;he su bsentence. Now, the process becomes ~s in Pig. d. All through the process in Pig. 4, C-and Bcombinators are used repeatedly as well as ordinary type inference (l) and (2). The second men:,brane s2 requires an agent role (the variable x' of 'make). There is a record in 0t that it bit agent, so that the comparison should be marie between 01 and 04(= O(~r,j)). llowever, because both of 0t and 04 unifies nominative case and agent role, the costs are. equivalent. In such a. case, linguistic heuristics will solve the problenl. In this cas G the agent of make shouJd be the nominative of the whole sentence, and the co-agent o[' make is (;he datiw~ of tlhe whole sentence, so that K and N are bit by newly a.):rive(I utake, t1 remains bound to rcad, because there is no A-variable of that type iu ?)ta,~e. The process is depicte.d in fig. 5 . ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In th.e Japanese language, the causative ~nd the change of voice a.re realized by agglutinations of those auxiliary verbs at the tail of current verbs. ", "mid_sen": "These auxiliary verbs as well a.s ordinary verbs can dominate some cases so that these agglutinations may change the whole syntax [9] . ", "after_sen": "Namely the scope of the operation of these auxiliary verbs is not the operated verb but the whole sentence. "}
{"citeStart": 56, "citeEnd": 88, "citeStartToken": 56, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993) , decision lists (Yarowsky, 1994) , and Bayesian hybrids (Golding, 1995) , have had varying degrees of success for the problem of context-sensitive spelling correction. However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic. Train Test  Most freq. Base  their, there, they're  3265  850  than, then  2096  514  its, it's  1364  366  your, you're  750  187  begin, being  559  146  passed, past  307  74  quiet, quite  264  66  weather, whether  239  61  accept, except  173  50  lead, led  173 Table 1 : Performance of the baseline method for 18 confusion sets. \"Train\" and \"Test\" give the number of occurrences of any word in the confusion set in the training and test corpora. \"Most freq.\" is the word in the confusion set that occurred most often in the training corpus. \"Base\" is the percentage of correct predictions of the baseline system on the test corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense.", "mid_sen": "Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993) , decision lists (Yarowsky, 1994) , and Bayesian hybrids (Golding, 1995) , have had varying degrees of success for the problem of context-sensitive spelling correction. ", "after_sen": "However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic. "}
{"citeStart": 25, "citeEnd": 39, "citeStartToken": 25, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "To ensure that no information is lost as a result of applying a lexical rule, it seems to be necessary to split up the lexical rule to make each instance deal with a specific case. In the above example, this would result in two lexical rules: one for words with tl as their c value and one for those with t2 as their c value. In the latter case, we can also take care of transferring the value of z. However, as discussed by Meurers (1994) , creating several instances of lexical rules can be avoided. Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. This is accomplished by having each lexical rule predicate call a so-called frame predicate, which can have multiple defining clauses. So for the lexical rule 1, the frame specification is taken care of by extending the predicate in Figure 6 with a call to a frame predicate, as shown in Figure 8 .17", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the latter case, we can also take care of transferring the value of z. ", "mid_sen": "However, as discussed by Meurers (1994) , creating several instances of lexical rules can be avoided. ", "after_sen": "Instead, the disjunctive possibilities introduced by the frame specification are attached as a constraint to a lexical rule. "}
{"citeStart": 90, "citeEnd": 104, "citeStartToken": 90, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "The amount of web documents is increasing in a very fast way in the last years, what makes more and more complicated its organization. For this reason, web page classification has gained importance as a task to ease and improve information access. Web page classification can be defined as the task of labeling and organizing web documents within a set of predefined categories. In this work, we focus on web page classification based on Support Vector Machines (SVM, (Joachims, 1998) ). This kind of classification tasks rely on a previously labeled training set of documents, with which the classifier acquires the required ability to classify new unknown documents.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Web page classification can be defined as the task of labeling and organizing web documents within a set of predefined categories. ", "mid_sen": "In this work, we focus on web page classification based on Support Vector Machines (SVM, (Joachims, 1998) ). ", "after_sen": "This kind of classification tasks rely on a previously labeled training set of documents, with which the classifier acquires the required ability to classify new unknown documents."}
{"citeStart": 94, "citeEnd": 111, "citeStartToken": 94, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008) . The citation summary data from the ACL Anthology Network 1 (Radev et al., 2009) was used. This dataset is rather large (8736 citations) and since manual annotation of context for each citation is a time consuming task, a subset of 20 papers were selected corresponding to approximately 20% of the original dataset.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We chose the dataset used by Athar (2011) comprising 310 papers taken from the ACL Anthology (Bird et al., 2008) . ", "after_sen": "The citation summary data from the ACL Anthology Network 1 (Radev et al., 2009) was used. "}
{"citeStart": 116, "citeEnd": 139, "citeStartToken": 116, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "This functionality has been used in van Noord and Gerdemann (1999) to provide an implementation of the algorithm in Mohri and Sproat (1996) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In such cases, Prolog hooks for manipulating states and transitions may be used.", "mid_sen": "This functionality has been used in van Noord and Gerdemann (1999) to provide an implementation of the algorithm in Mohri and Sproat (1996) .", "after_sen": ""}
{"citeStart": 83, "citeEnd": 94, "citeStartToken": 83, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "It is widely acknowledged that a full account of natural language utterances cannot be given in terms of only syntactic or semantic phenomena. For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990) . To understand con-versationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994) . Some presuppositions are inferrable when certain lexical constructs (factives, aspectuals, etc) or syntactic constructs (cleft and pseudo-cleft sentences) are used. Despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be defeated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, Hirschberg (1985) has shown that in order to understand a scalar implicature, one must analyze the conversants' beliefs and intentions. ", "mid_sen": "To recognize normal state implicatures one must consider mutual beliefs and plans (Green, 1990) . ", "after_sen": "To understand con-versationM implicatures associated with indirect replies one must consider discourse expectations, discourse plans, and discourse relations (Green, 1992; Green and Carberry, 1994) . "}
{"citeStart": 8, "citeEnd": 19, "citeStartToken": 8, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "Ripper (Cohen, 1995) is an algorithm that learns rules from example tuples in a relation. Attributes in the tuples can be integers (e.g., length of an article, in words), sets (e.g., semantic features), or bags (e.g., words that appear in a sentence or document). We use Ripper to learn rules that correlate context and other linguistic indicators with the semantics of the description being extracted and subsequently reused. It is important to notice that Ripper is designed to learn rules that classify data into atomic classes (e.g., \"good\", \"average\", and \"bad\"). We had to modify its algorithm in order to classify data into sets of atoms. For example, a rule can have the form \"if CONDITION then [( 07063762} { 02864326} { 0001795~}] \"3 . This rule states that if a certain \"CONDITION\" (which is a function of the indicators related to the description) is met, then the description is likely to contain words that are semantically related to the three WordNet nodes", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "while its hypernym, \"head, chief, top dog\" has a synset offset of \"~07311393} \".", "mid_sen": "Ripper (Cohen, 1995) is an algorithm that learns rules from example tuples in a relation. ", "after_sen": "Attributes in the tuples can be integers (e.g., length of an article, in words), sets (e.g., semantic features), or bags (e.g., words that appear in a sentence or document). "}
{"citeStart": 124, "citeEnd": 148, "citeStartToken": 124, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems that we should be able to use a function ¢ to assign any probability distribution to the strings in L(G) and then expect that we can assign appropriate probabilites to the adjunctions in G such that the language generated by G has the same distribution as that given by ¢. However a function ¢ that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs (see (Vijay-Shanker, 1987) , p. 104). An example of such a function ¢ is a simple Poisson distribution (2), which in fact was also used as the counterexample in (Booth and Thompson, 1973) for CFGs, since CFGs also have the constant growth property.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It seems that we should be able to use a function ¢ to assign any probability distribution to the strings in L(G) and then expect that we can assign appropriate probabilites to the adjunctions in G such that the language generated by G has the same distribution as that given by ¢. However a function ¢ that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs (see (Vijay-Shanker, 1987) , p. 104). ", "mid_sen": "An example of such a function ¢ is a simple Poisson distribution (2), which in fact was also used as the counterexample in (Booth and Thompson, 1973) for CFGs, since CFGs also have the constant growth property.", "after_sen": ""}
{"citeStart": 133, "citeEnd": 146, "citeStartToken": 133, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "In the area of word sense disambiguation, Black (1988) developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words. Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990 Zernik ( , 1991 , Hearst (1991) , Leacock, Towell, and Voorhees (1993) , Yarowsky (1992d, 1993) , Bruce and Wiebe (1994) , Miller et al. (1994) , Niwa and Nitta (1994) , Lehman (1994) , among others. However, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the area of word sense disambiguation, Black (1988) developed a model based on decision trees using a corpus of 22 million tokens, after manually sense-tagging approximately 2,000 concordance lines for five test words. ", "mid_sen": "Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990 Zernik ( , 1991 , Hearst (1991) , Leacock, Towell, and Voorhees (1993) , Yarowsky (1992d, 1993) , Bruce and Wiebe (1994) , Miller et al. (1994) , Niwa and Nitta (1994) , Lehman (1994) , among others. ", "after_sen": "However, despite the availability of increasingly large corpora, two major obstacles impede the acquisition of lexical knowledge from corpora: the difficulties of manually sense-tagging a training corpus, and data sparseness."}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "(3) This rule treats the conjunction in the same manner as a modifier, and results in the incorrect derivation shown in Figure 1 (a). Our work creates the correct CCG derivation, shown in Figure 1(b) , and removes the need for the grammar rule in (3). Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. PropBank (Palmer et al., 2005 ) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. ", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our work creates the correct CCG derivation, shown in Figure 1(b) , and removes the need for the grammar rule in (3). ", "mid_sen": "Honnibal and Curran (2007) have also made changes to CCGbank, aimed at better differentiating between complements and adjuncts. ", "after_sen": "PropBank (Palmer et al., 2005 ) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. "}
{"citeStart": 166, "citeEnd": 185, "citeStartToken": 166, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "First, \"tokenizer\" processes \"documents\" in a given collection to produce an inverted file (\"surrogates\"). Since our system is bidirectional, tokenization differs depending on the target language. In the case where documents are in English, tokenization involves eliminating stopwords and identifying root forms for inflected words, for which we used \"Word-Net\" (Miller et al., 1993) . On the other hand, we segment Japanese documents into lexical units using the \"ChaSen\" morphological analyzer (Matsumoto et al., 1997) and discard stopwords. In the current implementation, we use word-based uni-gram indexing for both English and Japanese documents. In other words, compound words are decomposed into base words in the surrogates. Note that indexing and retrieval methods are theoretically independent of the translation method.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since our system is bidirectional, tokenization differs depending on the target language. ", "mid_sen": "In the case where documents are in English, tokenization involves eliminating stopwords and identifying root forms for inflected words, for which we used \"Word-Net\" (Miller et al., 1993) . ", "after_sen": "On the other hand, we segment Japanese documents into lexical units using the \"ChaSen\" morphological analyzer (Matsumoto et al., 1997) and discard stopwords. "}
{"citeStart": 73, "citeEnd": 89, "citeStartToken": 73, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . Research has been carried out into the automatic assignment of Manner values to events (Nawaz et al., In Press) . In addition, the EventMine-MK service (Miwa et al., In Press) , based on EventMine (Miwa et al., 2010) facilitates automatic extraction of biomedical events with meta-knowledge assigned. The performance of EventMine-MK in assigning different meta-knowledge values to events ranges between 57% and 87% (macro-averaged F-Score) on the BioNLP'09 Shared Task corpus (Kim et al, 2011) . EventMine-MK is available as a component of the U-Compare interoperable text mining system 4 (Kano et al., 2011) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Of these five dimensions, only KT, CL and Source were considered during the comparison with the other two schemes, since they are directly related to discourse analysis.", "mid_sen": "The GENIA event corpus, consisting of 1000 abstracts with 36,115 events (Kim et al., 2008) has been annotated with meta-knowledge by 2 annotators, supported by 64-page annotation guidelines 3 (Thompson et al., 2011) . ", "after_sen": "Interannotator agreement rates ranged between 0.84-0.93 (Cohen's Kappa) . "}
{"citeStart": 30, "citeEnd": 40, "citeStartToken": 30, "citeEndToken": 40, "sectionName": "UNKNOWN SECTION NAME", "string": "Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner. The first step is to label each node as either a head, complement, or adjunct based on the approaches of Magerman (1994) and Collins (1997) . Each node is subsequently assigned the relevant category based on its constituent type and surface configuration. The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees. Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. Manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified HPSG derivation trees: Head/argument/modifier distinctions are made for each node in the tree based on Magerman (1994) and Collins (1997) ; the whole tree is then converted to a binary tree; heuristics are applied to deal with phenomena such as LDDs and coordination and to correct some errors in the treebank, and finally an HPSG category is assigned to each node in the tree in accordance with its CFG category. In the next phase of the process (externalization), HPSG lexical entries are automatically extracted from the annotated trees through the application of \"inverse schemata.\"", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The algorithm handles \"like\" coordination and exploits the traces used in the treebank in order to interpret LDDs. ", "mid_sen": "Unlike our approach, those of Xia (1999) and Hockenmaier, Bierner, and Baldridge (2004) include a substantial initial correction and clean-up of the Penn-II trees. ", "after_sen": "Miyao, Ninomiya, and Tsujii (2004) and Nakanishi, Miyao, and Tsujii (2004) describe a methodology for acquiring an English HPSG from the Penn-II Treebank. "}
{"citeStart": 245, "citeEnd": 257, "citeStartToken": 245, "citeEndToken": 257, "sectionName": "UNKNOWN SECTION NAME", "string": "1For the experiments described in this paper we have used TiMBL, an MBL software package developed in the ILK-group (Daelemans et al., 1998) , TiMBL is available from: http://ilk.kub.nl/. memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For other", "mid_sen": "1For the experiments described in this paper we have used TiMBL, an MBL software package developed in the ILK-group (Daelemans et al., 1998) , TiMBL is available from: http://ilk.kub.nl/. memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998) .", "after_sen": ""}
{"citeStart": 218, "citeEnd": 237, "citeStartToken": 218, "citeEndToken": 237, "sectionName": "UNKNOWN SECTION NAME", "string": "l+et; us briefly re.consider otu: cxplmlat.iou o1' ddct.ion. I\",x+mqfle (6) was exl)lainc(l by saying t.hat t.he two sta'ings by Cho.msky and Sue galm are deleted iltldcr SOil,(+ IlOI,ioIt 0[' ideutity. Ilowever, we could e(lmdly ',veil hame descrilled this as a process whereby the first; in,'-+La,nl:e of by Clwm.sk9 is ,nerged with the second (tlllder some noLion of idenLity), and the second insLance of ,9'uc 9av+! is merged wit,h the [irst+ Merging word strings instead of deleting them does not help with the problems of deletion acconnts which we outlined earlier. In particular, it does not help to exclude examples (9a) and (gb) which suggest shared material must have identical syntactic structnre. However, once we have started to think in terms of merging, there is an obvious next step, which is to move f'rom inerging of word strings to rnerging of synt;ax trees. This is the move made by Goodall (1!)87), who advocates treating coordination as a. union of phrase markers: '% 'pasting together' one or, top of the other of two trees, with any identical nodes tnergil~g together\" ((;oo(hdl, 1987, p.20) . We can visualise the result in terms o[' a three-dimensional tree structure, where the merged material is on one plane, and the syntax trees for each eonjmlct are on two other planes. For example, consider the a-l) tree for example (17a.) given in Fig. 1. np vp The merged part of' the tree inch,des all the nodes which dominate the shared matel:ial Sue gave. The coujmlets retain separate pla.nes (denoted here ] U using dot, ted and dashed lines respectively). (k)odall's aeco,mt does not deal with examples such as (171)), which he argues to I)e examples of'a different phenomcaon, llowew;r flmse can be incorporated into a ad) aeco,nt (e.g. Moltmamh 1992).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, once we have started to think in terms of merging, there is an obvious next step, which is to move f'rom inerging of word strings to rnerging of synt;ax trees. ", "mid_sen": "This is the move made by Goodall (1!)87), who advocates treating coordination as a. union of phrase markers: '% 'pasting together' one or, top of the other of two trees, with any identical nodes tnergil~g together\" ((;oo(hdl, 1987, p.20) . ", "after_sen": "We can visualise the result in terms o[' a three-dimensional tree structure, where the merged material is on one plane, and the syntax trees for each eonjmlct are on two other planes. "}
{"citeStart": 155, "citeEnd": 176, "citeStartToken": 155, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "To perform automatic subjectivity analysis, good clues must be found. A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998 ]), there is no comprehensive dictionary of subjective language. In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice. An NLP system must disambiguate these expressions in context.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To perform automatic subjectivity analysis, good clues must be found. ", "mid_sen": "A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998 ]), there is no comprehensive dictionary of subjective language. ", "after_sen": "In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice. "}
{"citeStart": 69, "citeEnd": 81, "citeStartToken": 69, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "Even if repairs are de ned by syntactic and semantic well-formedness Levelt, 1983 we observe that most of them are local phenomena. At this point w e h a ve to di erentiate between restarts and other repairs 3 modi cation repairs. Modi cation repairs have a strong correspondence between reparandum and reparans,", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a consequence a speech understanding system that cannot handle repairs will lose performance on these turns.", "mid_sen": "Even if repairs are de ned by syntactic and semantic well-formedness Levelt, 1983 we observe that most of them are local phenomena. ", "after_sen": "At this point w e h a ve to di erentiate between restarts and other repairs 3 modi cation repairs. "}
{"citeStart": 104, "citeEnd": 124, "citeStartToken": 104, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994) . The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994) . ", "mid_sen": "The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. ", "after_sen": "We propose that the similarity values of short text segments is statistically insignificant. "}
{"citeStart": 306, "citeEnd": 329, "citeStartToken": 306, "citeEndToken": 329, "sectionName": "UNKNOWN SECTION NAME", "string": "The last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is best collected automatically) The desire to combine hand-coded and automatically learned knowledge 1A point made by Church and Hanks (1989) . Arbitrary gaps in listing can be smoothed with a program such as the work presented here. For example, among the 27 verbs that most commonly cooccurred with from, Church and Hanks found 7 for which this suggests that we should aim for a high precision learner (even at some cost in coverage), and that is the approach adopted here.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Dictionaries produced by hand always substantially lag real language use.", "mid_sen": "The last two points do not argue against the use of existing dictionaries, but show that the incomplete information that they provide needs to be supplemented with further knowledge that is best collected automatically) The desire to combine hand-coded and automatically learned knowledge 1A point made by Church and Hanks (1989) . ", "after_sen": "Arbitrary gaps in listing can be smoothed with a program such as the work presented here. "}
{"citeStart": 77, "citeEnd": 90, "citeStartToken": 77, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• Estimating parameters for statistically-based machine translation [Brown et al., 1992] .", "mid_sen": "The work described here makes use of the aligned Canadian Hansards [Gale and Church, 1991b] to obtain noun phrase correspondences between the English and French text.", "after_sen": "The term \"correspondence\" is used here to signify a mapping between words in two aligned sentences. "}
{"citeStart": 313, "citeEnd": 333, "citeStartToken": 313, "citeEndToken": 333, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003) .", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We developed FiESTA (an acronym for \"format for extensive spatiotemporal annotations\"), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001) , the NITE object model (Evert et al., 2003) , the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008) , and the (X)CES standard (Ide et al., 2000) . ", "after_sen": "There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. "}
{"citeStart": 56, "citeEnd": 70, "citeStartToken": 56, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "In a real annotation setting, it is important to decide when to stop adding new instances to the training set. In this work, we used the confidence method proposed by Vlachos (2008) . This is an method that measures the model's confidence on a held-out non-annotated dataset every time a new instance is added to the training set and stops the AL procedure when this confidence starts to drop. In our experiments, we used the average test set variance as the confidence measure.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In a real annotation setting, it is important to decide when to stop adding new instances to the training set. ", "mid_sen": "In this work, we used the confidence method proposed by Vlachos (2008) . ", "after_sen": "This is an method that measures the model's confidence on a held-out non-annotated dataset every time a new instance is added to the training set and stops the AL procedure when this confidence starts to drop. "}
{"citeStart": 41, "citeEnd": 54, "citeStartToken": 41, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "Figure 1: Two analysis models and the associations they compare of left and right-branching compounds. Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time). In the test set used here and in that of Resnik (1993) , the proportion of leftbranching compounds is 67% and 64% respectively. In contrast, the adjacency model appears to predict a proportion of 50%.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Lauer and Dras (1994) show that under a dependency model, left-branching compounds should occur twice as often as right-branching compounds (that is twothirds of the time). ", "mid_sen": "In the test set used here and in that of Resnik (1993) , the proportion of leftbranching compounds is 67% and 64% respectively. ", "after_sen": "In contrast, the adjacency model appears to predict a proportion of 50%."}
{"citeStart": 94, "citeEnd": 115, "citeStartToken": 94, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "A small principle-based parser was built, following the proper branch formalism developed in [Crocker and Lewin1992] . Although the grammar is very limited, the use of probabilities in ranking the [mrscr's output can be seen as a first step towards implementing a principle-based parser using a more fully specified collection of grammar modules. The grammar is loosely based on three modules taken from Government-Binding Theory --X-bar theory, Theta Theory and Case Theory. Although these embody the spirit of the constraints found in Choresky [Chomsky1981] they are not intended to be entirely faithful to this specification of syntactic theory. There is also only a single level of representation (which is explicitly constructed for output purposes but not consulted by the parser). This representation is interpreted as S-structure.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A small principle-based parser was built, following the proper branch formalism developed in [Crocker and Lewin1992] . ", "after_sen": "Although the grammar is very limited, the use of probabilities in ranking the [mrscr's output can be seen as a first step towards implementing a principle-based parser using a more fully specified collection of grammar modules. "}
{"citeStart": 21, "citeEnd": 45, "citeStartToken": 21, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "Most natural language processing (NLP) systems are designed to work on certain specific domains and porting them to other domains is often a very timeconsuming and human-intenslve process. As the need for applying NLP systems to more and varied domains grows, it becomes increasingly important that some techniques be used to make these systems more portable. Several researchers (Lang and Hirschman, 1988; Rau et al., 1989; Pustejovsky, 1992; Grishman and Sterling, 1993; Basili et al., 1994) , either directly or indirectly, have addressed issues that assist in making it easier to move an NLP system from one domain to another. One of the reasons for the lack of portability is the need for domain-specific semantic features that such systems often use for lexical, syntactic, and semantic disambiguation. One such feature is the knowledge of the semantic clusters in a domain.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As the need for applying NLP systems to more and varied domains grows, it becomes increasingly important that some techniques be used to make these systems more portable. ", "mid_sen": "Several researchers (Lang and Hirschman, 1988; Rau et al., 1989; Pustejovsky, 1992; Grishman and Sterling, 1993; Basili et al., 1994) , either directly or indirectly, have addressed issues that assist in making it easier to move an NLP system from one domain to another. ", "after_sen": "One of the reasons for the lack of portability is the need for domain-specific semantic features that such systems often use for lexical, syntactic, and semantic disambiguation. "}
{"citeStart": 9, "citeEnd": 22, "citeStartToken": 9, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "We assume a sign-based grammar with binary rules, each of which may be used to combine two signs by unifying them with the daughter categories and returning the mother. Combination is the commutative equivalent of rule application; the linear ordering of the daughters that leads to successful rule application determines the orthography of the mother. Whitelock's Shake-and-Bake generation algorithm attempts to arrange the bag of target signs until a grammatical ordering (an ordering which allows all of the signs to combine to yield a single sign) is found. However, the target derivation information itself is not used to assist the algorithm. Even in (Beaven, 1992a) , the derivation information is used simply to cache previous results to avoid exact recomputation at a later stage, not to improve on previous guesses. The reason why we believe such improvement is possible is that, given adequate information from the previous stages, two target signs cannot combine by accident; they must do so because the underlying semantics within the signs licenses it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the target derivation information itself is not used to assist the algorithm. ", "mid_sen": "Even in (Beaven, 1992a) , the derivation information is used simply to cache previous results to avoid exact recomputation at a later stage, not to improve on previous guesses. ", "after_sen": "The reason why we believe such improvement is possible is that, given adequate information from the previous stages, two target signs cannot combine by accident; they must do so because the underlying semantics within the signs licenses it."}
{"citeStart": 35, "citeEnd": 53, "citeStartToken": 35, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "1.0000 coinage, mintage, specie, metal money: subconcept of cash Word 'reseller' Not in WordNet This cluster was one presented by Brown et al. as a randomly-selected class, rather than one hand-picked for its coherence. (I hand-selected it from that group forpresentation here, however.) As noted in Section 2.1, this group represents a set of words similar to burglar, according to Schtltze's method for deriving vector representation from corpus behavior. In this case, words rob and robbing were excluded because they were not nouns in WordNet. The word stray probably should be excluded also, since it most likely appears on this list as an adjective (as in \"stray bullet\").Machine-generated thesaurus entry (Grefenstette, 1994) : method, test, mean, procedure, techniqueWord 'method' (2 alternatives) 1.0000 method: a way of doing something, esp. a systematic one 0.0000 wise, method: a way of doing or being: \"in no wise\"; \"in this wise\"Word 'test' (7 alternatives) 0.6817 trial, test, tryout: trying something to find out about it; \"ten days free trial\" 0.6817 assay, check, test: subeoncept of appraisal assessanent 0.0000 examination, exam, test: a set of questions or exercises evaluating skill or knowledge 0.3183 test, mental test, mental testing, psychometric test 0.0000 test: a hard outer covering as of some amoebas and sea urchins", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The word stray probably should be excluded also, since it most likely appears on this list as an adjective (as in \"stray bullet\").", "mid_sen": "Machine-generated thesaurus entry (Grefenstette, 1994) : method, test, mean, procedure, techniqueWord 'method' (2 alternatives) 1.0000 method: a way of doing something, esp. a systematic one 0.0000 wise, method: a way of doing or being: \"in no wise\"; \"in this wise\"Word 'test' (7 alternatives) 0.6817 trial, test, tryout: trying something to find out about it; \"ten days free trial\" 0.6817 assay, check, test: subeoncept of appraisal assessanent 0.0000 examination, exam, test: a set of questions or exercises evaluating skill or knowledge 0.3183 test, mental test, mental testing, psychometric test 0.0000 test: a hard outer covering as of some amoebas and sea urchins", "after_sen": ""}
{"citeStart": 187, "citeEnd": 210, "citeStartToken": 187, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems. More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993) . We would now propose that HMM's have successfully been applied to the problem of name-finding.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the past decade, the speech recognition community has had huge successes in applying hidden Markov models, or HMM's to their problems. ", "mid_sen": "More recently, the natural language processing community has effectively employed these models for part-ofspeech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al., 1993) . ", "after_sen": "We would now propose that HMM's have successfully been applied to the problem of name-finding."}
{"citeStart": 0, "citeEnd": 14, "citeStartToken": 0, "citeEndToken": 14, "sectionName": "UNKNOWN SECTION NAME", "string": "One important type of lexical information is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998) . Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998) . ", "mid_sen": "Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. ", "after_sen": "Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue."}
{"citeStart": 274, "citeEnd": 284, "citeStartToken": 274, "citeEndToken": 284, "sectionName": "UNKNOWN SECTION NAME", "string": "The learning policy is on-line and mistakedriven; several update rules can be used within SNOW. The most successful update rule, and the only one used in this work is a variant of Littlestone's (1988) Winnow update rule, a multiplicative update rule tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992) . This mechanism is implemented via the sparse architecture of SNOW. That is, (1) input features are allocated in a data driven way -an input node for the feature i is allocated only if the feature i was active in any input sentence and (2) a link (i.e., a non-zero weight) exists between a target node t and a feature i if and only if i was active in an example labeled t.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The learning policy is on-line and mistakedriven; several update rules can be used within SNOW. ", "mid_sen": "The most successful update rule, and the only one used in this work is a variant of Littlestone's (1988) Winnow update rule, a multiplicative update rule tailored to the situation in which the set of input features is not known a priori, as in the infinite attribute model (Blum, 1992) . ", "after_sen": "This mechanism is implemented via the sparse architecture of SNOW. "}
{"citeStart": 54, "citeEnd": 74, "citeStartToken": 54, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "1. a linear acceptor, i.e. a sequence of nodes with one incoming arc and one outgoing arc, the words of source language text are placed consecutively in the arcs of the acceptor, 2. an acceptor containing possible permutations. To limit the permutations, we used an approach as in (Kanthak et al., 2005) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1. a linear acceptor, i.e. a sequence of nodes with one incoming arc and one outgoing arc, the words of source language text are placed consecutively in the arcs of the acceptor, 2. an acceptor containing possible permutations. ", "mid_sen": "To limit the permutations, we used an approach as in (Kanthak et al., 2005) .", "after_sen": "Each of these two acceptors results in different constraints for the generation of the hypotheses. "}
{"citeStart": 67, "citeEnd": 94, "citeStartToken": 67, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the framework for the allocation and transfer of control of Whittaker and Stenton[WS88] . The analysis is based on a classification of utterances into 4 types 5. These are:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use the framework for the allocation and transfer of control of Whittaker and Stenton[WS88] . ", "after_sen": "The analysis is based on a classification of utterances into 4 types 5. "}
{"citeStart": 129, "citeEnd": 161, "citeStartToken": 129, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Results are better using the AA (ρ max = 0.70 for AA-cos) than using the TAA (ρ max = 0.56). Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (ρ max,wlm = 0.60 vs ρ max,cos = 0.70). These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008 ) (ρ = 0.69) and ESA (Gabrilovich and Markovitch, 2007) ", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Secondly, the AA-cos spreading strategy significantly outperforms the AA-wlm strategy over this sample set (ρ max,wlm = 0.60 vs ρ max,cos = 0.70). ", "mid_sen": "These results compare favourably to similar inter-concept results reported for WLM (Witten and Milne, 2008 ) (ρ = 0.69) and ESA (Gabrilovich and Markovitch, 2007) ", "after_sen": "(ρ = 0.75)."}
{"citeStart": 181, "citeEnd": 194, "citeStartToken": 181, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "If the word is unknown, the system reconsiders analysis from the point where it broke down with the added possibility of an error rule. There are currently four error rules, corresponding to the four Damerau transformations: omission, insertion, transposition, substitution (Damerau, 1964) -considered in that order (Pollock, 1983) . The error rules are in two level format and integrate seamlessly into morphological analysis.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the word is unknown, the system reconsiders analysis from the point where it broke down with the added possibility of an error rule. ", "mid_sen": "There are currently four error rules, corresponding to the four Damerau transformations: omission, insertion, transposition, substitution (Damerau, 1964) -considered in that order (Pollock, 1983) . ", "after_sen": "The error rules are in two level format and integrate seamlessly into morphological analysis."}
{"citeStart": 140, "citeEnd": 164, "citeStartToken": 140, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Before determining the referents of noun phrases, sentences were at first transformed into a case structure by the case structure analyzer (Kurohashi and Nagao 1994) . Tile errors made by the case analyzer were corrected by hand. Table 1 shows the results of determining the referents of noun phrases. To confirm that the three constraints (referential property, modifier, and possessor) are effective, we experimented under several different conditions and compared them. The results are shown in Table 2 . Precision is the fraction of noun phrases which were judged to have antecedents. Recall is the fraction of noun phrases which have antecedents.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Before determining the referents of noun phrases, sentences were at first transformed into a case structure by the case structure analyzer (Kurohashi and Nagao 1994) . ", "after_sen": "Tile errors made by the case analyzer were corrected by hand. "}
{"citeStart": 131, "citeEnd": 142, "citeStartToken": 131, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "The possibility of inserting a word into a domain of some transitive head raises the questions of how to require continuity (as needed in nmst cases), and how to limit the distance between the governor and the modifier. Both questions will be soh,ed with reference to the dependency relation. From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981) . In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited 1)3-\" bounding nodes (e.g., Haegeman (1994) , Becker et al. (1991) ). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. Given this correspondence, it is natural to employ dependencies in the description of discontinuities as follows: For each modifier, a set of dependency types is defined which may link the direct head and the positional head of the modifier (\"gesehen\" and \"hat\", respectively). If this set is empty, both heads are identical and a continuous attachment results. The impossibility of extraction from, e.g., a finite verb phrase follows from the fact that the dependency embedding finite verbs, propo, may not appear on any path 2Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only apply to sister nodes. 32 between a direct and a positional head.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Both questions will be soh,ed with reference to the dependency relation. ", "mid_sen": "From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981) . ", "after_sen": "In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited 1)3-\" bounding nodes (e.g., Haegeman (1994) , Becker et al. (1991) ). "}
{"citeStart": 92, "citeEnd": 106, "citeStartToken": 92, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "We can draw a number of conclusions from our investigation of semantic parsing in Chinese. First, reasonably good performance can be achieved with a very small (1100 sentences) training set. Second, the features that we extracted for English semantic parsing worked well when applied to Chinese. Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing. Finally, we showed that semantic parsing is significantly easier in Chinese than in English. We show that this counterintuitive result seems to be due to the strict constraints on adjunct ordering in Chinese, making adjuncts easier to find and label.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, the features that we extracted for English semantic parsing worked well when applied to Chinese. ", "mid_sen": "Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing. ", "after_sen": "Finally, we showed that semantic parsing is significantly easier in Chinese than in English. "}
{"citeStart": 137, "citeEnd": 151, "citeStartToken": 137, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "# correct constituents in P Recall = # constituents in T # correct constituents in P Precision = # constituents in P A constituent in P is \"correct\" if there exists a constituent in T of the same label that spans the same words. Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of (Collins, 1996) and (Magerman, 1995) . Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995) ~, which have the best previously published parsing accuracies on the Wall St. Journal domain. It is often advantageous to produce the top N parses instead of just the top 1, since additional information can be used in a secondary model that reorders the top N and hopefully improves the quality of the top ranked parse. Suppose there exists a \"perfect\" reranking scheme that, for each sentence, magically picks the best parse from the top N parses produced by the maximum entropy parser, where the best parse has the highest average precision and recall when compared to the treebank parse. The performance of this \"perfect\" scheme is then an upper bound on the performance of any reranking scheme that might be used to reorder the top N parses. Figure 9 shows that the \"perfect\" scheme would achieve roughly 93% precision and recall, which is a dramatic increase over the top 1 accuracy of 87% precision and 86% recall. Figure 10 shows that the \"Exact Match\", which counts the percentage of times 2Results for SPATTER on section 23 are reported in (Collins, 1996) Parser Precision Maximum Entropy ° 86.8% Maximum Entropy* 87.5% (Collins, 1996) * 85.7% (Magerman, 1995) Table 5 : Results on 2416 sentences of section 23 (0 to 100 words in length) of the WSJ Treebank. Evaluations marked with ~ ignore quotation marks. Evaluations marked with * collapse the distinction between ADVP and PRT, and ignore all punctuation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "# correct constituents in P Recall = # constituents in T # correct constituents in P Precision = # constituents in P A constituent in P is \"correct\" if there exists a constituent in T of the same label that spans the same words. ", "mid_sen": "Table 5 shows results using the PARSEVAL measures, as well as results using the slightly more forgiving measures of (Collins, 1996) and (Magerman, 1995) . ", "after_sen": "Table 5 shows that the maximum entropy parser performs better than the parsers presented in (Collins, 1996) and (Magerman, 1995) ~, which have the best previously published parsing accuracies on the Wall St. Journal domain. "}
{"citeStart": 197, "citeEnd": 206, "citeStartToken": 197, "citeEndToken": 206, "sectionName": "UNKNOWN SECTION NAME", "string": "Some previous work on distributional similarity between nouns has used only a single grammatical relation (e.g., Lee 1999) , whereas other work has considered multiple grammatical relations (e.g., Lin 1998a) . We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from different relations. In previous work (Weeds 2003) , we found that considering the subject relation as well as the direct-object relation did not improve performance on a pseudo-disambiguation task.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Further, the noun hyponymy hierarchy in WordNet, which will be used as a pseudo-gold standard for comparison, is widely recognized in this area of research.", "mid_sen": "Some previous work on distributional similarity between nouns has used only a single grammatical relation (e.g., Lee 1999) , whereas other work has considered multiple grammatical relations (e.g., Lin 1998a) . ", "after_sen": "We consider only a single grammatical relation because we believe that it is important to evaluate the usefulness of each grammatical relation in calculating similarity before deciding how to combine information from different relations. "}
{"citeStart": 226, "citeEnd": 231, "citeStartToken": 226, "citeEndToken": 231, "sectionName": "UNKNOWN SECTION NAME", "string": "In the approach to discourse structure developed in [Sid83] and [GJW86], a discourse exhibits both global and local coherence. On this view, a key element of local coherence is centering, a system of rules and constraints that govern the relationship between what the discourse is about and some of the linguistic choices made by the discourse participants, e.g. choice of grammatical function, syntactic structure, and type of referring expression (proper noun, definite or indefinite description, reflexive or personal pronoun, etc.). Pronominalization in particular serves to focus attention on what is being talked about; inappropriate use or failure to use pronouns causes communication to be less fluent. For instance, it takes longer for hearers to process a pronominalized noun phrase that is no~ in focus than one that is, while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not [Gui85] .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Pronominalization in particular serves to focus attention on what is being talked about; inappropriate use or failure to use pronouns causes communication to be less fluent. ", "mid_sen": "For instance, it takes longer for hearers to process a pronominalized noun phrase that is no~ in focus than one that is, while it takes longer to process a non-pronominalized noun phrase that is in focus than one that is not [Gui85] .", "after_sen": "The [GJW86] centering model is based on the following assumptions. "}
{"citeStart": 176, "citeEnd": 190, "citeStartToken": 176, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "Rankboost, like other machine learning programs of the boosting family, can handle a very large number of features. Therefore, instead of carefully choosing a small number of features by hand which may be useful, we generated a very large number of features and let Rank-Boost choose the relevant ones. In total, we used 3,291 features in training the SPR. Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below, in a manner similar to that used by Collins (2000) . The motivation for the features was to capture declaratively decisions made by the randomized SPG. We avoided features specific to particular text plans by discarding those that occurred fewer than 10 times.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In total, we used 3,291 features in training the SPR. ", "mid_sen": "Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below, in a manner similar to that used by Collins (2000) . ", "after_sen": "The motivation for the features was to capture declaratively decisions made by the randomized SPG. "}
{"citeStart": 84, "citeEnd": 102, "citeStartToken": 84, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "However, different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997) , on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal communication), this is fine for information retrieval. Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As has been mentioned by John Carroll (personal communication), this is fine for information retrieval. ", "mid_sen": "Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999) .", "after_sen": "Unless the desired set of GRs matches the set already annotated in some large training corpus (e.g., the Buchholz et al. (1999) GR finder used the GRs annotated in the Penn Treebank (Marcus et al., 1993) ), one will have to either manually write rules to find the GRs or annotate a training corpus for the desired set. "}
{"citeStart": 65, "citeEnd": 81, "citeStartToken": 65, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "Word alignment models in general and the HMM in particular are very gross oversimplifications of the translation process and the optimal likelihood parameters learned often do not correspond to sensible alignments. One solution to this problem is to add more complexity to the model to better reflect the translation process. This is the approach taken by IBM Models 4+ (Brown et al. 1993b; Och and Ney 2003) , and more recently by the LEAF model (Fraser and Marcu 2007) . Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. Instead, we propose to use a learning framework called Posterior Regularization (Graça, Ganchev, and Taskar 2007) that incorporates side information into unsupervised estimation in the form of constraints on the model's posteriors. The constraints are expressed as inequalities on the expected values under the posterior distribution of user-defined constraint features (not necessarily the same features used by the model). Because in most applications what we are interested in are the latent variables (in this case the alignments), constraining the posteriors allows a more direct way to achieve the desired behavior. On the other hand, constraining the expected value of the features instead of adding them to the model allows us to express features that would otherwise make the model intractable. For example, enforcing that each hidden state of an HMM model should be used at most once per sentence would break the Markov property and make the model intractable. In contrast, we will show how to enforce the constraint that each hidden state is used at most once in expectation. The underlying model remains unchanged, but the learning method changes. During learning, our method is similar to the EM algorithm with the addition of solving an optimization problem similar to a maximum entropy problem inside the E Step. The following subsections present the Posterior Regularization framework, followed by a description of how to encode two pieces of prior information aimed at solving the problems described at the end of Section 2.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One solution to this problem is to add more complexity to the model to better reflect the translation process. ", "mid_sen": "This is the approach taken by IBM Models 4+ (Brown et al. 1993b; Och and Ney 2003) , and more recently by the LEAF model (Fraser and Marcu 2007) . ", "after_sen": "Unfortunately, these changes make the models probabilistically deficient and intractable, requiring approximations and heuristic learning and inference prone to search errors. "}
{"citeStart": 115, "citeEnd": 140, "citeStartToken": 115, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994 ). In addition, it is crucial for lexical choice and tense selection in machine translation (Moens and Steedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992) . Table 1 sun~narizes the three aspectual distinctions, which compose five aspectual categories. In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) from extended events, which have a time duration (e.g., She ran to the store). Therefore, four classes of events are derived: culmination, culminated process, process, and point.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994 ). ", "mid_sen": "In addition, it is crucial for lexical choice and tense selection in machine translation (Moens and Steedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992) . ", "after_sen": "Table 1 sun~narizes the three aspectual distinctions, which compose five aspectual categories. "}
{"citeStart": 86, "citeEnd": 99, "citeStartToken": 86, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "Sentence splitter. To cope with longer texts, we integrated the sentence splitter of the JTok tokeniser (Schäfer, 2005) into the system. WordNet interface. The WordNet interface now treats particle verbs like throw out correctly. More-over, we tested the usability of WordNet's verb entailment information as well as antonymy on nouns, verbs, and adjectives as basis for heuristic inferences in the graph matching process. However, for the given data, the number of text-hypothesis pairs where the relations are instantiated at all is marginal.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sentence splitter. ", "mid_sen": "To cope with longer texts, we integrated the sentence splitter of the JTok tokeniser (Schäfer, 2005) into the system. ", "after_sen": "WordNet interface. "}
{"citeStart": 135, "citeEnd": 161, "citeStartToken": 135, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the pruning algorithm of (Jonathan Graehl, p.c.; Huang, 2008) that is very similar to the method based on marginal probability (Charniak and Johnson, 2005) , except that it prunes hyperedges as well as nodes. Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost β(v) and the Viterbi outside cost α(v) for each node v, and then compute the merit αβ(e) for each hyperedge:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use the pruning algorithm of (Jonathan Graehl, p.c.; Huang, 2008) that is very similar to the method based on marginal probability (Charniak and Johnson, 2005) , except that it prunes hyperedges as well as nodes. ", "after_sen": "Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost β(v) and the Viterbi outside cost α(v) for each node v, and then compute the merit αβ(e) for each hyperedge:"}
{"citeStart": 10, "citeEnd": 24, "citeStartToken": 10, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Functional Generative Description (Sgall et al., 1986 ) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure~ which stratifies the theory and makes incremental processing difficult.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. ", "mid_sen": "Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. ", "after_sen": "He assumes associative categorial operators, permuting the arguments to yield the surface ordering. "}
{"citeStart": 137, "citeEnd": 150, "citeStartToken": 137, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "Exhaustive search for the global optimum is not an option when the search space is prohibitively large. In the present context, say for a sequence of 20 tones, the search space contains 6 ~° ~ 10 is possible tone transcriptions, and for each of these there are thousands of possible parameter settings, too large a search space for exhaustive search in a reasonable amount of compu- Non-deterministic search methods have been devised as a way of tackling large-scale combinatorial optimisation problems, problems that involve fin(ling optima of functions of discrete variables. 'I'hcse methods are only designed to yield an approximate solution, but they do so in a reasonable amount of computation time. The best known such methods are genetic search (Goldberg, 1989) and annealing search (van Laarhoven & Aarts, 1987) . Recently, annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata (Ellison, 1993 ). In the following sections I describe a genetic algorithm and an annealing algorithm for the tone transcription problem.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The best known such methods are genetic search (Goldberg, 1989) and annealing search (van Laarhoven & Aarts, 1987) . ", "mid_sen": "Recently, annealing search has been successfully applied to the learning of phonological constraints expressed as finite-state automata (Ellison, 1993 ). ", "after_sen": "In the following sections I describe a genetic algorithm and an annealing algorithm for the tone transcription problem."}
{"citeStart": 5, "citeEnd": 17, "citeStartToken": 5, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "A primary goal of abstract-syntax is to support recursion through abstractions with bound variables. This leads to the interpretation of a bound variable as a \"scoped constant\" -it acts like a constant that is not visible from the top of the term, but which becomes visible during the descent through the abstraction. See (Miller, 1991) for a discussion of how this may be used for evaluation of functional programs by \"pushing\" the evaluation through abstractions to reduce redexes that are not at the top-level. This technique is also used in the fl-reducer briefly mentioned at the end of the previous section, and a similar technique will be used here to implement coordination by recursively descending through the two arguments to be coordinated. Before describing the implementation of coordination, it is first necessary to mention how CCG categories are represented in the ~Prolog code. As shown in Figure 5 , cat is declared to be a primitive type, and np, s, conj, noun are the categories used in this implementation, fs and bs are declared to be constructors for forward and backward slash. For example, the CCG category for a transitive verb (s\\np)/np would be represented as (fs np (bs np s)). Also, the predicate atomic-type is declared to be true for the four atomic categories. This will be used in the implementation of coordination as a test for termination of the recursion. Since e is meant to be treated as a generic placeholder for any arbitrary z of the proper type, c must not appear in any terms instantiated for logic variables during the proof of [c/z]G. The significance of this restriction will be illustrated shortly.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This leads to the interpretation of a bound variable as a \"scoped constant\" -it acts like a constant that is not visible from the top of the term, but which becomes visible during the descent through the abstraction. ", "mid_sen": "See (Miller, 1991) for a discussion of how this may be used for evaluation of functional programs by \"pushing\" the evaluation through abstractions to reduce redexes that are not at the top-level. ", "after_sen": "This technique is also used in the fl-reducer briefly mentioned at the end of the previous section, and a similar technique will be used here to implement coordination by recursively descending through the two arguments to be coordinated. "}
{"citeStart": 161, "citeEnd": 188, "citeStartToken": 161, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we focus on interactive discourse. We model mixed-initiative using an utterance type classification and a set of rules for transfer of control between discourse participants that were proposed by Whittaker and Stenton[WS88] . We evaluate the generality of this analysis by applying the control rules to 4 sets of dialogues, including both advisory dialogues (ADs) and task-oriented dialogues (TODs). We analysed both financial and support ADs. The financial ADs are from the radio talk show \"Harry Gross: Speaking of Your Money \"1 The support ADs resulted from a client phoning an expert to help them diagnose and repair various software faults ~. The TODs are about the construction of a plastic water pump in both telephone and keyboard modality S.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this paper we focus on interactive discourse. ", "mid_sen": "We model mixed-initiative using an utterance type classification and a set of rules for transfer of control between discourse participants that were proposed by Whittaker and Stenton[WS88] . ", "after_sen": "We evaluate the generality of this analysis by applying the control rules to 4 sets of dialogues, including both advisory dialogues (ADs) and task-oriented dialogues (TODs). "}
{"citeStart": 81, "citeEnd": 99, "citeStartToken": 81, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the direct output of the 42-topic model of the 105th-108th Senates from (Quinn et al., 2006) to further divide the speech documents into topic clusters. In their paper, they use a model where the probabilities of a document belonging to a certain topic varies smoothly over time and the words within a given document have exactly the same probability of being drawn from a particular topic. These two properties make the model different than standard mixture models (McLachlan and Peel, 2000) and the latent Dirichlet allocation model of (Blei et al., 2003) . The model of (Quinn et al., 2006) is most closely related to the model of (Blei and Lafferty, 2006) , who present a generalization of the model used by (Quinn et al., 2006) . Table 1 lists the 42 topics and their related committees.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used the direct output of the 42-topic model of the 105th-108th Senates from (Quinn et al., 2006) to further divide the speech documents into topic clusters. ", "after_sen": "In their paper, they use a model where the probabilities of a document belonging to a certain topic varies smoothly over time and the words within a given document have exactly the same probability of being drawn from a particular topic. "}
{"citeStart": 199, "citeEnd": 218, "citeStartToken": 199, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "The context for each of the probability distributions includes all of the previous context. In principal, we could give all of this context to the decision tree algorithm and let it decide what information is relevant in constructing equivalence classes of the contexts. However, the amount of training data is limited (as are the learning techniques) and so we need to encode the context in order to simplify the task of constructing meaningflfl equivalence classes. We start with the words and their POS tags that are in the context and for each non-null tone, editing term (we also skip over E=ET), and repair tag, we insert it into the appropriate place, just as Kompe et al. (1994) do for boundary tones in their language model. Below we give the encoded context for the word \"know\" from Example 7", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the amount of training data is limited (as are the learning techniques) and so we need to encode the context in order to simplify the task of constructing meaningflfl equivalence classes. ", "mid_sen": "We start with the words and their POS tags that are in the context and for each non-null tone, editing term (we also skip over E=ET), and repair tag, we insert it into the appropriate place, just as Kompe et al. (1994) do for boundary tones in their language model. ", "after_sen": "Below we give the encoded context for the word \"know\" from Example 7"}
{"citeStart": 123, "citeEnd": 144, "citeStartToken": 123, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "(fly (argl (person))(arg2 (airplane))) 3 (fly (argl (company))(arg2 (airplane))) 2 (fly (argl (person))(arg2 (company))) (fly (argl (person))(to (place/)) 1 (fly (argl (person) )(from (place/)(to (place))) 1 (fly (argl (company))(from (place))(to (place))) that the slot of from and that of to should be considered dependent and the attachment site of one of the prepositional phrases (case slots) can be determined by that of the other with high accuracy and confidence. There has not been a general method proposed to date, however, that learns dependencies between case slots. Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth 1991) , or at most two case slots are dependent (Collins and Brooks 1995) . In this article, we propose an efficient and general method of learning dependencies between case frame slots.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There has not been a general method proposed to date, however, that learns dependencies between case slots. ", "mid_sen": "Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth 1991) , or at most two case slots are dependent (Collins and Brooks 1995) . ", "after_sen": "In this article, we propose an efficient and general method of learning dependencies between case frame slots."}
{"citeStart": 115, "citeEnd": 142, "citeStartToken": 115, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "We added some convenience methods for easy linking to some vocabularies or concept registries, among them, ISOcat (Windhouwer and Wright, 2012) , XML Schema, Dublin Core, FOAF, and others.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A category consists of (1) an identifier (which automatically is suffixed to the ontology URI to create an URI for the category), (2) a humanreadable label, (3) a human-readable definition (typically consisting of one or two sentences), (4) information about the class hierarchy, (5) information about possible domains and ranges, and (6) a number of relations, which express equivalence and similarity relations to other categories already existing outside the system (using appropriate vocabulary, such as rdfs:seeAlso or owl:sameAs).", "mid_sen": "We added some convenience methods for easy linking to some vocabularies or concept registries, among them, ISOcat (Windhouwer and Wright, 2012) , XML Schema, Dublin Core, FOAF, and others.", "after_sen": "At the moment, the ontology describing the Fi-ESTA data model (cf. Subsection 3.1) contains 23 categories and properties, resulting in triples. "}
{"citeStart": 118, "citeEnd": 137, "citeStartToken": 118, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "We extracted roughly 180,000 case fl:anles from the bracketed WSJ (Wall Street Journal) corpus of the Penn Tree Bank (Marcus et al., 1993) as co-occurrence data. We then eonstrucl.ed a number of thesauri based on these data, using our method. Figure 2 shows all example thesaurus for the 20 most frequently occurred nouns in the data, constructed based on their appearances as subject and object of roughly 2000 verbs. The obtained thesaurus seems to agree with human intuition to settle degr(~e. For example, 'million' and 'billion' are classilied in one IIOll[I chlster, alld 'stock' and 'share' arc classified together. Not all of tile IlOUII C]ltsters, however, seem to be meaningful in the useflll sense. This is probably because the. data size we had was not large enough. Pragmatically speaking, however, whethcl: the obtained thesaurus agrees with our intuition in itself is only of secondary concern, since the main lmrpose is to use the constructed t.hcsaurus to help i~uprow~ on a disaml)igual.ion I,ask. (('.over and Tl,omas, 1991) . ]t is Mways non-negative a.nd is zero iff the two distributions arc identical. ", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We extracted roughly 180,000 case fl:anles from the bracketed WSJ (Wall Street Journal) corpus of the Penn Tree Bank (Marcus et al., 1993) as co-occurrence data. ", "after_sen": "We then eonstrucl.ed a number of thesauri based on these data, using our method. "}
{"citeStart": 75, "citeEnd": 77, "citeStartToken": 75, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "To discus• the above issue• in a uniform way, we need a genera] framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take a• a l~sk a formalism developed by the second author in previous papers [15, 16] . The idea of this approach is to separate the dynamic programming construct• needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Gritfith & Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers 5. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not alway• terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. However ity. This approach may thus be used as a uniform framework for comparing chart parsers s.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The idea of this approach is to separate the dynamic programming construct• needed for efficient chart parsing from the chosen parsing schema. ", "mid_sen": "Comparison between the classifications of Kay [14] and Gritfith & Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers 5. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. ", "after_sen": "Their backtrack simulation does not alway• terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. "}
{"citeStart": 371, "citeEnd": 383, "citeStartToken": 371, "citeEndToken": 383, "sectionName": "UNKNOWN SECTION NAME", "string": "The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993) . However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system we have developed is straightforwardly extensible to nominal and adjectival predicates; the existing grammar distinguishes nominal and adjectival arguments from adjuncts structurally, so all that is required is extension of the classifier. ", "mid_sen": "Developing an analogous system for another language would be harder but not infeasible; similar taggers and parsers have been developed for a number of languages, but no extant subcategorization dictionaries exist to our knowledge, therefore the lexical statistics we utilize for statistical filtering would need to be estimated, perhaps using the technique described by Brent (1993) . ", "after_sen": "However, the entire approach to filtering needs improvement, as evaluation of our results demonstrates that it is the weakest link in our current system."}
{"citeStart": 80, "citeEnd": 92, "citeStartToken": 80, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "For the problem with multi-sentence discourses, and the \"threads\" that sentences continue, we use an implementation of tempo-rM centering (Kameyama et al., 1993; Poesio, 1994 ). This is a technique similar to the type of centering used for nominal anaphora (Sidner, 1983; Grosz et al., 1983) . Centering assumes that discourse understanding requires some notion of \"aboutness.\" While nominal centering assumes there is one object that the current discourse is \"about,\" temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread. Kameyama et al. (1993) confirmed these preferences when testing their ideas on the Brown corpus.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For the problem with multi-sentence discourses, and the \"threads\" that sentences continue, we use an implementation of tempo-rM centering (Kameyama et al., 1993; Poesio, 1994 ). ", "mid_sen": "This is a technique similar to the type of centering used for nominal anaphora (Sidner, 1983; Grosz et al., 1983) . ", "after_sen": "Centering assumes that discourse understanding requires some notion of \"aboutness.\" While nominal centering assumes there is one object that the current discourse is \"about,\" temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread. "}
{"citeStart": 53, "citeEnd": 66, "citeStartToken": 53, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "The main difference between our approach and that of Milward (1992 Milward ( , 1994 is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar. Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a function of a function, and given the type (n/n)/(n/n) when used as an adjectival modifier). The ability to deal with functions of functions has advantages in enabling more elegant linguistic descriptions, and in providing one kind of robust parsing: the parser never fails until the last word, since there could always be a final word which is a function over all the constituents formed so far. However, there is a corresponding problem of far greater non-determinism, with even unambiguous words allowing many possible transitions. It therefore becomes crucial to either perform some kind of ambiguity packing, or language tuning. This will be discussed in the final section of the paper.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This provides a state-transition or dynamic model of processing, with each state being a pair of a syntactic type and a semantic value.", "mid_sen": "The main difference between our approach and that of Milward (1992 Milward ( , 1994 is that it is based on a more expressive grammar formalism, Applicative Categorial Grammar, as opposed to Lexicalised Dependency Grammar. ", "after_sen": "Applicative Categorial Grammars allow categories to have arguments which are themselves functions (e.g. very can be treated as a function of a function, and given the type (n/n)/(n/n) when used as an adjectival modifier). "}
{"citeStart": 87, "citeEnd": 105, "citeStartToken": 87, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993) ; it measures the degree to which word m can be substituted into the contexts in which n appears. If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999) , as is the case for relative frequencies, then we may write the confusion probability as follows:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The confusion probability has been used by several authors to smooth word cooccurrence probabilities (Sugawara et al., 1985; Essen and Steinbiss, 1992; Grishman and Sterling, 1993) ; it measures the degree to which word m can be substituted into the contexts in which n appears. ", "mid_sen": "If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999) , as is the case for relative frequencies, then we may write the confusion probability as follows:", "after_sen": "P(m) conf(q, r, P(m) ) = E q(v)r(v) -p-~(v) \" V"}
{"citeStart": 33, "citeEnd": 51, "citeStartToken": 33, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "• CRF-Subj: We follow the method proposed by Zhao et al. (2008) , which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• CRF-Subj: ", "mid_sen": "We follow the method proposed by Zhao et al. (2008) , which regard the subjectivity of all clauses throughout a paragraph as a sequential flow of sentiments and use CRFs to model it. ", "after_sen": "The feature sets are similar as the local formulas for MLN including words, POS tags, dependency relations, and opinion lexicon."}
{"citeStart": 120, "citeEnd": 132, "citeStartToken": 120, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "The optimisations improve throughput by a factor of more than three. describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF 'backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the 'residue' of features in the unification grammar that are not encoded in the backbone. In this parser, the LALR(1) technique (Aho, Sethi Ullman, 1986 ) is used, in conjunction with a graph-structured stack (Tomita, 1987) , adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF 'backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the 'residue' of features in the unification grammar that are not encoded in the backbone. ", "mid_sen": "In this parser, the LALR(1) technique (Aho, Sethi Ullman, 1986 ) is used, in conjunction with a graph-structured stack (Tomita, 1987) , adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.", "after_sen": "On each reduction the parser performs the unifications specified by the unification grammar version of the CF backbone rule being applied. "}
{"citeStart": 138, "citeEnd": 158, "citeStartToken": 138, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan and Dorr, 2006) . However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daumé III and Marcu, 2005; Lopez and Resnik, 2005) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan and Dorr, 2006) . ", "mid_sen": "However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daumé III and Marcu, 2005; Lopez and Resnik, 2005) .", "after_sen": "We are particularly motivated by systems like the one described in Galley et al. (2006) , which constructs translations using tree-to-string transducer rules. "}
{"citeStart": 120, "citeEnd": 128, "citeStartToken": 120, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsingbased word clustering (Lin 1998) : conceptually similar words occur in structurally similar context. In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns. For example, this man can be substituted for the proper name John Smith in almost all structural patterns. Following the same rationale, a bootstrapping approach is applied to the semantic lexicon acquisition task [Thelen & Riloff. 2002] .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, at structural level, the concept-based seeds share the same or similar linguistic patterns (e.g. Subject-Verb-Object patterns) with the corresponding types of proper names.", "mid_sen": "The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsingbased word clustering (Lin 1998) : ", "after_sen": "conceptually similar words occur in structurally similar context. "}
{"citeStart": 141, "citeEnd": 165, "citeStartToken": 141, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005) . Given the set V s = {v i } containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence v i and sentence v j is induced from sentence similarity measure as follows:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005) . ", "after_sen": "Given the set V s = {v i } containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence v i and sentence v j is induced from sentence similarity measure as follows:"}
{"citeStart": 81, "citeEnd": 98, "citeStartToken": 81, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems, however, that Brown et al. expect that target word selection would be determined mainly by translation probabilities (the second factor in the above term), which should be derived from a bilingual corpus (Brown et al. 1990, p. 79) . This view is reflected also in their elaborate method for target word selection (Brown et al. 1991) , in which better estimates of translation probabilities are achieved as a result of word sense disambiguation. Our method, on the other hand, incorporates only target language probabilities and ignores any notion of translation probabilities. It thus demonstrates a possible trade-off between these two types of probabilities: using more informative statistics of the target language may compensate for the lack of translation probabilities. For our system, the more informative statistics are achieved by syntactic analysis of both the source and target languages, instead of the simple tri-gram model used by Brown et al. In a broader sense, this can be viewed as a tradeoff between the different components of a translation system: having better analysis and generation models may reduce some burden from the transfer model.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "expect that target word selection would be determined mainly by translation probabilities (the second factor in the above term), which should be derived from a bilingual corpus (Brown et al. 1990, p. 79) . ", "mid_sen": "This view is reflected also in their elaborate method for target word selection (Brown et al. 1991) , in which better estimates of translation probabilities are achieved as a result of word sense disambiguation. ", "after_sen": "Our method, on the other hand, incorporates only target language probabilities and ignores any notion of translation probabilities. "}
{"citeStart": 110, "citeEnd": 121, "citeStartToken": 110, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "This work was accomplished in the context of COBALT project (LRE 61-01 ), dealing with financial news. A detailed discussion about both procedures of anaphora resolution and PP attachment is largely developed in (Azzam, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This work was accomplished in the context of COBALT project (LRE 61-01 ), dealing with financial news. ", "mid_sen": "A detailed discussion about both procedures of anaphora resolution and PP attachment is largely developed in (Azzam, 1994) .", "after_sen": ""}
{"citeStart": 105, "citeEnd": 114, "citeStartToken": 105, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "There are several motivations for learning the graph weights Θ in this domain. First, some dependency relations -foremost, subject and object -are in general more salient than others (Lin, 1998; Padó and Lapata, 2007) . In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab, 2000) ). Weight tuning allows the adaption of edge weights to each task (i.e., distribution of queries).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are several motivations for learning the graph weights Θ in this domain. ", "mid_sen": "First, some dependency relations -foremost, subject and object -are in general more salient than others (Lin, 1998; Padó and Lapata, 2007) . ", "after_sen": "In addition, dependency relations may have varying importance per different notions of word similarity (e.g., noun vs. verb similarity (Resnik and Diab, 2000) ). "}
{"citeStart": 86, "citeEnd": 106, "citeStartToken": 86, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "PCFGs have the interesting property (which we expect most linguistically more realistic models to also possess) that the distributions they define are not always properly normalized or \"tight\". In a non-tight PCFG the partition function (i.e., sum of the \"probabilities\" of all the trees generated by the PCFG) is less than one. (Booth and Thompson, 1973 , called such non-tight PCFGs \"inconsistent\", but we follow Chi and Geman (1998) in calling them \"non-tight\" to avoid confusion with the consistency of statistical estimators). Chi (1999) showed that renormalized nontight PCFGs (which he called \"Gibbs CFGs\") define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In a non-tight PCFG the partition function (i.e., sum of the \"probabilities\" of all the trees generated by the PCFG) is less than one. ", "mid_sen": "(Booth and Thompson, 1973 , called such non-tight PCFGs \"inconsistent\", but we follow Chi and Geman (1998) in calling them \"non-tight\" to avoid confusion with the consistency of statistical estimators). ", "after_sen": "Chi (1999) showed that renormalized nontight PCFGs (which he called \"Gibbs CFGs\") define the same class of distributions over trees as do tight PCFGs with the same rules, and provided an algorithm for mapping any PCFG to a tight PCFG with the same rules that defines the same distribution over trees."}
{"citeStart": 119, "citeEnd": 138, "citeStartToken": 119, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm (Berger et al., 1996) , are set to maximize the likelihood of the training data. The probability of the sequence W = wl ... wn, given the attribute set A, (and also given that its length is n) is:", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The features used in NLG2", "mid_sen": "are described in the next section, and the feature weights aj, obtained from the Improved Iterative Scaling algorithm (Berger et al., 1996) , are set to maximize the likelihood of the training data. ", "after_sen": "The probability of the sequence W = wl ... wn, given the attribute set A, (and also given that its length is n) is:"}
{"citeStart": 78, "citeEnd": 90, "citeStartToken": 78, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003) . The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. The effort of searching for other clues indicating hyponymy relations is thus significant. We try to acquire hyponymy relations by combining three different types of clue obtainable from a wide range of words or phrases. The first type of clue is inclusion in itemizations or lists found in typical HTML documents on the WWW. The second consists of statistical measures such as the document frequency (df) and the inverse document frequency (idf), which are In our acquisition, we made the following assumptions.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The goal of this work is to become able to automatically acquire hyponymy relations for a wide range of words or phrases from HTML documents on the WWW. ", "mid_sen": "We do not use particular lexicosyntactic patterns, as previous attempts have (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003) . ", "after_sen": "The frequencies of use for such lexicosyntactic patterns are relatively low, and there can be many words or phrases that do not appear in such patterns even if we look at a large number of texts. "}
{"citeStart": 175, "citeEnd": 199, "citeStartToken": 175, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "As our n-best tagger, we use a first order MEMM model (McCallum et al., 2000) . Though CRFs (Lafferty et al., 2001 ) can be regarded as improved version of MEMMs, we have chosen MEMMs because MEMMs are usually much faster to train compared to CRFs, which enables extensive feature selection. Training a CRF tagger with features selected using an MEMM may result in yet another performance boost, but in this paper we concentrate on the MEMM as our n-best tagger, and consider CRFs as one of our future extensions. Table 1 shows the state transition table of our MEMM model. Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003) , our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 1 shows the state transition table of our MEMM model. ", "mid_sen": "Though existing studies suggest that changing the tag set of the original corpus, such as splitting of O tags, can contribute to the performances of named entity recognizers (Peshkin and Pfefer, 2003) , our system uses the original tagset of the training data, except that the 'BOS' label is added to represent the state before the beginning of sentences.", "after_sen": "Probability of state transition to the i-th label of a sentence is calculated by the following formula: where l i is the next BIO tag, l i−1 is the previous BIO tag, S is the target sentence, and f j and l j are feature functions and parameters of a log-linear model (Berger et al., 1996) . "}
{"citeStart": 25, "citeEnd": 44, "citeStartToken": 25, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "Neural Networks: Neural Networks are a special kind of \"non-symbolic\" eager learning algo-rithm. The neural network links the vector elements to the document categories The learning phase defines thresholds for the activation of neurons. In the categorization phase, a new document vector leads to the activation of a single category. For details we refer to (Wiener et al., 1995) . In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et al., 1996) . LVQ has been used in its default configuration only. No adaptation to the application domain has been made.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the categorization phase, a new document vector leads to the activation of a single category. ", "mid_sen": "For details we refer to (Wiener et al., 1995) . ", "after_sen": "In our application, we tried out the Learning Vector Quantization (LVQ) (Kohonen et al., 1996) . LVQ has been used in its default configuration only. "}
{"citeStart": 50, "citeEnd": 71, "citeStartToken": 50, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Nucor has pioneered the first mini-mill. no dering of edits, and multiple edits can operate on the same input spans), we find that this poses no great impediment to subsequent processing by the entailment model. Table 6 shows the performance of the NatLog system on RTE3 data. Relative to the Stanford RTE system, NatLog achieves high precision on its yes predictions-about 76% on the development set, and 68% on the test set-suggesting that hybridizing may be effective. For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases. NatLog makes positive predictions far more often-at a rate of 18% on the development set, and 24% on the test set. The Stanford RTE system makes yes/no predictions by thresholding a real-valued inference score. To construct a hybrid system, we adjust the Stanford inference scores by +x or −x, depending on whether NatLog predicts yes or no/unk. We choose the value of x by optimizing development set accuracy, while adjusting the threshold to generate bal-anced predictions (that is, equal numbers of yes and no predictions). As an additional experiment, we fix x at this value and then adjust the threshold to optimize development set accuracy, resulting in an excess of yes predictions. (Since this optimization is based solely on development data, its use on test data is fully legitimate.) Results for these two cases are shown in table 6. The parameters tuned on development data were found to yield good performance on test data. The optimized hybrid system attained an absolute accuracy gain of 3.12% over the Stanford system, corresponding to an extra 25 problems answered correctly. This result is statistically significant (p < 0.01, McNemar's test, 2-tailed).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Relative to the Stanford RTE system, NatLog achieves high precision on its yes predictions-about 76% on the development set, and 68% on the test set-suggesting that hybridizing may be effective. ", "mid_sen": "For comparison, the FOL-based system reported in (Bos and Markert, 2006) attained a similarly high precision of 76% on RTE2 problems, but was able to make a positive prediction in only about 4% of cases. ", "after_sen": "NatLog makes positive predictions far more often-at a rate of 18% on the development set, and 24% on the test set. "}
{"citeStart": 122, "citeEnd": 133, "citeStartToken": 122, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Semantic Relatedness Information. There bas recently been work in the detection of semantically related nouns via, for example, shared argument structures (Hindle 1990) , and shared dictionary definition context (Wilks e¢ al. 1990) . These approaches attempt to infer relationships among [exical terms by looking at very large text samples and determining which ones are related in a statistically significant way. The technique introduced in this paper can be seen as having a similar goal but an entirely different approach, since only one sample need be found in order to determine a salient relationship (and that sample may be infrequently occurring or nonexistent).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Semantic Relatedness Information. ", "mid_sen": "There bas recently been work in the detection of semantically related nouns via, for example, shared argument structures (Hindle 1990) , and shared dictionary definition context (Wilks e¢ al. 1990) . ", "after_sen": "These approaches attempt to infer relationships among [exical terms by looking at very large text samples and determining which ones are related in a statistically significant way. "}
{"citeStart": 26, "citeEnd": 36, "citeStartToken": 26, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach differs from Lin (1998) in three important ways: (a) by introducing dependency paths we can capture non-immediate relationships between words (i.e., between subjects and objects), whereas Lin considers only local context (dependency edges in our terminology); the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; (b) Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels (e.g., subject, object, modifier) and parametrize the space accordingly for different tasks; (c) considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words (see the leaves in Figure 1 ), parts of speech or dependency edges; in Lin's approach, it is only dependency edges (features in his terminology) that form the dimensions of the semantic space.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It extends prior work on syntax-based models (Grefenstette, 1994; Lin, 1998) , by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.", "mid_sen": "Our approach differs from Lin (1998) in three important ways: (a) by introducing dependency paths we can capture non-immediate relationships between words (i.e., between subjects and objects), whereas Lin considers only local context (dependency edges in our terminology); the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; (b) Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels (e.g., subject, object, modifier) and parametrize the space accordingly for different tasks; (c) considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words (see the leaves in Figure 1 ), parts of speech or dependency edges; in Lin's approach, it is only dependency edges (features in his terminology) that form the dimensions of the semantic space.", "after_sen": "Experiment 1 revealed that the dependency-based model adequately simulates semantic priming. "}
{"citeStart": 45, "citeEnd": 107, "citeStartToken": 45, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "To our knowledge, there have been few papers about identifying sentence boundaries. The most recent work will be described in (Pa.lmer and Hearst, To appear). There is also a less detailed description of Pahner and Hearst's system, SATZ, in (Pahuer and Hearst, 1994) . 1 The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries. The neural network achieves 98.5% accuracy on a corpus of Wall Str'eet Journal t~Ve recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The most recent work will be described in (Pa.lmer and Hearst, To appear). ", "mid_sen": "There is also a less detailed description of Pahner and Hearst's system, SATZ, in (Pahuer and Hearst, 1994) . 1 The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries. ", "after_sen": "The neural network achieves 98.5% accuracy on a corpus of Wall Str'eet Journal t~Ve recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here."}
{"citeStart": 70, "citeEnd": 89, "citeStartToken": 70, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The HMM part-of-speech tagging model and corresponding Viterbi algorithm were implemented based on their description in the updated version, http://www.cs.colorado.edu/˜martin/ SLP/updated.html , of chapter 8 of (Jurafsky and Martin, 2000) . A model was trained using Maximum Likelihood from the UPenn Treebank (Marcus et al., 1993 ). The input model file is encoded using XML and thus models built by other systems can be read in and displayed.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The HMM part-of-speech tagging model and corresponding Viterbi algorithm were implemented based on their description in the updated version, http://www.cs.colorado.edu/˜martin/ SLP/updated.html , of chapter 8 of (Jurafsky and Martin, 2000) . ", "mid_sen": "A model was trained using Maximum Likelihood from the UPenn Treebank (Marcus et al., 1993 ). ", "after_sen": "The input model file is encoded using XML and thus models built by other systems can be read in and displayed."}
{"citeStart": 1, "citeEnd": 15, "citeStartToken": 1, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "12This is a case in which the paxticulax LF assumed here fails to yield another available scoping. See footnote 10. Thus, \"generalized coordination\", instead of being a family of separate rules, can be expressed as a single rule on recursive descent through logical forms. (Steedman, 1990) also discusses \"generalized composition\", and it may well be that a similar implementation is possible for that family of rules as well.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, \"generalized coordination\", instead of being a family of separate rules, can be expressed as a single rule on recursive descent through logical forms. ", "mid_sen": "(Steedman, 1990) also discusses \"generalized composition\", and it may well be that a similar implementation is possible for that family of rules as well.", "after_sen": ""}
{"citeStart": 81, "citeEnd": 96, "citeStartToken": 81, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "Then, each sequence, together with any associated restrictions on orthographic features, undergoes analysis by the compiled spelling rules (Section 2.1), with the surface sequence and the root part of the lexical sequence initially uninstantiated. Rules are applied recursively and nondeterministically, somewhat in the style of Abramson (1992) , taking advantage of Prolog's unification mechanism to instantiate the part of the surface string corresponding to affixes and to place some spelling constraints on the start and/or end of the surface and/or lexical forms of the root.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Then, each sequence, together with any associated restrictions on orthographic features, undergoes analysis by the compiled spelling rules (Section 2.1), with the surface sequence and the root part of the lexical sequence initially uninstantiated. ", "mid_sen": "Rules are applied recursively and nondeterministically, somewhat in the style of Abramson (1992) , taking advantage of Prolog's unification mechanism to instantiate the part of the surface string corresponding to affixes and to place some spelling constraints on the start and/or end of the surface and/or lexical forms of the root.", "after_sen": "This process results in a set of spelling palterns, one for each distinct application of the spelling rules to each affix sequence suggested by the production rules. "}
{"citeStart": 124, "citeEnd": 146, "citeStartToken": 124, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper presents a general learning approach for identifying syntactic patterns, based on the SNoW learning architecture (Roth, 1998; Roth, 1999) . The SNoW learning architecture is a sparse network of linear ftmctions over a predefined or incrementally learned feature space. SNoW is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large -of which NLP is a principal example. Preliminary versions of it have already been used successfully on several tasks in natural language processing (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998) . In particular, SNoW's sparse architecture supports well chaining and combining predictors to produce a coherent inference. This property of the architecture is the base for the learning approach studied here in the context of shallow parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "SNoW is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large -of which NLP is a principal example. ", "mid_sen": "Preliminary versions of it have already been used successfully on several tasks in natural language processing (Roth, 1998; Golding and Roth, 1999; Roth and Zelenko, 1998) . ", "after_sen": "In particular, SNoW's sparse architecture supports well chaining and combining predictors to produce a coherent inference. "}
{"citeStart": 79, "citeEnd": 90, "citeStartToken": 79, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Research that is more similar in goal to that outlined in this paper is Vosse (Vosse, 1992) . Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (Granger, 1983) uses expectations generated by scripts to anMyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree.", "mid_sen": "Research that is more similar in goal to that outlined in this paper is Vosse (Vosse, 1992) . ", "after_sen": "Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. "}
{"citeStart": 169, "citeEnd": 194, "citeStartToken": 169, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt: it is that taken by Alshawi and Crouch (1992) . This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. Although the problem does not arise for the simple fragment they illustrate there, if their approach were extended to cover a wider range of constructions, it would be found that many QLFs subsumed RQLFs that are not actually permitted by the resolution rules: for example, those that can only arise via a violation of scoping or binding constraints. The role of resolution rules (for perfectly good presentational reasons) is completely ignored by their treatment. However, it is really the case that in giving the semantics of a QLF, one is interested only in the set of RQLFs that are obtainable from it under closure of the resolution rules. Ideally, therefore, we would like a formal reconstruction of resolution rules as well. This is so, not just for reasons of formal hygiene in trying to make logical sense out of underspecified representations, but also because resolution rules and the knowledge they express are an important object of study in their own right. Anyone who has built a wide-coverage system knows that the range of context-dependent phenomena encountered in real life is a lot wider than the preoccupations of many linguists might suggest. In the CLE, for example, contextual resolution forms a larger part of the system than do syntactic and semantic processing. Unfortunately, in the CLE there is no formal theory of resolution rules, and thus no prospect of capturing their role in assigning a semantics to QLFs.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The resolution mechanism is not intended to be reversible, although by redefining resolution rules, reversibility is achievable to some extent within the limitations just discussed (Hurst 1994) .", "mid_sen": "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt: it is that taken by Alshawi and Crouch (1992) . ", "after_sen": "This describes the semantics of QLFs via a supervaluation over the semantics of the RQLFs that they subsume. "}
{"citeStart": 161, "citeEnd": 187, "citeStartToken": 161, "citeEndToken": 187, "sectionName": "UNKNOWN SECTION NAME", "string": "(8) John ga [Oi ronbun wo kaita] seitoi wo hometa. John NOM essay ACC wrote student ACC praised \"John praised the student who wrote the essay\" Up to the first verb kaiia (\"wrote\"), the string is interpretable as a full clause (without a gap), meaning '~John wrote an essay\", and the incremental parser builds the requisite structure. However, the appearance of the head noun seito (student) means that at least part of the preceding clause must be reinterpreted as a relative clause including a gap (note that there is no overt relative pronoun in Japanese). One way of looking at what is happening here is to see the subject NP John ga as being dissociated from the clause in which it is originally attached, and reattached into the main clause. But looking at it from a different perspective, as Gorrell has noted (in press), one can see the subject NP as remaining in the main clause, and the constituent bracketed in (8), (tonbun wo kaita (\"wrote an essay\")) as being lowered into the relative clause. If this is possible, then we would expect examples like (8) to be unconscious garden paths, and this does indeed seem to be reflected in the intuitive data (see Mazuka and Itoh (in press) ). However, if we are to allow our parser to handle such examples, we must expand the definition of tree-lowering, since, in order to build a relative clause, we have to assert extra material (including the empty subject and the new S node), which is not justified solely by the lexical requirements of the disambiguating word, the head noun seito. This involves reconstructing all the clausal structure dominating the lowering site (including asserting empty argument positions), with reference to the verb's case frame, and attempting to attach the result as a relative clause to the head noun.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But looking at it from a different perspective, as Gorrell has noted (in press), one can see the subject NP as remaining in the main clause, and the constituent bracketed in (8), (tonbun wo kaita (\"wrote an essay\")) as being lowered into the relative clause. ", "mid_sen": "If this is possible, then we would expect examples like (8) to be unconscious garden paths, and this does indeed seem to be reflected in the intuitive data (see Mazuka and Itoh (in press) ). ", "after_sen": "However, if we are to allow our parser to handle such examples, we must expand the definition of tree-lowering, since, in order to build a relative clause, we have to assert extra material (including the empty subject and the new S node), which is not justified solely by the lexical requirements of the disambiguating word, the head noun seito. "}
{"citeStart": 205, "citeEnd": 219, "citeStartToken": 205, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "The mentioned studies use word-clusters for interpolated n-gram language models. Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger (Black et al., 1992 , Ushioda, 1996 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The mentioned studies use word-clusters for interpolated n-gram language models. ", "mid_sen": "Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger (Black et al., 1992 , Ushioda, 1996 .", "after_sen": "In this case a decision tree contains binary questions to decide the properties of a word."}
{"citeStart": 67, "citeEnd": 90, "citeStartToken": 67, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Obviously, if we assume that case slots are independent, then we only need to compare Pfly(Xfrom = 1) and Piet(Xfrom = 1). This is nearly equivalent to the disambiguafion method proposed by Hindle and Rooth (1991) . Their method, referred to here as the Lexical Association (LA) method, actually compares the two probabilities by means of hypothesis testing. Specifically, it calculates the so-called t-score, which is a statistic about the difference between the two probabilities.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Obviously, if we assume that case slots are independent, then we only need to compare Pfly(Xfrom = 1) and Piet(Xfrom = 1). ", "mid_sen": "This is nearly equivalent to the disambiguafion method proposed by Hindle and Rooth (1991) . ", "after_sen": "Their method, referred to here as the Lexical Association (LA) method, actually compares the two probabilities by means of hypothesis testing. "}
{"citeStart": 157, "citeEnd": 171, "citeStartToken": 157, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "While this paper focuses on reducing grammar size to minimize sparse data problems in PCFG estimation, the modi ed left-corner transforms described here are generally applicable wherever the original left-corner transform is. For example, the selective left-corner transform can be used in place of the standard left-corner transform in the construction of nite-state approximations Johnson, 1998a , often reducing the size of the intermediate automata constructed. The selective left-corner transform can be generalized to head-corner parsing van Noord, 1997, yielding a selective head-corner parser. This follows from generalizing the selective left-corner transform to Horn clauses.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While this paper focuses on reducing grammar size to minimize sparse data problems in PCFG estimation, the modi ed left-corner transforms described here are generally applicable wherever the original left-corner transform is. ", "mid_sen": "For example, the selective left-corner transform can be used in place of the standard left-corner transform in the construction of nite-state approximations Johnson, 1998a , often reducing the size of the intermediate automata constructed. ", "after_sen": "The selective left-corner transform can be generalized to head-corner parsing van Noord, 1997, yielding a selective head-corner parser. "}
{"citeStart": 41, "citeEnd": 59, "citeStartToken": 41, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). ", "mid_sen": "Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . ", "after_sen": "Our model is not based on sentence cohesion or structural adjacency. "}
{"citeStart": 99, "citeEnd": 122, "citeStartToken": 99, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Experiments used a model with 35 classes. From maximal probability parses for the British National Corpus derived with a statistical parser (Carroll and Rooth, 1998) , we extracted frequency tables for intransitve verb/subject pairs and transitive verb/subject/object triples. The 500 most frequent verbs were selected for slot labeling. Fig. 6 shows two verbs v for which the most probable class label is 5, a class which we earlier described as communicative action, together with the estimated frequencies of f(n)po(cln ) for those ten nouns n for which this estimated frequency is highest. Fig. 7 shows corresponding data for an intransitive scalar motion sense of increase. Fig. 8 shows the intransitive verbs which take 17 as the most probable label. Intuitively, the verbs are semantically coherent. When compared to Levin (1993)'s 48 top-level verb classes, we found an agreement of our classification with her class of \"verbs of changes of state\" except for the last three verbs in the list in Fig. 8 which is sorted by probability of the class label.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Experiments used a model with 35 classes. ", "mid_sen": "From maximal probability parses for the British National Corpus derived with a statistical parser (Carroll and Rooth, 1998) , we extracted frequency tables for intransitve verb/subject pairs and transitive verb/subject/object triples. ", "after_sen": "The 500 most frequent verbs were selected for slot labeling. "}
{"citeStart": 239, "citeEnd": 262, "citeStartToken": 239, "citeEndToken": 262, "sectionName": "UNKNOWN SECTION NAME", "string": "Based on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation, as shown, e.g. in Ratinov and Roth (2009) . The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks. The problem is then transformed into a simple, but constrained, 5-class classification problem.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, for another head h a ,b , we have the satisfying condition a − b > 0 or b − a < 0 ∀h a,b , h a ,b .", "mid_sen": "Based on this assumption, the problem of identifying mention heads is a sequential phrase identification problem, and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation, as shown, e.g. in Ratinov and Roth (2009) . ", "after_sen": "The BILOUrepresentation suggests learning classifiers that identify the Beginning, Inside and Last tokens of multi-token chunks as well as Unit-length chunks. "}
{"citeStart": 96, "citeEnd": 114, "citeStartToken": 96, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. We refer to these as linguistic level representations.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A visual display gives direct feedback on some of these parameters.", "mid_sen": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . ", "after_sen": "The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). "}
{"citeStart": 66, "citeEnd": 87, "citeStartToken": 66, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the Hebrew Treebank, (Sima'an et al., 2001) , provided by the knowledge center for processing Hebrew, in which sentences from the daily newspaper \"Ha'aretz\" are morphologically segmented and syntactically annotated. The treebank has two versions, v1.0 and v2.0, containing 5001 and 6501 sentences respectively. We use v1.0 mainly because previous studies on joint inference reported results w.r.t. v1.0 only. 5 We expect that using the same setup on v2.0 will allow a crosstreebank comparison. 6 We used the first 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and \"malformed\" 7 were removed. The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. (we ignored the 419 trees in their development set.)", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "6 We used the first 500 sentences as our dev set and the rest 4500 for training and report our main results on this split. ", "mid_sen": "To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and \"malformed\" 7 were removed. ", "after_sen": "The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing. "}
{"citeStart": 203, "citeEnd": 228, "citeStartToken": 203, "citeEndToken": 228, "sectionName": "UNKNOWN SECTION NAME", "string": "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991) , multidocument summarization (McKeown et al., 2002) , automatic evaluation of MT (Denkowski and Lavie, 2010) , and TE (Dinu and Wang, 2009) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. ", "mid_sen": "They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991) , multidocument summarization (McKeown et al., 2002) , automatic evaluation of MT (Denkowski and Lavie, 2010) , and TE (Dinu and Wang, 2009) .", "after_sen": "One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005) . "}
{"citeStart": 108, "citeEnd": 130, "citeStartToken": 108, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "'Fo date, psychologists have focused on two aspects of the speech segmentation problem. The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched; both psychologists (e.g., Cutler & Carter, 1987; Cutler & Butterliel (I, 1992) and designers of speech-recognition systems (e.g., (]hur (:h, 1987) have examined I~his problem. However, the problem we examined is dilferent---we want to know how infants segment speech before knowing which phonemic se-qllelW,('s form words. '1' he second aspect psychologists liaw~ focnsed (ill is the lirobleln of dcternihiilig the ill['Orluatioll SOllr(:(~s t() which ilifants are SCllSil,ive. Priluarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress. II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. Sensitivity to native-language phonotactics in 9-month-olds was re(:ently reported by Jusczyk, I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993) . 'i'lles~ sl, udi(~s deruoilstratcd infants' perceptive abilil.il's wil,hout deiilonsl.ral,hig tlw usefuhicss of hli'alil,s > ll(~rcel)l,ioils. I low do childl'(,n coiubine l,li(: iiiforiii;d,ion I, hey i)crc~,iw; froln dilrerenl, SOlll'l;es'. ? Aslili el, al. Sl)(~c-Illate that infants first learn words heard in isolation, then use distribution and prosody to refine and expand their w)cabulary; however, Jusczyk (1(,)93) sliggests that sound sequences learned in isolation dill~r too greatly from those in contexi. to bc useful. He goes on to say, \"just how far inforniation in the sound structure of the input can pies, we see that Hypothesis 1 uses 48 characters and Hypothesis 2 uses 75. However, this simplistic method is inefficient; for instance, the length of lexical indices are arbitrary with respect to properties of the words themselves (e.g., in Hypothesis 2, there is no reason why/jul/was assigned tile index '10'--length two--instead of '9'--length one). Our system improves upon this simple size metri(: I)y coml)uting sizes based on ;t ('Onll)act rel)rcs(,ntat.ion motivated I)y informati(m theory. W(: inmginc hypothes(:s r(qu'(~sented ;~ a string of ones and zeros. This binary string must r(,present not only the lexical entries, their indices (called code words) and the coded sample, but also overhead information specifying the number of items coded and their arrangement in the string (information implicitly given by spacing and sl)atial placement in the introductory cxamples). Furtherrnore, the string and its components must be self-delimiting, so that a decoder could identify the endpoints of components by itself. The next section describes the binary representation and the length formulm derived from it in detail; readers satisfied with the intuitive descriptions presented so far should skip ahead to the Phonotactics subsection.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Priluarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress. ", "mid_sen": "II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . ", "after_sen": "Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. "}
{"citeStart": 298, "citeEnd": 318, "citeStartToken": 298, "citeEndToken": 318, "sectionName": "UNKNOWN SECTION NAME", "string": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "b. That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.", "mid_sen": "While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998) , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994) .", "after_sen": "Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences. "}
{"citeStart": 109, "citeEnd": 131, "citeStartToken": 109, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "The system estimated the referential property of koutei buai (official rate) to be indefinite in the method (Murata and Nagao, 1993) . Following rule R3 (ection 3.2) the system took a candidate \"Indefinite,\" which means that the candidate is an indefinite noun phrase that does not have an indirect anaphoric referent. Following R4 (Section 3.2) the system took four possible antecedents, nisidoku (West Germany), jikokutuuka (own currency), kyoutyou (cooperation), dorudaka (dollar's surge). The possible antecedents were given points based on the weight of topics and foci, the distance from the anaphor, and so on. The system properly judged that nisidoku (West Germany), which had the best score, was the desired antecedent.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is explained as follows:", "mid_sen": "The system estimated the referential property of koutei buai (official rate) to be indefinite in the method (Murata and Nagao, 1993) . ", "after_sen": "Following rule R3 (ection 3.2) the system took a candidate \"Indefinite,\" which means that the candidate is an indefinite noun phrase that does not have an indirect anaphoric referent. "}
{"citeStart": 110, "citeEnd": 124, "citeStartToken": 110, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "Language reuse involves two components: a source text written by a human and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993) , the use of entire factual sentences extracted from corpora (e.g., \"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995) . In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. ", "mid_sen": "Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993) .", "after_sen": "Stydying the concept of language reuse is rewarding because it allows generation systems to leverage on texts written by humans and their deliberate choice of words, facts, structure."}
{"citeStart": 112, "citeEnd": 123, "citeStartToken": 112, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "We assume here that a translation of the surface forms of sentences into a logical formalism is possible. Its details are not important for our aim of giving a semantic interpretation of paragraphs; the main theses of our theory do not depend on a logical notation. So we will use a very simple formalism, like the one above, resembling the standard first order language. But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981) , which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. Jackendoff's (1983) formalism is richer and resembles more closely an English grammar. Jackendoff (1983, p. 14) writes \"it would be perverse not to take as a working assumption that language is a relatively efficient and accurate encoding of the information it conveys.\" Therefore a formalism of the kind he advocates would probably be most suitable for an implementation of our semantics. It will also be a model for our simplified logical notation (cf. Section 5). We can envision a system that uses data structures produced by a computational grammar to obtain the logical form of sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So we will use a very simple formalism, like the one above, resembling the standard first order language. ", "mid_sen": "But, obviously, there are other possibilities--for instance, the discourse representation structures (DRS's) of Kamp (1981) , which have been used to translate a subset of English into logical formulas, to model text (identified with a list of sentences), to analyze a fragment of English, and to deal with anaphora. ", "after_sen": "The logical notation of Montague (1970) is more sophisticated, and may be considered another possibility. "}
{"citeStart": 89, "citeEnd": 99, "citeStartToken": 89, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "2.2 Quantification over events (Partee, 1984) extends Hinrichs' treatment of temporal anaphora to the analysis of sentences, which contain a temporal adverbial and quantificatiort over eventualities. According to her analysis, these trigger box-splitting as do if or every clauses in DRT (Kamp, 1981) . Consider the following example from (Partee, 1984): (7) Whenever Mary telephoned, Sam was asleep. The subordinate clause cannot be interpreted relative to a single reference time, since Mary's telephoning is not specified to occur at some specific time. Still, the sentence needs to be interpreted relative to a reference time. This reference time can be a large interval, and should contain each of the relevant occurrences of Mary's telephoning during which Bill was asleep. This reference time is represented as r0 in the top sub-DRS.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2.2 Quantification over events (Partee, 1984) extends Hinrichs' treatment of temporal anaphora to the analysis of sentences, which contain a temporal adverbial and quantificatiort over eventualities. ", "mid_sen": "According to her analysis, these trigger box-splitting as do if or every clauses in DRT (Kamp, 1981) . ", "after_sen": "Consider the following example from (Partee, 1984): (7) Whenever Mary telephoned, Sam was asleep. "}
{"citeStart": 213, "citeEnd": 231, "citeStartToken": 213, "citeEndToken": 231, "sectionName": "UNKNOWN SECTION NAME", "string": "Thougll a 50\"/,, precision and recall might be reasonable for human assisted tasks, like in lexicography, supervised translation, etc., it is not \"fair enough\" if collocational analysis must serve a fully automated system. In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. ", "mid_sen": "Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "after_sen": "In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a \"large enough\" number of words. "}
{"citeStart": 115, "citeEnd": 141, "citeStartToken": 115, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "Connell and Ladd have devised a set of heuristics for identifying key points in an F0 contour to record F0 values (Connell & Ladd, 1990, 21If) . In the absence of a program which enshrines these heuristics, it was decided to develop a system for producing a tone transcription from a sequence of F0 values. Apart from the obvious benefits of automating the process, such as speed and accuracy, it ~'ould show up cases where there is more than one possible tone transcription, possibly with different parameter settings for the F0 scaling function. Having the set of tone transcriptions that are compatible with an utterance has consideral,le value to an analyst, searching for invariances in I.he tonal assignments to individual morphenaes.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Connell and Ladd have devised a set of heuristics for identifying key points in an F0 contour to record F0 values (Connell & Ladd, 1990, 21If) . ", "after_sen": "In the absence of a program which enshrines these heuristics, it was decided to develop a system for producing a tone transcription from a sequence of F0 values. "}
{"citeStart": 41, "citeEnd": 58, "citeStartToken": 41, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction. We train a MaxEnt model for relation detection on true positives and true negatives, which respectively are the subset of correct examples annotated by fp1 (and adjudicated as correct ones) and negative examples that are not annotated in adj, and use it to make predictions on the mixed pool of correct examples, missing examples and spurious ones.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If we adopt a good similarity metric, which captures the structural, lexical and semantic similarity between relation mentions, this analysis will help us to understand the similarity/difference from an extraction perspective.", "mid_sen": "We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction. ", "after_sen": "We train a MaxEnt model for relation detection on true positives and true negatives, which respectively are the subset of correct examples annotated by fp1 (and adjudicated as correct ones) and negative examples that are not annotated in adj, and use it to make predictions on the mixed pool of correct examples, missing examples and spurious ones."}
{"citeStart": 196, "citeEnd": 210, "citeStartToken": 196, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "In the training stage, a feature vector is constructed for each sense-annotated word covered by a semantic model. The features are model-specific, and feature vectors are added to the training set pertaining to the corresponding model. The label of each such feature vector consists of the target word and the corresponding sense, represented as word#sense. Table 1 shows the number of feature vectors constructed in this learning stage for each semantic model. To annotate new text, similar vectors are created for all content-words in the raw text. Similar to the training stage, feature vectors are created and stored separately for each semantic model. Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model. For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001) , which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002) , (Mihalcea, 2002) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Next, word sense predictions are made for all test examples, with a separate learning process run for each semantic model. ", "mid_sen": "For learning, we are using the Timbl memory based learning algorithm (Daelemans et al., 2001) , which was previously found useful for the task of word sense disambiguation (Hoste et al., 2002) , (Mihalcea, 2002) .", "after_sen": "Following the learning stage, each vector in the test data set is labeled with a predicted word and sense. "}
{"citeStart": 217, "citeEnd": 230, "citeStartToken": 217, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "(3) VERB OBJ: A VERB OBJ feature is created in a similar fashion as SUBJ VERB if NP i participates in a verb-object relation. Again, this represents our attempt to coarsely model subcategorization. (4) NE: We use BBN's IdentiFinder (Bikel et al., 1999) , a MUC-style NE recognizer to determine the NE type of NP i . If NP i is determined to be a PERSON or ORGANIZATION, we create an NE feature whose value is simply its MUC NE type. However, if NP i is determined to be a LOCATION, we create a feature with value GPE (because most of the MUC LOCA-TION NEs are ACE GPE NEs). Otherwise, no NE feature will be created (because we are not interested in the other MUC NE types). (5) WN CLASS: For each keyword w shown in the right column of Table 1 , we determine whether the head noun of NP i is a hyponym of w in WordNet, using only the first WordNet sense of NP i . 1 If so, we create a WN CLASS feature with w as its value. These keywords are potentially useful features because some of them are subclasses of the ACE SCs shown in the left column of Table 1 , while others appear to be correlated with these ACE SCs. 2 (6) INDUCED CLASS: Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992) ). Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. An example extraction would be <Eastern Airlines, the carrier>, where the first entry is a proper noun labeled with either one of the seven MUC-style NE types 4 or OTHERS 5 and the second entry is a common noun. We then infer the SC of a common noun as follows: (1) we compute the probability that the common noun co-occurs with each of the eight NE types 6 based on the extracted appositive relations, and (2) if the most likely NE type has a co-occurrence probability above a certain threshold (we set it to 0.7), we create a INDUCED CLASS fea-ture for NP i whose value is the most likely NE type. (7) NEIGHBOR: Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs (see Lin (1998a) ). Motivated by this observation, we create for each of NP i 's ten most semantically similar NPs a NEIGH-BOR feature whose value is the surface string of the NP. To determine the ten nearest neighbors, we use the semantic similarity values provided by Lin's dependency-based thesaurus, which is constructed using a distributional approach combined with an information-theoretic definition of similarity.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2 (6) INDUCED CLASS: ", "mid_sen": "Since the first-sense heuristic used in the previous feature may not be accurate in capturing the SC of an NP, we employ a corpusbased method for inducing SCs that is motivated by research in lexical semantics (e.g., Hearst (1992) ). ", "after_sen": "Given a large, unannotated corpus 3 , we use Identi-Finder to label each NE with its NE type and MINI-PAR to extract all the appositive relations. "}
{"citeStart": 1, "citeEnd": 22, "citeStartToken": 1, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popović and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. More recently, (Goldwater and McClosky, 2005) In general, this line of work focused on translating from morphologically rich languages into English; there has been limited research in MT in the opposite direction. Koehn (2005) includes a survey of statistical MT systems in both directions for the Europarl corpus, and points out the challenges of this task. A recent work (El-Kahlout and Oflazer, 2006) experimented with English-to-Turkish translation with limited success, suggesting that inflection generation given morphological features may give positive results. In the current work, we suggest a probabilistic framework for morphology generation performed as post-processing. It can therefore be considered as complementary to the techniques described above. Our approach is general in that it is not specific to a particular language pair, and is novel in that it allows modelling of agreement on the target side. The framework suggested here is most closely related to (Suzuki and Toutanova, 2006) , which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MT. This work can be viewed as a generalization of (Suzuki and Toutanova, 2006) in that our model generates inflected forms of words, and is not limited to generating a small, closed set of case markers. In addition, the morphology generation problem is more challenging in that it requires handling of complex agreement phenomena along multiple morphological dimensions.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. ", "mid_sen": "(Popović and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. ", "after_sen": "More recently, (Goldwater and McClosky, 2005) In general, this line of work focused on translating from morphologically rich languages into English; there has been limited research in MT in the opposite direction. "}
{"citeStart": 207, "citeEnd": 230, "citeStartToken": 207, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "To summarise, we have argued that discourse structure information will improve summarisation. Other researchers (Ono et al., 1994; Marcu, 1997) have argued similarly, although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987) . In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. Our categories are not hierarchical, and they are much less fine-grained than RST-relations. As mentioned above, we wanted them to a) provide context information for flexible summarisation, b) provide a higher degree of comparability between papers, and c) provide a fairer evaluation of superficially different sentences.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To summarise, we have argued that discourse structure information will improve summarisation. ", "mid_sen": "Other researchers (Ono et al., 1994; Marcu, 1997) have argued similarly, although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987) . ", "after_sen": "In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of appraisal extraction is a generalization of problem formulations developed in earlier works. Mullen and Collier's (2004) notion of classifying appraisal terms using a multidimensional set of attributes is closely tied to the definition of an appraisal expression, which is classified along several dimensions. In previous work (Whitelaw et al., 2005) , we presented a related technique of finding opinion phrases, using a multidimensional set of attributes and modeling the semantics of modifiers in these phrases. The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. Nigam and Hurst's (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni's (2005) opinion mining technique also fits well into our framework.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The use of multiple text classifiers by Wiebe and colleagues (Wilson et al., 2005; Wiebe et al., 2004) for various kinds of sentiment classification can also be viewed as a sentencelevel technique for analyzing appraisal expressions. ", "mid_sen": "Nigam and Hurst's (2004) work on detecting opinions about a certain topic presages our notion of connecting attitudes to targets, while Popescu and Etzioni's (2005) opinion mining technique also fits well into our framework.", "after_sen": "In this paper we describe a system for extracting adjectival appraisal expressions, based on a handbuilt lexicon, a combination of heuristic shallow parsing and dependency parsing, and expectationmaximization word sense disambiguation. "}
{"citeStart": 217, "citeEnd": 242, "citeStartToken": 217, "citeEndToken": 242, "sectionName": "UNKNOWN SECTION NAME", "string": "The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog, but instead needs to be implemented by additional programming. While theoretically possible, it becomes quite problematic to actually implement. The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While theoretically possible, it becomes quite problematic to actually implement. ", "mid_sen": "The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . ", "after_sen": "This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. "}
{"citeStart": 142, "citeEnd": 155, "citeStartToken": 142, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent work views lexical choice as the process of mapping fi'om a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996) . When the same concept admits more than one lexicalization, it is often difficult to choose which of these 'synonyms' is the most appropriate for achieving the desired pragmatic goals: but this is necessary for highquality machine translation and natural language generation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Recent work views lexical choice as the process of mapping fi'om a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996) . ", "after_sen": "When the same concept admits more than one lexicalization, it is often difficult to choose which of these 'synonyms' is the most appropriate for achieving the desired pragmatic goals: but this is necessary for highquality machine translation and natural language generation."}
{"citeStart": 204, "citeEnd": 224, "citeStartToken": 204, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997) . In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997) . ", "after_sen": "In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts."}
{"citeStart": 36, "citeEnd": 49, "citeStartToken": 36, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "The Buckeye corpus also provides an accurate phonetic transcription of its data, showing allophonic variation (e.g. glottal stop, dental/nasal flaps), segment deletions, quality shifts/uncertainty, and nasalization. Some words are \"massively\" reduced (Johnson, 2003) , going well beyond standard phonological rules. We represented its 64 phones using codes with 1-3 characters. Table 2 presents test results for the small corpora. The numbers for the four English dictionary and orthographic transcriptions are very similar. This confirms the finding of Batchelder (2002) that variations in transcription method have only minor impacts on segmenter performance. Performance seems to be largely determined by structural and lexical properties (e.g. word length, pause frequency).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Buckeye corpus also provides an accurate phonetic transcription of its data, showing allophonic variation (e.g. glottal stop, dental/nasal flaps), segment deletions, quality shifts/uncertainty, and nasalization. ", "mid_sen": "Some words are \"massively\" reduced (Johnson, 2003) , going well beyond standard phonological rules. ", "after_sen": "We represented its 64 phones using codes with 1-3 characters. "}
{"citeStart": 102, "citeEnd": 120, "citeStartToken": 102, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003) . Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002) . The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data. We measure translation performance by the BLEU score (Papineni et al., 2002) and Translation Error Rate (TER) (Snover et al., 2006) with one reference for each hypothesis. Word alignment models trained with different constraints are compared to show their effects on the resulting phrase translation tables and the final translation performance.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The maximum number of tokens in Arabic phrases is set to 5 for all conditions.", "mid_sen": "Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh (Koehn et al., 2003) . ", "after_sen": "Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, distortion model, and sentence length penalty. "}
{"citeStart": 235, "citeEnd": 258, "citeStartToken": 235, "citeEndToken": 258, "sectionName": "UNKNOWN SECTION NAME", "string": "It does not make sense to fix absolute v~dues li)r the retrievld, inference lind communication cost plu~uncters in relation to human processing. However, Design-World supports exploring issues about the relative costs of vlu'ions processes. These relative costs might v,'u'y depending,on the language that the agents are communicating with, properties of the communication channel, how smtu't tile agents ,arc, how much time they have, lind what the demands of the ti~sk are [Norm~m and Bobrow, 1975 ]. Below we vary tile relative cost of communication and retrieval.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, Design-World supports exploring issues about the relative costs of vlu'ions processes. ", "mid_sen": "These relative costs might v,'u'y depending,on the language that the agents are communicating with, properties of the communication channel, how smtu't tile agents ,arc, how much time they have, lind what the demands of the ti~sk are [Norm~m and Bobrow, 1975 ]. ", "after_sen": "Below we vary tile relative cost of communication and retrieval."}
{"citeStart": 132, "citeEnd": 141, "citeStartToken": 132, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "In this study, we explore a wider range of features for AVC, focusing particularly on various ways to mix syntactic with lexical information. Dependency relation (DR): Our way to overcome data sparsity is to break lexicalized frames into lexicalized slots (a.k.a. dependency relations). Dependency relations contain both syntactic and lexical information (4). However, augmenting PP with nouns selected by the preposition (e.g. PP(with:fork)) still gives rise to data sparsity. We therefore decide to break it into two individual dependency relations: PP(with), PP-fork. Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004) , their utility in AVC still remains untested.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We therefore decide to break it into two individual dependency relations: PP(with), PP-fork. ", "mid_sen": "Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004) , their utility in AVC still remains untested.", "after_sen": "Co-occurrence (CO): CO features mostly convey lexical information only and are generally considered not particularly sensitive to argument structures (Rohde et al., 2004) . "}
{"citeStart": 67, "citeEnd": 88, "citeStartToken": 67, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993) ; however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents. Sidner (1992; formulated an artificial language for modeling collaborative discourse using propo~acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, ff so, selects appropriate evidence from the system's private beliefs to support the claim.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993) ; however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit different behavior from collaborative agents. ", "after_sen": "Sidner (1992; formulated an artificial language for modeling collaborative discourse using propo~acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. "}
{"citeStart": 41, "citeEnd": 67, "citeStartToken": 41, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "The collaborative planning principle in (Whittak~ and Stenton, 1988; Walker, 1992) suggests that \"conversants must provide evidence of a detected discrepancy in belief as soon as possible.\" Thus, once an agent detects a relevant conflict, she must notify the other agent of the conflict and initiate a negotiation subdialogne to resolve it-to do otherwise is to fail in her responsibility as a collaborative agent. We capture the attempt to resolve a conflict with the problem-solving action Modify-Proposal, whose goal is to modify the proposal to a form that will potentially be accepted by both agents. When applied to belief modification, Modify-Proposal has two specializations: Correct-Node, for when a proposed belief is not accepted, and Correct-Relation, for when a proposed evidential relationship is not accepted. Figure 2 shows the problem-solving recipes 4 for Correct-Node and its subaction, Modify-Node, that is responsible for the actual modification of the proposal. The applicability conditions 5 of Correct-Node specify that the action can only be invoked when _sl believes that _node is not acceptable while _s2 believes that it is (when _sl and _s2 disagree about the proposed belief represented by ..node). However, since this is a collaborative interaction, the actual modification can only be performed when both ..sl and _s2 believe that _node is not acceptable w that is, the conflict between _sl and .s2 must have been resolved. This is captured by 4A recipe (Pollack, 1986 ) is a template for performing actions. It contains the applicabifity conditions for performing an action, the subactions comprising the body of an action, etc.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The collaborative planning principle in (Whittak~ and Stenton, 1988; Walker, 1992) suggests that \"conversants must provide evidence of a detected discrepancy in belief as soon as possible.\" Thus, once an agent detects a relevant conflict, she must notify the other agent of the conflict and initiate a negotiation subdialogne to resolve it-to do otherwise is to fail in her responsibility as a collaborative agent. ", "after_sen": "We capture the attempt to resolve a conflict with the problem-solving action Modify-Proposal, whose goal is to modify the proposal to a form that will potentially be accepted by both agents. "}
{"citeStart": 93, "citeEnd": 111, "citeStartToken": 93, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "There is typically a correspondence between the reparandum and the alteration, and following Bear et al. (1992) , we annotate this using the labels m for word matching and r for word replacements (words of the same syntactic category). Each pair is given a unique index. Other words in the reparandum and alteration are annotated with an x. Also, editing terms (filled pauses and clue words) are labeled with et, and the interruption point with ip, which will occur before any editing terms associated with the repair, and after a word fragment, if present. The interruption point is also marked as to whether the repair is a fresh start, modification repair, or abridged repair, in which cases, we use ip:ean, ip:mod and ip:abr, respectively. The example below illustrates how a repair is annotated in this scheme.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "There is typically a correspondence between the reparandum and the alteration, and following Bear et al. (1992) , we annotate this using the labels m for word matching and r for word replacements (words of the same syntactic category). ", "after_sen": "Each pair is given a unique index. "}
{"citeStart": 172, "citeEnd": 186, "citeStartToken": 172, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesni~re, 1959; Hudson, 1993) . The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesni~re, 1959; Hudson, 1993) . ", "after_sen": "The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals."}
{"citeStart": 40, "citeEnd": 56, "citeStartToken": 40, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "We used two datasets, customer reviews (Hu and Liu, 2004) and movie reviews (Pang and Lee, 2005) to evaluate sentiment classification of sentences. Both of these two datasets are often used for evaluation in sentiment analysis researches. The number of examples and other statistics of the datasets are shown in Table 1 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used two datasets, customer reviews (Hu and Liu, 2004) and movie reviews (Pang and Lee, 2005) to evaluate sentiment classification of sentences. ", "after_sen": "Both of these two datasets are often used for evaluation in sentiment analysis researches. "}
{"citeStart": 57, "citeEnd": 81, "citeStartToken": 57, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that the formulation of the adjunction generating function means that the values for ¢(X ~4 nil) for all X E V do not appear in the expectation matrix. This is a crucial difference between the test for consistency in TAGs as compared to CFGs. For CFGs, the expectation matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G). Using this it was shown in (Chaudhari et al., 1983) and (S£nchez and Bened~, 1997 ) that a single step of the inside-outside algorithm implies consistency for a probabilistic CFG. However, in the TAG case, the inclusion of values for ¢(X ~-+ nil) (which is essen-tim if we are to interpret the expectation matrix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For CFGs, the expectation matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G). ", "mid_sen": "Using this it was shown in (Chaudhari et al., 1983) and (S£nchez and Bened~, 1997 ) that a single step of the inside-outside algorithm implies consistency for a probabilistic CFG. ", "after_sen": "However, in the TAG case, the inclusion of values for ¢(X ~-+ nil) (which is essen-tim if we are to interpret the expectation matrix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent."}
{"citeStart": 154, "citeEnd": 172, "citeStartToken": 154, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, I discuss one such entity reference resolution algorithm for a general geo-political business domain developed for SRI's FASTUS TM system (Hobbs et al., 1996) , one of the leading IE systems, which can also be seen as a representative of today's IE technology.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, it is reasonable to assume that a largely domain-independent method of reference resolution can be developed, which need not be tailored anew each time a new target is defined.", "mid_sen": "In this paper, I discuss one such entity reference resolution algorithm for a general geo-political business domain developed for SRI's FASTUS TM system (Hobbs et al., 1996) , one of the leading IE systems, which can also be seen as a representative of today's IE technology.", "after_sen": "IThe IE technology has undergone a rapid development in the 1990s driven by the series of Message Understanding Conferences (MUCs) in the U.S. government-sponsored TIPSTER program (http ://www. tipster, org)."}
{"citeStart": 25, "citeEnd": 61, "citeStartToken": 25, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Event referential forms such as do it, do tha~, and do so constitute full verb phrases in the syntax. It has been often noted (Halliday and Hasan, 1976, inter alia) that it is the main verb do that is operative in these forms of anaphora, in contrast to the auxiliary do operative in VP-ellipsis/ It is the pronoun in event referential forms that is anaphoric; the fact that the pronouns refer to events results from the type constraints imposed by the main verb do. Therefore, such forms are anaphoric in the semantics, but do not leave behind an empty constituent in the syntax.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Event referential forms such as do it, do tha~, and do so constitute full verb phrases in the syntax. ", "mid_sen": "It has been often noted (Halliday and Hasan, 1976, inter alia) that it is the main verb do that is operative in these forms of anaphora, in contrast to the auxiliary do operative in VP-ellipsis/ It is the pronoun in event referential forms that is anaphoric; the fact that the pronouns refer to events results from the type constraints imposed by the main verb do. ", "after_sen": "Therefore, such forms are anaphoric in the semantics, but do not leave behind an empty constituent in the syntax."}
{"citeStart": 171, "citeEnd": 189, "citeStartToken": 171, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007) , or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006) . These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006) , who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Past work on discriminative SMT only address some of these problems. ", "mid_sen": "To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007) , or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006) . ", "after_sen": "These systems all include regularisation, thereby addressing Problem 2. "}
{"citeStart": 127, "citeEnd": 145, "citeStartToken": 127, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance. Acquiring labeled data is a difficult and expensive task. Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002) . The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000) . In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002) . On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; . This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels. One advantage of our formalism is that it allows capturing different levels of constraint violation. Our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data while learning (see Section 5. In the experimental part of this paper we use HMMs as the underlying model, and exhibit significant reduction in the number of training examples required in two information extraction problems.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets. ", "mid_sen": "However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000) . ", "after_sen": "In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002) . "}
{"citeStart": 183, "citeEnd": 204, "citeStartToken": 183, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "Example. Consider the following word group: 5 burglars thief rob mugging stray robbing lookout chase crate thieves 3An alternative worth mentioning, however, is the distributional approach of Pereira et al. (1993) : within their representational scheme, distributionaUy defined word senses emerge automatically in the form of cluster centroids.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Example. ", "mid_sen": "Consider the following word group: 5 burglars thief rob mugging stray robbing lookout chase crate thieves 3An alternative worth mentioning, however, is the distributional approach of Pereira et al. (1993) : within their representational scheme, distributionaUy defined word senses emerge automatically in the form of cluster centroids.", "after_sen": "4One could also say that ~ defines a fuzzy set. "}
{"citeStart": 29, "citeEnd": 53, "citeStartToken": 29, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "F-structures can mathematically be regarded as (finite) functions from a set of attributes to a set of atomic values, semantic forms and (recursively) f-structures. We will identify an occurrence of an f-structure with a path (from the root) to that occurrence; sets of occurrences of an f-structure can therefore be identified with path sets in the f-structure. We take, then, the domain of the a projection to be path sets in the root f-structure. Only those path sets S are considered which satisfy the property that the extensions of each path in S are identical. Therefore the f-structure reached by each of these paths is identical. Hence from a path set S, we can read off an f-structure S I. In the examples discussed in Dalrymple et al. (1993a) there is a one-to-one correspondence between the set of path sets S and the set of f-structures S I picked out by such path sets, so the two methods yield the same predictions for those cases.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Hence from a path set S, we can read off an f-structure S I. ", "mid_sen": "In the examples discussed in Dalrymple et al. (1993a) there is a one-to-one correspondence between the set of path sets S and the set of f-structures S I picked out by such path sets, so the two methods yield the same predictions for those cases.", "after_sen": "Relations between path sets are represented explicitly as resources in the logic by R-relations."}
{"citeStart": 35, "citeEnd": 47, "citeStartToken": 35, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "Our first goal was to find out whether system combination could improve performance of baseNP recognition and, if this was the fact, to select the best combination technique. For this purpose we performed a 10-fold cross validation experiment on the baseNP training data, sections 15-18 of the WSJ part of the Penn Treebank (211727 tokens). Like the data used by (Ramshaw and Marcus, 1995) , this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags 3. The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by (Ramshaw and Marcus, 1995) . The data was converted to the five data representations (IOB1, IOB2, IOE1, IOE2 and O+C) and", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For this purpose we performed a 10-fold cross validation experiment on the baseNP training data, sections 15-18 of the WSJ part of the Penn Treebank (211727 tokens). ", "mid_sen": "Like the data used by (Ramshaw and Marcus, 1995) , this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags 3. ", "after_sen": "The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by (Ramshaw and Marcus, 1995) . "}
{"citeStart": 235, "citeEnd": 254, "citeStartToken": 235, "citeEndToken": 254, "sectionName": "UNKNOWN SECTION NAME", "string": "Specialized glossaries serve two functions: Firstly, they are linguistic resources summarizing the terminological basis of a specialized domain. Secondly, they are knowledge resources, in that they provide definitions of concepts which the terms denote. Glossaries find obvious use as sources of reference. A survey on the use of lexicographical aids in specialized translation showed that glossaries are among the top five resources used (Durán-Muñoz, 2010) . Glossaries have also been shown to facilitate reception of texts and acquisition of knowledge during study (Weiten et al., 1999) , while selfexplanation of reasoning by referring to definitions has been shown to promote understanding (Aleven et al., 1999) . From a machine-processing point of view, glossaries may be used as input for domain ontology induction; see, e.g. (Bozzato et al., 2008) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A survey on the use of lexicographical aids in specialized translation showed that glossaries are among the top five resources used (Durán-Muñoz, 2010) . ", "mid_sen": "Glossaries have also been shown to facilitate reception of texts and acquisition of knowledge during study (Weiten et al., 1999) , while selfexplanation of reasoning by referring to definitions has been shown to promote understanding (Aleven et al., 1999) . ", "after_sen": "From a machine-processing point of view, glossaries may be used as input for domain ontology induction; see, e.g. (Bozzato et al., 2008) ."}
{"citeStart": 64, "citeEnd": 92, "citeStartToken": 64, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "TPLEX's boundary detectors are similar to those learned by BWI (Freitag and Kushmerick, 2000) . A boundary detector has two parts, a left pattern and a right pattern. Each of these patterns is a sequence of tokens, where each is either a literal or a generalized token. For example, the boundary detector would correctly find the start of a name in an utterance such as \"will be: Dr Robert Boyle\" and \"will be, Sandra Frederick\", but it will fail to identify the start of the name in \"will be Dr. Robert Boyle\". The boundary detectors that find the beginnings of fragments are called the pre-patterns, and the detectors that find the ends of fragments are called the post-patterns.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This strategy has previously been employed successfully (Freitag and Kushmerick, 2000; Ciravegna, 2001; Yangarber et al., 2002; Finn and Kushmerick, 2004) .", "mid_sen": "TPLEX's boundary detectors are similar to those learned by BWI (Freitag and Kushmerick, 2000) . ", "after_sen": "A boundary detector has two parts, a left pattern and a right pattern. "}
{"citeStart": 92, "citeEnd": 101, "citeStartToken": 92, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "The UG used in our experiments consists of 700 lexical entries and 60 rules. We used a variant of inside-outside training to estimate a model of UG derivations. It is a rule bigram model similar to PCFG with special extensions for UG type operations. The probability of future unifications is made dependent from the result type of earlier unifications. The model is described in more detail in (Weber 1994a; Weber 1995) ; it is very similar to (Brew 1995) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The probability of future unifications is made dependent from the result type of earlier unifications. ", "mid_sen": "The model is described in more detail in (Weber 1994a; Weber 1995) ; it is very similar to (Brew 1995) .", "after_sen": ""}
{"citeStart": 246, "citeEnd": 265, "citeStartToken": 246, "citeEndToken": 265, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. The PERSIVAL project, the most comprehensive study of such techniques applied on medical texts to date, leverages patient records to generate personalized summaries in response to physicians' queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al. 2005 ). Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. Patient information is no doubt important to answering clinical questions, and our work could certainly benefit from experiences gained in the PERSIVAL project.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition to question answering, multi-document summarization provides a complementary approach to addressing clinical information needs. ", "mid_sen": "The PERSIVAL project, the most comprehensive study of such techniques applied on medical texts to date, leverages patient records to generate personalized summaries in response to physicians' queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al. 2005 ). ", "after_sen": "Although the system incorporates both a user and a task model, it does not explicitly capture the principles of evidence-based medicine. "}
{"citeStart": 0, "citeEnd": 15, "citeStartToken": 0, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, the field of dialectometry, as introduced by S~guy (1971, 1973) , has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. There is however no firm agreement on just how to compute the distance matrices. Sfiguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make. The need to figure out such systems as the comparative phonology of various linguistic sites can be very time-consuming and fraught with arbitrary choices.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. ", "mid_sen": "Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. ", "after_sen": "There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make. "}
{"citeStart": 197, "citeEnd": 220, "citeStartToken": 197, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "The most relevant prior work is (Wiebe et al. 98) , who dealt with meeting scheduling dialogs (see also (Alexandersson et al. 97) , (Busemann et al. 97)) , where the goal is to schedule a time for the meeting. The temporal references in meeting scheduling are somewhat more constrained than in news, where (e.g., in a historical news piece on toxic dumping) dates and times may be relatively unconstrained. In addition, their model requires the maintenance of a focus stack. They obtained roughly .91 Precision and .80 Recall on one test set, and .87 Precision and .68 Recall on another. However, they adjust the reference time during processing, which is something that we have not yet addressed. More recently, (Setzer and Gaizauskas 2000) have independently developed an annotation scheme which represents both time values and more fine-grained interevent and event-time temporal relations. Although our work is much more limited in scope, and doesn't exploit the internal structure of events, their annotation scheme may be leveraged in evaluating aspects of our work. The MUC-7 task (MUC-7 98) did not require VALs, but did test TIMEX recognition accuracy. Our 98 F-measure on NYT can be compared for just TIMEX with MUC-7 (MUC-7 1998) results on similar news stories, where the best performance was .99 Precision and .88 Recall. (The MUC task required recognizing a wider variety of TIMEXs, including event-dependent ones. However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. ) Finally, there is a large body of work, e.g., (Moens and Steedman 1988) , (Passoneau 1988) , (Webber 1988) , (Hwang 1992) , (Song and Cohen 1991) , that has focused on a computational analysis of tense and aspect. While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(The MUC task required recognizing a wider variety of TIMEXs, including event-dependent ones. ", "mid_sen": "However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. ) Finally, there is a large body of work, e.g., (Moens and Steedman 1988) , (Passoneau 1988) , (Webber 1988) , (Hwang 1992) , (Song and Cohen 1991) , that has focused on a computational analysis of tense and aspect. ", "after_sen": "While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work."}
{"citeStart": 21, "citeEnd": 35, "citeStartToken": 21, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "Meaning-Text Theory (Melc'flk, 1988) assumes seven strata of representation. The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. These rules have not yet been formally specified (Melc'~k & Pertsov, 1987p.187f ) (but see the proposal by Rambow & Joshi (in print)).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary categorial operators; the second difference is the avoidance of an underlying structure, which stratifies the theory and makes incremental processing difficult.", "mid_sen": "Meaning-Text Theory (Melc'flk, 1988) assumes seven strata of representation. ", "after_sen": "The rules mapping from the unordered dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep-morphological representations include global ordering rules which allow discontinuities. "}
{"citeStart": 99, "citeEnd": 118, "citeStartToken": 99, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . ", "mid_sen": "Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "after_sen": ") ( ) ( ) , ( log ) , ( 2 f P w P f w P f w MI ="}
{"citeStart": 41, "citeEnd": 60, "citeStartToken": 41, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "Another common approach to lexical rules is to encode them as unary phrase structure rules. This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31) . A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990; Emele 1994 ). The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This approach is taken, for example, in LKB (Copestake 1992) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules (Copestake 1993, 31) . ", "mid_sen": "A similar method is included in PATR-II (Shieber et al. 1983) and can be used to encode lexical rules as binary relations in the CUF system (Dbrre and Eisele 1991; D6rre and Dorna 1993b) or the TFS system (Emele and Zajac 1990; Emele 1994 ). ", "after_sen": "The covariation approach described in this paper can be viewed as a domain-specific refinement of such a treatment of lexical rules."}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "The AdaBoost algorithm was presented by Freund and Schapire in 1996 (Freund and Schapire, 1996; Freund and Schapire, 1997) and has become a widely-known successful method in machine learning. The AdaBoost algorithm imposes one constraint on its underlying learner: it may abstain from making predictions about labels of some samples, 1This is the balanced version ofF-measure, where precision and recall are weighted equally. but it must consistently be able to get more than 50°-/o accuracy on the samples for which it commits to a decision. That accuracy is measured according to the distribution describing the importance of samples that it is given. The learner must be able to get more correct samples than incorrect samples by mass of importance on those that it labels. This statement of the restriction comes from Schapire and Singer's study (1998) . It is called the weak learning criterion. Schapire and Singer (1998) extended AdaBoost by describing how to choose the hypothesis mixing coefficients in certain circumstances and how to incorporate a general notion of confidence scores. They also provided a better characterization of its theoretical performance. The version of AdaBoost used in their work is shown in Algorithm 3, as it is the version that most amenable to parsing.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is called the weak learning criterion. ", "mid_sen": "Schapire and Singer (1998) extended AdaBoost by describing how to choose the hypothesis mixing coefficients in certain circumstances and how to incorporate a general notion of confidence scores. ", "after_sen": "They also provided a better characterization of its theoretical performance. "}
{"citeStart": 53, "citeEnd": 74, "citeStartToken": 53, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "The PAM output in terms of SL phrases is then handed over to the Phrasing model generator (PMG), which is trained to determine the phrasal structure of an input sentence. PMG reads the SL phrasing as defined by PAM and generates an SL phrasing model using a probabilistic methodology. This phrasing model is then applied in segmenting any arbitrary SL text being input to the PRESEMT system for translation. PMG is based on the Conditional Random Fields model (Lafferty et al., 1999) which has been found to provide the highest accuracy. The SL text segmented into phrases by PMG is then input to the 1 st translation phase. For a new language pair, the PAM-PMG chain is implemented without any manual correction of outputs.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This phrasing model is then applied in segmenting any arbitrary SL text being input to the PRESEMT system for translation. ", "mid_sen": "PMG is based on the Conditional Random Fields model (Lafferty et al., 1999) which has been found to provide the highest accuracy. ", "after_sen": "The SL text segmented into phrases by PMG is then input to the 1 st translation phase. "}
{"citeStart": 47, "citeEnd": 58, "citeStartToken": 47, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). The question then arises of whether the bound of two is tight. More generally, in this paper we investigate which re-strictions on TAGs are needed in order to lower tile O(n 6) time complexity, still retaining the generative power that is needed to capture the syntactic constructions of natural language that unrestricted TAGs can handle. The contribution of this paper is twofold:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More generally, in (Satta, 1994) it has been argued that methods for TAG parsing running in time asymptotically faster than (9(n 6) are unlikely to have small hidden constants.", "mid_sen": "A careful inspection of the proof provided in (Satta, 1994) reveals that the source of the claimed computational complexity of TAG parsing resides in the fact that auxiliary trees can get adjunctions at (at least) two distinct nodes in their spine (the path connecting the root and the foot nodes). ", "after_sen": "The question then arises of whether the bound of two is tight. "}
{"citeStart": 171, "citeEnd": 187, "citeStartToken": 171, "citeEndToken": 187, "sectionName": "UNKNOWN SECTION NAME", "string": "• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• Site based corpus annotation -in which the user can specify a web site to annotate • Domain based corpus annotation -in which the user specifies a content domain (with the use of keywords) to annotate • Crawler based corpus annotation -more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). ", "mid_sen": "Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002) .", "after_sen": "The second stage of our work will involve implementing the framework within a P2P environment. "}
{"citeStart": 184, "citeEnd": 198, "citeStartToken": 184, "citeEndToken": 198, "sectionName": "UNKNOWN SECTION NAME", "string": "5 Note that we are guaranteed to get exactly 50 standardized random baseline if random amb = 100 . sults is again somewhat di cult. Firstly, these approaches were evaluated on words with two clearly distant senses which w ere determined by the experimenters. In contrast, our method was evalutated on randomly selected actual translations of a large bilingual corpus. Furthermore, these approaches use large amounts of information in terms of linguistic categorizations, large context windows, or even manual intervention such as initial sense seeding Yarowsky, 1995 . Such information is easily obtainable, e.g., in IR applications, but often burdensome to gather or simply unavailable in situations such as incremental parsing or translation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, our method was evalutated on randomly selected actual translations of a large bilingual corpus. ", "mid_sen": "Furthermore, these approaches use large amounts of information in terms of linguistic categorizations, large context windows, or even manual intervention such as initial sense seeding Yarowsky, 1995 . ", "after_sen": "Such information is easily obtainable, e.g., in IR applications, but often burdensome to gather or simply unavailable in situations such as incremental parsing or translation."}
{"citeStart": 37, "citeEnd": 53, "citeStartToken": 37, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "The domain model represents a hierarchy of domain-relevant concept nodes, together with associated properties. It is expressed in the XI formalism (Gaizauskas, 1995) which provides a basic inheritance mechanism for property values and the ability to represent multiple classificatory dimensions in the hierarchy. Instances of concepts mentioned in a text are added to the domain model, populating it to become a text-, or discourse-, specific model.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The domain model represents a hierarchy of domain-relevant concept nodes, together with associated properties. ", "mid_sen": "It is expressed in the XI formalism (Gaizauskas, 1995) which provides a basic inheritance mechanism for property values and the ability to represent multiple classificatory dimensions in the hierarchy. ", "after_sen": "Instances of concepts mentioned in a text are added to the domain model, populating it to become a text-, or discourse-, specific model."}
{"citeStart": 161, "citeEnd": 172, "citeStartToken": 161, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "3 Since we did not have access to the output or to the system of (Subba and Di-Eugenio, 2009), we were not able to perform a significance test on the Instructional corpus. window approach to be more effective in finding those, without inducing much noise for the labels. This clearly demonstrates the potential of TSP SW for datasets with even more leaky boundaries e.g., the Dutch (Vliet and Redeker, 2011) and the German Potsdam (Stede, 2004) corpora. Error analysis reveals that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in a few cases, it tends to induce noise from the neighboring sentences for the labels. For example, when parsing was performed on the first sentence in Figure 1 in isolation using 1S-1S, our parser rightly identifies the Contrast relation between EDUs 2 and 3. But, when it is considered with its neighboring sentences by the sliding window, the parser labels it as Elaboration. A promising strategy to deal with this and similar problems that we plan to explore in future, is to apply both approaches to each sentence and combine them by consolidating three probabilistic decisions, i.e. the one from 1S-1S and the two from sliding window.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "window approach to be more effective in finding those, without inducing much noise for the labels. ", "mid_sen": "This clearly demonstrates the potential of TSP SW for datasets with even more leaky boundaries e.g., the Dutch (Vliet and Redeker, 2011) and the German Potsdam (Stede, 2004) corpora. ", "after_sen": "Error analysis reveals that although TSP SW finds more correct structures, a corresponding improvement in labeling relations is not present because in a few cases, it tends to induce noise from the neighboring sentences for the labels. "}
{"citeStart": 131, "citeEnd": 143, "citeStartToken": 131, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "paradigmatic relation: how the words are associated with each other. Similarity between words can be defined by either a syntagmatic or a paradigmatic relation. Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. This paper concentrates on paradigmatic similarity, because a paradigmatic relation can be established both inside a sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside a sentence --like syntax deals with sentence structure. The rest of this section focuses on two related works on measuring paradigmatic similarity --a psycholinguistic approach and a thesaurus-based approach.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Syntagmatic similarity is based on co-occurrence data extracted from corpora [Church and Hanks, 1990] , definitions in dictionaries [Wilks etal., 1989] , and so on. ", "mid_sen": "Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991] , psychological experiments [Osgood, 1952] , and so on. ", "after_sen": "This paper concentrates on paradigmatic similarity, because a paradigmatic relation can be established both inside a sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside a sentence --like syntax deals with sentence structure. "}
{"citeStart": 62, "citeEnd": 88, "citeStartToken": 62, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "An alternative solution to this second step was suggested in (Mihalcea and Faruque, 2004) , using semantic generalizations learned from dependencies identified between nodes in a conceptual network. Their approach however, although slightly more accurate, conflicted with our goal of creating an efficient WSD system, and therefore we opted for the simpler backoff method that employs WordNet sense frequencies.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The words that are not covered by these models (typically about 10-15% of the words in the test corpus) are assigned with the most frequent sense in WordNet.", "mid_sen": "An alternative solution to this second step was suggested in (Mihalcea and Faruque, 2004) , using semantic generalizations learned from dependencies identified between nodes in a conceptual network. ", "after_sen": "Their approach however, although slightly more accurate, conflicted with our goal of creating an efficient WSD system, and therefore we opted for the simpler backoff method that employs WordNet sense frequencies."}
{"citeStart": 197, "citeEnd": 215, "citeStartToken": 197, "citeEndToken": 215, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we use the Meta-Bootstrapping and Basilisk algorithms to learn lists of subjective nouns from a large collection of unannotated texts. Then we train a subjectivity classifier on a small set of annotated data, using the subjective nouns as features along with some other previously identified subjectivity features. Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al., 1999) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Then we train a subjectivity classifier on a small set of annotated data, using the subjective nouns as features along with some other previously identified subjectivity features. ", "mid_sen": "Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al., 1999) .", "after_sen": ""}
{"citeStart": 180, "citeEnd": 206, "citeStartToken": 180, "citeEndToken": 206, "sectionName": "UNKNOWN SECTION NAME", "string": "A test that does not use equation 1 but still makes an assumption of independence between x 1 and x 2 is that of using contingency tables with the chi-squared (χ 2 ) distribution (Box et al., 1978, Sec. 5.7) . When the assumption is valid, this test is good for comparing differences in the precision metric. Precision is the fraction of the items \"found\" by some technique that are actually of interest.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A test that does not use equation 1 but still makes an assumption of independence between x 1 and x 2 is that of using contingency tables with the chi-squared (χ 2 ) distribution (Box et al., 1978, Sec. 5.7) . ", "after_sen": "When the assumption is valid, this test is good for comparing differences in the precision metric. "}
{"citeStart": 75, "citeEnd": 90, "citeStartToken": 75, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Correlation coefficients for experiments measuring semantic relatedness are expected to be lower than results for semantic similarity, since the former also includes additional relations (like cooccurrence of words) and is thus a more complicated task. Judgments for such relations strongly depend on experience and cultural background of the test subjects. While most people may agree 10 Note that Resnik used the averaged correlation coefficient. We computed the summarized correlation coefficient using a Fisher Z-value transformation. that (car -vehicle) are highly related, a strong connection between (parts -speech) may only be established by a certain group. Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. Therefore, inter-subject correlation is lower than the results obtained by Gurevych (2006) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Due to the corpusbased approach, many domain-specific concept pairs are introduced into the test set. ", "mid_sen": "Therefore, inter-subject correlation is lower than the results obtained by Gurevych (2006) .", "after_sen": "In our experiment, intra-subject correlation was r=.670 for the first and r=.623 for the second individual who repeated the experiment, yielding a summarized intra-subject correlation of r=.647. "}
{"citeStart": 112, "citeEnd": 137, "citeStartToken": 112, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. Because their preparation needs a lot of manual assistance or an unerring tagger or parser, this requirement makes their algorithm~, troublesome in actual application environments. On the other hand, the algorithm in this paper has no such requirement, it requires only a minimum of linguistic knowledge, including parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules which lexicon based syntactic theories like HPSG CC etc. normally assume. The parser is not a deterministic parser, but a parser which produces all possible analyses. All of the results are used for calculation ant the system assumes that there is a correct answer among them. The algorithm builds correct structural descriptions of sentences and discovers semantic collocations at the same time. It works as a relaxation process.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We expect that the knowledge to be extracted will not only be useful for disambiguating sentences but also will contribute to discovering ontological classes in given subject domains.", "mid_sen": "Though several studies with similar objectives have been reported [Church, 1988] , [Zernik and Jacobs, 1990] , [Calzolari and Bindi, 1990] , [Garside and Leech, 1985] , [Hindle and Rooth, 1991] , [Brown et al., 1990] , they require that sample corpora be correctly analyzed or tagged in advance. ", "after_sen": "It must be a training corpus, which is tagged or parsed by human or it needs correspondence between two language corpora. "}
{"citeStart": 34, "citeEnd": 55, "citeStartToken": 34, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "Implementing the \"local\" or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "∀l < m − 1, a i,j,k,l •a i ,j ,l+1,l • |i − j − 1| < d", "mid_sen": "Implementing the \"local\" or MJ-d (Kumar and Byrne, 2005) reordering strategy is also straightforward, and implies using the following constraints:", "after_sen": "∀i, k, i ≤i a i ,j ,k ,l − k ≤k a i ,j ,k ,l ≤ d"}
{"citeStart": 207, "citeEnd": 226, "citeStartToken": 207, "citeEndToken": 226, "sectionName": "UNKNOWN SECTION NAME", "string": "The definition was used to manually annotate several corpora of newswire texts, using SGML markup to indicate relations between text strings. Automatically annotated texts, produced by systems using the same markup scheme, were then compared with the manually annotated versions, using scoring software made available to MUC participants, based on (Vilain et al., 1995) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The definition was used to manually annotate several corpora of newswire texts, using SGML markup to indicate relations between text strings. ", "mid_sen": "Automatically annotated texts, produced by systems using the same markup scheme, were then compared with the manually annotated versions, using scoring software made available to MUC participants, based on (Vilain et al., 1995) .", "after_sen": "The scoring software calculates the standard Information Retrieval metrics of 'recall' and 'precision', 2 together with an overall f-measure. "}
{"citeStart": 127, "citeEnd": 145, "citeStartToken": 127, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "Bilingnal (or parallel) texts are useful as resources of linguistic knowledge as well as in applications such as machine translation. One of the major approaches to analyzing bilingual texts is the statistical approach. The statistical approach involves the following: alignment of bilingual texts at the sentence level nsing statistical techniques (e.g. Brown, Lai and Mercer (1991) , Gale and Church (1993) , Chen (1993) , and Kay and RSscheisen (1993) ), statistical machine translation models (e.g. Brown, Cooke, Pietra, Pietra et al. (1990) ), finding character-level / word-level / phrase-level correspondences from bilingual texts (e.g. Gale and Church (1991) , Church (1993) , and Kupiec (1993)), and word sense disambiguation for MT (e.g. Dagan, Itai and Schwall (1991) ). In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics. For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters (Brown et al., 1991; Gale and Church, 1993) , or by statistically estimating word level correspondences (Chen, 1993; Kay and RSscheisen, 1993) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In general, the statistical approach does not use existing hand-written bilingual dictionaries, and depends solely upon statistics. ", "mid_sen": "For example, sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters (Brown et al., 1991; Gale and Church, 1993) , or by statistically estimating word level correspondences (Chen, 1993; Kay and RSscheisen, 1993) .", "after_sen": "The statistical approach analyzes unstructured sentences in bilingual texts, and it is claimed that the results are useful enough in real applications such as machine translation and word sense disambiguation. "}
{"citeStart": 168, "citeEnd": 180, "citeStartToken": 168, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about 'attributes'-only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network-and this information is still very sparse. On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in Word-Net: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002) . 2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of 'attribute' and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically (e.g., to supplement WordNet).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "our disposal, WordNet (Fellbaum, 1998) contains very little information that would be considered as being about 'attributes'-only information about parts, not about qualities such as height, or even to the values of such attributes in the adjective network-and this information is still very sparse. ", "mid_sen": "On the other hand, the only work on the extraction of lexical semantic relations we are aware of has concentrated on the type of relations found in Word-Net: hyponymy (Hearst, 1998; Caraballo, 1999) and meronymy (Berland and Charniak, 1999; Poesio et al, 2002) . 2 The work discussed here could be perhaps best described as an example of empirical ontology: using linguistics and philosophical ideas to improve the results of empirical work on lexical / ontology acquisition, and vice versa, using findings from empirical analysis to question some of the assumptions of theoretical work on ontology and the lexicon. ", "after_sen": "Specifically, we discuss work on the acquisition of (nominal) concept attributes whose goal is twofold: on the one hand, to clarify the notion of 'attribute' and its role in lexical semantics, if any; on the other, to develop methods to acquire such information automatically (e.g., to supplement WordNet)."}
{"citeStart": 44, "citeEnd": 62, "citeStartToken": 44, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "The development data was also annotated by three native English speaking experts (computational linguists with extensive linguistic background, two of whom are also authors of this paper). The inter-annotator agreement among these judges is very high, with pairwise observed agreements of 1.00, 0.90, and 0.90, and corresponding unweighted Kappa scores of 1.00, 0.79, and 0.79. The majority judgements of these annotators are the same as those obtained from AMT on the development data, giving us confidence in the reliability of the AMT judgements. These findings are consistent with those of Snow et al. (2008) in showing that AMT judgements can be as reliable as those of expert judges.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The majority judgements of these annotators are the same as those obtained from AMT on the development data, giving us confidence in the reliability of the AMT judgements. ", "mid_sen": "These findings are consistent with those of Snow et al. (2008) in showing that AMT judgements can be as reliable as those of expert judges.", "after_sen": "Finally, we remove a small number of items from the testing dataset which were difficult to paraphrase due to ellipsis of the verb participating in the target construction, or an extra negation in the verb phrase. "}
{"citeStart": 32, "citeEnd": 59, "citeStartToken": 32, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "To specify patterns, following (Davidov and Rappoport, 2006) we classify words into highfrequency words (HFWs) and content words (CWs). A word whose frequency is more (less) than F H (F C ) is considered to be a HFW (CW). Our patterns have the general form", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To specify patterns, following (Davidov and Rappoport, 2006) we classify words into highfrequency words (HFWs) and content words (CWs). ", "after_sen": "A word whose frequency is more (less) than F H (F C ) is considered to be a HFW (CW). "}
{"citeStart": 147, "citeEnd": 157, "citeStartToken": 147, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "Our goal is to learn a representation for each word of interest. Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space (Roth, 1998; Roth, 1999) . Therefore, in order to learn expressive representations one needs to compose complex features as a function of the information sources available. A linear function expressed directly in terms of those will not be expressive enough. We now define a language that allows 1We denote IS(s) as IS wherever it is obvious what the referred sentence we is, or whenever we want to indicate Information Source in general. one to define \"types\" of features 2 in terms of the information sources available to it.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our goal is to learn a representation for each word of interest. ", "mid_sen": "Most efficient learning methods known today and, in particular, those used in NLP, make use of a linear decision surface over their feature space (Roth, 1998; Roth, 1999) . ", "after_sen": "Therefore, in order to learn expressive representations one needs to compose complex features as a function of the information sources available. "}
{"citeStart": 153, "citeEnd": 166, "citeStartToken": 153, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. ", "mid_sen": "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . ", "after_sen": "But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. "}
{"citeStart": 12, "citeEnd": 27, "citeStartToken": 12, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we are interested in dealing with the second problem of the construction of an organized lexical resource i.e. discovering generalspecific noun relationships, so that correct nouns are chosen to label internal nodes of any hierarchical knowledge base, such as the one proposed in (Dias et al., 2006) . Most of the works proposed so far have (1) used predefined patterns or (2) automatically learned these patterns to identify hypernym/hyponym relationships. From the first paradigm, (Hearst, 1992) first identifies a set of lexico-syntactic patterns that are easily recognizable i.e. occur frequently and across text genre boundaries. These can be called seed patterns. Based on these seeds, she proposes a bootstrapping algorithm to semi-automatically acquire new more specific patterns. Similarly, (Caraballo, 1999) uses predefined patterns such as \"X is a kind of Y\" or \"X, Y, and other Zs\" to identify hypernym/hyponym relationships. This approach to information extraction is based on a technique called selective concept extraction as defined by (Riloff, 1993) . Selective concept extraction is a form of text skimming that selectively processes relevant text while effectively ignoring surrounding text that is thought to be irrelevant to the domain. A more challenging task is to automatically learn the relevant patterns for the hypernym/hyponym relationships. In the context of pattern extraction, there exist many approaches as summarized in (Stevenson and Greenwood, 2006) . The most well-known work in this area is certainly the one proposed by who use machine learning techniques to automatically replace hand-built knowledge. By using dependency path features extracted from parse trees, they introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, their algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. (Sang and Hof-mann, 2007 ) use a similar way as (Snow et al., 2006) to derive extraction patterns for hypernym/hyponym relationships by using web search engine counts from pairs of words encountered in WordNet. However, the most interesting work is certainly proposed by (Bollegala et al., 2007) who extract patterns in two steps. First, they find lexical relationships between synonym pairs based on snippets counts and apply wildcards to generalize the acquired knowledge. Then, they apply a SVM classifier to determine whether a new pair shows a relation of synonymy or not, based on a feature vector of lexical relationships. This technique could be applied to hypernym/hyponym relationships although the authors do not mention it. On the one hand, links between words that result from manual or semi-automatic acquisition of relevant predicative or discursive patterns (Hearst, 1992; Carballo, 1999) are fine and accurate, but the acquisition of these patterns is a tedious task that requires substantial manual work. On the other hand, works done by Snow et al., 2006; Sang and Hofmann, 2007; Bollegala et al., 2007) have proposed methodologies to automatically acquire these patterns mostly based on supervised learning to leverage manual work. However, training sets still need to be built. Unlike other approaches, we propose an unsupervised methodology which aims at discovering general-specific noun relationships which can be assimilated to hypernym/hyponym relationships detection 2 . The advantages of this approach are clear as it can be applied to any language or any domain without any previous knowledge, based on a simple assumption: specific words tend to attract general words with more strength than the opposite. As (Michelbacher et al., 2007) state: \"there is a tendency for a strong forward association from a specific term like adenocarcinoma to the more general term cancer, whereas the association from cancer to adenocarcinoma is weak\". Based on this assumption, we propose a methodology based on directed graphs and the Tex-tRank algorithm (Mihalcea and Tarau, 2004) to automatically induce general-specific noun relationships from web corpora frequency counts. Indeed, asymmetry in Natural Language Processing can be seen as a possible reason for 2 We must admit that other kinds of relationships may be covered. For that reason, we will speak about generalspecific relationships instead of hypernym/hyponym relationships.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Based on these seeds, she proposes a bootstrapping algorithm to semi-automatically acquire new more specific patterns. ", "mid_sen": "Similarly, (Caraballo, 1999) uses predefined patterns such as \"X is a kind of Y\" or \"X, Y, and other Zs\" to identify hypernym/hyponym relationships. ", "after_sen": "This approach to information extraction is based on a technique called selective concept extraction as defined by (Riloff, 1993) . "}
{"citeStart": 213, "citeEnd": 229, "citeStartToken": 213, "citeEndToken": 229, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. ", "mid_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "after_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . "}
{"citeStart": 138, "citeEnd": 160, "citeStartToken": 138, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "We hypothesize that one way to reduce the user's cognitive load is to make explicit two pieces of information: the purpose of the current system turn, and how the system turn relates to the overall discussion. This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz & Sidner theory of discourse (Grosz and Sidner, 1986) .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We hypothesize that one way to reduce the user's cognitive load is to make explicit two pieces of information: the purpose of the current system turn, and how the system turn relates to the overall discussion. ", "mid_sen": "This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz & Sidner theory of discourse (Grosz and Sidner, 1986) .", "after_sen": "Consequently, in this paper we propose using a graphical representation of the discourse structure as a way of improving the performance of complex-domain dialogue systems (note that graphical output is required). "}
{"citeStart": 187, "citeEnd": 203, "citeStartToken": 187, "citeEndToken": 203, "sectionName": "UNKNOWN SECTION NAME", "string": "Section 2 describes some of the previous transformations used in Linguistic Steganography. Note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al., 2008) . Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008) , in which the application of paraphrases from the dictionary encodes secret bits. One advantage of the dictionary is that it has wide coverage, being automatically extracted; however, a disadvantage is that it contains many paraphrases which are either inappropriate, or only appropriate in certain contexts. Since we require any changes to be imperceptible to a human observer, it is crucial to our system that any uses of paraphrasing are grammatical and retain the meaning of the original cover text.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Section 2 describes some of the previous transformations used in Linguistic Steganography. ", "mid_sen": "Note that we are concerned with transformations which are linguistic in nature, rather than dealing with superficial properties of the text, e.g. the amount of white space between words (Por et al., 2008) . ", "after_sen": "Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008) , in which the application of paraphrases from the dictionary encodes secret bits. "}
{"citeStart": 54, "citeEnd": 73, "citeStartToken": 54, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003) , and HM-BiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. 7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003) , and HM-BiTAM (Zhao and Xing, 2008) implemented by us. GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. ", "mid_sen": "7. We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006) .", "after_sen": "We trained the word alignment in two directions: English to French, and French to English. "}
{"citeStart": 103, "citeEnd": 113, "citeStartToken": 103, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "We experimented with a variety of distance measures such as cosine, Euclidean distance, L 1 norm, Jaccard's coefficient, Kullback-Leibler divergence and the Skew divergence (see Lee 1999 for an overview). We obtained the best results for cosine (Experiment 1) and Skew divergence (Experiment 2). The two measures are shown in Figure 2 . The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by Lee (1999) as a linguistically motivated distance measure. We use a value of α = .99.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The two measures are shown in Figure 2 . ", "mid_sen": "The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by Lee (1999) as a linguistically motivated distance measure. ", "after_sen": "We use a value of α = .99."}
{"citeStart": 49, "citeEnd": 69, "citeStartToken": 49, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2. We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work. The words that occurred less than 40 times in the data were discarded from the vocabulary. To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014) . Embeddings of 50, 200, 400 and 600 dimensions were trained.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2. ", "mid_sen": "We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work. ", "after_sen": "The words that occurred less than 40 times in the data were discarded from the vocabulary. "}
{"citeStart": 44, "citeEnd": 64, "citeStartToken": 44, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "For tasks where the number of classes is large, we can fix a number of negative classes to be considered at each example and select the one with the largest score to perform a gradient step. This approach is similar to the one used by Weston et al. (2014) to select negative examples.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For tasks where the number of classes is large, we can fix a number of negative classes to be considered at each example and select the one with the largest score to perform a gradient step. ", "mid_sen": "This approach is similar to the one used by Weston et al. (2014) to select negative examples.", "after_sen": "We use the backpropagation algorithm to compute gradients of the network. "}
{"citeStart": 15, "citeEnd": 36, "citeStartToken": 15, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "Value number of English entities 5M number of Chinese entities 4.7M number of full-abbreviation relations 51K number of translation entries added 210K total number of translation entries 50M Note that many of the \"abbreviations\" extracted by our algorithm are not true abbreviations in the linguistic sense, instead they are just continuous-span of words. This is analogous to the concept of \"phrase\" in phrase-based MT. Table 4 reports the precision on the extracted fullabbreviation relations. We classify the relations into several classes based on their occurrence counts. In the second column, we list the fraction of the relations in the given class among all the relations we have extracted (i.e., 51K relations). For each class, we randomly select 100 relations, manually tag them as correct or wrong, and then calculate the precision. Intuitively, a class that has a higher occurrence count should have a higher precision, and this is generally true as shown in the fourth column of Table 4 . In comparison, Chang and Teng (2006) reports a precision of 50% over relations between single-word fullforms and single-character abbreviations. One can imagine a much lower precision on general relations (e.g., the relations between multi-word full-forms and multi-character abbreviations) that we consider here. Clearly, our results are very competitive 3 .", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Intuitively, a class that has a higher occurrence count should have a higher precision, and this is generally true as shown in the fourth column of Table 4 . ", "mid_sen": "In comparison, Chang and Teng (2006) reports a precision of 50% over relations between single-word fullforms and single-character abbreviations. ", "after_sen": "One can imagine a much lower precision on general relations (e.g., the relations between multi-word full-forms and multi-character abbreviations) that we consider here. "}
{"citeStart": 47, "citeEnd": 73, "citeStartToken": 47, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors. All vectors were built from a lemmatised version of the BNC. The noun basis was the 2000 most common context words, basis weights were the probability of context words given the target word divided by the overall probability of the context word. These features were chosen to enable easy comparison of our experimental results with those of Mitchell and Lapata's original experiment, in spite of the fact that there may be more sophisticated lexical distributional models available.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The experiment is on the dataset developed in .", "mid_sen": "Parameters We used the parameters described by Mitchell and Lapata (2008) for the noun and verb vectors. ", "after_sen": "All vectors were built from a lemmatised version of the BNC. "}
{"citeStart": 1, "citeEnd": 15, "citeStartToken": 1, "citeEndToken": 15, "sectionName": "UNKNOWN SECTION NAME", "string": "Three papers mention having used the memorybased learning method IBI-IG. (Veenstra, 1998) introduced cascaded chunking, a two-stage process in which the first stage classifications are used to improve the performance in a second processing stage. This approach reaches the same performance level as Argamon et al. but it requires lexical information. (Daelemans et al., 1999a ) report a good performance for baseNP recognition but they use a different data set and do not mention precision and recall rates. (Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. Their baseNP results are slightly better than those of Ramshaw and Marcus (F~=1=92.37).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Three papers mention having used the memorybased learning method IBI-IG. ", "mid_sen": "(Veenstra, 1998) introduced cascaded chunking, a two-stage process in which the first stage classifications are used to improve the performance in a second processing stage. ", "after_sen": "This approach reaches the same performance level as Argamon et al. "}
{"citeStart": 187, "citeEnd": 207, "citeStartToken": 187, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "This section describes how we model the shallow parsing tasks studied here as learning problems. The goal is to detect NPs and SV phrases. Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al., 1998) and others. That is, a base NP is a non-recursive NP that includes determiners but excludes post-modifying prepositional phrases or clauses. For example:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The goal is to detect NPs and SV phrases. ", "mid_sen": "Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al., 1998) and others. ", "after_sen": "That is, a base NP is a non-recursive NP that includes determiners but excludes post-modifying prepositional phrases or clauses. "}
{"citeStart": 89, "citeEnd": 105, "citeStartToken": 89, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "It is difficult to compare these results with results from other studies such as that of Caraballo (1999) , as the data used is not the same. However, it seems that our figures are in the same range as those reported in previous studies.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One way to overcome this problem might be to give judges information about a sequence of higher ancestors, in order to make the judgement easier.", "mid_sen": "It is difficult to compare these results with results from other studies such as that of Caraballo (1999) , as the data used is not the same. ", "after_sen": "However, it seems that our figures are in the same range as those reported in previous studies."}
{"citeStart": 181, "citeEnd": 211, "citeStartToken": 181, "citeEndToken": 211, "sectionName": "UNKNOWN SECTION NAME", "string": "To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998) , trMned on the Penn Wall Street Journal Treebank (Marcus et al., 1993) . We defined cooccurrence in these constructions using the standard definitions of dominance and precedence. The relation is stipulated to be transitive, so that all head nouns in a list co-occur with each other (e.g. in the phrase planes, trains, and automobiles all three nouns are counted as co-occuring with each other). Two head nouns co-occur in this algorithm if they meet the following four conditions: 3 Statistics for selecting and ranking R&S used the same figure of merit both for selecting new seed words and for ranking words in the final output. Their figure of merit was simply the ratio of the times the noun coocurs with a noun in the seed list to the total frequency of the noun in the corpus. This statistic favors low frequency nouns, and thus necessitates the inclusion of a minimum occurrence cutoff. They stipulated that no word occuring fewer than six times in the corpus would be considered by the algorithm. This cutoff has two effects: it reduces the noise associated with the multitude of low frequency words, and it removes from consideration a fairly large number of certainly valid category members. Ideally, one would like to reduce the noise without reducing the number of valid nouns. Our statistics allow for the inclusion of rare occcurances. Note that this is particularly important given our algorithm, since we have restricted the relevant occurrences to a specific type of structure; even relatively common nouns m~v not occur in the corpus more than a handful of times in such a context.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We made the simplifying assumptions that a compound noun is a string of consecutive nouns (or, in certain cases, adjectives -see discussion below), and that the head of the compound is the rightmost noun.", "mid_sen": "To identify conjunctions, lists, and appositives, we first parsed the corpus, using an efficient statistical parser (Charniak et al., 1998) , trMned on the Penn Wall Street Journal Treebank (Marcus et al., 1993) . ", "after_sen": "We defined cooccurrence in these constructions using the standard definitions of dominance and precedence. "}
{"citeStart": 9, "citeEnd": 31, "citeStartToken": 9, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "item. However (Daelemans et al., 1999) report that for baseNP recognition better results can be obtained by making the algorithm consider the classification values of the three closest training items. We have tested this by repeating the first experiment series and part of the third experiment series for k=3. In this revised version we have repeated the best experiment of the third series with the results for k=l replaced by the k=3 results whenever the latter outperformed the first in the revised first experiment series. The results can be found in table 5. All formats benefited from this step. In this final experiment series the best results were obtained with IOB1 but the differences with the results of the other formats are not significant. We have used the optimal experiment configurations that we had obtained from the fourth experiment series for processing the complete (Ramshaw and Marcus, 1995) data set. The results can be found in table 6. They are better than the results for section 15 because more training data was used in these experiments. Again the best result was obtained with IOB1 (F~=I =92.37) which is an im-I)rovement of the best reported F,~=1 rate for this data set ((Ramshaw and Marcus, 1995) : 92.03).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "item. ", "mid_sen": "However (Daelemans et al., 1999) report that for baseNP recognition better results can be obtained by making the algorithm consider the classification values of the three closest training items. ", "after_sen": "We have tested this by repeating the first experiment series and part of the third experiment series for k=3. "}
{"citeStart": 75, "citeEnd": 89, "citeStartToken": 75, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "Our position will be that dependency relations are motivated semantically (Tesni~re, 1959) , and need not be projective (i.e., may cross if projected onto the surface ordering). We argue for so-called word order domains, consisting of partially ordered sets of words and associated with nodes in the dependency tree. These order domains constitute a tree defined by set inclusion, and surface word order is determined by traversing this tree. A syntactic analysis therefor consists of two linked, but dissimilar trees.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This result has recently been critizised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & Br6ker, 1997) .", "mid_sen": "Our position will be that dependency relations are motivated semantically (Tesni~re, 1959) , and need not be projective (i.e., may cross if projected onto the surface ordering). ", "after_sen": "We argue for so-called word order domains, consisting of partially ordered sets of words and associated with nodes in the dependency tree. "}
{"citeStart": 81, "citeEnd": 105, "citeStartToken": 81, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "We would like to apply our learning approach to the large data set mentioned in (Ramshaw and Marcus, 1995) : Wall Street Journal corpus sections 2-21 as training material and section 0 as test material. With our present hardware applying our optimal experiment configuration to this data would require several months of computer time. Therefore we have only used the best stage 1 approach with IOB1 tags: a left and right cont(,.xt of three words and three POS tags combined with k=3. This time the chunker achieved a F~=l score of 93.81 which is half a point better than the results obtained by (Ramshaw and Marcus, 1995) : 93.3 (other chunker rates for this data: accuracy: 98.04%; precision: 93.71%; recalh 93.90%).", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Again the best result was obtained with IOB1 (F~=I =92.37) which is an im-I)rovement of the best reported F,~=1 rate for this data set ((Ramshaw and Marcus, 1995) : 92.03).", "mid_sen": "We would like to apply our learning approach to the large data set mentioned in (Ramshaw and Marcus, 1995) : Wall Street Journal corpus sections 2-21 as training material and section 0 as test material. ", "after_sen": "With our present hardware applying our optimal experiment configuration to this data would require several months of computer time. "}
{"citeStart": 137, "citeEnd": 161, "citeStartToken": 137, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Note also, that, on standard X-bar assumptions, the attachment of post-modifiers may be derived via lowering at an X I node. In this case, the lowered node and its replacement will be of the same syntactic category (like the root and foot node of a TAG auxiliary tree). Researchers have noted a general preference for low attachment of postmodifiers (this is accounted for by the principle of late closure (Frazier and Rayner, 1982) ). This would suggest that a reasonable search strategy for English would be to search the set of accessible node in a bottom-up direction for English.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this case, the lowered node and its replacement will be of the same syntactic category (like the root and foot node of a TAG auxiliary tree). ", "mid_sen": "Researchers have noted a general preference for low attachment of postmodifiers (this is accounted for by the principle of late closure (Frazier and Rayner, 1982) ). ", "after_sen": "This would suggest that a reasonable search strategy for English would be to search the set of accessible node in a bottom-up direction for English."}
{"citeStart": 34, "citeEnd": 46, "citeStartToken": 34, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991) . All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a) , and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a) , and the first two are distributed as part of the ANLT package. ", "mid_sen": "The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node).", "after_sen": ""}
{"citeStart": 35, "citeEnd": 52, "citeStartToken": 35, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "In what follows, we present a new database consisting of articles, images, and their captions which we collected from an on-line news source. We then propose an image annotation model which can learn from our noisy annotations and the auxiliary documents. Specifically, we extend and modify Lavrenko's (2003) continuous relevance model to suit our task. Our experimental results show that this model can successfully scale to our database, without making use of explicit human annotations in any way. We also show that the auxiliary document contains important information for generating more accurate image descriptions.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We then propose an image annotation model which can learn from our noisy annotations and the auxiliary documents. ", "mid_sen": "Specifically, we extend and modify Lavrenko's (2003) continuous relevance model to suit our task. ", "after_sen": "Our experimental results show that this model can successfully scale to our database, without making use of explicit human annotations in any way. "}
{"citeStart": 91, "citeEnd": 112, "citeStartToken": 91, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal. We reviewed the entire lexicon to determine its accuracy and made numerous improvements.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each product type considered, we manually constructed a lexicon containing generic product words; we did not find it necessary to construct product-specific lexicons.", "mid_sen": "For adjectival attitudes, we used the lexicon developed we developed in our previous work (Whitelaw et al., 2005) on appraisal. ", "after_sen": "We reviewed the entire lexicon to determine its accuracy and made numerous improvements."}
{"citeStart": 211, "citeEnd": 219, "citeStartToken": 211, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We are not aware of other work that uses such collocations as we do.", "mid_sen": "Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999) .", "after_sen": "We are not aware of other work identifying and using density parameters as described in this article."}
{"citeStart": 14, "citeEnd": 27, "citeStartToken": 14, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "To resolve the opposition between surface order and the PAS in a free word order language, one can let the type shifted categories of terms proliferate, or reformulate CCG in such a way that arguments of the verbs are sets, rather than lists whose arguments are made available one at a time. The former alternative makes the spurious ambiguity problem of CG parsing (Karttunen, 1989 ) even more severe. Multiset CCG (Hoffman, 1995) is an example of the setoriented approach. It is known to be computationally tractable but less efficient than the polynomial time CCG algorithm of Vijay-Shanker and Weir (1993) . I try to show in this paper that the traditional curried notation of CG with type shifting can be maintained to account for Surface Form+-~PAS mapping without leading to proliferation of argument categories or to spurious ambiguity.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The former alternative makes the spurious ambiguity problem of CG parsing (Karttunen, 1989 ) even more severe. ", "mid_sen": "Multiset CCG (Hoffman, 1995) is an example of the setoriented approach. ", "after_sen": "It is known to be computationally tractable but less efficient than the polynomial time CCG algorithm of Vijay-Shanker and Weir (1993) . "}
{"citeStart": 140, "citeEnd": 164, "citeStartToken": 140, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "To train models using this information we use generalized expectation (GE) criteria. GE criteria are terms in a training objective function that assign scores to values of a model expectation. In particular we use a version of GE that prefers parameter settings in which certain model expectations are close to target distributions. Previous work has shown how to apply GE criteria to maximum entropy classifiers. In section 4, we extend GE criteria to semi-supervised learning of linear-chain conditional random fields, using conditional probability distributions of labels given features. To empirically evaluate this method we compare it with several competing methods for CRF training, including entropy regularization and expected gradient, showing that GE provides significant improvements. We achieve competitive performance in comparison to alternate model families, in particular generative models such as MRFs trained with EM (Haghighi and Klein, 2006) and HMMs trained with soft constraints (Chang et al., 2007) . Finally, in Section 5.3 we show that feature-labeling can lead to dramatic reductions in the annotation time that is required in order to achieve the same level of accuracy as traditional instance-labeling.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To empirically evaluate this method we compare it with several competing methods for CRF training, including entropy regularization and expected gradient, showing that GE provides significant improvements. ", "mid_sen": "We achieve competitive performance in comparison to alternate model families, in particular generative models such as MRFs trained with EM (Haghighi and Klein, 2006) and HMMs trained with soft constraints (Chang et al., 2007) . ", "after_sen": "Finally, in Section 5.3 we show that feature-labeling can lead to dramatic reductions in the annotation time that is required in order to achieve the same level of accuracy as traditional instance-labeling."}
{"citeStart": 53, "citeEnd": 64, "citeStartToken": 53, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "The direct evaluation of the rule-sets gave us the grounds for the comparison and selection of the best performing guessing rule-sets. The task of unknown word guessing is, however, a subtask of the overall part-of-speech tagging process. Thus we are mostly interested in how the advantage of one rule-set over another will affect the tagging performance. So, we performed an independent evaluation of the impact of the word guessers on tagging accuracy. In this evaluation we tried two different taggers. First, we used a tagger which was a c++ re-implementation of the LISP implemented HMM Xerox tagger described in (Kupiec, 1992) . The other tagger was the rule-based tagger of Brill (Brill, 1995) . Both of the taggers come with data and word-guessing components pre-trained on the Brown Corpus 6. This, actually gave us the search-space of four combinations: the Xerox tagger equipped with the original Xerox guesser, Brill's tagger with its original guesser, the Xerox tagger with our cascading Ps0+S60+E75 guesser and Brill's tagger with the cascading guesser. For words which failed to be guessed by the guessing rules we applied the standard method of classifying them as common nouns (NN) if they are not capitalised inside a sentence and proper nouns (NP) otherwise. As the base-line result we measured the performance of the taggers with all known words on the same word sample.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, we used a tagger which was a c++ re-implementation of the LISP implemented HMM Xerox tagger described in (Kupiec, 1992) . ", "mid_sen": "The other tagger was the rule-based tagger of Brill (Brill, 1995) . ", "after_sen": "Both of the taggers come with data and word-guessing components pre-trained on the Brown Corpus 6. "}
{"citeStart": 51, "citeEnd": 79, "citeStartToken": 51, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "For both languages, we resolved coreference by using SWIZZLE, our implementation of a bilingual coreference resolver. SWIZZLE is a multilingual enhancement of COCKTAIL (Harabagiu and Maiorano, 1999) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information 2. When COCKTAIL was applied separately on the English and the Romanian texts, coreferring links were identified for each English and Romanian document respectively. When aligned referential expressions corefer with non-aligned anaphors, SWIZZLE derived new heuristics for coreference. Our experiments show that SWIZZLE outperformed COCKTAIL on both English and Romanian test documents.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For both languages, we resolved coreference by using SWIZZLE, our implementation of a bilingual coreference resolver. ", "mid_sen": "SWIZZLE is a multilingual enhancement of COCKTAIL (Harabagiu and Maiorano, 1999) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information 2. ", "after_sen": "When COCKTAIL was applied separately on the English and the Romanian texts, coreferring links were identified for each English and Romanian document respectively. "}
{"citeStart": 113, "citeEnd": 137, "citeStartToken": 113, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a) , which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b) ). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992) .", "mid_sen": "The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a) , which is lexicalized and context-free equivalent. ", "after_sen": "We favor the PLTIG representation for two reasons. "}
{"citeStart": 127, "citeEnd": 140, "citeStartToken": 127, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "The work of Efron and Tibshirani (1993) enabled Breiman's refinement and application of their techniques for machine learning (Breiman, 1996) . His technique is called bagging, short for \"bootstrap aggregating\". In brief, bootstrap techniques and bag-ging in particular reduce the systematic biases many estimation techniques introduce by aggregating estimates made from randomly drawn representative resamplings of those datasets.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The work of Efron and Tibshirani (1993) enabled Breiman's refinement and application of their techniques for machine learning (Breiman, 1996) . ", "after_sen": "His technique is called bagging, short for \"bootstrap aggregating\". "}
{"citeStart": 88, "citeEnd": 102, "citeStartToken": 88, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "The result was a system which comes very close to the formalised dependency grammars of Gaifman (1965) and Hays (1964) . The only real difference is that Bar-Hillel allowed arguments to themselves be functions. For example, an adverb such as slowly could be given the type 4", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ix L~ ... L1 I(L1 .. • Ln) R1 . .. Rn ~ X r(R1 ...R~>", "mid_sen": "The result was a system which comes very close to the formalised dependency grammars of Gaifman (1965) and Hays (1964) . ", "after_sen": "The only real difference is that Bar-Hillel allowed arguments to themselves be functions. "}
{"citeStart": 0, "citeEnd": 29, "citeStartToken": 0, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the di erent trigger situations we performed two tests: One where we use only acoustic triggers and another where the existence of a perfect word fragment detector is assumed. The input were unsegmented transliterated utterance to exclude in uences a word recognizer. We restrict the processing time on a SUN ULTRA 300MHZ to 10 seconds. The parser was simulated by a w ord trigram. Training and testing were done on two separated parts of the German part of the Verbmobil corpus 12558 turns training 1737 turns test. Test 1 49  70  70  Test 2 71  85 62  83 A direct comparison to other groups is rather di cult due to very di erent corpora, evaluation conditions and goals. Nakatani and Hirschberg, 1993 suggest a acoustic prosodic detector to identify IPs but don't discuss the problem of nding the correct segmentation in depth. Also their results are obtained on a corpus where every utterance contains at least one repair. Shriberg, 1994 also addresses the acoustic aspects of repairs. Parsing approaches like in Bear et al., 1992; Hindle, 1983; Core and Schubert, 1999 must be proved to work with lattices rather than transliterated text. An algorithm which is inherently capable of lattice processing is proposed by Heeman Heeman, 1997 . He rede nes the word recognition problem to identify the best sequence of words, corresponding POS tags and special repair tags. He reports a recall rate of 81 and a precision of 83 for detection and 78 80 for correction. The test settings are nearly the same as test 2. Unfortunately, nothing is said about the processing time of his module.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Test 1 49  70  70  Test 2 71  85 62  83 A direct comparison to other groups is rather di cult due to very di erent corpora, evaluation conditions and goals. ", "mid_sen": "Nakatani and Hirschberg, 1993 suggest a acoustic prosodic detector to identify IPs but don't discuss the problem of nding the correct segmentation in depth. ", "after_sen": "Also their results are obtained on a corpus where every utterance contains at least one repair. "}
{"citeStart": 149, "citeEnd": 164, "citeStartToken": 149, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu) ; (Ma and Way, 2009) (Ma) ; (Xi et al., 2012) (Xi) ; (Zeng et al., 2014 . The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater generalizability.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.", "mid_sen": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu) ; (Ma and Way, 2009) (Ma) ; (Xi et al., 2012) (Xi) ; (Zeng et al., 2014 . ", "after_sen": "The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. "}
{"citeStart": 242, "citeEnd": 263, "citeStartToken": 242, "citeEndToken": 263, "sectionName": "UNKNOWN SECTION NAME", "string": "The corpora. Three corpora were used in this study. At the initial stage two development corpora were used: a digitalized early draft of the Jurafsky-Martin textbook (Jurafsky and Martin, 2000) and the WeScience Corpus, a set of Wikipedia articles in the domain of Natural Language Processing (Ytrestøl et al., 2009) . 1 The former served as a source of seed domain terms with definitions, while the latter was used for seed pattern creation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Three corpora were used in this study. ", "mid_sen": "At the initial stage two development corpora were used: a digitalized early draft of the Jurafsky-Martin textbook (Jurafsky and Martin, 2000) and the WeScience Corpus, a set of Wikipedia articles in the domain of Natural Language Processing (Ytrestøl et al., 2009) . 1 The former served as a source of seed domain terms with definitions, while the latter was used for seed pattern creation.", "after_sen": "For acquisition of definitional patterns and pattern refinement we used the ACL Anthology, a digital archive of scientific papers from conferences, workshops, and journals on Computational Linguistics and Language Technology (Bird et al., 2008) . 2 The corpus consisted of 18,653 papers published between 1965 and 2011, with a total of 66,789,624 tokens and 3,288,073 sentences. "}
{"citeStart": 63, "citeEnd": 78, "citeStartToken": 63, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The inspiration for the PLTIG formalism stems from the desire to lexicalize a context-free gram-mar. There are three ways in which one might do so. First, one can modify the tree structures so that all context-free productions contain lexical items. Greibach normal form provides a well-known example of such a lexicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994) . Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995) , Charniak (1997) , Collins (1997), and Goodman (1997) . A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988) , (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, one might propagate lexical information upward through the productions. ", "mid_sen": "Examples of formalisms using this approach include the work of Magerman (1995) , Charniak (1997) , Collins (1997), and Goodman (1997) . ", "after_sen": "A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. "}
{"citeStart": 346, "citeEnd": 360, "citeStartToken": 346, "citeEndToken": 360, "sectionName": "UNKNOWN SECTION NAME", "string": "Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996) . Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. ", "mid_sen": "At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "after_sen": "We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. "}
{"citeStart": 24, "citeEnd": 44, "citeStartToken": 24, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "In our formulation of the word ordering problem, a hypothesis is a phrase or sentence together with its CCG derivation. Hypotheses are constructed bottom-up: starting from single words, smaller phrases are combined into larger ones according to CCG rules. To allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted hypotheses. When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses. The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998) , and we adopt the term chart for this structure. However, note there are important differences to the parsing problem. First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling. Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. In our previous papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012) , we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT decoding (Koehn 2010 ). However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used. We will also investigate the possibility of applying dynamic-programming-style pruning to the chart.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. ", "mid_sen": "In our previous papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012) , we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT decoding (Koehn 2010 ). ", "after_sen": "However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used. "}
{"citeStart": 77, "citeEnd": 100, "citeStartToken": 77, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996) , KPML (Bateman, 1996) , MUMBLE (Meteer et al., 1987) , and RealPro (Lavoie and Rambow, 1997) , which produce natural language text from an abstract semantic representation. These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The approach of writing individual templates is convenient, but may not scale to complex domains in which hundreds or thousands of templates would be necessary, and may have shortcomings in maintainability and text quality (e.g., see (Reiter, 1995) for a discussion).", "mid_sen": "There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996) , KPML (Bateman, 1996) , MUMBLE (Meteer et al., 1987) , and RealPro (Lavoie and Rambow, 1997) , which produce natural language text from an abstract semantic representation. ", "after_sen": "These packages require linguistic sophistication in order to write the abstract semantic representation, but they are flexible because minor changes to the input can accomplish major changes to the generated text."}
{"citeStart": 47, "citeEnd": 66, "citeStartToken": 47, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000) , Lu and Getoor (2003) , Taskar et al. (2004) , Richardson and Domingos (2006) ). In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. Other joint models used in sentiment classification include the spin model (Takamura et al., 2007) , relaxation labeling (Popescu and Etzioni, 2005) , and label propagation (Goldberg and Zhu, 2006) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Turning to collective classification, there have been various collective classification frameworks proposed (for example, Neville and Jensen (2000) , Lu and Getoor (2003) , Taskar et al. (2004) , Richardson and Domingos (2006) ). ", "mid_sen": "In this paper, we use an approach proposed by (Bilgic et al., 2007) which iteratively predicts class and link existence using local classifiers. ", "after_sen": "Other joint models used in sentiment classification include the spin model (Takamura et al., 2007) , relaxation labeling (Popescu and Etzioni, 2005) , and label propagation (Goldberg and Zhu, 2006) ."}
{"citeStart": 142, "citeEnd": 160, "citeStartToken": 142, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. ", "mid_sen": "Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991) .", "after_sen": "In this article, we present the AT'R/Lancaster 7'reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University (UK)'s Unit for Computer Research on the English Language, according to specifications provided by ATR (Japan)'s Statistical Parsing Group. "}
{"citeStart": 203, "citeEnd": 223, "citeStartToken": 203, "citeEndToken": 223, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach also makes use of the scientific discourse for summarisation purposes. We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative. We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme (Liakata et al., 2010; Liakata et al., 2012) . The CoreSC scheme is \"uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence\" (White et al., 2011) , which are not readily identifiable by other annotation schemes. Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZ-II (Teufel et al., 2009) , it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZ-II focusses on the attribution of knowledge claims and the relation with previous work (Liakata et al., 2010) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative. ", "mid_sen": "We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme (Liakata et al., 2010; Liakata et al., 2012) . ", "after_sen": "The CoreSC scheme is \"uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence\" (White et al., 2011) , which are not readily identifiable by other annotation schemes. "}
{"citeStart": 22, "citeEnd": 46, "citeStartToken": 22, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "In our previous work (Salloum and Habash, 2011; Salloum and Habash, 2012) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only. We did not use a language model to pick the best path; instead we kept the ambiguity in the lattice and passed it to our SMT system. In contrast, in this paper, we run ELISSA on untokenized Arabic, we use feature, lemma, and surface form transfer rules, and we pick the best path of the generated MSA lattice through a language model. Certain aspects of our approach are similar to Riesa and Yarowsky (2006) 's, in that we use morphological analysis for DA to help DA-English MT; but unlike them, we use a rule-based approach to model DA morphology.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In contrast, we use hand-written morphosyntactic transfer rules that focus on translating DA morphemes and lemmas to their MSA equivalents.", "mid_sen": "In our previous work (Salloum and Habash, 2011; Salloum and Habash, 2012) , we applied our approach to tokenized Arabic and our DA-MSA transfer component used feature transfer rules only. ", "after_sen": "We did not use a language model to pick the best path; instead we kept the ambiguity in the lattice and passed it to our SMT system. "}
{"citeStart": 160, "citeEnd": 181, "citeStartToken": 160, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead (Stone and Webber 1998; Krahmer and Theune 2002) . We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. Consider, for instance, the list of properties L = size > 3 cm, size < 9 cm . If interleaving forced us to realize the two properties in L one by one, then it would no longer be possible to combine them into, for example, the largest mouse but one (if the facts in the KB support it), or even into the mice between 3 and 9 cm (since size > 3 cm is realized before size < cm). Clearly, sophisticated use of gradable adjectives requires a separation between CD and linguistic realization, unless one is willing to complicate linguistic realization considerably.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization, interleaving the two processes instead (Stone and Webber 1998; Krahmer and Theune 2002) . ", "after_sen": "We have separated the two phases because, in the case of vague descriptions, interleaving would tend to be difficult. "}
{"citeStart": 138, "citeEnd": 150, "citeStartToken": 138, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "The ratio of ambiguous tokens is usually calculated based on alternatives offered by a morphological lexicon (either built during the training process or furnished by an external application; see below). If the lexicon offers alternative analyses, the token is taken as ambiguous irrespective of the probability of the alternatives. If an external resource is used in the form of a morphological analyzer (MA), this will almost always overgenerate, yielding false ambiguity. But even if the MA is tight, a considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types (Church, 1988) . For example the word nem, can mean both 'not' and 'gender', so both ADV and NOUN are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, (12596 vs. 4 tokens in the 1 m word manually annotated Szeged Korpusz (Csendes et al., 2004) ).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If an external resource is used in the form of a morphological analyzer (MA), this will almost always overgenerate, yielding false ambiguity. ", "mid_sen": "But even if the MA is tight, a considerable proportion of ambiguous tokens will come from legitimate but rare analyses of frequent types (Church, 1988) . ", "after_sen": "For example the word nem, can mean both 'not' and 'gender', so both ADV and NOUN are valid analyses, but the adverbial reading is about five orders of magnitude more frequent than the noun reading, (12596 vs. 4 tokens in the 1 m word manually annotated Szeged Korpusz (Csendes et al., 2004) )."}
{"citeStart": 62, "citeEnd": 73, "citeStartToken": 62, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Possible interpretations of a given set of utterances with respect to a knowledge base are computed using an extension of the semantic tableau method. This extension has been proved to be both sound and complete (Marcu, 1994) . A partial ordering, <, determines the set of optimistic interpretations for a theory. An interpretation m0 is preferred to, or is more optimistic than, an interpretation ml (m0 < ml) if it contains more information and that information can be more easily updated in the future. That means that if an interpretation m0 makes an utterance true by assigning to a relation R a defensible status, while another interpretation ml makes the same utterance true by assigning the same relation R a stronger status, m0 will be the preferred or optimistic one, because it is as informative as mi and it allows more options in the future (R can be defeated).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Possible interpretations of a given set of utterances with respect to a knowledge base are computed using an extension of the semantic tableau method. ", "mid_sen": "This extension has been proved to be both sound and complete (Marcu, 1994) . ", "after_sen": "A partial ordering, <, determines the set of optimistic interpretations for a theory. "}
{"citeStart": 14, "citeEnd": 26, "citeStartToken": 14, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Slot Grammar (McCord, 1990 ) employs a number of rule types, some of which are exclusively concerned with precedence. So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers. Extractions (i.e., discontinuities) are merely handled by a mechanism built into the parser.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The approach requires that the parser interpretes several features specially, and it cannot restrict the scope of discontinuities.", "mid_sen": "Slot Grammar (McCord, 1990 ) employs a number of rule types, some of which are exclusively concerned with precedence. ", "after_sen": "So-called head/slot and slot/slot ordering rules describe the precedence in projective trees, referring to arbitrary predicates over head and modifiers. "}
{"citeStart": 1, "citeEnd": 13, "citeStartToken": 1, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "The analysis of temporal expressions in natural language discourse provides a challenge for contemporary semantic theories. (Partee, 1973) introduced the notion of temporal anaphora, to account for ways in which temporal expressions depend on surrounding elements in the discourse for their semantic contribution to the discourse. In this paper, we discuss the interaction of temporal anaphora and quantification over eventualities. Such interaction, while interesting in its own right, is also a good test-bed for theories of the semantic interpretation of temporal expressions. We discuss cases such as:", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The analysis of temporal expressions in natural language discourse provides a challenge for contemporary semantic theories. ", "mid_sen": "(Partee, 1973) introduced the notion of temporal anaphora, to account for ways in which temporal expressions depend on surrounding elements in the discourse for their semantic contribution to the discourse. ", "after_sen": "In this paper, we discuss the interaction of temporal anaphora and quantification over eventualities. "}
{"citeStart": 27, "citeEnd": 38, "citeStartToken": 27, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "A TAG is a tuple G --= (N, Z, I, A, S), where N, Z are the finite sets of nonterminal and terminal symbols, respectively, I, A are the finite sets of initial and auxiliary trees, respectively, and S E N is the initial symbol. Trees in I O A are also called elementary trees. The reader is referred to (Joshi, 1985) for the definitions of tree adjunction, tree substitution, and language derived by a TAG.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Trees in I O A are also called elementary trees. ", "mid_sen": "The reader is referred to (Joshi, 1985) for the definitions of tree adjunction, tree substitution, and language derived by a TAG.", "after_sen": "The spine of an auxiliary tree is the (unique) path that connects the root and the foot node. "}
{"citeStart": 0, "citeEnd": 25, "citeStartToken": 0, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work on polarity disambiguation has used contextual clues and reversal words (Wilson et al., 2005; Kennedy and Inkpen, 2006; Kanayama and Nasukawa, 2006; Devitt and Ahmad, 2007; Sadamitsu et al., 2008) . However, these do not capture discourse-level relations. Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. Similar to Somasundaran et al. (2008) , Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. However, these works do not provide an implementation for their insights. In this work we demonstrate a concrete way that discourse-level interpretation can improve recognition of individual opinions and their polarities.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, these do not capture discourse-level relations. ", "mid_sen": "Polanyi and Zaenen (2006) observe that a central topic may be divided into subtopics in order to perform evaluations. ", "after_sen": "Similar to Somasundaran et al. (2008) , Asher et al. (2008) advocate a discourse-level analysis in order to get a deeper understanding of contextual polarity and the strength of opinions. "}
{"citeStart": 169, "citeEnd": 193, "citeStartToken": 169, "citeEndToken": 193, "sectionName": "UNKNOWN SECTION NAME", "string": "Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm (Lewin, 1990) , or an inside-out algorithm with a free w~riable constraint (IIobbs and Shieber, 1987) . The propositions fbrlncd can then be judged for plausibility. To imitate jungle path phenomena, these pla.usi o bility judgements need to feed back into the scoping procedure for the next fragment. For example, if' every man is taken to be scoped outside a book after processing the fragment l?vcry man ga~c. a book, [;hen this preference should be preserved when deterlnining the scope for the full sentence l?very uza~t gave a book lo a child. Thus instead of doing ~dl quantitier scoping at the end of the sentence, each new quantilier is scoped relative to the existing quantifiers (and operators such as negation, intensional verbs etc.). A preliminary irnplemenl, ation achieves this by annotating the semantic representations with node nantes, a.nd recording which quantifiers are 'discharged' at. which nodes, and in which order.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "12) gives(< V,X,ITIall(X)>,< 3,y,book(y)>,< -~,z,'l'>)", "mid_sen": "Scoped propositions can then be obtained by using an outside-in quantifier scoping algorithm (Lewin, 1990) , or an inside-out algorithm with a free w~riable constraint (IIobbs and Shieber, 1987) . ", "after_sen": "The propositions fbrlncd can then be judged for plausibility. "}
{"citeStart": 167, "citeEnd": 188, "citeStartToken": 167, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "3.1 Referential Property Constraint First, our system estimates the referential property of a noun phrase by using the method described in one of our previous papers (Murata and Nagao 1993) . The method estimates a referential property using surface expressions in the sentences. For example, since the second \"OJIISAN (old man)\" in the following sentences is accompanied by a particle \"WA (topic)\" and the predicate is in the past tense, it is estimated to be a definite noun phrase. 6Next, our system determines the referent of a noun phrase by using its estimated referential property. When a noun phrase is estimated to be a definite noun phrase, our system judges that the noun phrase refers to the entity denoted by a previous noun phrase which has the same head noun. For example, the second \"OJIISAN\" in the above sentences is estimated to be a definite noun phrase, and our system judges that it refers to the entity denoted by the first \"OJIISAN\".", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "When two noun phrases which have the same head noun satisfy these three constraints, the system judges that the two noun phrases have the same referent.", "mid_sen": "3.1 Referential Property Constraint First, our system estimates the referential property of a noun phrase by using the method described in one of our previous papers (Murata and Nagao 1993) . ", "after_sen": "The method estimates a referential property using surface expressions in the sentences. "}
{"citeStart": 80, "citeEnd": 100, "citeStartToken": 80, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "All evaluations in this study were performed using a randomized cross-validation configuration. The 1,247 predicate instances were annotated document by document. In order to remove any confounding factors caused by specific documents, we first randomized the annotated predicate instances. Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold. This evaluation set-up is an improvement versus the one we previously reported (Gerber and Chai 2010) , in which fixed partitions were used for training, development, and testing. During training, the system was provided with annotated predicate instances. The system identified missing argument positions and generated a set of candidates for each such position. A candidate three-tuple p, iarg n , c was given a positive label if the candidate implicit argument c (the primary filler) was annotated as filling the missing argument position; otherwise, the candidate three-tuple was given a negative label. During testing, the system was presented with each predicate instance and was required to identify all implicit arguments for the predicate.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following this, we split the predicate instances evenly into ten folds and used each fold as testing data for a model trained on the instances outside the fold. ", "mid_sen": "This evaluation set-up is an improvement versus the one we previously reported (Gerber and Chai 2010) , in which fixed partitions were used for training, development, and testing. ", "after_sen": "During training, the system was provided with annotated predicate instances. "}
{"citeStart": 1, "citeEnd": 27, "citeStartToken": 1, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Corpora thus are used only as an inspiration. ", "mid_sen": "(Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "after_sen": ""}
{"citeStart": 43, "citeEnd": 72, "citeStartToken": 43, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. Their better performance is partly attributed to richer (speaker dependent) acoustic modeling, including phoneme duration, energy, and pitch. However, their model was trained and tested on professionally read speech, rather than spontaneous speech. Wang and Hirschberg (1992) did employ spontaneous speech, namely, the ATIS corpus. For turninternal boundary tones, they achieved a recall rate of 38.5% and a precision of 72.9% using a decision tree approach that combined both textual features, such as POS tags, and syntactic constituents with intonational features. One explanation for the difference in performance was that our model was trained on approximately ten times as much data. Secondly, their decision trees are used to classify each data point independently of the next, whereas we find the best interpretation over the entire turn, and incorporate speech repairs.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We feel that speech repair modeling must be combined with detecting utterance boundaries and discourse markers, and should take advantage of acoustic information.", "mid_sen": "For detecting boundary tones, the model of Wightman and Ostendorf (1994) achieves a recall rate of 78.1% and a precision of 76.8%. ", "after_sen": "Their better performance is partly attributed to richer (speaker dependent) acoustic modeling, including phoneme duration, energy, and pitch. "}
{"citeStart": 146, "citeEnd": 163, "citeStartToken": 146, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983 ). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. ", "mid_sen": "This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) .", "after_sen": "The task is illustrated in example 1: Example 1 The parses assigned to these two compounds differ, even though the sequence of parts of speech are identical. "}
{"citeStart": 140, "citeEnd": 151, "citeStartToken": 140, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "Following Strzalkowski and Brandow (1997) and Peters and Drexel (2004) we have implemented a transformation-based learning (TBL) algorithm (Brill, 1995) . This method iteratively improves the match (as measured by token error rate) of a collection of corresponding source and target token sequences by positing and applying a sequence of substitution rules. In each iteration the source and target tokens are aligned using a minimum edit distance criterion. We refer to maximal contiguous subsequences of non-matching tokens as error re-gions. These consist of paired sequences of source and target tokens, where either sequence may be empty. Each error region serves as a candidate substitution rule. Additionally we consider refinements of these rules with varying amounts of contiguous context tokens on either side. Deviating from Peters and Drexel (2004) , in the special case of an empty target sequence, i.e. a deletion rule, we consider deleting all (non-empty) contiguous subsequences of the source sequence as well. For each candidate rule we accumulate two counts: the number of exactly matching error regions and the number of false alarms, i.e. when its left-hand-side matches a sequence of already correct tokens. Rules are ranked by the difference in these counts scaled by the number of errors corrected by a single rule application, which is the length of the corresponding error region. This is an approximation to the total number of errors corrected by a rule, ignoring rule interactions and non-local changes in the minimum edit distance alignment. A subset of the topranked non-overlapping rules satisfying frequency and minimum impact constraints are selected and the source sequences are updated by applying the selected rules. Again deviating from Peters and Drexel (2004) , we consider two rules as overlapping if the left-hand-side of one is a contiguous subsequence of the other. This procedure is iterated until no additional rules can be selected. The initial rule set is populated by a small sequence of hand-crafted rules (e.g. \"impression colon\" → \"IMPRESSION:\"). A user-independent baseline rule set is generated by applying the algorithm to data from a collection of users. We construct speaker-dependent models by initializing the algorithm with the speakerindependent rule set and applying it to data from the given user.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following Strzalkowski and Brandow (1997) and Peters and Drexel (2004) we have implemented a transformation-based learning (TBL) algorithm (Brill, 1995) . ", "after_sen": "This method iteratively improves the match (as measured by token error rate) of a collection of corresponding source and target token sequences by positing and applying a sequence of substitution rules. "}
{"citeStart": 200, "citeEnd": 215, "citeStartToken": 200, "citeEndToken": 215, "sectionName": "UNKNOWN SECTION NAME", "string": "ing large amounts of text or in interactive applications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars. Previous comparisons have either focussed on context-free (CF) or augmented CF parsing (Tomita, 1987; Billot & Lang, 1989) , or have used relatively small, limited-coverage unification grammars and lexicons (Shann, 1989; Bouma & van Noord, 1993; Maxwell & Kaplan, 1993) . It is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based grammars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be constrained tightly with relatively simple predictive techniques. Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard & Sag, 1987) . However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be a confounding factor in any general conclusions made. ", "mid_sen": "Other research directed towards improving the throughput of unification-based parsing systems has been concerned with the unification operation itself, which can consume up to 90% of parse time (e.g. Tomabechi, 1991) in systems using lexicalist grammar formalisms (e.g. HPSG; Pollard & Sag, 1987) . ", "after_sen": "However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed."}
{"citeStart": 191, "citeEnd": 215, "citeStartToken": 191, "citeEndToken": 215, "sectionName": "UNKNOWN SECTION NAME", "string": "is obtained efficiently by the Viterbi algorithm. The optimal set of parameters λ is determined efficiently by the Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972) or Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) (Nocedal and Wright, 1999) method.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "is obtained efficiently by the Viterbi algorithm. ", "mid_sen": "The optimal set of parameters λ is determined efficiently by the Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972) or Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) (Nocedal and Wright, 1999) method.", "after_sen": ""}
{"citeStart": 38, "citeEnd": 62, "citeStartToken": 38, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "where h is the time in hours spent on the sentence, l is the number of tokens in the sentence, and c is the number of words in the sentence needing correction. For this model, the Relative Standard Error (RSE) is 89.5, and the adjusted correlation (R ) is 0.181. This model reflects the abilities of the annotators in the study and may not be representative of annotators in other projects. However, the purpose of this paper is to create a framework for accounting for cost in AL algorithms. In contrast to the model presented by Ngai and Yarowsky (2000) , which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, the purpose of this paper is to create a framework for accounting for cost in AL algorithms. ", "mid_sen": "In contrast to the model presented by Ngai and Yarowsky (2000) , which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence.", "after_sen": ""}
{"citeStart": 83, "citeEnd": 95, "citeStartToken": 83, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "Sentiment analysis, which seeks to analyze opinion in natural language text, has grown in interest in recent years. Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004) ; classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004) ; identifying or classifying appraisal targets (Nigam and Hurst, 2004) ; identifying the source of an opinion in a text (Choi et al., 2005) , whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005) . Much of this work has utilized the fundamental concept of 'semantic orientation', (Turney, 2002) ; however, sentiment analysis still lacks a 'unified field theory'.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sentiment analysis includes a variety of different problems, including: sentiment classification techniques to classify reviews as positive or negative, based on bag of words (Pang et al., 2002) or positive and negative words (Turney, 2002; Mullen and Collier, 2004) ; classifying sentences in a document as either subjective or objective (Riloff and Wiebe, 2003; Pang and Lee, 2004) ; identifying or classifying appraisal targets (Nigam and Hurst, 2004) ; identifying the source of an opinion in a text (Choi et al., 2005) , whether the author is expressing the opinion, or whether he is attributing the opinion to someone else; and developing interactive and visual opinion mining methods (Gamon et al., 2005; Popescu and Etzioni, 2005) . ", "mid_sen": "Much of this work has utilized the fundamental concept of 'semantic orientation', (Turney, 2002) ; however, sentiment analysis still lacks a 'unified field theory'.", "after_sen": "We propose in this paper that a fundamental task underlying many of these formulations is the extraction and analysis of appraisal expressions, defined as those structured textual units which express an evaluation of some object. "}
{"citeStart": 89, "citeEnd": 107, "citeStartToken": 89, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth. This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules. Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Micro-level simplification at each of its stages is done by means of a specific combination of rulebased and statistical techniques and relies on linguistic knowledge of different depth. ", "mid_sen": "This knowledge is structured following the methodology described in (Sheremetyeva, 1999; Sheremetyeva, 2003) and is mostly coded in the system lexicon as well as in analysis and generation rules. ", "after_sen": "Different modules of the micro-level simplification component use specific parts and types of linguistic knowledge included in the lexicon and their own specific sets of rules."}
{"citeStart": 141, "citeEnd": 150, "citeStartToken": 141, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "Correctly determining number is a difficult problem when translating from Japanese to English. This is because in Japanese, noun phrases are not normally marked with respect to number. Japanese nouns have no equivalent to the English singular and plural forms and verbs do not inflect to agree with the number of the subject (Kuno 1973) . In addition, there is no grammatical marking of countability, t", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is because in Japanese, noun phrases are not normally marked with respect to number. ", "mid_sen": "Japanese nouns have no equivalent to the English singular and plural forms and verbs do not inflect to agree with the number of the subject (Kuno 1973) . ", "after_sen": "In addition, there is no grammatical marking of countability, t"}
{"citeStart": 34, "citeEnd": 54, "citeStartToken": 34, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. The most relevant work is by Qazvinian and Radev (2010) who extract only the non-explicit citations for a given paper. They model each sentence as a node in a graph and experiment with various window boundaries to create edges between neighbouring nodes. However, their dataset consists of only 10 papers and their annotation scheme differs from our four-class annotation as they do not deal with any sentiment.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This task differs from citation sentiment, which is in a sense a \"lower level\" of analysis.", "mid_sen": "For implicit citation extraction, Kaplan et al. (2009) explore co-reference chains for citation extraction using a combination of co-reference resolution techniques. ", "after_sen": "However, their corpus consists of only 94 sentences of citations to 4 papers which is likely to be too small to be representative. "}
{"citeStart": 32, "citeEnd": 52, "citeStartToken": 32, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "Combinatory Categorial Grammar (CCG, Steedman (2000) ) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007) , sentence realization (White, 2006) , learning semantic parsers (Zettlemoyer and Collins, 2007) , dialog systems (Kruijff et al., 2007) , grammar engineering (Beavers, 2004; Baldridge et al., 2007) , and modeling syntactic priming (Reitter et al., 2006) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Combinatory Categorial Grammar (CCG, Steedman (2000) ) is a compositional, semantically transparent formalism that is both linguistically expressive and computationally tractable. ", "after_sen": "It has been used for a variety of tasks, such as wide-coverage parsing (Hockenmaier and Steedman, 2002; Clark and Curran, 2007) , sentence realization (White, 2006) , learning semantic parsers (Zettlemoyer and Collins, 2007) , dialog systems (Kruijff et al., 2007) , grammar engineering (Beavers, 2004; Baldridge et al., 2007) , and modeling syntactic priming (Reitter et al., 2006) ."}
{"citeStart": 84, "citeEnd": 100, "citeStartToken": 84, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "The domain of our work was the Conference Registration Telephony Conversations. The lexicon for the task contained about 500 English and 500 German words. There were 300 English/German f-structurepairs available from other research tasks (Osterholtz, 1992) . A separate set of 154 sentential f-structures was used to test the generalization performance of the system. The testing data was collected for an independent task (Jain, 1991) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The lexicon for the task contained about 500 English and 500 German words. ", "mid_sen": "There were 300 English/German f-structurepairs available from other research tasks (Osterholtz, 1992) . ", "after_sen": "A separate set of 154 sentential f-structures was used to test the generalization performance of the system. "}
{"citeStart": 125, "citeEnd": 138, "citeStartToken": 125, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to identify the set T of terms relevant to a particular document collection, we adopt the relative frequency ratio (Damerau, 1993) ,", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ated features based on those observable proxies are then used in classification as described in Section 5.", "mid_sen": "In order to identify the set T of terms relevant to a particular document collection, we adopt the relative frequency ratio (Damerau, 1993) ,", "after_sen": "R(t) = R t domain /R t reference , where R t c = f t c"}
{"citeStart": 98, "citeEnd": 111, "citeStartToken": 98, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech (Collins et al. 1999) . There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003) , but there has been no report on applying the Collins parser on Chinese. The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser. We analyzed the Penn Chinese Treebank data and worked out head rules for the Chinese Treebank grammar (we were unable to find any published head rules for Chinese in the literature). There are two major differences in the head rules between English and Chinese. First, NP heads in Chinese are rigidly rightmost, that is to say, no modifiers of an NP can follow the head. In contrast, in English a modifier may follow the head. Second, just as with NPs in Chinese, the head of ADJP is rigidly rightmost. In English, by contrast, the head of an ADJP is mainly the leftmost constituent. Our head rules for the Chinese Treebank grammar are given in the Appendix. In addition to the head rules, we modified the POS tags for all punctuation. This is because all cases of punctuation in the Penn Chinese Treebank are assigned the same POS tag \"PU\". The Collins parser, on the other hand, expects the punctuation tags in the English TreeBank format, where the tag for a punctuation mark is the punctuation mark itself. We therefore replaced the POS tags for all punctuation marks in the Chinese data to conform to the conventions in English. Finally, we made one further augmentation also related to punctuation. Chinese has one punctuation mark that does not exist in English. This commonly used mark, 'semi-stop', is used in Chinese to link coordinates within a sentence (for example between elements of a list). This function is represented in English by a comma. But the comma in English is ambiguous; in addition to its use in coordination and lists, it can also represent the end of a clause. In Chinese, by contrast the semi-stop has only the conjunction/list function. Chinese thus uses the regular comma only for representing clause boundaries. We investigated two ways to model the use of the Chinese semi-stop: (1) just converting the semi-stop to the comma, thus conflating the two functions as in English; and (2) by giving the semi-stop the POS tag \"CC\", a conjunction. We compared parsing results with these two methods; the latter (conjunction) method gained 0.5% net improvement in F-score over the former one. We therefore include it in our Collins parser port. We trained the Collins parser on the Penn Chinese Treebank(CTB) Release 2 with 250K words, first removing from the training set any sentences that occur in the test set for the semantic parsing experiments. We then tested on the test set used in the semantic parsing which includes 113 sentences(TEST1). The results of the syntactic parsing on the test set are shown in Table  7 . To compare the performance of the Collins parser on Chinese with those of other parsers, we conducted an experiment in which we used the same training and test data (Penn Chinese Treebank Release 1, with 100K words) as used in those reports. In this experiment, we used articles 1-270 for training and 271-300 as test(TEST2). Table 8 shows the results and the comparison with other parsers. Table 8 only shows the performance on sentences ≤ 40 words. Our performance on all the sentences TEST2 is P/R/F=82.2/83.3/82.7. It may seem surprising that the overall F-score on TEST2 (82.7) is higher than the overall F-score on TEST1 (81.0) despite the fact that our TEST1 system had more than twice as much training as our TEST2 system. The reason lies in the makeup of the two test sets; TEST1 consists of randomly selected long sentences; TEST2 consists of sequential text, including many short sentences. The average sentence length in TEST1 is 35.2 words, vs. 22.1 in TEST2. TEST1 has 32% long sentences (>40 words) while TEST2 has only 13%. ", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech (Collins et al. 1999) . ", "after_sen": "There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003) , but there has been no report on applying the Collins parser on Chinese. "}
{"citeStart": 168, "citeEnd": 180, "citeStartToken": 168, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "By construction we have ensured that the following theorem from (Booth and Thompson, 1973 ) applies to probabilistic TAGs. A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype Galton-Watson branching process (Harris, 1963) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By construction we have ensured that the following theorem from (Booth and Thompson, 1973 ) applies to probabilistic TAGs. ", "mid_sen": "A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype Galton-Watson branching process (Harris, 1963) .", "after_sen": "Theorem 4.1 A probabilistic grammar is consistent if the spectral radius p(A4) < 1, where ,h,4 is the stochastic expectation matrix computed from the grammar. "}
{"citeStart": 115, "citeEnd": 128, "citeStartToken": 115, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . ", "mid_sen": "Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . ", "after_sen": "We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training."}
{"citeStart": 216, "citeEnd": 235, "citeStartToken": 216, "citeEndToken": 235, "sectionName": "UNKNOWN SECTION NAME", "string": "Concerning downstep, I shall assume that the magnitude of downstep is independent of the tones on either side, and so ~OHL4H = 'PH$H ----\"])LSL ----~LII.I.L. A separate instrumental study supports this hypothesis t (Bird & Stegen, 1993) . Therefore, we lave l~st = 7Pt,s.Lt --dRstt, where s is any tone and t is 1I or L.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Therefore, we can write PHLH(X) = X, and by a result we showed above, RHL.RLH = 1. Given that RHL = d it follows that RLH = ~.", "mid_sen": "Concerning downstep, I shall assume that the magnitude of downstep is independent of the tones on either side, and so ~OHL4H = 'PH$H ----\"])LSL ----~LII.I.L. A separate instrumental study supports this hypothesis t (Bird & Stegen, 1993) . ", "after_sen": "Therefore, we lave l~st = 7Pt,s."}
{"citeStart": 16, "citeEnd": 27, "citeStartToken": 16, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999 ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 000 verbs and 25 000 verbal expressions. In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. ", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 000 verbs and 25 000 verbal expressions. ", "mid_sen": "In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. ", "after_sen": ""}
{"citeStart": 75, "citeEnd": 103, "citeStartToken": 75, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "The possibility of strict definition of each sense of a polysemous word, and the possibility of unambiguous assignment of a given sense in a given situation are, in themselves, nontrivial issues in philosophy (Quine 1960) and linguistics (Weinreich 1980; Cruse 1986) . Different dictionaries often disagree on the definitions; the split into senses may also depend on the task at hand. Thus, it is important to maintain the possibility of flexible distinction of the different senses, e.g., by letting this distinction be determined by an external knowledge source such as a thesaurus or a dictionary. Although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility. For example, defining the senses by the possible translations of the word (Dagan, Itai and Schwall 1991; Brown et al. 1991; Gale, Church, and Yarowsky 1992) , by the Roget's categories (Yarowsky 1992) , or by clustering (Schi~tze 1992) , yields a grouping that does not always conform to the desired sense distinctions. In comparison to these approaches, our reliance on the MRD for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions. Specifically, the user of our system may choose a certain dictionary definition, a combination of definitions from several dictionaries, or manually listed seed words for every sense that needs to be defined. Whereas pure MRD-based methods allow the same flexibility, their potential so far has not been fully tapped, because definitions alone do not contain enough information for disambiguation.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although this requirement may seem trivial, most corpus-based methods do not, in fact, allow such flexibility. ", "mid_sen": "For example, defining the senses by the possible translations of the word (Dagan, Itai and Schwall 1991; Brown et al. 1991; Gale, Church, and Yarowsky 1992) , by the Roget's categories (Yarowsky 1992) , or by clustering (Schi~tze 1992) , yields a grouping that does not always conform to the desired sense distinctions. ", "after_sen": "In comparison to these approaches, our reliance on the MRD for the definition of senses in the initialization of the learning process guarantees the required flexibility in setting the sense distinctions. "}
{"citeStart": 26, "citeEnd": 39, "citeStartToken": 26, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "Following previous works (Golding, 1995; Meknavin et al., 1997) , we have tried two types of features: context words and collocations. Context-word features is used to test for the presence of a particular word within ÷/-M words of the target word, and collocations test for a pattern of up to L contiguous words and/or part-of-speech tags around the target word. In our experiment M and L is set to 10 and 2, respectively. Examples of features for discriminating between snow and know include:", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, if a specific OCR has low probability of substituting t with w, \"knof' should be excluded from the set.", "mid_sen": "Following previous works (Golding, 1995; Meknavin et al., 1997) , we have tried two types of features: context words and collocations. ", "after_sen": "Context-word features is used to test for the presence of a particular word within ÷/-M words of the target word, and collocations test for a pattern of up to L contiguous words and/or part-of-speech tags around the target word. "}
{"citeStart": 161, "citeEnd": 173, "citeStartToken": 161, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "Here we compared two discourse strategies: Allhnplicit ~ul(l Explicit-Warr~mh Explicit-W;uranl is a type of discourse stralegy called ~m Attelition strategy in [W;dkel; 1993] because its main lunction is to manipulate agents' altenlional slate. Elsewhere we show that (1) some IRU strategies are only beneficial when inferential complexity is higher Ihall in the Standard \"l,u,~k [R~unbow and Walker, 1994; Walker, 1994al;  (2) IRUs that make intL'rences explicit can help inlbrence limited agents perlorm as well as logic;ally omniscient ones I Walker, 199311.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In other words, slrategies ~ue cooperative for certain conversational p;u'tners, in/der particular task delinitions, lor p~uticul~u communicalion situations.", "mid_sen": "Here we compared two discourse strategies: Allhnplicit ~ul(l Explicit-Warr~mh Explicit-W;uranl is a type of discourse stralegy called ~m Attelition strategy in [W;dkel; 1993] because its main lunction is to manipulate agents' altenlional slate. ", "after_sen": "Elsewhere we show that (1) some IRU strategies are only beneficial when inferential complexity is higher Ihall in the Standard \"l,u,~k [R~unbow and Walker, 1994; Walker, 1994al;  (2) IRUs that make intL'rences explicit can help inlbrence limited agents perlorm as well as logic;ally omniscient ones I Walker, 199311."}
{"citeStart": 47, "citeEnd": 71, "citeStartToken": 47, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "For Arabic, we use the head-finding rules from Green and Manning (2010) . For French, we use the head-finding rules of Dybro-Johansen 2004, which yielded an approximately 1% development set improvement over those of Arun (2004) .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For Arabic, we use the head-finding rules from Green and Manning (2010) . ", "after_sen": "For French, we use the head-finding rules of Dybro-Johansen 2004, which yielded an approximately 1% development set improvement over those of Arun (2004) ."}
{"citeStart": 66, "citeEnd": 78, "citeStartToken": 66, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993) . They require a relatively large tagged training text. Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. No pretagged text is nec-essary for Hidden Markov Models (Jelinek, 1985; Cutting et al., 1991; Kupiec, 1992) . Still, a lexicon is needed that specifies the possible parts of speech for every word. Brill and Marcus (1992a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably reduced by combining learning procedures and a partial part-of-speech categorization elicited from an informant.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The simplest part-of-speech taggers are bigram or trigram models (Church, 1989; Charniak et al., 1993) . ", "after_sen": "They require a relatively large tagged training text. "}
{"citeStart": 111, "citeEnd": 125, "citeStartToken": 111, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. ", "mid_sen": "Vadas and Curran (2007b) carry out supervised experiments using this data set of 36,584 NPs, outperforming the Collins (2003) parser.", "after_sen": "The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below:"}
{"citeStart": 221, "citeEnd": 239, "citeStartToken": 221, "citeEndToken": 239, "sectionName": "UNKNOWN SECTION NAME", "string": "For the task of genre adaptation to the genres newswire (NW) and UG comments or weblogs, we use a flexible translation model adaptation approach based on phrase pair weighting using a vector space model (VSM) inspired by Chen et al. (2013) . The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling. Second, Irvine et al. (2013) have shown that including relevant training data in a mixture modeling approach solves many coverage errors, but also introduces substantial amounts of new scoring errors. With phrase-pair weighting we aim to optimize phrase translation selection while keeping our training data fixed, and we can thus compare the impact of several methodological variants on genre adaptation for SMT.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For the task of genre adaptation to the genres newswire (NW) and UG comments or weblogs, we use a flexible translation model adaptation approach based on phrase pair weighting using a vector space model (VSM) inspired by Chen et al. (2013) . ", "after_sen": "The reason we choose an instanceweighting method rather than a mixture modeling approach is twofold: First, mixture modeling approaches intrinsically depend on subcorpus boundaries, which resemble provenance or require manual labeling. "}
{"citeStart": 23, "citeEnd": 37, "citeStartToken": 23, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "To see whether feature subsumption can improve classification performance, we trained an SVM classifier for each of the three opinion data sets. We used the SVM light (Joachims, 1998) package with a linear kernel. For the Polarity and OP data we discarded all features that have frequency < 5, and for the MPQA data we discarded features that have frequency < 2 because this data set is substantially smaller. All of our experimental results are averages over 3-fold cross-validation. First, we created 4 baseline classifiers: a 1Gram classifier that uses only the unigram features; a 1+2Gram classifier that uses unigram and bigram features; a 1+EP classifier that uses unigram and extraction pattern features, and a 1+2+EP classifier that uses all three types of features. Next, we created analogous 1+2Gram, 1+EP, and 1+2+EP classifiers but applied the subsumption hierarchy first to eliminate unnecessary features before training the classifier. We experimented with three delta values for the subsumption process: δ=.0005, .001, and .002. Figures 7, 8, and 9 show the results. The subsumption process produced small but consistent improvements on all 3 data sets. For example, Fig-ure 8 shows the results on the OP data, where all of the accuracy values produced after subsumption (the rightmost 3 columns) are higher than the accuracy values produced without subsumption (the Base[line] column). For all three data sets, the best overall accuracy (shown in boldface) was always achieved after subsumption. We also observed that subsumption had a dramatic effect on the F-measure scores on the OP data, which are shown in Figure 10 . The OP data set is fundamentally different from the other data sets because it is so highly skewed, with 91% of the documents belonging to the non-opinion class. Without subsumption, the classifier was conservative about assigning documents to the opinion class, achieving F-measure scores in the 82-88 range. After subsumption, the overall accuracy improved but the F-measure scores increased more dramatically. These numbers show that the subsumption process produced not only a more accurate classifier, but a more useful classifier that identifies more documents as being opinion articles.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To see whether feature subsumption can improve classification performance, we trained an SVM classifier for each of the three opinion data sets. ", "mid_sen": "We used the SVM light (Joachims, 1998) package with a linear kernel. ", "after_sen": "For the Polarity and OP data we discarded all features that have frequency < 5, and for the MPQA data we discarded features that have frequency < 2 because this data set is substantially smaller. "}
{"citeStart": 83, "citeEnd": 98, "citeStartToken": 83, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "The formalism for spelling rules (dimension (a)) is a syntactic variant of that of Ruessink (1989) and Pulman (1991) . A rule is of the form spell (Name, Surface Op Lexical, Classes, Features) . Rules may be optional (Op is \"~\") or obligatory (Op is \"¢~\"). Surface and Lexical are both strings of the form \" LContext I Target I RContext\" meaning that the surface and lexical targets may correspond if the left and right contexts and the Features specification are satisfied. The vertical bars simply separate the parts of the string and do not themselves match letters. The correspondence between surface and lexical strings for an entire word is licensed if there is a partitioning of both so that each partition (pair of corresponding surface and lexica] targets) is licensed by a rule, and no partition breaks an obligatory rule. A partition breaks an obligatory rule if the surface target does not match but everything else, including the feature specification, does.", "label": "Extends", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The formalism for spelling rules (dimension (a)) is a syntactic variant of that of Ruessink (1989) and Pulman (1991) . ", "after_sen": "A rule is of the form spell (Name, Surface Op Lexical, Classes, Features) . "}
{"citeStart": 6, "citeEnd": 17, "citeStartToken": 6, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "Events are occurrences that have a structure to them; in particular, their result, or their coming to an end is included in them: to destroy a building, to write a book. As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished. While Bach (1986) did not investigate the internal structure of events, others suggested that this needs to be done (e.g., Moens and Steedman 1988; Parsons 1990) . Pustejovsky (1991) treated Vendlerian accomplishments and achievements as transitions from a state Q(y) to NOT-Q(y), and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished. ", "mid_sen": "While Bach (1986) did not investigate the internal structure of events, others suggested that this needs to be done (e.g., Moens and Steedman 1988; Parsons 1990) . ", "after_sen": "Pustejovsky (1991) treated Vendlerian accomplishments and achievements as transitions from a state Q(y) to NOT-Q(y), and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state."}
{"citeStart": 183, "citeEnd": 199, "citeStartToken": 183, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004) . Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Because a tree-based gold standard for parser evaluation must adopt a particular style of linguistic analysis (reflected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser.", "mid_sen": "Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanfilippo 1998; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004) . ", "after_sen": "Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation."}
{"citeStart": 92, "citeEnd": 111, "citeStartToken": 92, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "We implemented, at least minimally, all 12 of these features, with a few additions of what (Ng and Cardie, 2002) hand selected as being most salient for increased performance. We also extended this list by adding a cosine-similarity metric between two noun phrases; it uses bag-ofwords to create a vector for each noun phrase (where each word is a term in the vector) to compute their similarity. The intuition behind this is that noun phrases with more similar surface forms should be more likely to corefer.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The antecedent in these steps is also always considered to be to the left of, or preceding, the anaphor; cataphors are not addressed in this technique.", "mid_sen": "We implemented, at least minimally, all 12 of these features, with a few additions of what (Ng and Cardie, 2002) hand selected as being most salient for increased performance. ", "after_sen": "We also extended this list by adding a cosine-similarity metric between two noun phrases; it uses bag-ofwords to create a vector for each noun phrase (where each word is a term in the vector) to compute their similarity. "}
{"citeStart": 25, "citeEnd": 38, "citeStartToken": 25, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "lIowever, even if we fix both syntactic (:ategory and lexieal meaning, we still get some weird coordinations. For example, consider: The unacceptability of these examples suggests that word by word identity is insufficient, and that deleted material must have identicM syntactic structure, as well as identical lexieal meanings. Some of the most compelling arguments against deletion have been semantic. For examqfle, Lakoff and Peters (1969) argued that deletion accounts are inap propriate tbr certain constituent coordinations such gtS : 10) John and Mary are alike since the 'antecedent' sentence John are alike and Mary arc alike is nonsensical (it is also ungrannnatieal if we consider number agreement).", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some of the most compelling arguments against deletion have been semantic. ", "mid_sen": "For examqfle, Lakoff and Peters (1969) argued that deletion accounts are inap propriate tbr certain constituent coordinations such gtS : 10) John and Mary are alike since the 'antecedent' sentence John are alike and Mary arc alike is nonsensical (it is also ungrannnatieal if we consider number agreement).", "after_sen": "However, semantically inappropriate or nonsensi-cM 'antecedents' are also possible when we consider non-constituent coordination. "}
{"citeStart": 156, "citeEnd": 170, "citeStartToken": 156, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "Our thesaurus brings up only alternatives that have the same part-of-speech with the target word. The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus (Roget, 1852), dictionaries of synonyms (Hayakawa, 1994) , or clusters acquired from corpora (Lin, 1998) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our thesaurus brings up only alternatives that have the same part-of-speech with the target word. ", "mid_sen": "The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus (Roget, 1852), dictionaries of synonyms (Hayakawa, 1994) , or clusters acquired from corpora (Lin, 1998) .", "after_sen": "In this paper we focus on the task of automatically selecting the best near-synonym that should be used in a particular context. "}
{"citeStart": 60, "citeEnd": 79, "citeStartToken": 60, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "We now prove the same result for the unsupervised MLE method, without any restrictive assumption on the rules of our CFGs. This solves a problem that was left open in the literature (Chi and Geman, 1998) ; see again Section 1 for discussion. Let C and C be defined as in Section 3. We define the empirical distribution of C as", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We now prove the same result for the unsupervised MLE method, without any restrictive assumption on the rules of our CFGs. ", "mid_sen": "This solves a problem that was left open in the literature (Chi and Geman, 1998) ; see again Section 1 for discussion. ", "after_sen": "Let C and C be defined as in Section 3. "}
{"citeStart": 355, "citeEnd": 367, "citeStartToken": 355, "citeEndToken": 367, "sectionName": "UNKNOWN SECTION NAME", "string": "The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: 1Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996) , (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus-but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997) , and (4) no figures about computational effort -space/time complexity-are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The above mentioned factors can affect either the evaluation or the comparison process. ", "mid_sen": "Factors affecting the evaluation process are: 1Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996) , (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus-but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997) , and (4) no figures about computational effort -space/time complexity-are usually reported, even from an empirical perspective. ", "after_sen": "A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance."}
{"citeStart": 83, "citeEnd": 102, "citeStartToken": 83, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to be able to compare our results with the results obtained by other researchers, we worked with the same data sets already used by (Ramshaw and Marcus, 1995; Argamon et al., 1998) for NP and SV detection. These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993) . For NP, the training and test corpus was prepared from sections 15 to 18 and section 20, respectively; the SV corpus was prepared from sections 1 to 9 for training and section 0 for testing. Instead of using the NP bracketing information present in the tagged Treebank data, Ramshaw and Marcus modified the data so as to include bracketing information related only to the non-recursive, base NPs present in each sentence while the subject verb phrases were taken as is. The data sets include POS tag information generated by Ramshaw and Marcus using Brill's transformational part-of-speech tagger (Brill, 1995) . The sizes of the training and test data are summarized in Table 1 and Table 2 .", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to be able to compare our results with the results obtained by other researchers, we worked with the same data sets already used by (Ramshaw and Marcus, 1995; Argamon et al., 1998) for NP and SV detection. ", "mid_sen": "These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993) . ", "after_sen": "For NP, the training and test corpus was prepared from sections 15 to 18 and section 20, respectively; the SV corpus was prepared from sections 1 to 9 for training and section 0 for testing. "}
{"citeStart": 187, "citeEnd": 204, "citeStartToken": 187, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997) . It was developed at the Saarland University in Saarbrficken 2. Part of it was tagged at the IMS Stuttgart. This evaluation only uses the partof-speech annotation and ignores structural annotations.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The German NEGRA corpus consists of 20,000 sentences (355,000 tokens) of newspaper texts (Frankfurter Rundschau) that are annotated with parts-ofspeech and predicate-argument structures (Skut et al., 1997) . ", "after_sen": "It was developed at the Saarland University in Saarbrficken 2. "}
{"citeStart": 50, "citeEnd": 64, "citeStartToken": 50, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "Language identification is an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible. It can be solved using many techniques, including knowledge-poor statistical approaches. Typically, the distribution of n-grams of characters or other objects is used to form a model. A comparison of the input against the model determines the language which matches best. Versions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994) , while an interesting practical implementation is described by Adams and Resnik (1997) .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A comparison of the input against the model determines the language which matches best. ", "mid_sen": "Versions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994) , while an interesting practical implementation is described by Adams and Resnik (1997) .", "after_sen": "A variant of the problem is considered by Sibun and Spitz (1994) , and Sibun and Reynar (1996) , who look at it from the point of view of Optical Character Recognition (OCR)."}
{"citeStart": 213, "citeEnd": 228, "citeStartToken": 213, "citeEndToken": 228, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ( [Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994 ]), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995] )--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 11, "citeEnd": 31, "citeStartToken": 11, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Annotators were both native English speakers who speak French as a second language. Each has a strong comprehension of written French. consider them to be equal. To avoid bias, the competing systems were presented anonymously and in random order. Following (Collins et al., 2005) , we provide the annotators with only short sentences: those with source sentences between 10 and 25 tokens long. Following (Callison-Burch et al., 2006) , we conduct a targeted evaluation; we only draw our evaluation pairs from the uncohesive subset targeted by our constraint. All 75 sentences that meet these two criteria are included in the evaluation.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To avoid bias, the competing systems were presented anonymously and in random order. ", "mid_sen": "Following (Collins et al., 2005) , we provide the annotators with only short sentences: those with source sentences between 10 and 25 tokens long. ", "after_sen": "Following (Callison-Burch et al., 2006) , we conduct a targeted evaluation; we only draw our evaluation pairs from the uncohesive subset targeted by our constraint. "}
{"citeStart": 313, "citeEnd": 340, "citeStartToken": 313, "citeEndToken": 340, "sectionName": "UNKNOWN SECTION NAME", "string": "Anaphora resolution is still present as a significant linguistic problem, both theoretically and practically, and interest has recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference (MUC) evaluations of Information Extraction (IE) systems (Grishman and Sundheim, 1996) . This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Anaphora resolution is still present as a significant linguistic problem, both theoretically and practically, and interest has recently been renewed with the introduction of a quantitative evaluation regime as part of the Message Understanding Conference (MUC) evaluations of Information Extraction (IE) systems (Grishman and Sundheim, 1996) . ", "after_sen": "This has made it possible to evaluate different (implementable) theoretical approaches against sizable corpora of real-world texts, rather than the small collections of artificial examples typically discussed in the literature."}
{"citeStart": 225, "citeEnd": 235, "citeStartToken": 225, "citeEndToken": 235, "sectionName": "UNKNOWN SECTION NAME", "string": "In a system like INTARC-1.3, the analysis tree is of much higher importance than the recovered string; for the goal of speech translation an adequate semantic representation for a string with word errors is more important than a good string with a wrong reading. The grammar scores have only indirect influence on the string; their main function is picking the right tree. We cannot measure something like a \"tree recognition rate\" or \"rule accuracy\", because there is no treebank for our grammar. The word accuracy results cannot be compared to word accuracy as usually applied to an acoustic decoder in isolation. We counted only those words as recognized which could be built into a valid parse from the beginning of the utterance. Words to the right which could not be integrated into a parse, were counted as deletions ---although they might have been correct in standard word accuracy terms. This evaluation method is much harder than standard word accuracy, but it appears to be a good approximation to \"rule accuracy\". Using this strict method we achieved a word accuracy of 47%, which is quite promising. Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989 ) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a) , and (Weber 1995) . Recognition rates had been improved there for read speech. In spontaneous speech we could not achieve the same effects.", "label": "ComOrCon", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using this strict method we achieved a word accuracy of 47%, which is quite promising. ", "mid_sen": "Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989 ) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a) , and (Weber 1995) . ", "after_sen": "Recognition rates had been improved there for read speech. "}
{"citeStart": 148, "citeEnd": 168, "citeStartToken": 148, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) . Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms (Schabes et al., 1988; van Noord, 1994; Yoshida et al., 1999; Torisawa et al., 2000) . However, these realizations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; ). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) . ", "mid_sen": "Along with the independent development of parsing techniques for individual grammar formalisms, some of them have been adapted to other formalisms (Schabes et al., 1988; van Noord, 1994; Yoshida et al., 1999; Torisawa et al., 2000) . ", "after_sen": "However, these realizations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; ). "}
{"citeStart": 75, "citeEnd": 102, "citeStartToken": 75, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "The SENSELEARNER system was evaluated on the SENSEVAL-2 and SENSEVAL-3 English all words data sets, each data set consisting of three texts from the Penn Treebank corpus annotated with WordNet senses. The SENSEVAL-2 corpus includes a total of 2,473 annotated content words, and the SENSEVAL-3 corpus includes annotations for an additional set of 2,081 words. Table 1 shows precision and recall figures obtained with each semantic model on these two data sets. A baseline, computed using the most frequent sense in WordNet, is also indicated. The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002) and 65.2% on SENSEVAL-3 data (Decadt et al., 2004) . Note however that both these systems rely on significantly larger training data sets, and thus the results are not directly comparable.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A baseline, computed using the most frequent sense in WordNet, is also indicated. ", "mid_sen": "The best results reported on these data sets are 69.0% on SENSEVAL-2 data (Mihalcea and Moldovan, 2002) and 65.2% on SENSEVAL-3 data (Decadt et al., 2004) . ", "after_sen": "Note however that both these systems rely on significantly larger training data sets, and thus the results are not directly comparable."}
{"citeStart": 10, "citeEnd": 33, "citeStartToken": 10, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007) . Each committee member's training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson & Dagan (1996) in the implementation of vote entropy for sentence selection using these models.", "label": "Uses", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each committee member's training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. ", "mid_sen": "We follow Engelson & Dagan (1996) in the implementation of vote entropy for sentence selection using these models.", "after_sen": "When comparing the relative performance of AL algorithms, learning curves can be challenging to in-terpret. "}
{"citeStart": 95, "citeEnd": 145, "citeStartToken": 95, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "'Fo date, psychologists have focused on two aspects of the speech segmentation problem. The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched; both psychologists (e.g., Cutler & Carter, 1987; Cutler & Butterliel (I, 1992) and designers of speech-recognition systems (e.g., (]hur (:h, 1987) have examined I~his problem. However, the problem we examined is dilferent---we want to know how infants segment speech before knowing which phonemic se-qllelW,('s form words. '1' he second aspect psychologists liaw~ focnsed (ill is the lirobleln of dcternihiilig the ill['Orluatioll SOllr(:(~s t() which ilifants are SCllSil,ive. Priluarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress. II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. Sensitivity to native-language phonotactics in 9-month-olds was re(:ently reported by Jusczyk, I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993) . 'i'lles~ sl, udi(~s deruoilstratcd infants' perceptive abilil.il's wil,hout deiilonsl.ral,hig tlw usefuhicss of hli'alil,s > ll(~rcel)l,ioils. I low do childl'(,n coiubine l,li(: iiiforiii;d,ion I, hey i)crc~,iw; froln dilrerenl, SOlll'l;es'. ? Aslili el, al. Sl)(~c-Illate that infants first learn words heard in isolation, then use distribution and prosody to refine and expand their w)cabulary; however, Jusczyk (1(,)93) sliggests that sound sequences learned in isolation dill~r too greatly from those in contexi. to bc useful. He goes on to say, \"just how far inforniation in the sound structure of the input can pies, we see that Hypothesis 1 uses 48 characters and Hypothesis 2 uses 75. However, this simplistic method is inefficient; for instance, the length of lexical indices are arbitrary with respect to properties of the words themselves (e.g., in Hypothesis 2, there is no reason why/jul/was assigned tile index '10'--length two--instead of '9'--length one). Our system improves upon this simple size metri(: I)y coml)uting sizes based on ;t ('Onll)act rel)rcs(,ntat.ion motivated I)y informati(m theory. W(: inmginc hypothes(:s r(qu'(~sented ;~ a string of ones and zeros. This binary string must r(,present not only the lexical entries, their indices (called code words) and the coded sample, but also overhead information specifying the number of items coded and their arrangement in the string (information implicitly given by spacing and sl)atial placement in the introductory cxamples). Furtherrnore, the string and its components must be self-delimiting, so that a decoder could identify the endpoints of components by itself. The next section describes the binary representation and the length formulm derived from it in detail; readers satisfied with the intuitive descriptions presented so far should skip ahead to the Phonotactics subsection.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. ", "mid_sen": "Sensitivity to native-language phonotactics in 9-month-olds was re(:ently reported by Jusczyk, I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993) . 'i'lles~ sl, udi(~s deruoilstratcd infants' perceptive abilil.", "after_sen": "il's wil,hout deiilonsl.ral,hig tlw usefuhicss of hli'alil,s > ll(~rcel)l,ioils. "}
{"citeStart": 50, "citeEnd": 55, "citeStartToken": 50, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "Bidirectionality of grammar is a research topic in natural language processing that is enjoying increasing attention (Strzalkowski, 1993a) . This is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, Appelt, 1987) . We address this topic in describing a novel approach to HPSG (Pollard and Sag, 1994) based language processing that uses an off-line compiler to automatically prime a declarative grammar for generation or parsing, and hands the primed grammar to an advanced Earley processor. The developed techniques are direction independent in the sense that they can be used for both generation and parsing with HPSG grammars. In this paper, we focus on the application of the developed techniques in the context of the comparatively neglected area of HPSG generation. Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not *The presented research was sponsored by 'l~eilprojekt B4 \"Constraints on Grammar for Efficient Generation\" of the Sonderforschungsbereich 340 \"Sprachtheoretische Grundlagen fiir die Computerllnguistik\" of the Deutsche Forschungsgemeinschaft. The authors wish to thank Paul King, Detmar Meurers and Shuly Wintner for valuable comments and discussion. Of course, the authors are responsible for all remaining errors. use the prediction step to restrict feature instantiations on the predicted phrases, and thus lacks goaldirectedness. Though Gerdemann (1991) showed how to modify the restriction function to make topdown information available for the bottom-up completion step, Earley generation with top-down prediction still has a problem in that generating the subparts of a construction in the wrong order might lead to massive nondeterminacy or even nontermination. Gerdemann (1991) partly overcame this problem by incorpQrating a head-driven strategy into Earley's algorithm. However, evaluating the head of a construction prior to its dependent subparts still suffers from efficiency problems when the head of a construction is either missing, displaced or underspecified. Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. It is based on the notion of essential arguments, arguments which must be instantiated to ensure the efficient and terminating execution of a node. Minnen et al. (1995) observe that the EAA is computationally infeasible, because it demands the investigation of almost all possible permutations of a grammar. Moreover, the interchanging of arguments in recursive procedures as proposed by Strzalkowski fails to guarantee that input and output grammars are semantically equivalent. The Direct Inversion Approach (DI,~) of Minnen et al. (1995) overcomes these problems by making the reordering process more goal-directed and developing a reformulation technique that allows the successful treatment of rules which exhibit head-recursion. Both the EAA and the DIA were presented as approaches to the inversion of parseroriented grammars into grammars suitable for generation. However, both approaches can just as well take a declarative grammar specification as input to produce generator and/or parser-oriented grammars as in Dymetman et al. (1990) . In this paper we adopt the latter theoretically more interesting perspective.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, Martinovid and Strzalkowski (1992) and others have observed that a simple headfirst reordering of the grammar rules may still make insufficient restricting information available for generation unless the form of the grammar is restricted to unary or binary rules. ", "mid_sen": "Strzalkowski's Essential Arguments Approach (Ehh; 1993b ) is a top-down approach to generation and parsing with logic grammars that uses off-line compilation to automatically invert parser-oriented logic grammars. ", "after_sen": "The inversion process consists of both the automatic static reordering of nodes in the grammar, and the interchanging of arguments in rules with recursively defined heads. "}
{"citeStart": 29, "citeEnd": 43, "citeStartToken": 29, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "This problem is analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach. We were led to seek a solution for this problem within DRT, because of DRT's advantages as a general theory of discourse, and its choice as the underlying formalism in another research project of ours, which deals with sentences such as 1-4, in the context of natural language specifications of computerized systems. In this paper, we propose such a solution, based on a careful distinction between different roles of Reichenbach's reference time (Reichenbach, 1947) , adapted from (Kamp and Reyle, 1993) . Figure 1 shows a 'minimal pair' of DRS's for sentence 1, one according to Partee's(1984) analysis and one according to ours.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In DRT, such sentences trigger box-splitting with the eventuality of the subordinate clause and an updated reference time in the antecedent box, and the eventuality of the main clause in the consequent box, causing undesirable universal quantification over the reference time.", "mid_sen": "This problem is analyzed in (de Swart, 1991) as an instance of the proportion problem and given a solution from a Generalized Quantifier approach. ", "after_sen": "We were led to seek a solution for this problem within DRT, because of DRT's advantages as a general theory of discourse, and its choice as the underlying formalism in another research project of ours, which deals with sentences such as 1-4, in the context of natural language specifications of computerized systems. "}
{"citeStart": 126, "citeEnd": 147, "citeStartToken": 126, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, relevance models originally developed for information retrieval, have been successfully applied to image annotation (Lavrenko et al., 2003; Feng et al., 2004) . A key idea behind these models is to find the images most similar to the test image and then use their shared keywords for annotation.", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More sophisticated graphical models have also been employed including Gaussian Mixture Models (GMM) and Latent Dirichlet Allocation (LDA).", "mid_sen": "Finally, relevance models originally developed for information retrieval, have been successfully applied to image annotation (Lavrenko et al., 2003; Feng et al., 2004) . ", "after_sen": "A key idea behind these models is to find the images most similar to the test image and then use their shared keywords for annotation."}
{"citeStart": 160, "citeEnd": 177, "citeStartToken": 160, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "to detransform the parse trees produced using the PCFG estimated from trees transformed by T . By calculating the labelled precision and recall scores for the detransformed trees in the usual manner, we can systematically compare the parsing accuracy of di erent kinds of stochastic generalized left-corner parsers. Table 5 presents the results of this comparison. As reported previously, the standard left-corner grammar embeds su cient non-local information in its productions to signi cantly improve the labelled precision and recall of its MLPs with respect to MLPs of the PCFG estimated from the untransformed trees Manning and Carpenter, 1997; Roark and Johnson, 1999 . Parsing accuracy drops o as grammar size decreases, presumably because smaller PCFGs have fewer adjustable parameters with which t o d escribe this non-local information. There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy Johnson, 1998b . Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak 1997 and Collins 1997 .", "label": "Background", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are other kinds of non-local information which can be incorporated into a PCFG using a transform-detransform approach that result in an even greater improvement of parsing accuracy Johnson, 1998b . ", "mid_sen": "Ultimately, however, it seems that a more complex approach incorporating back-o and smoothing is necessary in order to achieve the parsing accuracy achieved by Charniak 1997 and Collins 1997 .", "after_sen": ""}
