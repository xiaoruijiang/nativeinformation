{"citeStart": 136, "citeEnd": 152, "citeStartToken": 136, "citeEndToken": 152, "sectionName": "UNKNOWN SECTION NAME", "string": "The resulting specialized grammar was compiled into LR parsing tables, and a special LR parser exploited their special properties, see [Samuelsson 1994b ].", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation.", "mid_sen": "The resulting specialized grammar was compiled into LR parsing tables, and a special LR parser exploited their special properties, see [Samuelsson 1994b ].", "after_sen": "The technical vehicle previously used to extract the specialized grammar is explanation-based generalization (EBG), see e.g. "}
{"citeStart": 64, "citeEnd": 78, "citeStartToken": 64, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The extraction procedure fragments a parse tree from the PTB that is provided as input into elementary trees. See Figure 4 . These elementary trees can be composed by TAG operations to form the original parse tree. The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics. Salient heuristics include the use of a head percolation table (Magerman, 1995) , and another table that distinguishes between complements and adjunct nodes in the tree. For our current work, we use the head percolation table to determine heads of phrases. Also, we treat a PropBank argument (ARG0 . . . ARG9) as a complement and a PropBank adjunct (ARGM's) as an adjunct when such annotation is available. 1 Otherwise, we basically follow the approach of (Chen, 2001 ). 2 Besides introducing one kind of TAG extraction procedure, (Chen, 2001) introduces the notion of grouping linguistically-related extracted tree frames together. In one approach, each tree frame is decomposed into a feature vector. Each element of this vector describes a single linguistically-motivated characteristic of the tree.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics. ", "mid_sen": "Salient heuristics include the use of a head percolation table (Magerman, 1995) , and another table that distinguishes between complements and adjunct nodes in the tree. ", "after_sen": "For our current work, we use the head percolation table to determine heads of phrases. "}
{"citeStart": 78, "citeEnd": 93, "citeStartToken": 78, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "In Section 2, we review previous approaches to the semantics of coordination and argument shar-ing. and make note of some of their drawbacks. We describe the revised sere.antic framework in Section 3. and work through several examples of non-constituent coordination (specifically, rightnode raising) in Section 4. We discnss examples involving intensioual verbs in Section 5, 2 Previous Work Steedman (198.5; 1989; , working in the framework of Combinatory Categorial Grammar (CCG), presents what is probably the most adequate analysis of non-constituent coordination to date. As noted by Steedman and discussed by Oehrle (1990) , the addition of the rule of function composition to the inventory of syntactic rules in Categorial Grammar enables the formation of constituents with right-peripheral gaps, providing a basis for a clean treatment of cases of right node raising as exemplified by sentence (1). Such examples are handled by a coordination schema which allows like categories to be conjoined, shown in (2).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "antic framework in Section 3. and work through several examples of non-constituent coordination (specifically, rightnode raising) in Section 4. ", "mid_sen": "We discnss examples involving intensioual verbs in Section 5, 2 Previous Work Steedman (198.5; 1989; , working in the framework of Combinatory Categorial Grammar (CCG), presents what is probably the most adequate analysis of non-constituent coordination to date. ", "after_sen": "As noted by Steedman and discussed by Oehrle (1990) , the addition of the rule of function composition to the inventory of syntactic rules in Categorial Grammar enables the formation of constituents with right-peripheral gaps, providing a basis for a clean treatment of cases of right node raising as exemplified by sentence (1). "}
{"citeStart": 108, "citeEnd": 122, "citeStartToken": 108, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993) , decision lists (Yarowsky, 1994) , and Bayesian hybrids (Golding, 1995) , have had varying degrees of success for the problem of context-sensitive spelling correction. However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic. Train Test  Most freq. Base  their, there, they're  3265  850  than, then  2096  514  its, it's  1364  366  your, you're  750  187  begin, being  559  146  passed, past  307  74  quiet, quite  264  66  weather, whether  239  61  accept, except  173  50  lead, led  173 Table 1 : Performance of the baseline method for 18 confusion sets. \"Train\" and \"Test\" give the number of occurrences of any word in the confusion set in the training and test corpora. \"Most freq.\" is the word in the confusion set that occurred most often in the training corpus. \"Base\" is the percentage of correct predictions of the baseline system on the test corpus.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, word trigrams are ineffective at capturing longdistance properties such as discourse topic and tense.", "mid_sen": "Feature-based approaches, such as Bayesian classifters (Gale, Church, and Yarowsky, 1993) , decision lists (Yarowsky, 1994) , and Bayesian hybrids (Golding, 1995) , have had varying degrees of success for the problem of context-sensitive spelling correction. ", "after_sen": "However, we report experiments that show that these methods are of limited effectiveness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic. "}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Inoue (1991), describes a \"minimal expulsion strategy\", which predicts a preference, on reanalysis, towards expelling the minimum amount of material from the clause. In our terms, this means that (assuming a binary right-branching clause structure, with the verb in its right corner) the node selected for lowering must be as high as possible. This means that the bottom-up search which we use for English will wrongly predict a Maximal expulsion strategy. In cases such as (8), assuming the bottom-up search, when a postclausal noun has been reached in the input, the parser starts its search from the node immediately dominating the last word to be incorporated, (i.e. the verb of what will become the relative clause). This means that, in cases such as (8), the first preference will be to lower the verb (and therefore \"expel\" both subject and object), whereas the human preference, (to lower the object and verb, and therefore expel only the subject) is the parser's second choice on the bottom-up search strategy. Mazuka and Itoh (in press) note that examples where both subject and object must be expelled from the relative clause, as would be the first choice in a bottom-up search, often cause a conscious garden path effect. An example, adapted from Mazuka and Itoh is the following:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This means that, in cases such as (8), the first preference will be to lower the verb (and therefore \"expel\" both subject and object), whereas the human preference, (to lower the object and verb, and therefore expel only the subject) is the parser's second choice on the bottom-up search strategy. ", "mid_sen": "Mazuka and Itoh (in press) note that examples where both subject and object must be expelled from the relative clause, as would be the first choice in a bottom-up search, often cause a conscious garden path effect. ", "after_sen": "An example, adapted from Mazuka and Itoh is the following:"}
{"citeStart": 36, "citeEnd": 58, "citeStartToken": 36, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus far, we have added all semantic filters by hand, and they are implemented in a hard-fail mode, i.e., if the semantic restrictions fail, the node dies. This strategy seems to be adequate for the limited domains that we have worked with thus far, but they will probably be inadequate for more complex domains. In principle, one could parse a large set of sentences with semantics turned off, collecting the semantic conditions that occurred at each node of interest. Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. This approach resembles the work by Grishman et al. (1986) and Hirschman et al. (1975) on selectional restrictions. The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. There is obviously a great deal more work to be done in this important area.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Then the system could propose to a human expert a set of filters for each node, based on its observations, and the human could make the final decision on whether to accept the proposals. ", "mid_sen": "This approach resembles the work by Grishman et al. (1986) and Hirschman et al. (1975) on selectional restrictions. ", "after_sen": "The semantic conditions that pass could even ultimately be associated with probabilities, obtained by frequency counts on their occurrences. "}
{"citeStart": 73, "citeEnd": 91, "citeStartToken": 73, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "First, we adhere to the commonly used scenario in which adaptation is guided by manual subcorpus labels that resemble provenance of training documents. In this formulation, each weight w i (f ,ē) in Equation (1) is a standard tf-idf weight capturing the relative occurrence of phrase pair (f ,ē) in different subcorpora. Since our aim is to adapt to multiple genres in a test corpus, we follow Chen et al. (2013) and manually group our training data into subcorpora that reflect various genres (see Table 3 ). While this definition of the vector space can approximate genres at different levels of granularity, manual subcorpus labels are labor intensive to generate, particularly in the scenario where provenance information is not available, and may not generalize well to new translation tasks.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this formulation, each weight w i (f ,ē) in Equation (1) is a standard tf-idf weight capturing the relative occurrence of phrase pair (f ,ē) in different subcorpora. ", "mid_sen": "Since our aim is to adapt to multiple genres in a test corpus, we follow Chen et al. (2013) and manually group our training data into subcorpora that reflect various genres (see Table 3 ). ", "after_sen": "While this definition of the vector space can approximate genres at different levels of granularity, manual subcorpus labels are labor intensive to generate, particularly in the scenario where provenance information is not available, and may not generalize well to new translation tasks."}
{"citeStart": 244, "citeEnd": 256, "citeStartToken": 244, "citeEndToken": 256, "sectionName": "UNKNOWN SECTION NAME", "string": "Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007) . We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. Thus, unlike McDonald (2006) , Clarke and Lapata (2006) and Cohn and Lapata (2007) , we do not insist on finding a globally optimal solution in the space of 2 n possible compressions for an n word long sentence. Rather we insist on finding a most plausible compression among those that are explicitly warranted by the grammar.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An obvious benefit of using CRFs for sentence compression is that the model provides a general (and principled) probabilistic framework which permits information from various sources to be integrated towards compressing sentence, a property K&M do not share.", "mid_sen": "Nonetheless, there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates (Nomoto, 2007) . ", "after_sen": "We tackle the issues by harnessing CRFs with what we might call dependency truncation, whose goal is to restrict CRFs to working with candidates that conform to the grammar. "}
{"citeStart": 90, "citeEnd": 114, "citeStartToken": 90, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "Our results are summarized in Table 1 . We compare the annotation performance of the model proposed in this paper (ExtModel) with Lavrenko et al.'s (2003) original continuous relevance model (Lavrenko03) and two other simpler models which do not take the image into account (tf * idf and Doc-Title). First, note that the original relevance model performs best when the annotation output is restricted to 10 words with an F1 of 11.81% (recall is 9.05 and precision 16.01). F1 is marginally worse with 15 output words and decreases by 2% with 20. This model does not take any document-based information into account, it is trained solely on imagecaption pairs. On the Corel test set the same model obtains a precision of 19.0% and a recall of 16.0% with a vocabulary of 260 words. Although these results are not strictly comparable with ours due to the different nature of the training data (in addition, we output 10 annotation words, whereas Lavrenko et al. (2003) output 5), they give some indication of the decrease in performance incurred when using a more challenging dataset. Unlike Corel, our images have greater variety, non-overlapping content and employ a larger vocabulary (2,167 vs. 260 words).", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our results are summarized in Table 1 . ", "mid_sen": "We compare the annotation performance of the model proposed in this paper (ExtModel) with Lavrenko et al.'s (2003) original continuous relevance model (Lavrenko03) and two other simpler models which do not take the image into account (tf * idf and Doc-Title). ", "after_sen": "First, note that the original relevance model performs best when the annotation output is restricted to 10 words with an F1 of 11.81% (recall is 9.05 and precision 16.01). "}
{"citeStart": 132, "citeEnd": 156, "citeStartToken": 132, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "The paper thus sheds light on two questions. A very early result on the weak generative equivalence of context-free grammars and DGs suggested that DGs are incapable of describing surface word order (Gaifman, 1965) . J This result has been criticised to apply only to impoverished DGs which do not properly represent formally the expressivity of contemporary DG variants (Neuhaus & BrSker, 1997) , and our use of a context-free backbone with further constraints imposed by dependency relations further supports the view that DG is not a notational ~riant of context-free grammar. The second question addressed is that of efficient processing of discontinuous DGs. By converting a native DG grammar into LFG rules, we are able to profit from the state of the art in context-free parsing technology. A context-free base (or skeleton) has often been cited as a prerequisite for practical applicability of a natural language grammar (Erbach & Uszkoreit, 1990 ), and we here show that a DG can meet this criterion with ease.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By converting a native DG grammar into LFG rules, we are able to profit from the state of the art in context-free parsing technology. ", "mid_sen": "A context-free base (or skeleton) has often been cited as a prerequisite for practical applicability of a natural language grammar (Erbach & Uszkoreit, 1990 ), and we here show that a DG can meet this criterion with ease.", "after_sen": "Sec. 2 will briefly review approaches to word order in DG, and Sec. 3 introduces word order domains as our proposal. "}
{"citeStart": 347, "citeEnd": 361, "citeStartToken": 347, "citeEndToken": 361, "sectionName": "UNKNOWN SECTION NAME", "string": "This pure model of SA has several significant problems, the most notable being that activation can saturate the entire network unless certain constraints are imposed, namely limiting how far activation can spread from the initially activated nodes (distance constraint), and limiting the effect of very highly-connected nodes (fanout constraint) (Crestani, 1997) . In the following three sections we discuss how these constraints were implemented in our model for SA.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "EQUATION", "mid_sen": "This pure model of SA has several significant problems, the most notable being that activation can saturate the entire network unless certain constraints are imposed, namely limiting how far activation can spread from the initially activated nodes (distance constraint), and limiting the effect of very highly-connected nodes (fanout constraint) (Crestani, 1997) . ", "after_sen": "In the following three sections we discuss how these constraints were implemented in our model for SA."}
{"citeStart": 0, "citeEnd": 22, "citeStartToken": 0, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "We say a set of parameters S is identifiable (in terms of x) if p θ (x) = p θ (x) for every θ, θ ∈ S where θ = θ . 7 In general, identifiability error is incurred when the set of maximizers of E log p θ (x) is non-identifiable. 8 Label symmetry is perhaps the most familiar example of non-identifiability and is intrinsic to models with hidden labels (HMM and PCFG, but not DMV). We can permute the hidden labels without changing the objective function or even the nature of the solution, so there is no reason to prefer one permutation over another. While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1). Grenager et al. (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q. Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution. This is a case where our notion of desired structure is absent in the likelihood, and a prior over parameters could help break ties. For our three model families, θ is identifiable in terms of (x, y), but not in terms of x alone. 8 We emphasize that non-identifiability is in terms of x, so two parameter settings could still induce the same marginal distribution on x (weak generative capacity) while having different joint distributions on (x, y) (strong generative capacity). Recall that discrepancy depends on the latter.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1). ", "mid_sen": "Grenager et al. (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q. Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution. ", "after_sen": "This is a case where our notion of desired structure is absent in the likelihood, and a prior over parameters could help break ties. "}
{"citeStart": 198, "citeEnd": 216, "citeStartToken": 198, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "In the longer term, it is clear that neither the contents nor form of any existing published dictionary meet all the requirements of a natural language processing system. A substantial component of the research reported above has been devoted to restructuring LDOCE to make it more suitable for automatic analysis. However, even after this process much of the information in LDOCE remains difficult to access, essentially because it is aimed at a human reader, as opposed to a computer system. This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. Leech et al., 1983) . However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This suggests that the automatic construction of dictionaries from published sources intended for other purposes will have a limited life unless lexicography is heavily influenced by the requirements of automated natural language analysis. ", "mid_sen": "In the longer term, therefore, the automatic construction of dictionaries for natural language processing systems may need to be based on techniques for the automatic analysis of large corpora (eg. Leech et al., 1983) . ", "after_sen": "However, in the short term, the approach outlined in this paper will allow us to produce a relatively sophisticated and useful dictionary rapidly."}
{"citeStart": 89, "citeEnd": 105, "citeStartToken": 89, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "We further improved a typed extension of Gerdemann's Earley generator with a number of techniques that reduce the number of edges created during generation. Three optimizations were especially helpful. The first supplies each edge in the chart with two indices, a backward index pointing to the state in the chart that the edge is predicted from, and a forward index poinfing to the states that are predicted from the edge. By matching forward and backward indices, the edges that must be combined for completion can be located faster. This indexing technique, as illustrated below, improves upon the more complex indices in Gerdemann (1991) and is closely related to OLDT-resolution (Tamaki and Sato, 1986) .", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By matching forward and backward indices, the edges that must be combined for completion can be located faster. ", "mid_sen": "This indexing technique, as illustrated below, improves upon the more complex indices in Gerdemann (1991) and is closely related to OLDT-resolution (Tamaki and Sato, 1986) .", "after_sen": "1) active(Xo---~Xl*X2,1~2~ 2) active(X:--~.Y1Y2,~3)) 3) active(X2---*Y1."}
{"citeStart": 16, "citeEnd": 34, "citeStartToken": 16, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been growing interest in using techniques from natural language processing in the area of political science. In (Porter et al., 2005 ) the authors performed a network analysis of members and committees of the US House of Representatives. They found connections between certain committees and political positions that suggest that committee membership is not determined at random. In (Thomas et al., 2006) , the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. In (Wang et al., 2005) , the authors use a generative model to simultaneously discover groups of voters and topics using the voting records and the text from bills of the US Senate and the United Nations. The authors of (Quinn et al., 2006) introduce a multinomial mixture model to perform unsupervised clustering of Congressional speech documents into topically related categories. We rely on the output of this model to cluster the speeches from the Record in order to compare speaker rankings within a topic to related committees.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (Wang et al., 2005) , the authors use a generative model to simultaneously discover groups of voters and topics using the voting records and the text from bills of the US Senate and the United Nations. ", "mid_sen": "The authors of (Quinn et al., 2006) introduce a multinomial mixture model to perform unsupervised clustering of Congressional speech documents into topically related categories. ", "after_sen": "We rely on the output of this model to cluster the speeches from the Record in order to compare speaker rankings within a topic to related committees."}
{"citeStart": 148, "citeEnd": 175, "citeStartToken": 148, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "The differences in the beliefs and knowledge states of the participants can be interpreted in the terms of the collaborative planning principles of Whittaker and Stenton[WS88] . We generalize the principles of INFORMATION QUALITY and PLAN QUALITY, which predict when an interrupt should occur.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "S/he is merely present to execute the actions indicated by the knowledgeable participant.", "mid_sen": "The differences in the beliefs and knowledge states of the participants can be interpreted in the terms of the collaborative planning principles of Whittaker and Stenton[WS88] . ", "after_sen": "We generalize the principles of INFORMATION QUALITY and PLAN QUALITY, which predict when an interrupt should occur."}
{"citeStart": 284, "citeEnd": 296, "citeStartToken": 284, "citeEndToken": 296, "sectionName": "UNKNOWN SECTION NAME", "string": "The core of the disambiguation algorithm is a computation of semantic similarity using the WordNet taxonomy, a topic recently investigated by a number of people (Leacock and Chodorow, 1994; Resnik, 1995; Sussna, 1993) . In this paper, I restrict my attention to WordNet's IS-A taxonomy for nouns, and take an approach in which semantic similarity is evaluated on the basis of the information content shared by the items being compared. The intuition behind the approach is simple: the more similar two words are, the more informative will be the most specific concept that subsumes them both. (That is, their least upper bound in the taxonomy; here a concept corresponds to a WordNet synset.) The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes (Lee et al., 1993; Rada et al., 1989) also captures this, albeit indirectly, when the semantic network is just an IS-A hierarchy: if the minimal path of IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find their least upper bound. However, there are problems with the simple path-length definition of semantic similarity, and experiments using WordNet show that other measures of semantic similarity, such as the one employed here, provide a better match to human similarity judgments than simple path length does (Resnik, 1995) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(That is, their least upper bound in the taxonomy; here a concept corresponds to a WordNet synset.) The traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes (Lee et al., 1993; Rada et al., 1989) also captures this, albeit indirectly, when the semantic network is just an IS-A hierarchy: if the minimal path of IS-A links between two nodes is long, that means it is necessary to go high in the taxonomy, to more abstract concepts, in order to find their least upper bound. ", "mid_sen": "However, there are problems with the simple path-length definition of semantic similarity, and experiments using WordNet show that other measures of semantic similarity, such as the one employed here, provide a better match to human similarity judgments than simple path length does (Resnik, 1995) .", "after_sen": "Given two words wl and w2, their semantic similarity is calculated as"}
{"citeStart": 84, "citeEnd": 106, "citeStartToken": 84, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Accordingly, we try to extract a hierarchical relation of words automatically and statistically. In previous research, ways of extracting from definition sentences in dictionaries (Tsurumaru et al., 1986; Shoutsu et al., 2003) or from a corpus by using patterns such as \"a part of\", \"is-a\", or \"and\" (Berland and Charniak, 1999; Caraballo, 1999) have been proposed. Also, there is a method that uses the dependence relation between words taken from a corpus (Matsumoto et al., 1996) . In contrast, we propose a method based on the inclusion relation of appearance patterns from corpora.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Accordingly, we try to extract a hierarchical relation of words automatically and statistically. ", "mid_sen": "In previous research, ways of extracting from definition sentences in dictionaries (Tsurumaru et al., 1986; Shoutsu et al., 2003) or from a corpus by using patterns such as \"a part of\", \"is-a\", or \"and\" (Berland and Charniak, 1999; Caraballo, 1999) have been proposed. ", "after_sen": "Also, there is a method that uses the dependence relation between words taken from a corpus (Matsumoto et al., 1996) . "}
{"citeStart": 128, "citeEnd": 138, "citeStartToken": 128, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "Algorithm and Heuristics 2.1 General algorithm for least-errors recognition The general algorithm for least-errors recognition (Lyon, 1974) , which is based on Earley's algorithm, assumes that sentences may have insertion,", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Algorithm and Heuristics 2.1 General algorithm for least-errors recognition The general algorithm for least-errors recognition (Lyon, 1974) , which is based on Earley's algorithm, assumes that sentences may have insertion,", "after_sen": "RULE : T ~ a INPUT :t(i) lnutation-error °,\" ............. \"., ';\" ................ \". hypothesis .:' \". • perfec~ match : deletion-error ~ \"''',\" T --* u . O ! hypothe,is i : ' :~T --* a. , 1 } insertion-error .'~T --*. a, 1 i ,,"}
{"citeStart": 142, "citeEnd": 160, "citeStartToken": 142, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Thus the present system is unlike SMT (Och and Ney, 2003) , where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002) , which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus the present system is unlike SMT (Och and Ney, 2003) , where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. ", "mid_sen": "It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002) , which allows a one-to-one correspondence irrespective of the context. ", "after_sen": "Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent."}
{"citeStart": 48, "citeEnd": 61, "citeStartToken": 48, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Several types of research have involved document-level subjectivity classification. Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ( (Turney, 2002; Pang et al., 2002) ). Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. Research in genre classification may include recognition of subjective genres such as editorials (e.g., (Karlgren and Cutting, 1994; Kessler et al., 1997; Wiebe et al., 2001) ). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al., 1999) . Sentence-level subjectivity classification is useful because most documents contain a mix of subjective and objective sentences. For example, newspaper articles are typically thought to be relatively objective, but (Wiebe et al., 2001) reported that, in their corpus, 44% of sentences (in arti-cles that are not editorials or reviews) were subjective.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several types of research have involved document-level subjectivity classification. ", "mid_sen": "Some work identifies inflammatory texts (e.g., (Spertus, 1997)) or classifies reviews as positive or negative ( (Turney, 2002; Pang et al., 2002) ). ", "after_sen": "Tong's system (Tong, 2001) generates sentiment timelines, tracking online discussions and creating graphs of positive and negative opinion messages over time. "}
{"citeStart": 214, "citeEnd": 234, "citeStartToken": 214, "citeEndToken": 234, "sectionName": "UNKNOWN SECTION NAME", "string": "Being able to interpret the rhetorical status of a citation at a glance would add considerable value to citation indexes, as shown in Fig. 1 . Here differences and similarities are shown between the example paper (Pereira et al., 1993) and the papers it cites, as well as the papers that cite it. Contrastive links are shown in grey -links to rival papers and papers the current paper contrasts itself to. Continuative links are shown in black -links to papers that are taken as starting point of the current research, or as part of the methodology of the current paper. The most important textual sentence about each citation could be extracted and displayed. For instance, we see which aspect of Hindle (1990) the Pereira et al. paper criticises, and in which way Pereira et al.'s work was used by Dagan et al. (1994) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We argue that the automatic recognition of citation function is interesting for two reasons: a) it serves to build better citation indexers and b) in the long run, it will help constrain interpretations of the overall argumentative structure of a scientific paper.", "mid_sen": "Being able to interpret the rhetorical status of a citation at a glance would add considerable value to citation indexes, as shown in Fig. 1 . Here differences and similarities are shown between the example paper (Pereira et al., 1993) and the papers it cites, as well as the papers that cite it. ", "after_sen": "Contrastive links are shown in grey -links to rival papers and papers the current paper contrasts itself to. "}
{"citeStart": 41, "citeEnd": 60, "citeStartToken": 41, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "We applied the three classification methods described in Section 4.3 to Task 4 data. For supervised classification we strictly followed the SemEval datasets and rules. For unsupervised classification we did not use any training data. Using the k-means algorithm, we obtained two nearly equal unlabeled clusters containing test samples. For evaluation we assigned a negative/positive label to these two clusters according to the best alignment with true labels. Table shows our results, along with the best Task 4 result not using WordNet labels (Costello, 2007) . For reference, the best results overall (Beamer et al., 2007) are also shown. The table shows precision (P) recall (R), F-score (F), and Accuracy (Acc) (percentage of correctly classified examples).", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table shows our results, along with the best Task 4 result not using WordNet labels (Costello, 2007) . ", "mid_sen": "For reference, the best results overall (Beamer et al., 2007) are also shown. ", "after_sen": "The table shows precision (P) recall (R), F-score (F), and Accuracy (Acc) (percentage of correctly classified examples)."}
{"citeStart": 113, "citeEnd": 140, "citeStartToken": 113, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "The second data set is the Romanian-English parallel corpus from the 2005 ACL shared task (Martin et al., 2005) . This consists of approximately 50,000 aligned sentences and 448 wordaligned sentences, which are split into a 248 sentence trial set and a 200 sentence test set. We used these as our training and test sets, respectively. For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003) . For this task we have used the same test data as the competition entrants, and therefore can directly compare our results. The word alignments in this corpus were only annotated with sure (S) alignments, and therefore the AER is equivalent to the F 1 score. In the shared task it was found that models which were trained on only the first four letters of each word obtained superior results to those using the full words (Martin et al., 2005) . We observed the same result with our model on the trial set and thus have only used the first four letters when training the Dice and Model 1 translation probabilities. Tables 1 and 2 show the results when all feature types are employed on both language pairs. We report the results for both translation directions and when combined using the refined and intersection methods. The Model 4 results are from GIZA++ with the default parameters and the training data lowercased. For Romanian, Model 4 was trained using the first four letters of each word.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We used these as our training and test sets, respectively. ", "mid_sen": "For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003) . ", "after_sen": "For this task we have used the same test data as the competition entrants, and therefore can directly compare our results. "}
{"citeStart": 0, "citeEnd": 11, "citeStartToken": 0, "citeEndToken": 11, "sectionName": "UNKNOWN SECTION NAME", "string": "We share the view of using global inference to improve event extraction with some recent research. Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. We expand the idea to more general event types and use informa-tion retrieval techniques to obtain wider background knowledge from related documents.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. ", "mid_sen": "Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task. ", "after_sen": "In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction. "}
{"citeStart": 130, "citeEnd": 161, "citeStartToken": 130, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "This algorithm differs from some commonly used methods. In feed forward networks trained in supervised mode to perform a classification task different penalty measures can be used to trigger a weight update. Back propagation and some single layer training methods typically minimise a metric based on the least squared error (LSE) between desired and actual activation of the output nodes. The reason why a differentiable error measure of this sort is necessary for multi-layer nets is well documented, for example see (Rumelhart and Mc-Clelland, 1986 ). However, for single layer nets we can choose to update weights directly: the error at an output node can trigger weight updates on the connections that feed it. Solutions with LSE are not necessarily the same as minimising the number of misclassifications, and for certain types of data this second method of direct training may be appropriate. Now, in the natural language domain it is desirable to get information from infrequent as well as common events. Rare events, rather than being noise, can make a useful contribution to a classification task. We need a method that captures information from infrequent events, and adopt a direct measure of misclassification. This may be better suited to data with a \"Zipfian\" distribution (Shannon, 1951) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Back propagation and some single layer training methods typically minimise a metric based on the least squared error (LSE) between desired and actual activation of the output nodes. ", "mid_sen": "The reason why a differentiable error measure of this sort is necessary for multi-layer nets is well documented, for example see (Rumelhart and Mc-Clelland, 1986 ). ", "after_sen": "However, for single layer nets we can choose to update weights directly: the error at an output node can trigger weight updates on the connections that feed it. "}
{"citeStart": 104, "citeEnd": 122, "citeStartToken": 104, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Identifying transliteration pairs is an important component in many linguistic applications such as machine translation and information retrieval, which require identifying out-of-vocabulary words. In our settings, we have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997) . Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998) . Unlike other approaches,our approach is based on minimizing the distance between the feature distribution of a comprehensive reference set and the sampled set.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. ", "mid_sen": "The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998) . ", "after_sen": "Unlike other approaches,our approach is based on minimizing the distance between the feature distribution of a comprehensive reference set and the sampled set."}
{"citeStart": 68, "citeEnd": 87, "citeStartToken": 68, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. This is noticeable for German (Brants et al., 2002) and Portuguese (Afonso et al., 2002) , which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Böhmová et al., 2003) , Dutch (van der Beek et al., 2002) and Slovene (Džeroski et al., 2006) , where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. A very similar pattern is found for Spanish (Civit Torruella and Martí Antonín, 2002) , although this cannot be explained by a high proportion of non-projective structures. One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A second observation is that a high proportion of non-projective structures leads to fragmentation in the parser output, reflected in lower precision for roots. ", "mid_sen": "This is noticeable for German (Brants et al., 2002) and Portuguese (Afonso et al., 2002) , which still have high overall accuracy thanks to very high attachment scores, but much more conspicuous for Czech (Böhmová et al., 2003) , Dutch (van der Beek et al., 2002) and Slovene (Džeroski et al., 2006) , where root precision drops more drastically to about 69%, 71% and 41%, respectively, and root recall is also affected negatively. ", "after_sen": "On the other hand, all three languages behave like high-accuracy languages with respect to attachment score. "}
{"citeStart": 51, "citeEnd": 65, "citeStartToken": 51, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. The Temporal Centering framework (Kameyama et al., 1993) integrates lWe will limit the scope of this paper by restricting the discussion to accomplishments and achievements. aspects of both approaches, but patterns with the first in treating tense as anaphoric.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . ", "mid_sen": "Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . ", "after_sen": "Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. "}
{"citeStart": 111, "citeEnd": 131, "citeStartToken": 111, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Dependencies In order to find the marginal probabilities of x i s in a MRF we can use Belief Propagation (BP) (Yedidia et al., 2003 ). If we assume the y i s are fixed and show φ i (x i , y i ) by φ i (x i ), we can find the joint probability distribution for unknown variables", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": ", where ne(v) is the set of neighbors of v, and cl(v) = {v} ∪ ne(v) is the closed neighborhood of v. Thus, the state of a node is assumed to statistically depend only upon its hidden node and each of its neighbors, and independent of any other node in the graph given its neighbors.", "mid_sen": "Dependencies In order to find the marginal probabilities of x i s in a MRF we can use Belief Propagation (BP) (Yedidia et al., 2003 ). ", "after_sen": "If we assume the y i s are fixed and show φ i (x i , y i ) by φ i (x i ), we can find the joint probability distribution for unknown variables"}
{"citeStart": 108, "citeEnd": 122, "citeStartToken": 108, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Obligations also cannot be reduced to simple expectations, although obligations may act as a source of expectations. Expectations can be used to guide the action interpretation and plan-recognition processes (as proposed by [Carberry, 1990] ), but expectations do not in and of themselves provide a sufficient motivation for an agent to perform the expected action -in many cases there is nothing wrong with doing the unexpected or not performing an expected action. The interpretation of an utterance will often be clear even without coherence with prior expectations. We need to allow for the possibility that an agent has performed an action even when this violates expectations. If an agent actually violates obligations as well then the agent can be held accountable. 1 Specific obligations arise from a variety of sources. In a conversational setting, an accepted offer or a promise will incur an obligation. Also, a command or request by the other party will bring about an obligation to perform the requested action. If the obligation is to say something then we call this a discourse obligation. Our model of obligation is very simple. We use a set of rules that encode discourse conventions. Whenever a new conversation act is determined to have been performed, then any future action that can be inferred from the conventional rules becomes an obligation. We use a simple forward chaining technique to introduce obligations.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Obligations also cannot be reduced to simple expectations, although obligations may act as a source of expectations. ", "mid_sen": "Expectations can be used to guide the action interpretation and plan-recognition processes (as proposed by [Carberry, 1990] ), but expectations do not in and of themselves provide a sufficient motivation for an agent to perform the expected action -in many cases there is nothing wrong with doing the unexpected or not performing an expected action. ", "after_sen": "The interpretation of an utterance will often be clear even without coherence with prior expectations. "}
{"citeStart": 35, "citeEnd": 55, "citeStartToken": 35, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "phrase boundaries. However, experimental results are poor. Early results using neural nets by Cairns et al. (1997) and Christiansen et al (1998) are discouraging. Rytting (2007) seems to have the best result: 61.0% boundary recall with 60.3% precision 2 on 26K words of modern Greek data, average word length 4.4 phones. This algorithm used mutual information plus phrase-final 2-phone sequences. He obtained similar results (Rytting, 2004) using phrase-final 3-phone sequences.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, experimental results are poor. ", "mid_sen": "Early results using neural nets by Cairns et al. (1997) and Christiansen et al (1998) are discouraging. ", "after_sen": "Rytting (2007) seems to have the best result: 61.0% boundary recall with 60.3% precision 2 on 26K words of modern Greek data, average word length 4.4 phones. "}
{"citeStart": 189, "citeEnd": 213, "citeStartToken": 189, "citeEndToken": 213, "sectionName": "UNKNOWN SECTION NAME", "string": "As noted above, the precise granunaticalil;y predictions depend on the kind of parsing lnodel which is encoded in the states. In Milward (1992a), the dynarnics specifies a word-by-word incremental parser lbr a lexicalised version of dependency gramlnar. Fach state is a recursively defined category, similar to ~/cal;egory in Categorial Gramlnar. For exarnple, after parsing You can call me one possible state is a sentence missing a sentence lnoditier 1°. This state is appropriate as the initial state R~r a parse of both dirccll?/, or of after ,Tpm thro,ugh my sccTvtary, resulting in a final state of category sentence. Thus examples Stlch as (19a) are dealt with, since the syntactic context after You can call me dots not distingnish between one or lnore than one subsequent modilier. This lack of distinction as to whethel: one or lnore modilier is expected is actually a necessary prereqnisite ['or performing decidable fl,lly word-by-word incremental interpretation (see Milward and Cooper, 1994 , in these proceedings).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus examples Stlch as (19a) are dealt with, since the syntactic context after You can call me dots not distingnish between one or lnore than one subsequent modilier. ", "mid_sen": "This lack of distinction as to whethel: one or lnore modilier is expected is actually a necessary prereqnisite ['or performing decidable fl,lly word-by-word incremental interpretation (see Milward and Cooper, 1994 , in these proceedings).", "after_sen": "Some of the problelns with eategorial granlmar accounts of coordination do reoccur with a dynamic acconnt based on the parser used ill Milward (1992a Fred won the seholarship This second batch of examples is particularly dillicult to exclude withont malting changes to the eharacterisation of the states. "}
{"citeStart": 193, "citeEnd": 204, "citeStartToken": 193, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "To this we now add the assumption that each auxiliary tree can have only one complement adjunction site projecting from y0, where y0 is the lexical category that projects ymax. This 1is justified in order to prevent projections of y0 from receiving more than one theta role from complement adjuncts, which would violate the underlying theta criterion in Government and Binding Theory (Chomsky, 1981) .We also assume that an auxiliary tree can not have complement adjunetion sites on its spine projecting from lexical heads other than y0 in order to preserve the minimality of elementary trees (Kroch, 1989; Frank, 1992) . Thus there (2) can be no more than one complement adjunction site on the spine of any complement auxiliary tree, and no complement adjunction site on the spine of any athematie auxiliary tree, since the foot node of an athematie tree lies outside of the maximal projection of the head. 4", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This 1is justified in order to prevent projections of y0 from receiving more than one theta role from complement adjuncts, which would violate the underlying theta criterion in Government and Binding Theory (Chomsky, 1981) .", "mid_sen": "We also assume that an auxiliary tree can not have complement adjunetion sites on its spine projecting from lexical heads other than y0 in order to preserve the minimality of elementary trees (Kroch, 1989; Frank, 1992) . ", "after_sen": "Thus there (2) can be no more than one complement adjunction site on the spine of any complement auxiliary tree, and no complement adjunction site on the spine of any athematie auxiliary tree, since the foot node of an athematie tree lies outside of the maximal projection of the head. "}
{"citeStart": 166, "citeEnd": 194, "citeStartToken": 166, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "Generative Capacity HG are more powerful than CI=G and are known to be weakly equivalent to Tree Adjoining Grammar, Combinatory Categorial Grammar, and Head Grammar (Vijay-Shanker and Weir, 1994) . PI_IG are more powerful than I_IG since they can generate the k-copy language for any fixed k (see Example 2). Slightly more generally, PI_IG can generate the language {w~]weR} for any k > 1 and regular language R. We believe that the language involving copies of strings of matching brackets described in Example 5 cannot be generated by PI_IG but, as shown in Exampie 5, it can be generated by P/T(:; and therefore PLPATR. Slightly more generally, PLTG can gener-ate the language {w k Iw~L } for any k > 1 and context-free language L. It appears that the class of languages generated by PI_TG is included in those languages generated by Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987) since the construction involved in a proof of this underlies the recognition algorithm discussed in the next section. As is the case for the tree sets of 16, 1_16 and Tree Adjoining Grammar, the tree sets generated by PI_TG have path sets that. are context-free languages. In other words, the set of all strings labelling root to frontier paths of derivation trees is a context-free language. While the tree sets of lAG and Tree Adjoining Grammars have independent branches, PI_T6 tree sets exhibit dependent branches, where the number of dependent branches in any tree is bounded by the grammar. Note that the number of dependent branches in the tree sets of 16 is not bounded by the grammar (e.g., they generate sets of all full binary trees).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Generative Capacity HG are more powerful than CI=G and are known to be weakly equivalent to Tree Adjoining Grammar, Combinatory Categorial Grammar, and Head Grammar (Vijay-Shanker and Weir, 1994) . PI_IG are more powerful than I_IG since they can generate the k-copy language for any fixed k (see Example 2). ", "after_sen": "Slightly more generally, PI_IG can generate the language {w~]weR} for any k > 1 and regular language R. "}
{"citeStart": 189, "citeEnd": 212, "citeStartToken": 189, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "Over the last 20 years, statistical language models (SLMs) have been used successfully in many tasks in natural language processing, and the data available for modeling has steadily grown (Lapata and Keller, 2005) . Langkilde and Knight (1998) first applied SLMs to statistical natural language generation (SNLG) , showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. Since then, research in SNLG has explored a range of models for both dialogue and text generation.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Over the last 20 years, statistical language models (SLMs) have been used successfully in many tasks in natural language processing, and the data available for modeling has steadily grown (Lapata and Keller, 2005) . ", "after_sen": "Langkilde and Knight (1998) first applied SLMs to statistical natural language generation (SNLG) , showing that high quality paraphrases can be generated from an underspecified representation of meaning, by first applying a very underconstrained, rule-based overgeneration phase, whose outputs are then ranked by an SLM scoring phase. "}
{"citeStart": 121, "citeEnd": 146, "citeStartToken": 121, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "where type1 to typen are exhaustive and disjoint subtypes of type entry, entry need not necessarily be a single type; it can be a logical expression over types formed with the connectors AND and oR. A systemic grammar therefore resembles more a type lattice than a type hierarchy in the HPSG tradition. In systemic grammar, these basic type axioms, the systems, are named; we will use entry(s) to denote the left-hand side of some named system s, and out(s) to denote the set of subtypes {type1, type2, ..., type,}- the output of the system. The following type axioms taken from the large systemic English grammar NXGI~L (Matthiessen, 1983) The meaning of these type axioms is fairly obvious: Nominal groups can be subcategorized in classnames and individual-names on the one hand, they can be subcategorized with respect to their WHcontainment into WH-containing nominal-groups and nominal-groups without WH-element on the other hand. Universal principles and rules are in systemic grammar not factored out. The lexicon contains stem forms and has a detailed word class type hierarchy at its top. Morphology is also organized as a monotonic type hierarchy. Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Morphology is also organized as a monotonic type hierarchy. ", "mid_sen": "Currently used implementations of SFG are the PENMAN system (Penman Project, 1989 ), the KPML system (Bateman, 1997) and WAG-KRL (O'Donnell, 1994) .", "after_sen": "Our subgrammar extraction has been applied and tested in the context of the KPML environment. "}
{"citeStart": 106, "citeEnd": 131, "citeStartToken": 106, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998) . In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The theory has also been validated empirically.", "mid_sen": "Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998) . ", "after_sen": "In both cases the investigators were able to achieve significant improvements over the previous best tagging results. "}
{"citeStart": 3, "citeEnd": 12, "citeStartToken": 3, "citeEndToken": 12, "sectionName": "UNKNOWN SECTION NAME", "string": "Our explicit division of V-space into various support regions has been implicitly considered in other work. Smadja et al. (1996) observe that for two potential mutual translations X and Y, the fact that X occurs with translation Y indicates association; X's occurring with a translation other than Y decreases one's belief in their association; but the absence of both X and Y yields no information. In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema. The Figure 4 : Performance of the skew divergence with respect to the best functions from Figure 2 . definition of commonality is left to the user (several different definitions are proposed for different tasks).", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In essence, Smadja et al. argue that information from the union of supports, rather than the just the intersection, is important. ", "mid_sen": "D. Lin (1997; 1998a) takes an axiomatic approach to determining the characteristics of a good similarity measure. ", "after_sen": "Starting with a formalization (based on certain assumptions) of the intuition that the similarity between two events depends on both their commonality and their differences, he derives a unique similarity function schema. "}
{"citeStart": 30, "citeEnd": 46, "citeStartToken": 30, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "While other systems, such as (Hu and Liu, 2004; Turney, 2002) , have addressed these tasks to some degree, OPINE is the first to report results. We first ran OPINE on 13841 sentences and 538 previously extracted features. OPINE searched for a SO label assignment for 1756 different words in the context of the given features and sentences. We compared OPINE against two baseline methods, PMI++ and Hu++.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section we evaluate OPINE's performance on the following tasks: finding SO labels of words in the context of known features and sentences (SO label extraction) ; distinguishing between opinion and non-opinion phrases in the context of known features and sentences (opinion phrase extraction); finding the correct polarity of extracted opinion phrases in the context of known features and sentences (opinion phrase polarity extraction).", "mid_sen": "While other systems, such as (Hu and Liu, 2004; Turney, 2002) , have addressed these tasks to some degree, OPINE is the first to report results. ", "after_sen": "We first ran OPINE on 13841 sentences and 538 previously extracted features. "}
{"citeStart": 60, "citeEnd": 88, "citeStartToken": 60, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Lazy Learning: Lazy Learners are also known as memory-based, instance-based, exemplarbased, case-based, experience-based, or knearest neighbor algorithms. They store all documents as vectors during the learning phase. In the categorization phase, the new document vector is compared to the stored ones and is categorized to same class as the k-nearest neighbors. The distance is measured by computing e.g. the Euclidean distance between the vectors. By changing the number of neighbors k or the kind of distance measure, the amount of generalization can be controlled. We used IB (Aha, 1992), which is part of the MLC++ library (Kohavi and Sommerfield, 1996) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By changing the number of neighbors k or the kind of distance measure, the amount of generalization can be controlled. ", "mid_sen": "We used IB (Aha, 1992), which is part of the MLC++ library (Kohavi and Sommerfield, 1996) .", "after_sen": ""}
{"citeStart": 180, "citeEnd": 191, "citeStartToken": 180, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001 ). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing. One advantage of this approach is that, although less information is derived from the treebank, it gen-eralizes better to parsers which make different representational assumptions, and it is easier, as Pereira and Schabes did, to map unlabeled bracketings to a format more consistent with the target grammar. Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain.", "mid_sen": "Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001 ). ", "after_sen": "However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. "}
{"citeStart": 65, "citeEnd": 85, "citeStartToken": 65, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "Preprocessing. The corpora have been sentence and word-tokenized using regular expression-based sentence boundary detection and tokenization tools. Sentences have been part-of-speech tagged using the TnT tagger (Brants, 2000) trained on the Penn Treebank (Marcus et al., 1993) . 3 Next, domain terms were identified using the C-Value approach (Frantzi et al., 1998) . C-Value is a domain-independent method of automatic multiword term recognition that rewards high frequency and high-order n-gram candidates, but penalizes those which frequently occur as sub-strings of another candidate. 10,000 top-ranking multi-word token sequences, according to C-Value, were used. Domain terms. The set of domain terms was compiled from the following sub-sets: 1) the 10,000 automatically identified multi-word terms, 2) the set of terms appearing on the margins of the Jurafsky-Martin textbook; the intuition being that these are domain-specific terms which are likely to be defined or explained in the text along which they appear, 3) a set of 5,000 terms obtained by expanding frequent abbreviations and acronyms retrieved from the ACL Anthology corpus using simple pattern matching. The token spans of domain terms have been marked in the corpora as these are used in the course of definition pattern acquisition (Section 4.2).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sentences have been part-of-speech tagged using the TnT tagger (Brants, 2000) trained on the Penn Treebank (Marcus et al., 1993) . ", "mid_sen": "3 Next, domain terms were identified using the C-Value approach (Frantzi et al., 1998) . ", "after_sen": "C-Value is a domain-independent method of automatic multiword term recognition that rewards high frequency and high-order n-gram candidates, but penalizes those which frequently occur as sub-strings of another candidate. "}
{"citeStart": 34, "citeEnd": 56, "citeStartToken": 34, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "Various learning algorithms have been used for relation classification. Common choices include variations of SVM (Girju et al., 2004; Nastase et al., 2006) , decision trees and memory-based learners. Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007) . In this paper we did not focus on a single ML algorithm, letting algorithm selection be automatically based on cross-validation results on the training set, as in (Hendrickx et al., 2007) but using more algorithms and allowing a more flexible parameter choice.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Common choices include variations of SVM (Girju et al., 2004; Nastase et al., 2006) , decision trees and memory-based learners. ", "mid_sen": "Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007) . ", "after_sen": "In this paper we did not focus on a single ML algorithm, letting algorithm selection be automatically based on cross-validation results on the training set, as in (Hendrickx et al., 2007) but using more algorithms and allowing a more flexible parameter choice."}
{"citeStart": 53, "citeEnd": 78, "citeStartToken": 53, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "lation is also added to word-based translation. Results show that selecting and translating phrases improve the three best performers of word-based selection. The best performer, shown in the last raw, suggests using phrase-based selection and restricted word-based selection. The restriction is to include OOV words and selected low frequency words that have at least one dialectal analysis or appear in our dialectal dictionaries. Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation. This is a similar conclusion to our previous work in Salloum and Habash (2011) .", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Comparing the best performer to the OOV selection mode system shows that translating low frequency in-vocabulary dialectal words and phrases to their MSA paraphrases can improve the English translation. ", "mid_sen": "This is a similar conclusion to our previous work in Salloum and Habash (2011) .", "after_sen": ""}
{"citeStart": 110, "citeEnd": 125, "citeStartToken": 110, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. Indeed, such rich semantic links can be used to extend indices or reformulate queries (similar to the work by Voorhees (1994) with WordNet relations).", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Even though this work is carried out for terminographical and lexicographical purposes, it can certainly be of use in other applications, namely information retrieval. ", "mid_sen": "Indeed, such rich semantic links can be used to extend indices or reformulate queries (similar to the work by Voorhees (1994) with WordNet relations).", "after_sen": ""}
{"citeStart": 35, "citeEnd": 49, "citeStartToken": 35, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we proposed a framework for exploiting contextual information in a process of grammar refinement. In this framework, a rough grammar is first learned from a bracketed corpus and then the grammar is refined by the combination of rulebased and corpus-based methods. Unlike stochastic parsing such as (Magerman, 1995) (Collins, 1996) , our approach can parse sentences which fall out the current grammar and suggest the plausible hypothesis rules and the best parses. The grammar is not acquired from scratch like the approaches shown in Table 3 : Parsing Accuracy (Pereira and Schabes, 1992) (Mort and Nagao, 1995) . Through some experiments, our method can achieve effective hypothesis selection and parsing accuracy to some extent. As our further work, we are on the way to consider the correctness of the selected hypothesis of the most plausible parses proposed by the parser. Some improvements are needed to grade up the parsing accuracy. Another work is to use an existing grammar, instead of an automatically learned one, to investigate the effectiveness of contextual information. By providing a user interface, this method will be useful for grammar developers.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this framework, a rough grammar is first learned from a bracketed corpus and then the grammar is refined by the combination of rulebased and corpus-based methods. ", "mid_sen": "Unlike stochastic parsing such as (Magerman, 1995) (Collins, 1996) , our approach can parse sentences which fall out the current grammar and suggest the plausible hypothesis rules and the best parses. ", "after_sen": "The grammar is not acquired from scratch like the approaches shown in Table 3 : Parsing Accuracy (Pereira and Schabes, 1992) (Mort and Nagao, 1995) . "}
{"citeStart": 16, "citeEnd": 28, "citeStartToken": 16, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "Such distributions are referred to as 'dendroid distributions' in tile literature. A dendroid distribution can be represenled by a dependency forest (i.e. a set of dependency trees), whose nodes represent the random variaMes, and whose directed arcs represent the dependencies that exist between these random w/riahles, each labeled with a number of parameters specil}'ing the probabilistic dependency. (A dendroid distribution can also be considered as a re.stricted form of the Bayesian Network (Pearl, 1988) .) It is not difficult t.o see tha.t there are 7 and only 7 such representations for the joint distribution P(X1, X,2, X3) disregarding the actual nmnerical values of t.he probability parameters. Now we turn to the problem of how to select the best dendroid distribution fi:om among all possible ones to approximate a target joint distribution based on input data generated by it. This problem has been inw?stiga.ted in the area of machine learning and related fields. A classical method is Chow & Liu's algorMnn for estimating a nmltidimensional .joint distribution as a dependency tree, ill a way which is both el-~cient and theoretically sound (C.how and I,iu, 1968). More recent.ly (Suzuki, 1993) extended their algorithm so that it estimates the target ,joint. distribution as a dependency Forest. or 'dendroid distrihution', allowing for the possibility of learning one group of random variables to be completely independent of another. Since nlany of the random variables (case slots) in case flame patterns are esseutially independent, this feature is crucial in our context, and we thus employ Suzuki's algorithm for learning our case frame patterns. Figure 1 shows the detail of this Mgorithm, where ki denotes the nun> her of possible values assumed by node (random variable) Xi, N the input data size, and qog' denotes the logarithm to the base 2. It is easy to see that the nulnber of parameters in a dendroid distribution is of the order O(k2ne), where k is the maxinmni of all ki, and n is the. number of random variables, and the time complexity of the algorithm is of the same order, as it is linear in the number of parameters.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A classical method is Chow & Liu's algorMnn for estimating a nmltidimensional .joint distribution as a dependency tree, ill a way which is both el-~cient and theoretically sound (C.how and I,iu, 1968). ", "mid_sen": "More recent.ly (Suzuki, 1993) extended their algorithm so that it estimates the target ,joint. ", "after_sen": "distribution as a dependency Forest. "}
{"citeStart": 50, "citeEnd": 67, "citeStartToken": 50, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "The first directed Applicative CG was proposed by Bar-Hillel (1953) . Functional types included a list of arguments to the left, and a list of arguments to the right. Translating Bar-Hillel's notation into a feature based notation similar to that in HPSG (Pollard and Sag 1994) , we obtain the following category for a ditransitive verb such as put:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is therefore worth giving some brief indications of how it fits in with these developments.", "mid_sen": "The first directed Applicative CG was proposed by Bar-Hillel (1953) . ", "after_sen": "Functional types included a list of arguments to the left, and a list of arguments to the right. "}
{"citeStart": 61, "citeEnd": 78, "citeStartToken": 61, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "We extracted word pairs from three different domain-specific corpora (see Table 2 ). This is motivated by the aim to enable research in information retrieval incorporating SR measures. In particular, the \"Semantic Information Retrieval\" project (SIR Project, 2006) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is motivated by the aim to enable research in information retrieval incorporating SR measures. ", "mid_sen": "In particular, the \"Semantic Information Retrieval\" project (SIR Project, 2006) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems.", "after_sen": "The BERUFEnet (BN) corpus 7 consists of descriptions of 5,800 professions in Germany and therefore contains many terms specific to professional training. "}
{"citeStart": 104, "citeEnd": 120, "citeStartToken": 104, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "Following the work of, for example, Marslen-Wilson (1973), .lust and Carpenter (1980) and Altma.nn al]d Steedrnan (1988) , it has heroine widely accepted that semantic i11terpretation in hnman sentence processing can occur beibre sentence boundaries and even before clausal boundaries. It is less widely accepted that there is a need for incremental interpretation in computational applications.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following the work of, for example, Marslen-Wilson (1973), .lust and Carpenter (1980) and Altma.nn al]d Steedrnan (1988) , it has heroine widely accepted that semantic i11terpretation in hnman sentence processing can occur beibre sentence boundaries and even before clausal boundaries. ", "after_sen": "It is less widely accepted that there is a need for incremental interpretation in computational applications."}
{"citeStart": 159, "citeEnd": 185, "citeStartToken": 159, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992) .", "after_sen": "The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a) , which is lexicalized and context-free equivalent. "}
{"citeStart": 37, "citeEnd": 49, "citeStartToken": 37, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "The essential contribution of our study is that we treat preverbal and postverbal parts of the sentence differently. The sentence-initial position, which in German is the VF, has been shown to be cognitively more prominent than other positions (Gernsbacher & Hargreaves, 1988) . Motivated by the theoretical work by Chafe (1976) and Jacobs (2001) , we view the VF as the place for elements which modify the situation described in the sentence, i.e. for so called frame-setting topics (Jacobs, 2001) . For example, temporal or locational constituents, or anaphoric adverbs are good candidates for the VF. We hypothesize that the reasons which bring a constituent to the VF are different from those which place it, say, to the beginning of the MF, for the order in the MF has been shown to be relatively rigid (Keller, 2000; Kempen & Harbusch, 2004) . Speakers have the freedom of selecting the outgoing point for a sentence. Once they have selected it, the remaining constituents are arranged in the MF, mainly according to their grammatical properties.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The sentence-initial position, which in German is the VF, has been shown to be cognitively more prominent than other positions (Gernsbacher & Hargreaves, 1988) . ", "mid_sen": "Motivated by the theoretical work by Chafe (1976) and Jacobs (2001) , we view the VF as the place for elements which modify the situation described in the sentence, i.e. for so called frame-setting topics (Jacobs, 2001) . ", "after_sen": "For example, temporal or locational constituents, or anaphoric adverbs are good candidates for the VF. "}
{"citeStart": 107, "citeEnd": 125, "citeStartToken": 107, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "For a cogent introduction to genetic search and an explanation of why it works, the reader is referred to (South et al., 1993) . Before presenting the version of the algorithm used in the implementation, ! .~hall informally define the key data types it uses ah,ng with tim standard operations on those types. g,,ne A line;at encoding of a solution. In the present setti,Lg, it is an array of n tones, where each tone is oim of H, SH, TH, L, SL or tL. A gene also contains 16 bit eucodings of the parameters h, l and ,I. These encodings were scaled to be floating i)oint numbers in the range [90, 110] for /,, [70, I0,) ] for t and [0.6, 0.9] for d.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "For a cogent introduction to genetic search and an explanation of why it works, the reader is referred to (South et al., 1993) . ", "after_sen": "Before presenting the version of the algorithm used in the implementation, ! .~hall informally define the key data types it uses ah,ng with tim standard operations on those types. "}
{"citeStart": 126, "citeEnd": 147, "citeStartToken": 126, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977] . In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness. Unlike the other methods that have been mentioned, the approach has the capability to accommodate more context to improve performance.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An arbitrarily large corpus can be accommodated by segmenting it appropriately.", "mid_sen": "The algorithm described here is an instance of a general approach to statistical estimation, represented by the EM algorithm [Dempster et al., 1977] . ", "after_sen": "In contrast to reservations that have been expressed [Gale and Church, 1991a] about using the EM algorithm to provide word correspondences, there have been no indications that prohibitive amounts of memory might be required, or that the approach lacks robustness. "}
{"citeStart": 139, "citeEnd": 151, "citeStartToken": 139, "citeEndToken": 151, "sectionName": "UNKNOWN SECTION NAME", "string": "De Swart (1991) sees Partee's quantification problem as a temporal manifestation of the proportion problem, which arises in cases such as (Kadmon, 1990 ):", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "De Swart (1991) sees Partee's quantification problem as a temporal manifestation of the proportion problem, which arises in cases such as (Kadmon, 1990 ):", "after_sen": "(8) Most women who own a cat are happy."}
{"citeStart": 35, "citeEnd": 47, "citeStartToken": 35, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "A Galton-Watson branching process (Harris, 1963 ) is simply a model of processes that have objects that can produce additional objects of the same kind, i.e. recursive processes, with certain properties. There is an initial set of objects in the 0-th generation which produces with some probability a first generation which in turn with some probability generates a second, and so on. We will denote by vectors Z0, Z1, Z2,... the 0-th, first, second, ... generations. There are two assumptions made about Z0, Z1, Z2,...:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To show that Theorem 4.1 in Section 4 holds for any probabilistic TAG, it is sufficient to show that the derivation process in TAGs is a Galton-Watson branching process.", "mid_sen": "A Galton-Watson branching process (Harris, 1963 ) is simply a model of processes that have objects that can produce additional objects of the same kind, i.e. recursive processes, with certain properties. ", "after_sen": "There is an initial set of objects in the 0-th generation which produces with some probability a first generation which in turn with some probability generates a second, and so on. "}
{"citeStart": 14, "citeEnd": 27, "citeStartToken": 14, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "More recently, the field of dialectometry, as introduced by S~guy (1971, 1973) , has addressed these issues by developing several techniques for summarizing and presenting variation along multiple dimensions. They replace isoglosses with a distance matrix, which compares each site directly with all other sites, ultimately yielding a single figure that measures the linguistic distances between each pair of sites. There is however no firm agreement on just how to compute the distance matrices. Sfiguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. Babitch and Lebrun (1989) did a similar analysis based on the varying pronunciation of/r/. Elsie (1986) grouped the Gaelic dialects on the basis of whether the vocabulary matched. Ebobisse (1989) grouped the Sawabantu languages of Cameroon by whether phonological correspondences in matching vocabulary items were complete, partial, or lacking. There seems to be a certain bias in favour of working with lexical correspondences, which is understandable, since deciding whether two sites use the same word for the same concept is perhaps one of the easiest linguistic judgements to make. The need to figure out such systems as the comparative phonology of various linguistic sites can be very time-consuming and fraught with arbitrary choices.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sfiguy's earliest work (1971) was based on lexical correspondences: sites differed in the extent to which they used different words for the same concept. ", "mid_sen": "S~guy (1973), Philps (1987) , and Durand (1989) use some combination of lexical, phonological, and morphological data. ", "after_sen": "Babitch (1988) described the dialectal distances in Acadian villages by the degree to which their fishing terminology varied. "}
{"citeStart": 82, "citeEnd": 94, "citeStartToken": 82, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "Our research builds on earlier work defining illocutionary points of speech acts (Searle, 1975) , and relating such speech acts to email and workflow tracking (Winograd, 1987 , Flores & Ludlow, 1980 , Weigant et al, 2003 . Winograd suggested that research explicating the speech-act based \"language-action perspective\" on human communication could be used to build more useful tools for coordinating joint activities. The Coordinator (Winograd, 1987) was one such system, in which users augmented email messages with additional annotations indicating intent.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our research builds on earlier work defining illocutionary points of speech acts (Searle, 1975) , and relating such speech acts to email and workflow tracking (Winograd, 1987 , Flores & Ludlow, 1980 , Weigant et al, 2003 . ", "after_sen": "Winograd suggested that research explicating the speech-act based \"language-action perspective\" on human communication could be used to build more useful tools for coordinating joint activities. "}
{"citeStart": 192, "citeEnd": 206, "citeStartToken": 192, "citeEndToken": 206, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 76, "citeEnd": 95, "citeStartToken": 76, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993) ), which are very sensitive to noise. This paper describes a new program, called word_align, that starts with an initial \"rough\" alignment (e.g., the output of char_align or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. The alignment algorithm consists of two steps: (1) estimate translation probabilities, and (2) use these probabilities to search for most probable alignment path. The two steps are described in the following section.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To deal with these robustness issues, Church (1993) developed a character-based alignment method called char_align. ", "mid_sen": "The method was intended as a replacement for sentence-based methods (e.g., (Brown et al., 1991a; Gale and Church, 1991b; Kay and Rosenschein, 1993) ), which are very sensitive to noise. ", "after_sen": "This paper describes a new program, called word_align, that starts with an initial \"rough\" alignment (e.g., the output of char_align or a sentence-based alignment method), and produces improved alignments by exploiting constraints at the word-level. "}
{"citeStart": 79, "citeEnd": 90, "citeStartToken": 79, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The problem of word segmentation presents an important challenge in language acquisition. The child learner must segment a continuous stream of sounds into words without knowing what the individual words are until the stream has been segmented. Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child may be using to guide her acquisition. In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner. We seek to determine how these limitations in the learner's input and memory affect the learner's performance and to demonstrate that the presented learner is robust even under non-ideal conditions. * Portions of this work were adapted from an earlier manuscript, Word Segmentation: Quick But Not Dirty.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Computational models present an opportunity to test the potentially innate constraints, structures, and algorithms that a child may be using to guide her acquisition. ", "mid_sen": "In this work we develop a segmentation model from the constraints suggested by Yang (2004) and evaluate it in idealized conditions and conditions that better approximate the environment of a child learner. ", "after_sen": "We seek to determine how these limitations in the learner's input and memory affect the learner's performance and to demonstrate that the presented learner is robust even under non-ideal conditions. "}
{"citeStart": 83, "citeEnd": 90, "citeStartToken": 83, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Abstract generation is, like Machine Translation, one of the ultimate goal of Natural Language Processing. IIowever, since conventional word-frequencybased abstract generation systems(e.g. [Kuhn 58]) are lacking in inter-sentential or discourse-structural analysis, they are liable to generate incoherent abstracts. On the other hand, conventional knowledge or script-based abstract generation systems(e.g.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Abstract generation is, like Machine Translation, one of the ultimate goal of Natural Language Processing. ", "mid_sen": "IIowever, since conventional word-frequencybased abstract generation systems(e.g. [Kuhn 58]) are lacking in inter-sentential or discourse-structural analysis, they are liable to generate incoherent abstracts. ", "after_sen": "On the other hand, conventional knowledge or script-based abstract generation systems(e.g."}
{"citeStart": 95, "citeEnd": 108, "citeStartToken": 95, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "Given the basic signature provided by FrameNet (and any extension of it that will prove necessary to account for the data), the grammar must then specify a compositional semantics which will derive identical representations for the types of paraphrases captured by our typology. In essence, this implies assigning the same semantic representations to synonyms, converses and alternations. Concretely, this involves two different subtasks : first, a modeling of the synonymic relation between syntactically divergent constructs (e.g., between a predicative noun, a support verb construction and a verb) and second, the identification of the synonymic sets (which are the words and multi word expressions that stand in a parallel, shuffling or definitional paraphrastic relation?). Modeling intercategorial synonymic links. A first investigation of Anne Abeillé's TAG for French suggests that modeling the synonymic relations across syntactic constructs is reasonably straightforward. For instance, as Figures 3, 4 and 5 show, the FTAG trees assigned on syntactic grounds by Anne Abeillé FTAG to predicative nouns, support verb constructions and transitive verbs can be equiped with a flat semantics in such a way as to assign the three sentences in 1 a unique semantic representation namely the one given above. Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. To address this problem, we are currently working on developing a metagrammar in the sense of (Candito, 1999) . This metagrammar allows us to factorise both syntactic and semantic information. Syntactic information is factorised in the usual way. For instance, there will be a class NOVN1 which groups together all the initial trees representing the possible syntactic configurations in which a transitive verb with two nominal arguments can occur. But additionnally there will be semantic classes such as, \"binary predicate of semantic type X\" which will be associated with the relevant syntactic classes for instance, NOVN1 (the class of transitive verbs with nominal arguments), BINARY NPRED (the class of binary predicative nouns), NOVSUPNN1 , the class of support verb constructions taking two nominal arguments. By further associating semantic units (e.g., \"cost\") with the appropriate semantic classes (e.g., \"binary predicate of semantic type X\"), we can in this way capture both intra and intercategorial paraphrasing links in a general way.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Generally, the problem is not so much to state the correspondances between synonymic but syntactically different constructs as to do this in a general way while not overgeneralising. ", "mid_sen": "To address this problem, we are currently working on developing a metagrammar in the sense of (Candito, 1999) . ", "after_sen": "This metagrammar allows us to factorise both syntactic and semantic information. "}
{"citeStart": 25, "citeEnd": 73, "citeStartToken": 25, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Fixed part is a skip-gram with 1 to 4 length. Each group of words is a sequence. A skipgram, according to Guthrie, Allison, Liu, Guthrie, and Wilks (2006) , is a generalization of an n-gram, where text leave not considered spaces, while a skip-gram does consider spaces between word sequences.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each group of words is a sequence. ", "mid_sen": "A skipgram, according to Guthrie, Allison, Liu, Guthrie, and Wilks (2006) , is a generalization of an n-gram, where text leave not considered spaces, while a skip-gram does consider spaces between word sequences.", "after_sen": ""}
{"citeStart": 394, "citeEnd": 405, "citeStartToken": 394, "citeEndToken": 405, "sectionName": "UNKNOWN SECTION NAME", "string": "We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al. (2007) for technical details. The models we examine are derived from the collocational model of by varying three parameters, resulting in 6 models: two baselines that do not take advantage of stress cues and either do or do not use phonotactics, as described in Section 3.2; and four stress models that differ with respect to the use of phonotactics, and as to whether they embody the Unique Stress Constraint introduced by Yang (2004) . We describe these models in section 3.3.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We give an intuitive description of the mathematical background of Adaptor Grammars in 3.1, referring the reader to Johnson et al. (2007) for technical details. ", "mid_sen": "The models we examine are derived from the collocational model of by varying three parameters, resulting in 6 models: two baselines that do not take advantage of stress cues and either do or do not use phonotactics, as described in Section 3.2; and four stress models that differ with respect to the use of phonotactics, and as to whether they embody the Unique Stress Constraint introduced by Yang (2004) . ", "after_sen": "We describe these models in section 3.3."}
{"citeStart": 114, "citeEnd": 123, "citeStartToken": 114, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003) , have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. ", "mid_sen": "Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003) , have been applied to machine translation (MT), and have provided significant improvements. ", "after_sen": "In this paper, we introduce two novel machine learning algorithms specialized for the MT task."}
{"citeStart": 312, "citeEnd": 337, "citeStartToken": 312, "citeEndToken": 337, "sectionName": "UNKNOWN SECTION NAME", "string": "The most obvious way to make a knowledge source more accurate is to increase the amount of structure or context that it takes account of. For example, a bigram model may be replaced by a trigram one, and the fact that dependencies exist among the likelihoods of occurrence of grammar rules at different locations in a parse tree can be modeled by associating probabilities with states in a parsing table rather than simply with the rules themselves (Briscoe and Carroll, 1993) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The most obvious way to make a knowledge source more accurate is to increase the amount of structure or context that it takes account of. ", "mid_sen": "For example, a bigram model may be replaced by a trigram one, and the fact that dependencies exist among the likelihoods of occurrence of grammar rules at different locations in a parse tree can be modeled by associating probabilities with states in a parsing table rather than simply with the rules themselves (Briscoe and Carroll, 1993) .", "after_sen": "However, such remedies have their drawbacks. "}
{"citeStart": 141, "citeEnd": 162, "citeStartToken": 141, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "Classifier design and feature selection. The overall approach to classifying spurts uses a decision tree classifier (Breiman et al., 1984) to combine the word based and prosodic cues. In order to facilitate learning of cues for the less frequent classes, the data was upsampled (duplicated) so that there were the same number of training points per class. The decision tree size was determined using error-based cost-complexity pruning with 4-fold cross validation. To reduce our initial candidate feature set, we used an iterative feature selection algorithm that involved running multiple decision trees (Shriberg et al., 2000) . The algorithm combines elements of brute-force search (in a leave-one-out paradigm) with previously de-termined heuristics for narrowing the search space. We used entropy reduction of the tree after cross-validation as a criterion for selecting the best subtree.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The decision tree size was determined using error-based cost-complexity pruning with 4-fold cross validation. ", "mid_sen": "To reduce our initial candidate feature set, we used an iterative feature selection algorithm that involved running multiple decision trees (Shriberg et al., 2000) . ", "after_sen": "The algorithm combines elements of brute-force search (in a leave-one-out paradigm) with previously de-termined heuristics for narrowing the search space. "}
{"citeStart": 74, "citeEnd": 84, "citeStartToken": 74, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983 ). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . ", "mid_sen": "While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. ", "after_sen": "This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) ."}
{"citeStart": 19, "citeEnd": 34, "citeStartToken": 19, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Example 2 (d93-83.3 utt73) that's all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers. Discourse markers (Schiffrin, 1987; Hirschberg and Litman, 1993; Byron and Heeman, 1997) are used to relate new speech to the current discourse state. Lexical items that can function as discourse markers, such as \"well\" and \"okay,\" are ambiguous as to whether they are being used as discourse markers or not. The complication is that discourse markers tend to be used to introduce a new utterance, or can be an utterance all to themselves (such as the acknowledgment \"okay\" or \"alright\"), or can be used as part of the editing term of a speech repair, or to begin the alteration. Hence, the problem of identifying discourse markers also needs to be addressed with the segmentation and speech repair problems.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Example 2 (d93-83.3 utt73) that's all you need % you only need one boxcar Intonational phrases and speech repairs also interact with the identification of discourse markers. ", "mid_sen": "Discourse markers (Schiffrin, 1987; Hirschberg and Litman, 1993; Byron and Heeman, 1997) are used to relate new speech to the current discourse state. ", "after_sen": "Lexical items that can function as discourse markers, such as \"well\" and \"okay,\" are ambiguous as to whether they are being used as discourse markers or not. "}
{"citeStart": 84, "citeEnd": 102, "citeStartToken": 84, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "Distance-weighted averaging induces classes of similar words from word co-occurrences without making reference to a taxonomy. A key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies. Several measures of distributional similarity have been proposed in the literature (Dagan et al., 1999; Lee, 1999) . We used two measures, the Jensen-Shannon divergence and the confusion probability. Those two measures have been previously shown to give promising performance for the task of estimating the frequencies of unseen verb-argument pairs (Dagan et al., 1999; Grishman and Sterling, 1994; Lapata, 2000; Lee, 1999) . In the following we describe these two similarity measures and show how they can be used to recreate the frequencies for unseen adjective-noun pairs.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A key feature of this type of smoothing is the function which measures distributional similarity from cooccurrence frequencies. ", "mid_sen": "Several measures of distributional similarity have been proposed in the literature (Dagan et al., 1999; Lee, 1999) . ", "after_sen": "We used two measures, the Jensen-Shannon divergence and the confusion probability. "}
{"citeStart": 459, "citeEnd": 482, "citeStartToken": 459, "citeEndToken": 482, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition, a χ 2 dependency analysis showed that the NM presence interacts significantly with both AsrMis (p<0.02) and SemMis (p<0.001), with fewer than expected AsrMis and SemMis in the 3 Due to random assignment to conditions, before the first problem the F and S populations are similar (e.g. no difference in pretest); thus any differences in metrics can be attributed to the NM presence/absence. However, in the second problem, the two populations are not similar anymore as they have received different forms of instruction; thus any difference has to be attributed to the NM presence/absence in this problem as well as to the NM absence/presence in the previous problem. 4 Due to logging issues, 2 S users are excluded from this analysis (13 F and 13 S users remaining). We run the subjective metric analysis from Section 5.1 on this subset and the results are similar. NM condition. The fact that in the second problem the differences are much smaller (e.g. 2% for AsrMis) and that the NM-AsrMis and NM-SemMis interactions are not significant anymore, suggests that our observations can not be attributed to a difference in population with respect to system's ability to recognize their speech. We hypothesize that these differences are due to the NM text influencing users' lexical choice. Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993) , essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001) , generation/interpretation of anaphoric expressions (Allen et al., 2001) , performance modeling (Rotaru and Litman, 2006) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We hypothesize that these differences are due to the NM text influencing users' lexical choice. ", "mid_sen": "Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993) , essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001) , generation/interpretation of anaphoric expressions (Allen et al., 2001) , performance modeling (Rotaru and Litman, 2006) ).", "after_sen": "In this paper, we study the utility of the discourse structure on the user side of a dialogue system. "}
{"citeStart": 20, "citeEnd": 34, "citeStartToken": 20, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a) . A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be im-portant to provide more accurate tree structures and labels (Nivre and Scholz, 2004) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a) . ", "mid_sen": "A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. ", "after_sen": "On the other hand, joint learning models can benefit from edge-label information that has proven to be im-portant to provide more accurate tree structures and labels (Nivre and Scholz, 2004) ."}
{"citeStart": 216, "citeEnd": 225, "citeStartToken": 216, "citeEndToken": 225, "sectionName": "UNKNOWN SECTION NAME", "string": "First, a feature extractor is run over each parsed corpus file to extract occurrences of the sub-parse and their features. The feature extractor reads in a template for each phrase in the form of dependency relations over lemmas. It checks each sentence parse against the template (taking care that the same word form is indeed the same occurrence of the word in the sentence). When a match is found, the other grammatical relations 5 for each word in the sub-parse are output as features. When the sub-parse is only a word, the process is simplified to finding grammatical relations containing that word. The raw feature file is then converted into a cooccurrence vector by counting the occurrences of each feature type. Table 1 shows the number of feature types and tokens extracted for each phrase. This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus (Lin, 1998; Weeds and Weir, 2003) .", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 1 shows the number of feature types and tokens extracted for each phrase. ", "mid_sen": "This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus (Lin, 1998; Weeds and Weir, 2003) .", "after_sen": "We then computed the distributional similarity between each co-occurrence vector using the α-skew divergence measure (Lee, 1999) . "}
{"citeStart": 129, "citeEnd": 157, "citeStartToken": 129, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "Very interestingly, the distributional methods based on intra-sentence relations (Lin, 1998; Padó and Lapata, 2003) outperformed Garera and Yarowsky's (2006) association measure when used for ranking, which may due to sparse data problems or simply too much noise for the latter. For the association measures, the fact that they are relation-free also means that they can profit from added semantic filtering.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While none of the information sources can match the precision of the hypernymy information encoded in GermaNet, or that of using a combination of high-precision patterns with the World Wide Web as a very large corpus, it is possible to achieve a considerable improvement in terms of recall without sacrificing too much precision by combining these methods.", "mid_sen": "Very interestingly, the distributional methods based on intra-sentence relations (Lin, 1998; Padó and Lapata, 2003) outperformed Garera and Yarowsky's (2006) association measure when used for ranking, which may due to sparse data problems or simply too much noise for the latter. ", "after_sen": "For the association measures, the fact that they are relation-free also means that they can profit from added semantic filtering."}
{"citeStart": 3, "citeEnd": 22, "citeStartToken": 3, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "Our opinion annotation scheme (Wiebe et al., 2005) is centered on the notion of private state, a general term that covers opinions, beliefs, thoughts, sentiments, emotions, intentions and evaluations. As Quirk et al. (1985) define it, a private state is a state that is not open to objective observation or verification. We can further view private states in terms of their functional components -as states of experiencers holding attitudes, optionally toward targets. For example, for the private state expressed in the sentence John hates Mary, the experiencer is John, the attitude is hate, and the target is Mary.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our opinion annotation scheme (Wiebe et al., 2005) is centered on the notion of private state, a general term that covers opinions, beliefs, thoughts, sentiments, emotions, intentions and evaluations. ", "mid_sen": "As Quirk et al. (1985) define it, a private state is a state that is not open to objective observation or verification. ", "after_sen": "We can further view private states in terms of their functional components -as states of experiencers holding attitudes, optionally toward targets. "}
{"citeStart": 129, "citeEnd": 142, "citeStartToken": 129, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "If parsing is taken to be the first step in taming the natural language understanding task, then broad coverage NLP remains a jungle inhabited by wild beasts. For instance, parsing noun compounds appears to require detailed world knowledge that is unavailable outside a limited domain (Sparek Jones, 1983 ). Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens el al, 1987; Vanderwende, 1993 and Sproat, 1994) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language. ", "mid_sen": "It has already made five appearances in this paragraph and at least one diachronic study shows a veritable population explosion (Leonard, 1984) . ", "after_sen": "While substantial work on noun compounds exists in both linguistics (e.g. Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984) , techniques suitable for broad coverage parsing remain unavailable. "}
{"citeStart": 39, "citeEnd": 52, "citeStartToken": 39, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "of incremental description refinement (Mellish, 1988) than to exploit the usual metaphors of constraint-based grammar. In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings, because all that we need in order to train our stochastic procedures is a corpus of signs which are known to be valid descriptions of strings.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We take a slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework", "mid_sen": "of incremental description refinement (Mellish, 1988) than to exploit the usual metaphors of constraint-based grammar. ", "after_sen": "In fact we can afford to remain entirely agnostic about the means by which the HPSG grammar associates signs with linguistic strings, because all that we need in order to train our stochastic procedures is a corpus of signs which are known to be valid descriptions of strings."}
{"citeStart": 0, "citeEnd": 11, "citeStartToken": 0, "citeEndToken": 11, "sectionName": "UNKNOWN SECTION NAME", "string": "Given marker chunks such as those in (16), we are able to extract automatically a further bilingual dictionary, the word-level lexicon. We take advantage of the assumption that where a chunk contains just one non-marker word in both source and target, these words are translations of each other. Where a marker-headed pair contains just two words, as in (16), for instance, we can extract the word-level translations in 23 That is, using the marker hypothesis method of segmentation, smaller aligned segments can be extracted from the phrasal lexicon without recourse to any detailed parsing techniques or complex co-ocurrence measures. Juola (1994 Juola ( , 1997 assumes that words ending in -ed are verbs. However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. Instead, we take advantage of the fact that the initial phrasal chunks correspond to rule right-hand sides. That is, for a rule in the Penn Treebank VP −→ VBG, NP, PP, we are certain (if the annotators have done their job correctly) that the first word in each of the strings corresponding to this right-hand side is a VBG, that is, a present participle. Given this information, in such cases we tag such words with the <LEX> tag. Taking expanding the board to 14 members −→ augmente le conseilà 14 membres as an example, we extract the chunks in 24 We ignore here the trivially true lexical chunk \"<QUANT> 14 : 14.\" In a final processing stage, we generalize over the marker lexicon following a process found in Block (2000) . In Block's approach, word alignments are assigned probabilities by means of a statistical word alignment tool. In a subsequent stage, chunk pairs are extracted, which are then generalized to produce a set of translation templates for each source, target segment.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Where a marker-headed pair contains just two words, as in (16), for instance, we can extract the word-level translations in 23 That is, using the marker hypothesis method of segmentation, smaller aligned segments can be extracted from the phrasal lexicon without recourse to any detailed parsing techniques or complex co-ocurrence measures. ", "mid_sen": "Juola (1994 Juola ( , 1997 assumes that words ending in -ed are verbs. ", "after_sen": "However, given that verbs are not a closed class, in our approach we do not mark chunks beginning with a verb with any marker category. "}
{"citeStart": 60, "citeEnd": 81, "citeStartToken": 60, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "We utilized the factored translation framework in Moses, to enrich the baseline system with an additional target sequence model. For English we used part-of-speech tags obtained using Tree-Tagger (Schmid, 1994) , enriched with more finegrained tags for the number of determiners, in order to target more agreement issues, since nouns already have number in the tagset. For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008) , that contains morphological information such as case, number, and gender for nouns and tense for verbs. We used the extra factor in an additional sequence model on the target side, which can improve word order Table 2 : Results for morphological processing, German→English and agreement between words. For German the factor was also used for compound merging. Prior to training and translation, compound processing was performed, using an empirical method (Koehn and Knight, 2003; Stymne, 2008 ) that splits words if they can be split into parts that occur in a monolingual corpus, choosing the splitting option with the highest arithmetic mean of its part frequencies in the corpus. We split nouns, adjectives and verbs, into parts that are content words or particles. We imposed a length limit on parts of 3 characters for translation from German and of 6 characters for translation from English, and we had a stop list of parts that often led to errors, such as arische (Aryan) in konsularische (consular). We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were given a special part-of-speech tag that matches the head word.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For English we used part-of-speech tags obtained using Tree-Tagger (Schmid, 1994) , enriched with more finegrained tags for the number of determiners, in order to target more agreement issues, since nouns already have number in the tagset. ", "mid_sen": "For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008) , that contains morphological information such as case, number, and gender for nouns and tense for verbs. ", "after_sen": "We used the extra factor in an additional sequence model on the target side, which can improve word order Table 2 : Results for morphological processing, German→English and agreement between words. "}
{"citeStart": 109, "citeEnd": 111, "citeStartToken": 109, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "The string positions known from chart parsing are also inadequate for generation, as pointed out by Shieber [13] in whose generator all items go from position 0 to 0 so that any item can be combined with any item.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The string positions known from chart parsing are also inadequate for generation, as pointed out by Shieber [13] in whose generator all items go from position 0 to 0 so that any item can be combined with any item.", "after_sen": "Itowever, the string positions are useful as an indexing of the items so that it can be easily detected whether their combination can contribute to a proof of the goal. "}
{"citeStart": 145, "citeEnd": 169, "citeStartToken": 145, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "Two other tests for comparing how two techniques perform by comparing how well they perform on each test sample are the sign and Wilcoxon tests (Harnett, 1982, Sec. 15.5) . Unlike the matched-pair t test, neither of these two tests assume that the sum of the differences has a normal (Gaussian) distribution. The two tests are so-called nonparametric tests, which do not make assumptions about how the results are distributed (Harnett, 1982, Ch. 15) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This means for example, that even if the values from techniques 1 and 2 vary on different test samples, s d will now be 0 if on each test sample, technique 1 produces a value that is the same constant amount more than the value from technique 2.", "mid_sen": "Two other tests for comparing how two techniques perform by comparing how well they perform on each test sample are the sign and Wilcoxon tests (Harnett, 1982, Sec. 15.5) . ", "after_sen": "Unlike the matched-pair t test, neither of these two tests assume that the sum of the differences has a normal (Gaussian) distribution. "}
{"citeStart": 79, "citeEnd": 102, "citeStartToken": 79, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method . This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011) . The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it. The detailed results are given in Table 2 . Table 2 : Results for joint context and sentiment detection.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. ", "after_sen": "The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method . "}
{"citeStart": 121, "citeEnd": 140, "citeStartToken": 121, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Two advantages of this approach to implement SThis is only a syntactic variant of attaching constraints solely to types (Carpenter et al. 91) and does not permit general conditional structnres ms used in Pollard & Sag 87.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These antecedent checks trigger either the application of the princiltle, its abandomnent, or its delay (by annotating the variables wlfich are not sufficiently constrained to decide on the antecedent with the delayed goals).", "mid_sen": "Two advantages of this approach to implement SThis is only a syntactic variant of attaching constraints solely to types (Carpenter et al. 91) and does not permit general conditional structnres ms used in Pollard & Sag 87.", "after_sen": "6pred/n is the usual notation for a n-ary Prolog predicate."}
{"citeStart": 70, "citeEnd": 91, "citeStartToken": 70, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Although the sentences that belong to the same cluster are similar, they are not necessarily equally important. We rank the sentences within each cluster by computing their LexRank (Erkan and Radev, 2004) . Sentences with higher rank are more important.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although the sentences that belong to the same cluster are similar, they are not necessarily equally important. ", "mid_sen": "We rank the sentences within each cluster by computing their LexRank (Erkan and Radev, 2004) . ", "after_sen": "Sentences with higher rank are more important."}
{"citeStart": 131, "citeEnd": 145, "citeStartToken": 131, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997) , Charniak (1997) and Ratnaparkhi (1997) . These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993) . We used these three parsers to explore parser combination techniques.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998) .", "mid_sen": "The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997) , Charniak (1997) and Ratnaparkhi (1997) . ", "after_sen": "These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993) . "}
{"citeStart": 141, "citeEnd": 153, "citeStartToken": 141, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "Even moderately long documents typically address sew~ral topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of linear text segmentation is to discover the topic boundaries. ", "mid_sen": "The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "after_sen": "This paper focuses on domain independent methods for segmenting written text. "}
{"citeStart": 19, "citeEnd": 41, "citeStartToken": 19, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "In our prior work (Xiong and Litman, 2011) , we examined whether techniques used for predicting the helpfulness of product reviews (Kim et al., 2006) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review. While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. In fact, we observe that peer-review helpfulness seems to differ not only between students and experts (example 1), but also between types of experts (example 2).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such an intelligent component could enable peer-review systems to 1) control the quality of peer reviews that are sent back to authors, so authors can focus on the helpful ones; and 2) provide feedback to reviewers with respect to their reviewing performance, so students can learn to write better reviews.", "mid_sen": "In our prior work (Xiong and Litman, 2011) , we examined whether techniques used for predicting the helpfulness of product reviews (Kim et al., 2006) could be tailored to our peer-review domain, where the definition of helpfulness is largely influenced by the educational context of peer review. ", "after_sen": "While previously we used the average of two expert-provided ratings as our gold standard of peer-review helpfulness 1 , there are other types of helpfulness rating (e.g. author perceived helpfulness) that could be the gold standard, and that could potentially impact the features used to build the helpfulness model. "}
{"citeStart": 97, "citeEnd": 113, "citeStartToken": 97, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "As another example, certain techniques to deal with ill-formed input can be characterized as finite state transducers (Lang, 1989) ; the composition of an input string with such a finite state transducer results in a FSA that can then be input for syntactic parsing. Such an approach allows for the treatment of missing, extraneous, interchanged or misused words (Teitelbaum, 1973; Saito and Tomita, 1988; Nederhof and Bertsch, 1994) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As another example, certain techniques to deal with ill-formed input can be characterized as finite state transducers (Lang, 1989) ; the composition of an input string with such a finite state transducer results in a FSA that can then be input for syntactic parsing. ", "mid_sen": "Such an approach allows for the treatment of missing, extraneous, interchanged or misused words (Teitelbaum, 1973; Saito and Tomita, 1988; Nederhof and Bertsch, 1994) .", "after_sen": "Such techniques might be of use both in the case of written and spoken language input. "}
{"citeStart": 122, "citeEnd": 134, "citeStartToken": 122, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992) , (Church, 1988) , (Hajji, Hladk~, 1997) ) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a \"direct\" approach to modeling, for which we have chosen an exponential probabilistic model. Such model (when predicting an event 5 y E Y in a context x) has the general form", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992) , (Church, 1988) , (Hajji, Hladk~, 1997) ) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a \"direct\" approach to modeling, for which we have chosen an exponential probabilistic model. ", "after_sen": "Such model (when predicting an event 5 y E Y in a context x) has the general form"}
{"citeStart": 74, "citeEnd": 87, "citeStartToken": 74, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990) , although Hindle did not apply it to information retrieval.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although Stairmand (1997) and Richardson (1995) proposed the use of WordNet in information retrieval, they did not use WordNet in the query expansion framework.", "mid_sen": "Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990) , although Hindle did not apply it to information retrieval.", "after_sen": "Hindle only extracted subject-verb and object-verb relations, while we also extract adjective-noun and noun-noun relations, in the manner of Grefenstette (1994) , who applied his syntactically-based thesaurus to information retrieval with mixed results. "}
{"citeStart": 155, "citeEnd": 173, "citeStartToken": 155, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. ", "mid_sen": "The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "after_sen": "Reichenbach's well -known account of the interpretation of the different tense forms uses the temporal relations between three temporal indices: the utterance time, event time and reference time. "}
{"citeStart": 216, "citeEnd": 245, "citeStartToken": 216, "citeEndToken": 245, "sectionName": "UNKNOWN SECTION NAME", "string": "Taken for themselves these variants of I_ are of little use in linguistic descriptions. But in Moortgat's mixed system all the different resource management modes of the different systems are left intact in the combination and can be exploited in different parts of the grammar. The relative pronoun which would, for instance, receive category (np\\np)/ (np --o s) with --o being implication in LP, 1 i.e., it requires 1The Lambek calculus with permutation I_P is also called the \"nondirectional Lambek calculus\" (Benthem 88) . In it the leftward and rightward implication The present paper studies the computational complexity of a variant of the Lambek Calculus that lies between / and tP, the Semidirectional Lambek Calculus SDk. 3 Since tP derivability is known to be NPcomplete, it is interesting to study restrictions on the use of the I_P operator -o. A restriction that leaves its proposed linguistic applications intact is to admit a type B -o A only as the argument type in functional applications, but never as the functor. Stated prove-theoretically for Gentzen-style systems, this amounts to disallowing the left rule for -o. Surprisingly, the resulting system SD[. can be stated without the need for structural rules, i.e., as a monolithic system with just one structural connective, because the ability of the abstracted-over formula to permute can be directly encoded in the right rule for --o. 4", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But in Moortgat's mixed system all the different resource management modes of the different systems are left intact in the combination and can be exploited in different parts of the grammar. ", "mid_sen": "The relative pronoun which would, for instance, receive category (np\\np)/ (np --o s) with --o being implication in LP, 1 i.e., it requires 1The Lambek calculus with permutation I_P is also called the \"nondirectional Lambek calculus\" (Benthem 88) . ", "after_sen": "In it the leftward and rightward implication The present paper studies the computational complexity of a variant of the Lambek Calculus that lies between / and tP, the Semidirectional Lambek Calculus SDk. "}
{"citeStart": 47, "citeEnd": 69, "citeStartToken": 47, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "Interactive spoken dialog provides many new challenges for natural language understanding systems. One of the most critical challenges is simply determining the speaker's intended utterances: both segmenting the speaker's turn into utterances and determining the intended words in each utterance. Since there is no well-agreed to definition of what an utterance is, we instead focus on intonational phrases (Silverman et al., 1992) , which end with an acoustically signaled boundary lone. Even assuming perfect word recognition, the problem of determining the intended words is complicated due to the occurrence of speech repairs, which occur where the speaker goes back and changes (or repeats) something she just said. The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. The following example, from the Trains corpus (Heeman and Allen, 1995) , gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The words that are replaced or repeated are no longer part of the intended utterance, and so need to be identified. ", "mid_sen": "The following example, from the Trains corpus (Heeman and Allen, 1995) , gives an example of a speech repair with the words that the speaker intends to be replaced marked by reparandum, the words that are the intended replacement marked as alteration, and the cue phrases and filled pauses that tend to occur in between marked as the editing term.", "after_sen": "Example 1 (d92a-5.2 utt34) we'll pick up ~. uh the tanker of oranges reparandu \"q'ml ~"}
{"citeStart": 105, "citeEnd": 124, "citeStartToken": 105, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "Graph-based approaches for joint inference in sentiment analysis have been explored previously by many researchers. The biggest difference between this work and theirs is in what the links represent linguistically. Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . Our model is not based on sentence cohesion or structural adjacency. The relations due to the opinion frames are based on relationships between targets and discourse-level functions of opinions being mutually reinforcing or non-reinforcing. Adjacent instances need not be related via opinion frames, while long distant relations can be present if opinion targets are same or alternatives. Also, previous efforts in graph-based joint inference in opinion analysis has been textbased, while our work is over multi-party conversations. McDonald et al. (2007) propose a joint model for sentiment classification based on relations defined by granularity (sentence and document). Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (topic) model. Their aspects would be related as same and their high contrast relations would correspond to (a subset of) the non-reinforcing frames.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some of these are not related to discourse at all (e.g., lexical similarities (Takamura et al., 2007) , morphosyntactic similarities (Popescu and Etzioni, 2005) and word based measures like TF-IDF (Goldberg and Zhu, 2006) ). ", "mid_sen": "Some of these work on sentence cohesion (Pang and Lee, 2004) or agreement/disagreement between speakers (Thomas et al., 2006; Bansal et al., 2008) . ", "after_sen": "Our model is not based on sentence cohesion or structural adjacency. "}
{"citeStart": 358, "citeEnd": 376, "citeStartToken": 358, "citeEndToken": 376, "sectionName": "UNKNOWN SECTION NAME", "string": "The first trial application of the automatic subgrammar extraction tool has been carried out for an information system with an output component that generates integrated text and graphics. This information system has been developed for the domain of art history and is capable of providing short biography articles for around l0 000 artists. The underlying knowledge base, comprising half a million semantic concepts, includes automatically extracted information from 14 000 encyclopedia articles from McMillans planned publication \"Dictionary of Art\" combined with several additional information sources such as the Getty \"Art and Architecture Thesaurus\"; the application is described in detail in (Kamps et al., 1996) . As input the user clicks on an artist name. The system then performs content selection, text planning, text and diagram generation and page layout automatically. Possible output languages are English and German.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This information system has been developed for the domain of art history and is capable of providing short biography articles for around l0 000 artists. ", "mid_sen": "The underlying knowledge base, comprising half a million semantic concepts, includes automatically extracted information from 14 000 encyclopedia articles from McMillans planned publication \"Dictionary of Art\" combined with several additional information sources such as the Getty \"Art and Architecture Thesaurus\"; the application is described in detail in (Kamps et al., 1996) . ", "after_sen": "As input the user clicks on an artist name. "}
{"citeStart": 71, "citeEnd": 93, "citeStartToken": 71, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "We also added stress information to the Brent-Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999) , following the procedure just outlined. This corpus is a de-facto standard for evaluat- .00 .00 .00 .00 .00 .00 Table 2 : Relative frequencies for stress patterns for the corpora used in our study. X * stands for 0 or more, X + for one or more repetitions of X, and S for a stressed and W for an unstressed syllable. Note the stark asymmetry between type and token frequencies for unstressed words. Up to two-decimal places, patterns other than the ones given have relative frequency 0.00 (frequencies might not sum to 1 as an artefact of rounding to 2 decimal places).", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As function words account for roughly 50% of the tokens but only roughly 5% of the types in our corpora, this means that the type and token distribution of stress patterns differs dramatically in all our corpora, as can be seen from Table 2 .", "mid_sen": "We also added stress information to the Brent-Bernstein-Ratner corpus (Bernstein-Ratner, 1987; Brent, 1999) , following the procedure just outlined. ", "after_sen": "This corpus is a de-facto standard for evaluat- .00 .00 .00 .00 .00 .00 Table 2 : Relative frequencies for stress patterns for the corpora used in our study. "}
{"citeStart": 10, "citeEnd": 24, "citeStartToken": 10, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "The Penn Treebank (PTB) (Marcus et al., 1994) has been used for a rather simple approach to deriving large grammars automatically: one where the grammar rules are simply 'read off' the parse trees in the corpus, with each local subtree providing the left and right hand sides of a rule. Charniak (Charniak, 1996) reports precision and recall figures of around 80% for a parser employing such a grammar. In this paper we show that the huge size of such a treebank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Penn Treebank (PTB) (Marcus et al., 1994) has been used for a rather simple approach to deriving large grammars automatically: one where the grammar rules are simply 'read off' the parse trees in the corpus, with each local subtree providing the left and right hand sides of a rule. ", "mid_sen": "Charniak (Charniak, 1996) reports precision and recall figures of around 80% for a parser employing such a grammar. ", "after_sen": "In this paper we show that the huge size of such a treebank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved."}
{"citeStart": 153, "citeEnd": 172, "citeStartToken": 153, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "The processing described in this paper has been implemented in NTI\" Communication Science Laboratories' experimental machine translation system ALTJ/E ([kehara et al. 1991) . Along with new processing for the generation of articles, wifich is not discussed in detail in this paper, it improved the percent-age of noun phrases with correctly generated detemfiners and number flom 65% to 73%.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is followed by some examples of sentences translated by the proposed method and a discussion of the results.", "mid_sen": "The processing described in this paper has been implemented in NTI\" Communication Science Laboratories' experimental machine translation system ALTJ/E ([kehara et al. 1991) . ", "after_sen": "Along with new processing for the generation of articles, wifich is not discussed in detail in this paper, it improved the percent-age of noun phrases with correctly generated detemfiners and number flom 65% to 73%."}
{"citeStart": 81, "citeEnd": 105, "citeStartToken": 81, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000) they use their citing area identification algorithm to improve citation type classification and automatic survey generation.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section we review the research lines that are relevant to our work and show how our work is different.", "mid_sen": "One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. ", "after_sen": "Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. "}
{"citeStart": 140, "citeEnd": 165, "citeStartToken": 140, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "The algorithm is constructed in such a way that lowering is only attempted in cases where simple attachment fails. This means that arguments (which are incorporated via simple attachment) will be attached preferentially to adjuncts (which are incorporated via lowering). This captures the general preference for argument over adjunct attachment, which is accounted for by the principle of Minimal attachment in Frazier and Rayner (1982) , and by the principle of simplicity in Gottell (in press).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This means that arguments (which are incorporated via simple attachment) will be attached preferentially to adjuncts (which are incorporated via lowering). ", "mid_sen": "This captures the general preference for argument over adjunct attachment, which is accounted for by the principle of Minimal attachment in Frazier and Rayner (1982) , and by the principle of simplicity in Gottell (in press).", "after_sen": ""}
{"citeStart": 60, "citeEnd": 72, "citeStartToken": 60, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Nevertheless, a good estimation of this probability seems quite problematic because of the lack of tagged training material. In absence of a better estimator we use a rather poor one as the uniform distribution, c) = (cln) = e el Is ,,ses(,,)l also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). First, it doesn't take dependency relationships introduced by hyperonymy into account. Second, nouns categorized in lower levels in the taxonomy provide less weight to each class than higher nouns.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In absence of a better estimator we use a rather poor one as the uniform distribution, c) = (cln) = e el Is ,,ses(,,)l also uses a local normalization technique but he normalizes by the total number of classes in the hierarchy. ", "mid_sen": "This scheme seems to present two problematic features (see (Ribas, 1994b) for more details). ", "after_sen": "First, it doesn't take dependency relationships introduced by hyperonymy into account. "}
{"citeStart": 171, "citeEnd": 194, "citeStartToken": 171, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper presents Genpex, a system for generating narrative exercises expressing probability problems. Genpex was created in the context of an inter-national project on item generation for testing student competencies in solving probability problems. Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009) . The goal of our item generation project is to develop a model to support optimal problem and test construction. A large collection of narrative exercises is needed to test the developed models in field trials. All of these narrative exercises should be different, but the properties that define the difficulty of the exercise should be known. Genpex was designed to enable easy creation of new exercises meeting these requirements. Figure 1 shows a narrative probability exercise generated by Genpex. The text of the exercise is in German, because the target group of our project are German high school students. The texts produced by Genpex are based on a set of example narrative exercises that were created earlier within the project (Zeuch, In preparation).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Genpex was created in the context of an inter-national project on item generation for testing student competencies in solving probability problems. ", "mid_sen": "Automatic item generation is an effective way of constructing many items with controlled difficulties, based on a set of predefined task parameters (Enright et al., 2002; Deane and Sheehan, 2003; Arendasy et al., 2006; Holling et al., 2009) . ", "after_sen": "The goal of our item generation project is to develop a model to support optimal problem and test construction. "}
{"citeStart": 43, "citeEnd": 55, "citeStartToken": 43, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains. Surface ordering is determined by the sequence of constituents associated with the root node. The order domain of a mother node is the sequence union of the order domains of the daughter nodes, which means that the relative order of elements in an order domain is retained, but material from several domains may be interleaved, resulting in discontinuities. Whether an order domain allows interleaving with other domains is a parameter of the constituent. This approach is very similar to ours in that order domains separate word order from the syntactic tree, but there is one important difference: Word order domains in HPSG do not completely free the hierarchical structure from ordering considerations, because discontinuity is specified per phrase, not per modifier. For example, two projections are required for an NP, the lower one for the continuous material (determiner, adjective, noun, genitival and prepositional attributes) and the higher one for the possibly discontinuous relative clause. This dependence of hierarchical structure on ordering is absent from our proposal.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is not necessary in our approach, which drastically reduces the search space for parsing.", "mid_sen": "This property is shared by the proposal of Reape (1993) to associate HPSG signs with sequences of constituents, also called word order domains. ", "after_sen": "Surface ordering is determined by the sequence of constituents associated with the root node. "}
{"citeStart": 25, "citeEnd": 44, "citeStartToken": 25, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "The Penn Treebank (PTB) (Marcus et al., 1994) has been used for a rather simple approach to deriving large grammars automatically: one where the grammar rules are simply 'read off' the parse trees in the corpus, with each local subtree providing the left and right hand sides of a rule. Charniak (Charniak, 1996) reports precision and recall figures of around 80% for a parser employing such a grammar. In this paper we show that the huge size of such a treebank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Penn Treebank (PTB) (Marcus et al., 1994) has been used for a rather simple approach to deriving large grammars automatically: one where the grammar rules are simply 'read off' the parse trees in the corpus, with each local subtree providing the left and right hand sides of a rule. ", "after_sen": "Charniak (Charniak, 1996) reports precision and recall figures of around 80% for a parser employing such a grammar. "}
{"citeStart": 30, "citeEnd": 50, "citeStartToken": 30, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "We use all the verbs that their lemmatized form appears in at least three sentences that belong to the same category in the training set. Auxiliary verbs are excluded. In our annotated dataset, for example, the verb propose appeared in 67 sentences from the Methodology category, while the verbs outperform and achieve appeared in 33 Result sentences. (2007) . These sentences belong to the same category (i.e Method). Both Sentences (6) and (7) convey the same information about Goldwater and Griffiths (2007) contribution. Sentence (8), however, describes a different aspect of the paper methodology. Clustering divides the sentences of each category into groups of similar sentences. Following Qazvinian and Radev (2008) , we build a cosine similarity graph out of the sentences of each category. This is an undirected graph in which nodes are sen-tences and edges represent similarity relations. Each edge is weighted by the value of the cosine similarity (using TF-IDF vectors) between the two sentences the edge connects. Once we have the similarity network constructed, we partition it into clusters using a community finding technique. We use the Clauset algorithm (Clauset et al., 2004) , a hierarchical agglomerative community finding algorithm that runs in linear time.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Once we have the similarity network constructed, we partition it into clusters using a community finding technique. ", "mid_sen": "We use the Clauset algorithm (Clauset et al., 2004) , a hierarchical agglomerative community finding algorithm that runs in linear time.", "after_sen": ""}
{"citeStart": 154, "citeEnd": 168, "citeStartToken": 154, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "Why should the set of rules continue to grow in this way? Putting aside the possibility that natural languages do not have finite rule sets, we can think of two possible answers. First, it may be that the full \"underlying grammar\" is much larger than the rule set that has so far been produced, requiring a much larger tree-banked corpus than is now available for its extraction. If this were true, then the outlook would be bleak for achieving near-complete grammars from treebanks, given the resource demands of producing hand-parsed text. However, the radical incompleteness of grammar that this alternative implies seems incompatible with the promising parsing results that Charniak reports (Charniak, 1996) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If this were true, then the outlook would be bleak for achieving near-complete grammars from treebanks, given the resource demands of producing hand-parsed text. ", "mid_sen": "However, the radical incompleteness of grammar that this alternative implies seems incompatible with the promising parsing results that Charniak reports (Charniak, 1996) .", "after_sen": "A second answer is suggested by the presence in the extracted grammar of rules such as (1). "}
{"citeStart": 141, "citeEnd": 160, "citeStartToken": 141, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "The results of the Xerox experiment appear very encouraging. Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993) , it is a good thing to reduce the human involvement as much as possible. However, some careful examination of the experiment is needed. In the first place, Cutting et al. do not compare the success rate in their work with that achieved from a hand-tagged training text with no re-estimation. Secondly, it is unclear how much the initial biasing contributes the success rate. If significant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such intervention is needed on each new text domain. The kind of biasing Cutting et al. describe reflects linguistic insights combined with an understanding of the predictions a tagger could reasonably be expected to make and the ones it could not.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results of the Xerox experiment appear very encouraging. ", "mid_sen": "Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993) , it is a good thing to reduce the human involvement as much as possible. ", "after_sen": "However, some careful examination of the experiment is needed. "}
{"citeStart": 48, "citeEnd": 74, "citeStartToken": 48, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases (Hirschberg and Litman 1993) can serve as explicit indicators of discourse structure. Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh's occur in BACKCHANNELS, and 88.4% of the trigrams \"<start> do you\" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type. 5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs. All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [Katz 1987 ] with Witten-Bell discounting [Witten and Bell 1991] ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "DA classification using words is based on the observation that different DAs use distinctive word strings. ", "mid_sen": "It is known that certain cue words and phrases (Hirschberg and Litman 1993) can serve as explicit indicators of discourse structure. ", "after_sen": "Similarly, we find distinctive correlations between certain phrases and DA types. "}
{"citeStart": 102, "citeEnd": 113, "citeStartToken": 102, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Our dataset is skewed as there are many more objective sentences than subjective ones. In such scenarios, average micro-F scores tend to be slightly higher as they are a weighted measure. To avoid this bias, we also report the macro-F scores. Furthermore, to ensure there is enough data for training each class, we use 10-fold cross-validation (Lewis, 1991) in all our experiments.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To avoid this bias, we also report the macro-F scores. ", "mid_sen": "Furthermore, to ensure there is enough data for training each class, we use 10-fold cross-validation (Lewis, 1991) in all our experiments.", "after_sen": "We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework. "}
{"citeStart": 139, "citeEnd": 153, "citeStartToken": 139, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000) . While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993) ). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000) . ", "after_sen": "While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. "}
{"citeStart": 121, "citeEnd": 135, "citeStartToken": 121, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010) , G-index (Egghe, 2006) , and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An objective and fair evaluation of the impact of published research requires both quantitative and qualitative assessment. ", "mid_sen": "Existing bibliometric measures such as H-Index (Hirsch, 2005; Hirsch, 2010) , G-index (Egghe, 2006) , and Impact Factor (Garfield, 1994) focus on the quantitative aspect of this evaluation which dose not always correlate with the qualitative aspect.", "after_sen": "For example, the number of papers published by a researcher only tells how productive she or he is. "}
{"citeStart": 34, "citeEnd": 46, "citeStartToken": 34, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "The bilingual dual-coding theory (Paivio, 1986 ) partially answers the above questions. It depicts the verbal representations for two different languages as two separate but connected logogen systems, characterizes the translation process as the activation along the connections between the logogen systems, and attributes the acquisition of the representation to some unspecified statistical processes.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As for machine translation, we should take advantage of our understandings of (1) how the languages are represented in human mind; (2) how the representation is mapped from one language to another; (3) how the representation and mapping are acquired by human.", "mid_sen": "The bilingual dual-coding theory (Paivio, 1986 ) partially answers the above questions. ", "after_sen": "It depicts the verbal representations for two different languages as two separate but connected logogen systems, characterizes the translation process as the activation along the connections between the logogen systems, and attributes the acquisition of the representation to some unspecified statistical processes."}
{"citeStart": 24, "citeEnd": 45, "citeStartToken": 24, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "The problem we solve encompasses the generation of referring expressions (REs) as a special case. Unlike some approaches (Dale and Reiter, 1995; Heeman and Hirst, 1995) , we do not have to distinguish between generating NPs and expressions of other syntactic categories. We develop a new perspective on the lifecycle of a distractor, which allows us to generate more succinct REs by taking the rest of the utterance into account. More generally, we do not split the process of sentence generation into two separate steps of sentence planning and realization, as most other systems do, but solve the joint problem in a single integrated step. This can potentially allow us to generate higher-quality sentences. We share these advantages with systems such as SPUD (Stone et al., 2003) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The problem we solve encompasses the generation of referring expressions (REs) as a special case. ", "mid_sen": "Unlike some approaches (Dale and Reiter, 1995; Heeman and Hirst, 1995) , we do not have to distinguish between generating NPs and expressions of other syntactic categories. ", "after_sen": "We develop a new perspective on the lifecycle of a distractor, which allows us to generate more succinct REs by taking the rest of the utterance into account. "}
{"citeStart": 4, "citeEnd": 24, "citeStartToken": 4, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "In (Shen and Joshi, 2004) , we have introduced a new perceptron-like ordinal regression algorithm for parse reranking. In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks. In addition, the uneven margin technique has been used for the purpose of adapting ordinal regression to reranking tasks. In this paper, we apply this algorithm to MT reranking, and we also introduce a new perceptron-like reranking algorithm for MT.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The reranking problem is reduced to a classification problem by using pairwise samples.", "mid_sen": "In (Shen and Joshi, 2004) , we have introduced a new perceptron-like ordinal regression algorithm for parse reranking. ", "after_sen": "In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks. "}
{"citeStart": 292, "citeEnd": 319, "citeStartToken": 292, "citeEndToken": 319, "sectionName": "UNKNOWN SECTION NAME", "string": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This strategy is certainly the right one to start out with, since anaphora is always the more typical direction of reference in English prose (Halliday and Hasan 1976, p. 329) .", "mid_sen": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. ", "after_sen": "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . "}
{"citeStart": 189, "citeEnd": 212, "citeStartToken": 189, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "(fly (argl (person))(arg2 (airplane))) 3 (fly (argl (company))(arg2 (airplane))) 2 (fly (argl (person))(arg2 (company))) (fly (argl (person))(to (place/)) 1 (fly (argl (person) )(from (place/)(to (place))) 1 (fly (argl (company))(from (place))(to (place))) that the slot of from and that of to should be considered dependent and the attachment site of one of the prepositional phrases (case slots) can be determined by that of the other with high accuracy and confidence. There has not been a general method proposed to date, however, that learns dependencies between case slots. Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth 1991) , or at most two case slots are dependent (Collins and Brooks 1995) . In this article, we propose an efficient and general method of learning dependencies between case frame slots.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There has not been a general method proposed to date, however, that learns dependencies between case slots. ", "mid_sen": "Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth 1991) , or at most two case slots are dependent (Collins and Brooks 1995) . ", "after_sen": "In this article, we propose an efficient and general method of learning dependencies between case frame slots."}
{"citeStart": 290, "citeEnd": 301, "citeStartToken": 290, "citeEndToken": 301, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we will address the problem of sense discrimination as defined above. That is, we will not be concerned with the sense-labeling component of word sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. ", "mid_sen": "Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996) , thesauri (Yarowsky 1992; Walker and Amsler 1986) , bilingual corpora (Brown et al. 1991; Church and Gale 1991) , or hand-labeled training sets (Hearst 1991; Niwa and Nitta 1994; Bruce and Wiebe 1994) , providing information for sense definitions can be a considerable burden.", "after_sen": "What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses."}
{"citeStart": 151, "citeEnd": 174, "citeStartToken": 151, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al. (2005) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages.", "mid_sen": "Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al. (2005) .", "after_sen": ""}
{"citeStart": 159, "citeEnd": 183, "citeStartToken": 159, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991) , 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs 3 (Finkelstein et al., 2001) . We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. We measure the correlation with Pearson's Correlation Coefficient. A preliminary experiment set out to determine whether there is any advantage to indexing the words in a phrase separately, for example, whether the phrase \"change of direction\" should be indexed only as a whole, or as all of \"change\", \"of\", \"direction\" and \"change of direction\". The outcome of this experiment appears in Table 4 . There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget's. In the remaining experiments, we have each word in a phrase indexed.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do not count links between words in the same semicolon group, so in effect these methods find distances between semicolon groups, that is to say, these two functions will give the same results. ", "mid_sen": "The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs (Miller and Charles, 1991) , 65 pairs (Rubenstein and Goodenough, 1965) and 353 pairs 3 (Finkelstein et al., 2001) . ", "after_sen": "We assume that all terms are nouns, so that we can have a fair comparison of the two Thesauri with WordNet. "}
{"citeStart": 66, "citeEnd": 80, "citeStartToken": 66, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "We hypolhcsizcd lhal a warrant Inllsi be ,'qAIJENT for hoth agents (as shown by example 2). In l)esign-Wodd, salience is modeled by AWM model, adapted lronl [Landauer, 1975 I. While the AWM model is extremely sin> pie, [,andauer showed thai it could be pm'ameterized lo 1il many empirical resells on human memory and learning [Baddeley, 19861 . AWM consists of a three dimensional slsace in which propositions acquired Dora perceiving the world are stored in chronological sequence according 1o tile localion o1' a moving memory pointef. The sequence of memory loci iised lof slotage constitutes a fI.Itldolll walk lhrough memory wilh each loci a shorl dislance lfonl tile previous one. If items are encountefed illtllliple times, they me stored nmlliple limes [Itinlzmann and Block, 1971] .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We hypolhcsizcd lhal a warrant Inllsi be ,'qAIJENT for hoth agents (as shown by example 2). ", "mid_sen": "In l)esign-Wodd, salience is modeled by AWM model, adapted lronl [Landauer, 1975 I. ", "after_sen": "While the AWM model is extremely sin> pie, [,andauer showed thai it could be pm'ameterized lo 1il many empirical resells on human memory and learning [Baddeley, 19861 . AWM consists of a three dimensional slsace in which propositions acquired Dora perceiving the world are stored in chronological sequence according 1o tile localion o1' a moving memory pointef. "}
{"citeStart": 53, "citeEnd": 64, "citeStartToken": 53, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "A remarkable point about SDL's ability to cover this language is that neither L nor LP can generate it. Hence, this example substantiates the claim made in (Moortgat 94 ) that the inferential capacity of mixed Lambek systems may be greater than the sum of its component parts. Moreover, the attentive reader will have noticed that our encoding also extends to languages having more groups of n symbols, i.e., to languages of the form n n n al a2 ... a k • Finally, we note in passing that for this grammar the rules (/R) and (\\R) are irrelevant, i.e. that it is at the same time an SOL-grammar.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A remarkable point about SDL's ability to cover this language is that neither L nor LP can generate it. ", "mid_sen": "Hence, this example substantiates the claim made in (Moortgat 94 ) that the inferential capacity of mixed Lambek systems may be greater than the sum of its component parts. ", "after_sen": "Moreover, the attentive reader will have noticed that our encoding also extends to languages having more groups of n symbols, i.e., to languages of the form n n n al a2 ... a k • Finally, we note in passing that for this grammar the rules (/R) and (\\R) are irrelevant, i.e. that it is at the same time an SOL-grammar."}
{"citeStart": 107, "citeEnd": 121, "citeStartToken": 107, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006) . Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007) . All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics. 4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. Figure 8 (a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a) . At the lowest level of search error, the relative speed-up from cube growing and cube pruning compared with full-integration is by a factor of 9.8 and 4.1, respectively. Figure 8(b) is a similar comparison in terms of BLEU scores and shows an even bigger advantage of cube growing and cube pruning over the baseline. ", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although different from the SCFG-based systems in Section 2, its derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. ", "mid_sen": "The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006) . ", "after_sen": "Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007) . "}
{"citeStart": 16, "citeEnd": 37, "citeStartToken": 16, "citeEndToken": 37, "sectionName": "UNKNOWN SECTION NAME", "string": "The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words. The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004) . Both identify product features from reviews, but OPINE significantly improves on both. (Hu and Liu, 2004) doesn't assess candidate features, so its precision is lower than OPINE's. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE's focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . ", "mid_sen": "Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. ", "after_sen": "The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. "}
{"citeStart": 59, "citeEnd": 70, "citeStartToken": 59, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , some Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and some subjectivity clues listed in (Wiebe, 1990) . We represented each set as a three-valued feature based on the presence of 0, 1, or ≥ 2 members of the set. We will refer to these as the manual features.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also added manually-developed features found by other researchers. ", "mid_sen": "We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , some Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and some subjectivity clues listed in (Wiebe, 1990) . ", "after_sen": "We represented each set as a three-valued feature based on the presence of 0, 1, or ≥ 2 members of the set. "}
{"citeStart": 323, "citeEnd": 341, "citeStartToken": 323, "citeEndToken": 341, "sectionName": "UNKNOWN SECTION NAME", "string": "If attacking the evidence for _bel does not appear to be sufficient to convince the user of -~_bel, the algorithm checks whether directly attacking _bel will accomplish this goal. If providing evidence directly against _bel is predicted to be successful, then the focus of modification is _bcl itself (step 4.3). If directly attacking _bel is also predicted to fail, the algorithm considers the effect of attacking both ..bel and its unaccepted proposed evidence by combining the previous two prediction processes (step 4.4). If the combined evidence is still predicted to fail, the system does not have sufficient evidence to change the user's view of_bel; thus, the focus of modification for .bel is nil (step 4.5). 7 Notice that steps 2 and 4 of the algorithm invoke a function, Predict, that makes use of the belief revision mechanism (Galliers, 1992) discussed in Section 4.1 to predict the user's acceptance or unacceptance of..bel based on the system's knowledge of the user's beliefs and the evidence that could be presented to him (Logan et al., 1994) . The result of Select-Focus-Modification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user's belief about the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Mod/fy actions.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the combined evidence is still predicted to fail, the system does not have sufficient evidence to change the user's view of_bel; thus, the focus of modification for .bel is nil (step 4.5). ", "mid_sen": "7 Notice that steps 2 and 4 of the algorithm invoke a function, Predict, that makes use of the belief revision mechanism (Galliers, 1992) discussed in Section 4.1 to predict the user's acceptance or unacceptance of..bel based on the system's knowledge of the user's beliefs and the evidence that could be presented to him (Logan et al., 1994) . ", "after_sen": "The result of Select-Focus-Modification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user's belief about the unaccepted top-level belief. "}
{"citeStart": 239, "citeEnd": 251, "citeStartToken": 239, "citeEndToken": 251, "sectionName": "UNKNOWN SECTION NAME", "string": "The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data. This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features that have been shown to be optimal for word sense disambiguation.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997) . ", "mid_sen": "A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , ). ", "after_sen": "In many ensemble approaches the member classitiers are learned with different algorithms that are trained with the same data. "}
{"citeStart": 148, "citeEnd": 172, "citeStartToken": 148, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "The role of objective information. Results from the previous subsection suggest that our method for extracting objective materials and removing them from the reviews is not effective in terms of improving performance. To determine the reason, we examine the n-grams and the dependency relations that are extracted from the nonreviews. We find that only in a few cases do these extracted objective materials appear in our set of 25000 features obtained in Section 4.2. This explains why our method is not as effective as we originally thought. We conjecture that more sophisticated methods would be needed in order to take advantage of objective information in polarity classification (e.g., Koppel and Schler (2005) ).", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This explains why our method is not as effective as we originally thought. ", "mid_sen": "We conjecture that more sophisticated methods would be needed in order to take advantage of objective information in polarity classification (e.g., Koppel and Schler (2005) ).", "after_sen": ""}
{"citeStart": 112, "citeEnd": 127, "citeStartToken": 112, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with \"context-free\" derivations, such as (multi-component) tree-adjoining grammars (Joshi and Schabes, 1997; Weir, 1988) , IO macro grammars (Fisher, 1968) , and (parallel) multiple contextfree grammars (Seki et al., 1991) . For instance, the TAG in Figure 3 is represented by the Datalog program in Figure 4 . Moreover, the method of reduc- Figure 3 : A TAG with one initial tree (left) and one auxiliary tree (right) tion extends to the problem of tactical generation (surface realization) for these grammar formalisms coupled with Montague semantics (under a certain restriction). Our method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars (de Groote, 2001) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Moreover, the method of reduc- Figure 3 : A TAG with one initial tree (left) and one auxiliary tree (right) tion extends to the problem of tactical generation (surface realization) for these grammar formalisms coupled with Montague semantics (under a certain restriction). ", "mid_sen": "Our method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars (de Groote, 2001) .", "after_sen": "S(p 1 , p ) :− A(p 1 , p 3 , p 2 , p 2 ). A(p 1 , p 8 , p 4 , p 5 ) :− A(p 2 , p 7 , p 3 , p 6 ), a(p 1 , p 2 ), b(p 3 , p 4 ), c(p 5 , p 6 ), d(p 7 , p 8 ). A(p 1 , p 2 , p 1 , p )."}
{"citeStart": 146, "citeEnd": 158, "citeStartToken": 146, "citeEndToken": 158, "sectionName": "UNKNOWN SECTION NAME", "string": "It could be argued that it is meaningless to compare the phonetics of different words, as in the case of eallaigh vs. crodh mentioned above. Therefore the feature string comparison was also computed only for pairs of citations that used the same word, so that [AL:i] All of these distance matrices were compared with the isogloss matrix, to see which of them gives results closest to that base method. I used two different methods of comparison, Pearson's p computed between all corresponding cells in the which is a derivative of Kendall's r that Dietz (1983) empirically found particularly accurate as a test statistic for comparing distance matrices, a Table 1 shows that the two measures give parallel results. More importantly, it shows that the approaches based on string comparisons of the phonetic transcriptions correlate more highly with the isogloss approach than do the word or etymon identity measures. Furthermore, comparing whole phone letters works better than the more sophi.sticated technique of comparing features, and restricting comparison to pairs based on the same words does not make the latter any better. Of course, I do not expect that this technique using flat 1-unit costs will prove superior to all methods that are more sensitive to phonetic details. Feature comparison may work better if features were weighted differentially, or if the numeric values they assume were assigned less arbitrarily, or if the Manhattan-style distance computation were replaced by some formula that did not assume that the features are independent of each other. An ideal comparison would be based on data telling how likely it is for the one phone to turn into the other in the course of normal language change. In the method described here, Is] is adjudged closer to [g] than to [h]. But [s] often changes into [hi in the world's languages, and so the pair should have a small distance; whereas the change of [s] to [g] has never occurred to my knowledge, and so should have a very large distance. The unfortunate fact that such ideal data are lacking is compensated for by the fact that the inexpensive phone-string comparison employed in this study performs quite well.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Therefore the feature string comparison was also computed only for pairs of citations that used the same word, so that [AL:i] All of these distance matrices were compared with the isogloss matrix, to see which of them gives results closest to that base method. ", "mid_sen": "I used two different methods of comparison, Pearson's p computed between all corresponding cells in the which is a derivative of Kendall's r that Dietz (1983) empirically found particularly accurate as a test statistic for comparing distance matrices, a Table 1 shows that the two measures give parallel results. ", "after_sen": "More importantly, it shows that the approaches based on string comparisons of the phonetic transcriptions correlate more highly with the isogloss approach than do the word or etymon identity measures. "}
{"citeStart": 90, "citeEnd": 110, "citeStartToken": 90, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The process study most similar to ours is that of Koehn (2009a) , who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000) , Caitra (Koehn, 2009b) , Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b) , and CasmaCat (Alabau et al., 2013) . However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, he used undergraduate, non-professional subjects, and did not consider re-tuning. ", "mid_sen": "Our experimental design with professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. ", "after_sen": "Many research translation UIs have been proposed including TransType (Langlais et al., 2000) , Caitra (Koehn, 2009b) , Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b) , and CasmaCat (Alabau et al., 2013) . "}
{"citeStart": 219, "citeEnd": 251, "citeStartToken": 219, "citeEndToken": 251, "sectionName": "UNKNOWN SECTION NAME", "string": "A total of 1,741 citations were annotated. Although this annotation was performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable interannotator agreement (Teufel et al., 2006 ). An example annotation for Smadja (1993) is given in Figure  2 , where the first column shows the line number and the second one shows the class label. To compare our work with Athar (2011), we also applied a three-class annotation scheme. In this method of annotation, we merge the citation context into a single sentence. Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and Mac-Roberts, 1984) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this method of annotation, we merge the citation context into a single sentence. ", "mid_sen": "Since the context introduces more than one sentiment per citation, we marked the citation sentiment with the last sentiment mentioned in the context window as this is pragmatically most likely to be the real intention (MacRoberts and Mac-Roberts, 1984) .", "after_sen": "As is evident from Table 1, including the 4 sentence window around the citation more than doubles the instances of subjective sentiment, and in the case of negative sentiment, this proportion rises to 3. "}
{"citeStart": 94, "citeEnd": 111, "citeStartToken": 94, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "In a system like INTARC-1.3, the analysis tree is of much higher importance than the recovered string; for the goal of speech translation an adequate semantic representation for a string with word errors is more important than a good string with a wrong reading. The grammar scores have only indirect influence on the string; their main function is picking the right tree. We cannot measure something like a \"tree recognition rate\" or \"rule accuracy\", because there is no treebank for our grammar. The word accuracy results cannot be compared to word accuracy as usually applied to an acoustic decoder in isolation. We counted only those words as recognized which could be built into a valid parse from the beginning of the utterance. Words to the right which could not be integrated into a parse, were counted as deletions ---although they might have been correct in standard word accuracy terms. This evaluation method is much harder than standard word accuracy, but it appears to be a good approximation to \"rule accuracy\". Using this strict method we achieved a word accuracy of 47%, which is quite promising. Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989 ) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a) , and (Weber 1995) . Recognition rates had been improved there for read speech. In spontaneous speech we could not achieve the same effects.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using this strict method we achieved a word accuracy of 47%, which is quite promising. ", "mid_sen": "Results using top down prediction of possible word hypotheses by the parser work inspired by (Kita et. al. 1989 ) have already been published in (Hauenstein and Weber 1994a; ltmlenstein and Weber 1994b), (Weber 1994a) , and (Weber 1995) . ", "after_sen": "Recognition rates had been improved there for read speech. "}
{"citeStart": 181, "citeEnd": 208, "citeStartToken": 181, "citeEndToken": 208, "sectionName": "UNKNOWN SECTION NAME", "string": "Direct saturation of a quale role. The complement is identified as a subtype of the experiencing event or the intellectual act. In (16b) and (17b), for example, the complement directly saturates the event e2 or e3 (as the qualia structures in (18) and (19) make explicit). Partir is indeed a subtype of the experiencing event sort (parfir < ezperiencing_event) and les dchecs (chess) of the intellectual act one (les dchecs < creative-intellectual_act). By contrast, in order for (20) to be an acceptable sentence, ~tre malade 'be ill' must be reconstructed, non-standardly, as an intellectual act. Saturation of the object of the experiencing or intellectual act event. In (16c) and (17c), the complement is the object (y) of an implicit event and the saturation of the quale is only possible because the complement can be coerced to the type expected for the complement (the experiencing event or intellectual act): (16c) means that I'm sad~furious because I ezperience your leaving (as (21) makes explicit) and (17c) that I'm ingenious at performing ~he intellectual ac~ whose object is the departure (as in (22) ). There is no further specification available for the ezp_ev or intellectual-acLev variable. These two ways of saturating a quale explain what Croft (Croft, 1984) and Ernst (1985) have called the verbal/factive ambiguity of two arguments agent-oriented adjectives (see also Kiparsky and Kiparsky, 1979) . When the event is saturated, we get the eventual sense: in (17b), cleverness is predicated of the manner of playing chess (structure 19); when the object of the event is saturated, we get the faetive sense so that in (17c) cleverness is predicated of the fact of leaving (structure 22).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There is no further specification available for the ezp_ev or intellectual-acLev variable. ", "mid_sen": "These two ways of saturating a quale explain what Croft (Croft, 1984) and Ernst (1985) have called the verbal/factive ambiguity of two arguments agent-oriented adjectives (see also Kiparsky and Kiparsky, 1979) . ", "after_sen": "When the event is saturated, we get the eventual sense: in (17b), cleverness is predicated of the manner of playing chess (structure 19); when the object of the event is saturated, we get the faetive sense so that in (17c) cleverness is predicated of the fact of leaving (structure 22)."}
{"citeStart": 24, "citeEnd": 42, "citeStartToken": 24, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W × LW matricization of DM.DE, the German version (Padó and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010) . DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faaß et al., 2010) , lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More specifically, we use the W × LW matricization of DM.", "mid_sen": "DE, the German version (Padó and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010) . ", "after_sen": "DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faaß et al., 2010) , lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010) ."}
{"citeStart": 22, "citeEnd": 42, "citeStartToken": 22, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "In our previous work (Zhang and Chai, 2009) , conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d 1 ∧ . . . ∧ d m , and a hypothesis H represented by another set of clauses H = h 1 ∧ . . . ∧ h n , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause h j is entailed from all the conversation segment clauses d 1 . . . d m as follows. This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In our previous work (Zhang and Chai, 2009) , conversation entailment is formulated as the following: given a conversation segment D which is represented by a set of clauses D = d 1 ∧ . . . ∧ d m , and a hypothesis H represented by another set of clauses H = h 1 ∧ . . . ∧ h n , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause h j is entailed from all the conversation segment clauses d 1 . . . d m as follows. ", "after_sen": "This is based on a simple assumption that whether a clause is entailed from a conversation segment is conditionally independent from other clauses."}
{"citeStart": 0, "citeEnd": 22, "citeStartToken": 0, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "In general, LexRank is a graph-based method for computing relative importance of textual units. Erkan and Radev (2004) use it to compute the sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. They use a connectivity matrix based on intra-sentence cosine similarity as the adjacency matrix of the graph representation of sentences. We adapt LexRank to compute the importance of tweets.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In general, LexRank is a graph-based method for computing relative importance of textual units. ", "mid_sen": "Erkan and Radev (2004) use it to compute the sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. ", "after_sen": "They use a connectivity matrix based on intra-sentence cosine similarity as the adjacency matrix of the graph representation of sentences. "}
{"citeStart": 43, "citeEnd": 67, "citeStartToken": 43, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "Data We used the full paper corpus used by Contractor et al. (2012) which contains 8171 sentences from 50 biomedical journal articles. The corpus is annotated according to the AZ scheme described in Table  1 . AZ describes the logical structure, scientific argumentation and intellectual attribution of a scientific paper. It was originally introduced by Teufel and Moens (2002) and applied to computational linguistics papers, and later adapted to other domains such as biology (Mizuta et al., 2006 ) -which we used in this work -and chemistry (Teufel et al., 2009) . Table 4 shows the AZ class distribution in full articles as well as in individual sections. Since section names vary across scientific articles, we grouped similar sections before calculating the statistics (e.g. Discussion and Conclusions sections were grouped under Discussion). We can see that although there is a major category in each section (e.g. CON in Discussion), up to 36.5% of the sentences in each section still belong to other categories.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At test time we first apply the first classifier and based on its prediction we apply either the classifier that distinguishes between OWN categories or the one that distinguishes between OTHER categories.", "mid_sen": "Data We used the full paper corpus used by Contractor et al. (2012) which contains 8171 sentences from 50 biomedical journal articles. ", "after_sen": "The corpus is annotated according to the AZ scheme described in Table  1 . AZ describes the logical structure, scientific argumentation and intellectual attribution of a scientific paper. "}
{"citeStart": 368, "citeEnd": 380, "citeStartToken": 368, "citeEndToken": 380, "sectionName": "UNKNOWN SECTION NAME", "string": "Sec. 2 will briefly review approaches to word order in DG, and Sec. 3 introduces word order domains as our proposal. LFG is briefly introduced in Sec. 4, and the encoding of DG within the LFG framework is the topic of Sec. 5. I  I  I   I   I  I  I  i  I   l   I  I  i  I  !   i 2 Word Order in DG A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesni~re, 1959; Hudson, 1993) . If these relations are motivated semantically, such dependency trees can be non-projective. Consider the extracted NP in \"Beans, I know John likes\". A projective tree would require \"Beans\" to be connected to either \"1\" or \"know\"none of which is conceptually directly related to \"Beans\". It is \"likes\" that determines syntactic features of \"Beans\" and which provides a semantic role for it. The only connection between \"know\" and \"Beans\" is that the finite verb allows the extraction of\"Beans\", thus defining order restrictions for the NP. The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sec. 2 will briefly review approaches to word order in DG, and Sec. 3 introduces word order domains as our proposal. ", "mid_sen": "LFG is briefly introduced in Sec. 4, and the encoding of DG within the LFG framework is the topic of Sec. 5. I  I  I   I   I  I  I  i  I   l   I  I  i  I  !   i 2 Word Order in DG A very brief characterization of DG is that it recognizes only lexical, not phrasal nodes, which are linked by directed, typed, binary relations to form a dependency tree (Tesni~re, 1959; Hudson, 1993) . ", "after_sen": "If these relations are motivated semantically, such dependency trees can be non-projective. "}
{"citeStart": 273, "citeEnd": 299, "citeStartToken": 273, "citeEndToken": 299, "sectionName": "UNKNOWN SECTION NAME", "string": "In previous work with the ANLT , throughput with raw corpus data was worse than that observed in these experiments, though probably only by a constant factor. This could be due to the fact that the vocabulary of the corpus concerned exhibits significantly higher lexical ambiguity; however, for sentences taken from a specific corpus, constructional bias observed in a training phase could be exploited to improve performance (e.g. Samuelsson &: Rayner, 1991) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In previous work with the ANLT , throughput with raw corpus data was worse than that observed in these experiments, though probably only by a constant factor. ", "mid_sen": "This could be due to the fact that the vocabulary of the corpus concerned exhibits significantly higher lexical ambiguity; however, for sentences taken from a specific corpus, constructional bias observed in a training phase could be exploited to improve performance (e.g. Samuelsson &: Rayner, 1991) .", "after_sen": ""}
{"citeStart": 117, "citeEnd": 137, "citeStartToken": 117, "citeEndToken": 137, "sectionName": "UNKNOWN SECTION NAME", "string": "To calculate a shortest common cover link for an utterance, I will use an incremental parser. Incrementality means that the parser reads the words of the utterance one by one and, as each word is read, the parser is only allowed to add links which have one of their ends at that word. Words which have not yet been read are not available to the parser at this stage. This restriction is inspired by psycholinguistic research which suggests that humans process language incrementally (Crocker et al., 2000) . If the incrementality of the parser roughly resembles that of human processing, the result is a significant restriction of parser search space which does not lead to too many parsing errors.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Words which have not yet been read are not available to the parser at this stage. ", "mid_sen": "This restriction is inspired by psycholinguistic research which suggests that humans process language incrementally (Crocker et al., 2000) . ", "after_sen": "If the incrementality of the parser roughly resembles that of human processing, the result is a significant restriction of parser search space which does not lead to too many parsing errors."}
{"citeStart": 58, "citeEnd": 75, "citeStartToken": 58, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "The SVM classification model is built from lexical and syntactic features assigned to tokens and entity pairs prior to classification. We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006) . These features are split into 11 sets, as described in Table 3 .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The SVM classification model is built from lexical and syntactic features assigned to tokens and entity pairs prior to classification. ", "mid_sen": "We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006) . ", "after_sen": "These features are split into 11 sets, as described in Table 3 ."}
{"citeStart": 52, "citeEnd": 61, "citeStartToken": 52, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that Jaccard's coefficient differs from all the other measures we consider in that it is essentially combinatorial, being based only on the sizes of the supports of q, r, and q • r rather than the actual values of the distributions.", "mid_sen": "Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions:", "after_sen": "JS(q,r)=-~l [D(q aVgq,r)+D(r aVgq,r) ]"}
{"citeStart": 58, "citeEnd": 86, "citeStartToken": 58, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "is the difference found between x 1 and x 2 , the results for the new and current techniques, respectively. E[d] is the expected difference (which is 0 under the null hypothesis) and s d is an estimate of the standard deviation of d. Standard deviation is the square root of the variance, a measure of how much a random variable is expected to vary. The results of equation 1 are compared to tables (c.f. in Box et al. (1978, Appendix)) to find out what the chances are of equaling or exceeding the equation 1 results if the null hypothesis were true. The larger the equation 1 results, the more unusual it would be under the null hypothesis. A complication of using equation 1 is that one usually does not have s d , but only s 1 and s 2 , where s 1 is the estimate for x 1 's standard deviation and similarly for s 2 . How does one get the former from the latter? It turns out that (Box et al., 1978, Ch. 3)", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "E[d] is the expected difference (which is 0 under the null hypothesis) and s d is an estimate of the standard deviation of d. Standard deviation is the square root of the variance, a measure of how much a random variable is expected to vary. ", "mid_sen": "The results of equation 1 are compared to tables (c.f. in Box et al. (1978, Appendix)) to find out what the chances are of equaling or exceeding the equation 1 results if the null hypothesis were true. ", "after_sen": "The larger the equation 1 results, the more unusual it would be under the null hypothesis. "}
{"citeStart": 152, "citeEnd": 166, "citeStartToken": 152, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "Our non-verbal features attempt to capture similarity between the speaker's hand gestures; similar gestures are thought to suggest semantic similarity (Mc-Neill, 1992) . For example, two noun phrases may be more likely to corefer if they are accompanied by identically-located pointing gestures. In this section, we describe features that quantify various aspects of gestural similarity.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our non-verbal features attempt to capture similarity between the speaker's hand gestures; similar gestures are thought to suggest semantic similarity (Mc-Neill, 1992) . ", "after_sen": "For example, two noun phrases may be more likely to corefer if they are accompanied by identically-located pointing gestures. "}
{"citeStart": 96, "citeEnd": 118, "citeStartToken": 96, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "The resulting training procedure is analogous to the one presented in (Brown et al., 1993) and (Tillmann and Ney, 1997) .", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The function δ(•, •) denotes the Kronecker delta.", "mid_sen": "The resulting training procedure is analogous to the one presented in (Brown et al., 1993) and (Tillmann and Ney, 1997) .", "after_sen": "The next section presents variants of the basic unconstrained model by putting restrictions on the valid regions of triggers (in-phrase vs. out-ofphrase) and using alignments obtained from either GIZA++ training or forced alignments in order to reduce the model size and to incorporate knowledge already obtained in previous training steps."}
{"citeStart": 93, "citeEnd": 111, "citeStartToken": 93, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998) . ", "mid_sen": "Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998) . ", "after_sen": "The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy."}
{"citeStart": 106, "citeEnd": 126, "citeStartToken": 106, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Figure 2(a) , we find the distance between x 11 and x 9 is 3.", "mid_sen": "Using the implicit modeling of argument consistency, we follow the same approach as in our previous work (Zhang and Chai, 2009) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .", "after_sen": ""}
{"citeStart": 188, "citeEnd": 207, "citeStartToken": 188, "citeEndToken": 207, "sectionName": "UNKNOWN SECTION NAME", "string": "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or 'biTAM' (Zhao and Xing, 2006) . In contrast to our source language approach, these authors use both source and target information.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data.", "mid_sen": "To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or 'biTAM' (Zhao and Xing, 2006) . ", "after_sen": "In contrast to our source language approach, these authors use both source and target information."}
{"citeStart": 22, "citeEnd": 35, "citeStartToken": 22, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "In very recent work, (Demberg, 2007) developed an unsupervised algorithm (f-meas: 68%; an extension of RePortS) whose segmentations improve g2p when using a the decision tree (PER: 3.45%). Table 6 : The effect of morphological preprocessing on phoneme error rates (PER) and word error rates (WER) in grapheme-to-phoneme conversion.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Without the constraints, the performance difference is smaller: the joint n-gram model then achieves a word error rate of 21.5% on the nomorphology-condition.", "mid_sen": "In very recent work, (Demberg, 2007) developed an unsupervised algorithm (f-meas: 68%; an extension of RePortS) whose segmentations improve g2p when using a the decision tree (PER: 3.45%). ", "after_sen": "Table 6 : The effect of morphological preprocessing on phoneme error rates (PER) and word error rates (WER) in grapheme-to-phoneme conversion."}
{"citeStart": 253, "citeEnd": 279, "citeStartToken": 253, "citeEndToken": 279, "sectionName": "UNKNOWN SECTION NAME", "string": "Generative Capacity HG are more powerful than CI=G and are known to be weakly equivalent to Tree Adjoining Grammar, Combinatory Categorial Grammar, and Head Grammar (Vijay-Shanker and Weir, 1994) . PI_IG are more powerful than I_IG since they can generate the k-copy language for any fixed k (see Example 2). Slightly more generally, PI_IG can generate the language {w~]weR} for any k > 1 and regular language R. We believe that the language involving copies of strings of matching brackets described in Example 5 cannot be generated by PI_IG but, as shown in Exampie 5, it can be generated by P/T(:; and therefore PLPATR. Slightly more generally, PLTG can gener-ate the language {w k Iw~L } for any k > 1 and context-free language L. It appears that the class of languages generated by PI_TG is included in those languages generated by Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987) since the construction involved in a proof of this underlies the recognition algorithm discussed in the next section. As is the case for the tree sets of 16, 1_16 and Tree Adjoining Grammar, the tree sets generated by PI_TG have path sets that. are context-free languages. In other words, the set of all strings labelling root to frontier paths of derivation trees is a context-free language. While the tree sets of lAG and Tree Adjoining Grammars have independent branches, PI_T6 tree sets exhibit dependent branches, where the number of dependent branches in any tree is bounded by the grammar. Note that the number of dependent branches in the tree sets of 16 is not bounded by the grammar (e.g., they generate sets of all full binary trees).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We believe that the language involving copies of strings of matching brackets described in Example 5 cannot be generated by PI_IG but, as shown in Exampie 5, it can be generated by P/T(:; and therefore PLPATR. ", "mid_sen": "Slightly more generally, PLTG can gener-ate the language {w k Iw~L } for any k > 1 and context-free language L. It appears that the class of languages generated by PI_TG is included in those languages generated by Linear Context-Free Rewriting Systems (Vijay-Shanker et al., 1987) since the construction involved in a proof of this underlies the recognition algorithm discussed in the next section. ", "after_sen": "As is the case for the tree sets of 16, 1_16 and Tree Adjoining Grammar, the tree sets generated by PI_TG have path sets that. "}
{"citeStart": 200, "citeEnd": 213, "citeStartToken": 200, "citeEndToken": 213, "sectionName": "UNKNOWN SECTION NAME", "string": "A left-corner parsing algorithm with top-down filtering has been reported to show very efficient performance for unification-based systems (Carroll, 1994) . In particular, top-down filtering seems to be very effective in increasing parse efficiency (Shann, 1991) . Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time. However, in the context of unification-based parsing, left-recursive grammars have the formal power of a Turing machine, therefore detection of all infinite loops due to left-recursion is impossible (Shieber, 1992) . So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Ideally all top-down expectation should be propagated down to the input word so that unsuccessful rule applications are pruned at the earliest time. ", "mid_sen": "However, in the context of unification-based parsing, left-recursive grammars have the formal power of a Turing machine, therefore detection of all infinite loops due to left-recursion is impossible (Shieber, 1992) . ", "after_sen": "So, top-down constraints must be weakened in order for parsing to be guaranteed to terminate."}
{"citeStart": 130, "citeEnd": 131, "citeStartToken": 130, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "In this su])section~ we. introduce two com])inators theft enable us to change the order of ,\\co nversio:u, proposed by Stee(hnan [6] , as a kind of type chAuge [ II ~z'.s= I = ',,~,al.<(~Clo~,NIo~,Z) .1", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In this su])section~ we. introduce two com])inators theft enable us to change the order of ,\\co nversio:u, proposed by Stee(hnan [6] , as a kind of type chAuge [ II ~z'.s= I = ',,~,al.<(~Clo~,NIo~,Z) .1", "after_sen": "II .~= I: ,o.ake (KI0~, NIo~, .Xy.', '~ad( N, e:, 13)) [?igur(, 4: Process"}
{"citeStart": 61, "citeEnd": 85, "citeStartToken": 61, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "Our computational paradigm suggests using a SNoW based predictor as a building block that learns to perform each of the required predictions, and writing a simple program that activates these predictors with the appropriate input, aggregates their output and controls the interaction between the predictors. Two instantiations of this paradigm are studied and evaluated on two different shallow parsing tasksidentifying base NPs and SV phrases. The first instantiation of this para4igm uses predictors to decide whether each word belongs to the in-terior of a phrase or not, and then groups the words into phrases. The second instantiation finds the borders of phrases (beginning and end) and then pairs !them in an \"optimal\" way into different phrases. These problems formulations are similar to those studied in (Ramshaw and Marcus, 1995) and (Church, 1988; Argamon et al., 1998) , respectively.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second instantiation finds the borders of phrases (beginning and end) and then pairs !them in an \"optimal\" way into different phrases. ", "mid_sen": "These problems formulations are similar to those studied in (Ramshaw and Marcus, 1995) and (Church, 1988; Argamon et al., 1998) , respectively.", "after_sen": "The experimental results presented using the SNoW based approach compare favorably with previously published results, both for NPs and SV phrases. "}
{"citeStart": 277, "citeEnd": 295, "citeStartToken": 277, "citeEndToken": 295, "sectionName": "UNKNOWN SECTION NAME", "string": "However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be oriented towards a particular task such as language modelling (Dagan et al., 1999; Lee, 1999) . The first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard. Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area. However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003) . Thus, applying a distributional similarity technique to a new application necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). ", "mid_sen": "Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be oriented towards a particular task such as language modelling (Dagan et al., 1999; Lee, 1999) . ", "after_sen": "The first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard. "}
{"citeStart": 289, "citeEnd": 323, "citeStartToken": 289, "citeEndToken": 323, "sectionName": "UNKNOWN SECTION NAME", "string": "To perform automatic subjectivity analysis, good clues must be found. A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998 ]), there is no comprehensive dictionary of subjective language. In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice. An NLP system must disambiguate these expressions in context.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To perform automatic subjectivity analysis, good clues must be found. ", "mid_sen": "A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998 ]), there is no comprehensive dictionary of subjective language. ", "after_sen": "In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Most of the previous approaches discussed in Section 3 have been evaluated to different degrees. In general, a small number of frequently occurring verbs is selected, and the subcategorization frames extracted for these verbs (from some quantity of unseen test data) are compared to a gold standard. The gold standard is either manually custom-made based on the test data or adapted from an existing external resource such as the OALD (Hornby 1980) or COMLEX (MacLeod, Grishman, and Meyers 1994 ). There are advantages and disadvantages to both types of gold standard. While it is time-consuming to manually construct a custom-made standard, the resulting standard has the advantage of containing only the subcategorization frames exhibited in the test data. Using an existing externally produced resource is quicker, but the gold standard may contain many more frames than those which occur in the data from which the test lexicon is induced or, indeed, may omit relevant correct frames contained in the data. As a result, systems generally score better against custom-made, manually established gold standards. Carroll and Rooth (1998) achieve an F-score of 77% against the OALD when they evaluate a selection of 100 verbs with absolute frequency of greater than 500 each. Their system recognizes 15 frames, and these do not contain details of subcategorizedfor prepositions. Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%. However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system. The largest lexical evaluation we know of is that of Schulte im Walde (2002b) for German. She evaluates 3,000 German verbs with a token frequency between 10 and 2,000 against the Duden (Dudenredaktion 2001). We will refer to this work and the methods and results presented by Schulte im Walde again in Sections 6.2 and 6.3.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Still, to date this is the largest number of verbs used in any of the evaluations of the systems for English described in Section 3. ", "mid_sen": "Sarkar and Zeman (2000) evaluate 914 Czech verbs against a custom-made gold standard and record a token recall of 88%. ", "after_sen": "However, their evaluation does not examine the extracted subcategorization frames but rather the argument-adjunct distinctions posited by their system. "}
{"citeStart": 282, "citeEnd": 292, "citeStartToken": 282, "citeEndToken": 292, "sectionName": "UNKNOWN SECTION NAME", "string": "In Figure 1 , characters in each group differ at the stroke level. Similar characters in every group in the first row in Figure 2 share a common part, but the shared part is not the radical of these characters. Similar characters in every group in the second row in Figure 2 share a common part, which is the radical of these characters. Similar characters in every group in Figure 2 have different pronunciations. We show six groups of homophones that also share a component in Figure 3 . Characters that are similar in both pronunciations and internal structures are most confusing to new learners. It is not difficult to list all of those characters that have the same or similar pronunciations, e.g., \"試場\" and \"市場\", if we have a machine readable lexicon that provides information about pronunciations of characters and when we ignore special patterns for tone sandhi in Chinese (Chen, 2000) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Characters that are similar in both pronunciations and internal structures are most confusing to new learners. ", "mid_sen": "It is not difficult to list all of those characters that have the same or similar pronunciations, e.g., \"試場\" and \"市場\", if we have a machine readable lexicon that provides information about pronunciations of characters and when we ignore special patterns for tone sandhi in Chinese (Chen, 2000) .", "after_sen": "In contrast, it is relatively difficult to find characters that are written in similar ways, e.g., \"構\" with \"購\", in an efficient way. "}
{"citeStart": 10, "citeEnd": 36, "citeStartToken": 10, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "We use all the verbs that their lemmatized form appears in at least three sentences that belong to the same category in the training set. Auxiliary verbs are excluded. In our annotated dataset, for example, the verb propose appeared in 67 sentences from the Methodology category, while the verbs outperform and achieve appeared in 33 Result sentences. (2007) . These sentences belong to the same category (i.e Method). Both Sentences (6) and (7) convey the same information about Goldwater and Griffiths (2007) contribution. Sentence (8), however, describes a different aspect of the paper methodology. Clustering divides the sentences of each category into groups of similar sentences. Following Qazvinian and Radev (2008) , we build a cosine similarity graph out of the sentences of each category. This is an undirected graph in which nodes are sen-tences and edges represent similarity relations. Each edge is weighted by the value of the cosine similarity (using TF-IDF vectors) between the two sentences the edge connects. Once we have the similarity network constructed, we partition it into clusters using a community finding technique. We use the Clauset algorithm (Clauset et al., 2004) , a hierarchical agglomerative community finding algorithm that runs in linear time.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clustering divides the sentences of each category into groups of similar sentences. ", "mid_sen": "Following Qazvinian and Radev (2008) , we build a cosine similarity graph out of the sentences of each category. ", "after_sen": "This is an undirected graph in which nodes are sen-tences and edges represent similarity relations. "}
{"citeStart": 191, "citeEnd": 206, "citeStartToken": 191, "citeEndToken": 206, "sectionName": "UNKNOWN SECTION NAME", "string": "For comparison, a variety of other data has been collected. Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983) , using confusion matrices where appropriate (Kernighan, 1990) . Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). Suggestions have been made to look for low frequency words in corpora and news/mail archives, and to the Longmans learner corpus (not native speakers).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For comparison, a variety of other data has been collected. ", "mid_sen": "Preliminary tests used generated errors, from a program that produces random Damerau slips according to an observed distribution (Pollock, 1983) , using confusion matrices where appropriate (Kernighan, 1990) . ", "after_sen": "Assembled data includes the Birkbeck corpus (Mitton, 1986) and multifarious misspelling lists (without context). "}
{"citeStart": 186, "citeEnd": 194, "citeStartToken": 186, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity (Lin 1998) (Section 3.4) . We hypothesized that two words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre). In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One or more positions in the collocation may be filled by any word (of an appropriate part of speech) that is unique in the test data.", "mid_sen": "The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity (Lin 1998) (Section 3.4) . ", "after_sen": "We hypothesized that two words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre). "}
{"citeStart": 33, "citeEnd": 50, "citeStartToken": 33, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "Reichenbach's well -known account of the interpretation of the different tense forms uses the temporal relations between three temporal indices: the utterance time, event time and reference time. The reference time according to (Reichenbach, 1947) is determined either by context, or by temporal adverbials.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Reichenbach's well -known account of the interpretation of the different tense forms uses the temporal relations between three temporal indices: the utterance time, event time and reference time. ", "mid_sen": "The reference time according to (Reichenbach, 1947) is determined either by context, or by temporal adverbials.", "after_sen": "2.1 St unified analysis of temporal anaphora Hinrichs' and Partee's use of a notion of reference time, provides for a unified treatment of temporal anaphoric relations in discourse, which include narrative progression especially in sequences of simple past tense sentences, temporal adverbs and temporal adverbial clauses, introduced by a temporal connective. "}
{"citeStart": 118, "citeEnd": 145, "citeStartToken": 118, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "The linguistic studies, used for the typologies of CoL verbs and spatial prepositions, have been realized on verbs considered without any adjuncts, in their atemporal form and independently of any context, on the one hand, and on prepositions considered independently of any context, on the other. This methodology, discussed in Borillo & Sablayrolles (1993) , has allowed us to extract the intrinsic semantics of these lexical items. Since natural languages put together verbs and prepositions in a sentence, we have developped a formal calculus (see (Asher & Sablayrolles, 1994b) ), based on these two typologies, which computes, in a compositional way, the spatiotemporal properties of a motion complex from the semantic properties of the verb and of the preposition. For reason of space we cannot detail our formalism here, but we intend to present it in the talk.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This methodology, discussed in Borillo & Sablayrolles (1993) , has allowed us to extract the intrinsic semantics of these lexical items. ", "mid_sen": "Since natural languages put together verbs and prepositions in a sentence, we have developped a formal calculus (see (Asher & Sablayrolles, 1994b) ), based on these two typologies, which computes, in a compositional way, the spatiotemporal properties of a motion complex from the semantic properties of the verb and of the preposition. ", "after_sen": "For reason of space we cannot detail our formalism here, but we intend to present it in the talk."}
{"citeStart": 1, "citeEnd": 28, "citeStartToken": 1, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "Following Gruber (1965), Jackendoff (1976), Boons (1985) , we approach motion verbs in terms of some \"localist semantical\" role labels. The linguistic study of French intransitive motion verbs (see eg. (Asher & Sablayrolles, 1994a) ) we have realized has allowed the definition of an ontology for \"location\" in three basic concepts:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The linguistic study of French intransitive motion verbs (see eg. ", "mid_sen": "(Asher & Sablayrolles, 1994a) ) we have realized has allowed the definition of an ontology for \"location\" in three basic concepts:", "after_sen": "• locations which are concrete places (a room; a house; a street);"}
{"citeStart": 47, "citeEnd": 69, "citeStartToken": 47, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "The syntax/prosody misalignment may be viewed as resulting in part from semantic considerations. Both predicateargument relations and discourse factors have been examined for their possible input to prosodic phrasing. Crystal (1969) claims that prosodic phrase boundaries will co-occur with grammatical functions such as subject, predicate, modifier, and adjunct. Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. The problem here is that the phrasing in observed data often ignores the argument status of constituents. In 17a-f, for example, the phrasing makes no distinction between arguments and adjuncts. All of the sentences have the same X(VY) pattern even though Y is a complement in the first case (thefirst serious attempt) and an adjunct in the others. ( The relation between discourse and prosodic phrasing has been examined in some detail by Bing (1985) , who argues that each noun phrase in an utterance constitutes a separate prosodic phrase unless it is destressed because of reference to previous discourse. Bing also observes that constituents that refer to items newly introduced into a discourse tend to be longer. This may be the reason that word count and syllable count play a prominent role in prosodic phrasing (see Section 2.1.3.). To our knowledge, no work has explicitly explored the relation between the length of a constituent and its status in the discourse. Hirschberg and Litman (1987) and Litman and Hirschberg (1990) also examine the relation between discourse and prosodic phrasing. Their work succeeds in distinguishing the use of items like now, so, and well as discourse cues from their denotative lexical use on the basis of a complex combination of pitch accent type and phrasing.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Selkirk (1984) and Nespor and Vogel (1986) take a similar approach, but within a different theoretical framework. ", "mid_sen": "Previous versions of our work, as described in Bachenko et al. (1986) also assume that phrasing is dependent on predicate-argument structure. ", "after_sen": "The problem here is that the phrasing in observed data often ignores the argument status of constituents. "}
{"citeStart": 159, "citeEnd": 177, "citeStartToken": 159, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "We also added manually-developed features found by other researchers. We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , some Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and some subjectivity clues listed in (Wiebe, 1990) . We represented each set as a three-valued feature based on the presence of 0, 1, or ≥ 2 members of the set. We will refer to these as the manual features.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We also added manually-developed features found by other researchers. ", "mid_sen": "We created 14 feature sets representing some classes from (Levin, 1993; Ballmer and Brennenstuhl, 1981) , some Framenet lemmas with frame element experiencer (Baker et al., 1998) , adjectives manually annotated for polarity (Hatzivassiloglou and McKeown, 1997) , and some subjectivity clues listed in (Wiebe, 1990) . ", "after_sen": "We represented each set as a three-valued feature based on the presence of 0, 1, or ≥ 2 members of the set. "}
{"citeStart": 194, "citeEnd": 219, "citeStartToken": 194, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been surprisingly little work on precisely the problem that we tackle in this paper. The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001 ) and from free text (Hearst, 1992; Caraballo, 1999) . Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. For two terms x and y, x is said to subsume y if the following conditions hold:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The literature on automated text categorization is enormous, but assumes that a set of categories has already been created, whereas the problem here is to determine the categories of interest. ", "mid_sen": "There has also been extensive work on finding synonymous terms and word associations, as well as automatic acquisition of IS-A (or genus-head) relations from dictionary definitions and glosses (Klavans and Whitman, 2001 ) and from free text (Hearst, 1992; Caraballo, 1999) . Sanderson and Croft (1999) propose a method called subsumption for building a hierarchy for a set of documents retrieved for a query. ", "after_sen": "For two terms x and y, x is said to subsume y if the following conditions hold:"}
{"citeStart": 65, "citeEnd": 96, "citeStartToken": 65, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "The extracted sentences were independently annotated by two judges. One is a coauthor of this article (judge 1), and the other has performed subjectivity annotation before, but is not otherwise involved in this research (judge 2). Sentences were annotated according to the coding instructions of Wiebe, Bruce, and O'Hara (1999) which, recall, are to classify a sentence as subjective if there is a significant expression of subjectivity of either the writer or someone mentioned in the text, in the sentence.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "One is a coauthor of this article (judge 1), and the other has performed subjectivity annotation before, but is not otherwise involved in this research (judge 2). ", "mid_sen": "Sentences were annotated according to the coding instructions of Wiebe, Bruce, and O'Hara (1999) which, recall, are to classify a sentence as subjective if there is a significant expression of subjectivity of either the writer or someone mentioned in the text, in the sentence.", "after_sen": ""}
{"citeStart": 71, "citeEnd": 90, "citeStartToken": 71, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "We had hoped that some approach to a limit would be seen using PTB II (Marcus et al., 1994) , which larger and more consistent for bracketting than PTB I. As shown in Figure 1 , however, the rule number growth continues unabated even after more than 1 million part-ofspeech tokens have been processed.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If there is, the number of extracted rules will approach a limit as more sentences are processed, i.e. as the rule number approaches the size of such an underlying and finite grammar.", "mid_sen": "We had hoped that some approach to a limit would be seen using PTB II (Marcus et al., 1994) , which larger and more consistent for bracketting than PTB I. ", "after_sen": "As shown in Figure 1 , however, the rule number growth continues unabated even after more than 1 million part-ofspeech tokens have been processed."}
{"citeStart": 36, "citeEnd": 47, "citeStartToken": 36, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "As far as we know, the first SVM-based NE system was proposed by for Japanese. His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001 ) that gave the best performance at CoNLL-2000 shared tasks. In their system, every word in a sentence is classified sequentially from the beginning or the end of a sentence. However, since Yamada has not compared it with other methods under the same conditions, it is not clear whether his NE system is better or not. Here, we show that our SVM-based NE system is more accurate than conventional systems. Our system uses the Viterbi search (Allen, 1995) instead of sequential determination.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here, we show that our SVM-based NE system is more accurate than conventional systems. ", "mid_sen": "Our system uses the Viterbi search (Allen, 1995) instead of sequential determination.", "after_sen": "For training, we use 'CRL data', which was prepared for IREX (Information Retrieval and Extraction Exercise 1 , Sekine and Eriguchi (2000) ). "}
{"citeStart": 114, "citeEnd": 136, "citeStartToken": 114, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992) ). A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986) , RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988) . We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992) ). ", "mid_sen": "A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986) , RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988) . ", "after_sen": "We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion."}
{"citeStart": 91, "citeEnd": 102, "citeStartToken": 91, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "To estimate the inter-annotator agreement, we picked 500 random sentences from our data set and assigned them to two different annotators. The interannotator agreement was perfect on both the reference tagging annotation and the reference syntacticality annotation. This is expected since both are objective, clear, and easy tasks. To measure the interannotator agreement on the scope annotation task, we deal with it as a word classification task. This allows us to use the popular classification agreement measure, the Kappa coefficient (Cohen, 1968) . The Kappa coefficient is defined as follows:", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To measure the interannotator agreement on the scope annotation task, we deal with it as a word classification task. ", "mid_sen": "This allows us to use the popular classification agreement measure, the Kappa coefficient (Cohen, 1968) . ", "after_sen": "The Kappa coefficient is defined as follows:"}
{"citeStart": 46, "citeEnd": 59, "citeStartToken": 46, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "In one of our experiments, the 400 most frequent nouns in the Merck Veterinary Manual were clustered. Three experts were used to evaluate the generated noun clusters. Some examples of the classes that were generated by the system for the veterinary medicine domain are PROBLEM, TREAT-MENT, ORGAN, DIET, ANIMAL, MEASURE-MENT, PROCESS, and so on. The results obtained by comparing these noun classes to the clusterings provided by three different experts are shown in Table 3. We have also experimented with the use of WordNet to improve the classes obtained by a distributional technique. Some initial experiments have shown that WordNet consistently improves the Fmeasures for these noun classes by about 0.05 on an average. Details of these experiments can be found in (Agarwal, 1995) . It is our belief that the evaluation scheme presented in this paper is useful for comparing different clusterings produced by the same system or those produced by different systems against one provided by an expert. The resulting precision, recall, and F-measure should not be treated as a kind of \"gold standard\" to represent the quality of these classes in some absolute sense. It has been our experience that, as semantic clustering is a highly subjective task, evaluating a given clustering against different experts may yield numbers that vary considerably. However, when different clusterings generated by a system are compared against the same expert (or the same set of experts), such relative comparisons are useful.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some initial experiments have shown that WordNet consistently improves the Fmeasures for these noun classes by about 0.05 on an average. ", "mid_sen": "Details of these experiments can be found in (Agarwal, 1995) . ", "after_sen": "It is our belief that the evaluation scheme presented in this paper is useful for comparing different clusterings produced by the same system or those produced by different systems against one provided by an expert. "}
{"citeStart": 56, "citeEnd": 79, "citeStartToken": 56, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "We have developed an Argumentative Zoning (zone) classifier using a ME model. We compare our zone classifier to a reimplementation of Teufel and Moens (2002) 's NB classifier and features on their original Computational Linguistics corpus. Like Teufel (1999) , we model zone classification as a sequence tagging task. Our zone classifier achieves an F-score of 96.88%, a 20% improvement. We also show how Argumentative Zoning can be applied to other domains by evaluating our system on a corpus of Astronomy journal articles, achieving an F-measure of 97.9%. Teufel (1999) introduced a new rhetorical analysis for scientific texts called Argumentative Zoning. Each sentence of an article from the scientific literature is classified into one of seven basic rhetorical structures shown in Table 1 . The first three: Background, Other, and Own, are part of the basic schema and represent attribution of intellectual ownership. The four additional categories: aim, textual, contrast, and basis, are based upon Swales (1990) 's Creating A Research Space (CARS) model, and provide pointed information about the author's stance and the paper itself. Teufel assumes that each sentence only requires a single classification and that all sentences clearly fit into the above structure. The assumption is clearly not always correct, but is a useful approximation nevertheless.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have developed an Argumentative Zoning (zone) classifier using a ME model. ", "mid_sen": "We compare our zone classifier to a reimplementation of Teufel and Moens (2002) 's NB classifier and features on their original Computational Linguistics corpus. ", "after_sen": "Like Teufel (1999) , we model zone classification as a sequence tagging task. "}
{"citeStart": 98, "citeEnd": 119, "citeStartToken": 98, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "Previous work has examined applying models often used in MT to the paired corpus described above (Fleischman and Roy, 2006) . Recent work in automatic image annotation (Barnard et al., 2003; and natural language processing (Steyvers et al., 2004) , however, have demonstrated the advantages of using hierarchical Bayesian models for related tasks. In this work we follow closely the Author-Topic (AT) model (Steyvers et al., 2004) which is a generalization of Latent Dirichlet Allocation (LDA) (Blei et al., 2005) . 3 LDA is a technique that was developed to model the distribution of topics discussed in a large corpus of documents. The model assumes that every document is made up of a mixture of topics, and that each word in a document is generated from a probability distribution associated with one of those topics. The AT model generalizes LDA, saying that the mixture of topics is not dependent on the document itself, but rather on the authors who wrote it. According to this model, for each word (or phrase) in a document, an author is chosen uniformly from the set of the authors of the document. Then, a topic is chosen from a distribution of topics associated with that particular author. Finally, the word is generated from the distribution associated with that chosen topic. We can express the probability of the words in a document (W) given its authors (A) as:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Previous work has examined applying models often used in MT to the paired corpus described above (Fleischman and Roy, 2006) . ", "mid_sen": "Recent work in automatic image annotation (Barnard et al., 2003; and natural language processing (Steyvers et al., 2004) , however, have demonstrated the advantages of using hierarchical Bayesian models for related tasks. ", "after_sen": "In this work we follow closely the Author-Topic (AT) model (Steyvers et al., 2004) which is a generalization of Latent Dirichlet Allocation (LDA) (Blei et al., 2005) . "}
{"citeStart": 62, "citeEnd": 113, "citeStartToken": 62, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first n sentences of a randomly selected document from the Brown corpus 4. A sample is characterised by the range of n. The corpus was generated by an automatic procedure 5. Speed performance is measured by the average number of CPU seconds required to process a test sample 6. Segmentation accuracy is measured by th(,. error metric (equation 6, fa --+ false alarms) 1)roposed in (Beeferman et al., 1999) . Low error probability indicates high accuracy. Other performanc(; measures include the popular precision and recall metric (PR) (Hearst, 1994) , fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997). The l)roblems associated with these metrics are discussed in (Beeferman et al., 1999) . The accuracy of the last two algorithms are com-puWA analytically. We consider the status of m potential bomldaries as a bit string (1 --+ topic boundary). The terms p(miss) and p(fa) in equation 6 corresponds to p(samelk ) and p(difflk ) = 1-p(same[k). Equatioll 7, 8 and 9 gives the general form of p(samelk ), B (.,.?) and B(r,b), respectively 7. Table 2 presents the experimental results. The values in row two and three, four and five are not actually the same. However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992) . 3-11 3-5 6-8 9-11 B~ 45% 38% 39% 36% B,, 47% 47% 47% 46% B(,.,~) 47% 47% 47% 46% B, 53% 53% 53% 54% B(~,?) 53% 53% 53% 54% (8) (9) ", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The values in row two and three, four and five are not actually the same. ", "mid_sen": "However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992) . 3-11 3-5 6-8 9-11 B~ 45% 38% 39% 36% B,, 47% 47% 47% 46% B(,.,~) 47% 47% 47% 46% B, 53% 53% 53% 54% B(~,?) 53% 53% 53% 54% (8) (9) ", "after_sen": "2ri+t,i + si,i + si+l,i+l 8i,i+ 1 : 8i+1, i for i E {1,...,n-1} 3. 8iTj, i -~ 2ri+j,i \"}-8iWj--l,i-l- 8i+j,i+ 1 --8i+j_l,i+ 1 Si,i+ j --~-"}
{"citeStart": 66, "citeEnd": 78, "citeStartToken": 66, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "Halteren et al., 1998), the best voting technique did not outperform the best stacked classifier. Furthermore the performance differences between the combination methods are not significant (p>0.05). To our surprise the five voting techniques performed the same. We assume that this has happened because the accuracies of the individual classifiers do not differ much and because the classification involves a binary choice. Since there is no significant difference between the combination methods, we can use any of them in the remaining experiments. We have chosen to use majority voting because it does not require tuning data. We have applied it to the two data sets mentioned in (Ramshaw and Marcus, 1995) . The first data set uses WSJ sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have chosen to use majority voting because it does not require tuning data. ", "mid_sen": "We have applied it to the two data sets mentioned in (Ramshaw and Marcus, 1995) . ", "after_sen": "The first data set uses WSJ sections 15-18 as training data (211727 tokens) and section 20 as test data (47377 tokens)."}
{"citeStart": 302, "citeEnd": 328, "citeStartToken": 302, "citeEndToken": 328, "sectionName": "UNKNOWN SECTION NAME", "string": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975) , it has been applied computationally to IR with various levels of success (Preece, 1982) , with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997) . Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) , have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001) , arguably due to Wikipedia's larger conceptual coverage.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975) , it has been applied computationally to IR with various levels of success (Preece, 1982) , with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997) . ", "mid_sen": "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) , have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001) , arguably due to Wikipedia's larger conceptual coverage.", "after_sen": "WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them. "}
{"citeStart": 34, "citeEnd": 68, "citeStartToken": 34, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "In spite of the permanent interest in German word order in the linguistics community, most studies have limited their scope to the order of verb arguments and few researchers have implemented -and even less evaluated -a generation algorithm. In this paper, we present an algorithm, which orders not only verb arguments but all kinds of constituents, and evaluate it on a corpus of biographies. For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text. We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones. We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. Similar to Langkilde & Knight (1998) we utilize statistical methods. Unlike overgeneration approaches (Varges & Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similar to Langkilde & Knight (1998) we utilize statistical methods. ", "mid_sen": "Unlike overgeneration approaches (Varges & Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation.", "after_sen": ""}
{"citeStart": 42, "citeEnd": 54, "citeStartToken": 42, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "In other cases, however, we observe a diversity of realisations. We highlight here three cases: modality in goal, polarity in constraint, and mood in substep. In such cases, we must appeal to another source of control over the apparently available choices. We have looked to the construct of genre (Martin, 1992) to provide this additional control, on two grounds:", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In such cases, we must appeal to another source of control over the apparently available choices. ", "mid_sen": "We have looked to the construct of genre (Martin, 1992) to provide this additional control, on two grounds:", "after_sen": "(1) since genres are distinguished by their communicative purposes, we can view each of the functional sections already identified as a distinct genre; (2) genre is presented as controlling text structure and realisation. "}
{"citeStart": 172, "citeEnd": 194, "citeStartToken": 172, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper reports on a comparison between the transformation-based error-driven learner described in Ferro et al. (1999) and the memory-based learner for GRs described in Buchholz et al. (1999) on finding GRs to verbs 1 by retraining the memory-based learner with the data used in Ferro et al. (1999) . We find that the transformation versus memory-based difference only seems to cause a small difference in the results. Most of the result differences seem to instead be caused by differences in the representations and information used by the learners. An example is that different GR length measures are used. In English, one measure seems better for recovering simple argument GRs, while another measure seems better for modifier GRs. We also find that partitioning the data sometimes helps memory-based learn-That is, GRs that have a verb as the relation target. For example, in Cats eat., there is a \"subject\" relation that has eat as the target and Cats as the source.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our starting point is the work described in Ferro et al. (1999) , which used a fairly small training set.", "mid_sen": "This paper reports on a comparison between the transformation-based error-driven learner described in Ferro et al. (1999) and the memory-based learner for GRs described in Buchholz et al. (1999) on finding GRs to verbs 1 by retraining the memory-based learner with the data used in Ferro et al. (1999) . ", "after_sen": "We find that the transformation versus memory-based difference only seems to cause a small difference in the results. "}
{"citeStart": 141, "citeEnd": 161, "citeStartToken": 141, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010) . We analyze all co-occurrence patterns rather than just repetitions.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Later in Section 6, we examine syntactic patterns in individual sentences (assumption 1) using a corpus of academic articles where sentences were manually annotated with communicative goals.", "mid_sen": "Prior work has reported that certain grammatical productions are repeated in adjacent sentences more often than would be expected by chance (Reitter et al., 2006; Cheung and Penn, 2010) . ", "after_sen": "We analyze all co-occurrence patterns rather than just repetitions."}
{"citeStart": 11, "citeEnd": 32, "citeStartToken": 11, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "Similar to Ringger et al. (2004) , we find the order with the highest probability conditioned on syntactic and semantic categories. Unlike them we use dependency parses and compute the probability of the top node only, which is modified by all constituents. With these adjustments the probability of an order O given the history h, if conditioned on syntactic functions of constituents (s 1 ...s n ), is simply:", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Similar to Ringger et al. (2004) , we find the order with the highest probability conditioned on syntactic and semantic categories. ", "after_sen": "Unlike them we use dependency parses and compute the probability of the top node only, which is modified by all constituents. "}
{"citeStart": 79, "citeEnd": 103, "citeStartToken": 79, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Inspired by the aforementioned studies and the concepts of information theory (Shannon and Weaver, 1949) we try to quantitatively capture the amount of redundancy found across the consonant plosive voiced voiceless dental /d/ /t/ bilabial /b/ /p/ Table 1: The table shows four plosives. If a language has in its consonant inventory any three of the four phonemes listed in this table, then there is a higher than average chance that it will also have the fourth phoneme of the table in its inventory.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, this definition does not take into account the complexity that is involved in communicating the information about the inventory in terms of its constituent features.", "mid_sen": "Inspired by the aforementioned studies and the concepts of information theory (Shannon and Weaver, 1949) we try to quantitatively capture the amount of redundancy found across the consonant plosive voiced voiceless dental /d/ /t/ bilabial /b/ /p/ Table 1: The table shows four plosives. ", "after_sen": "If a language has in its consonant inventory any three of the four phonemes listed in this table, then there is a higher than average chance that it will also have the fourth phoneme of the table in its inventory."}
{"citeStart": 160, "citeEnd": 161, "citeStartToken": 160, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "A key part of this model is that some types of evidence provide better support for beliefs than other types. The types of evidence considered are categorized and ordered based on the source of the evidence: hypothesis < default < inference < linguistic < physical(See [2, 4] ). This ordering reflects the relative defeasibility of different assumptions. Augmenting the strength of an assumption thus decreases its relative defensibility.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A key part of this model is that some types of evidence provide better support for beliefs than other types. ", "mid_sen": "The types of evidence considered are categorized and ordered based on the source of the evidence: hypothesis < default < inference < linguistic < physical(See [2, 4] ). ", "after_sen": "This ordering reflects the relative defeasibility of different assumptions. "}
{"citeStart": 415, "citeEnd": 436, "citeStartToken": 415, "citeEndToken": 436, "sectionName": "UNKNOWN SECTION NAME", "string": "In this dialogue, speaker B is not confident that he will be able to identify the intersection at Lowell Street, and so suggests that the intersection might be marked. Speaker A replies with an elaboration of the initial expression, and B finds that he is now confident, and so accepts the reference. This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent (Appelt, 1985a; Appelt and Kronfeld, 1987; Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1992; Searle, 1969) , are copreseut with the referent (Heeman and Hirst, 1992; Cohen, 1981) , or have the referent in their focus of attention (Reiter and Dale, 1992) . In these theories, the speaker has the intention that the hearer either know the referent or identijy it immediately.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Speaker A replies with an elaboration of the initial expression, and B finds that he is now confident, and so accepts the reference. ", "mid_sen": "This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent (Appelt, 1985a; Appelt and Kronfeld, 1987; Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1992; Searle, 1969) , are copreseut with the referent (Heeman and Hirst, 1992; Cohen, 1981) , or have the referent in their focus of attention (Reiter and Dale, 1992) . ", "after_sen": "In these theories, the speaker has the intention that the hearer either know the referent or identijy it immediately."}
{"citeStart": 80, "citeEnd": 89, "citeStartToken": 80, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The paper is structured as follows. We start by reviewing the training and use of probabilistic context-free grammars (PCFGs). We then de: velop a technique to allow analogous probabilistic annotations on type hierarchies. This gives us a clear account of the relationship between a large class of feature structures and their probabilities, but does not treat re-entrancy. We conclude by sketching a technique which does treat such structures. While we know of previous work which associates scores with feature structures (Kim, 1994) are not aware of any previous treatment which makes explicit the link to classical probability theory.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We conclude by sketching a technique which does treat such structures. ", "mid_sen": "While we know of previous work which associates scores with feature structures (Kim, 1994) are not aware of any previous treatment which makes explicit the link to classical probability theory.", "after_sen": "We take a slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework"}
{"citeStart": 150, "citeEnd": 171, "citeStartToken": 150, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008) , sentence compression (Knight and Marcu, 2002) , and reformulation (Barzilay et al., 2001; Saggion, 2011) ; while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. News-Blaster 3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring language generation to produce a grammatical and coherent summary, and better suites the scenario of tweet summarization. Note that our method considers each tweet as the unit for summarization, which often cannot provide reliable information to compute the salience. This is one main difference between our system and the existing studies.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Abstraction and selection are two strategies employed for multi-document summarization. ", "mid_sen": "The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008) , sentence compression (Knight and Marcu, 2002) , and reformulation (Barzilay et al., 2001; Saggion, 2011) ; while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. ", "after_sen": "News-Blaster 3 and our method are examples of abstraction and selection based methods, respectively. "}
{"citeStart": 110, "citeEnd": 122, "citeStartToken": 110, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998) . In both cases the investigators were able to achieve significant improvements over the previous best tagging results. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In both cases the investigators were able to achieve significant improvements over the previous best tagging results. ", "mid_sen": "Similar advances have been made in machine translation (Frederking and Nirenburg, 1994) , speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998) .", "after_sen": "The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997) , Charniak (1997) and Ratnaparkhi (1997) . "}
{"citeStart": 97, "citeEnd": 122, "citeStartToken": 97, "citeEndToken": 122, "sectionName": "UNKNOWN SECTION NAME", "string": "In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks. We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007) . Latent variables we wish to consider are an increased number of word classes; more flexible regions-see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition-and phonological features and syllable boundaries. Indeed, our local log-linear features over several aligned latent strings closely resemble the soft constraints used by phonologists (Eisner, 1997) . Finally, rather than define a fixed set of feature templates as in Fig. 2 , we would like to refine empirically useful features during training, resulting in language-specific backoff patterns and adaptively sized n-gram windows. Many of these enhancements will increase the computational burden, and we are interested in strategies to mitigate this, including approximation methods.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In future work, we would like to identify a set of features, latent variables, and training methods that port well across languages and string-transduction tasks. ", "mid_sen": "We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al., 2007) . ", "after_sen": "Latent variables we wish to consider are an increased number of word classes; more flexible regions-see Petrov et al. (2007) on learning a state transition diagram for acoustic regions in phone recognition-and phonological features and syllable boundaries. "}
{"citeStart": 70, "citeEnd": 92, "citeStartToken": 70, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008) . We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005) , or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008) . ", "mid_sen": "We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005) , or secondary links M ij (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006) .", "after_sen": "Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. "}
{"citeStart": 38, "citeEnd": 59, "citeStartToken": 38, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "In the framework of the EM algorithm (Dempster et al., 1977) , we can formalize clustering as an estimation problem for a latent class (LC) model as follows. We are given: (i) a sample space y of observed, incomplete data, corre-17: scalar change sponding to pairs from VxN, (ii) a sample space X of unobserved, complete data, corresponding", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "p(v,n) = ~~p(c,v,n)= ~-']p(c)p(vJc)p(nJc) cEC cEC The joint distribution p(c,v,n) is defined by p(c, v, n) = p(c)p(vlc)p(", "mid_sen": "In the framework of the EM algorithm (Dempster et al., 1977) , we can formalize clustering as an estimation problem for a latent class (LC) model as follows. ", "after_sen": "We are given: (i) a sample space y of observed, incomplete data, corre-17: scalar change sponding to pairs from VxN, (ii) a sample space X of unobserved, complete data, corresponding"}
{"citeStart": 47, "citeEnd": 66, "citeStartToken": 47, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "We calculate the lexical similarity between the two sentences based on BOW. We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. We did not use HIRAGANA expressions because they are also used in function words. Next, to evaluate inter annotator agreement, 207 randomly selected statement pairs were annotated by two human annotators. The annotators agreed in their judgment for 81.6% of the examples, which corresponds to a kappa level of 0.49. The annotation results are evaluated by calculating recall and precision in which one annotation result is treated as a gold standard and the other's as the output of the system, as shown in Talbe 2. ", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We calculate the lexical similarity between the two sentences based on BOW. ", "mid_sen": "We also used hyponym and synonym dictionaries (Sumida et al., 2008) and a database of relations between predicate argument structures (Matsuyoshi et al., 2008) as resources. ", "after_sen": "According to our preliminary experiments, unigrams of KANJI and KATAKANA expressions, single and compound nouns, verbs and adjectives worked well as features, and we calculate the similarity using cosine distance. "}
{"citeStart": 181, "citeEnd": 204, "citeStartToken": 181, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994 ). In addition, it is crucial for lexical choice and tense selection in machine translation (Moens and Steedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992) . Table 1 sun~narizes the three aspectual distinctions, which compose five aspectual categories. In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) from extended events, which have a time duration (e.g., She ran to the store). Therefore, four classes of events are derived: culmination, culminated process, process, and point.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "the second sentence describes a state, which begins before the event described by the first sentence.", "mid_sen": "Aspectual classification is also a necessary prerequisite for interpreting certain adverbial adjuncts, as well as identifying temporal constraints between sentences in a discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994 ). ", "after_sen": "In addition, it is crucial for lexical choice and tense selection in machine translation (Moens and Steedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992) . "}
{"citeStart": 142, "citeEnd": 156, "citeStartToken": 142, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the DL learner as described in Collins and Singer (1999) , motivated by its success in the related tasks of word sense disambiguation (Yarowsky, 1995) and NE classification (Collins and Singer, 1999) . We apply add-one smoothing to smooth the class posteriors.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use the DL learner as described in Collins and Singer (1999) , motivated by its success in the related tasks of word sense disambiguation (Yarowsky, 1995) and NE classification (Collins and Singer, 1999) . ", "after_sen": "We apply add-one smoothing to smooth the class posteriors."}
{"citeStart": 82, "citeEnd": 104, "citeStartToken": 82, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "10 Both algorithms are provided by Weka (http://www.cs.waikato.ac.nz/ml/weka/). provide complementary perspectives. While the former can directly tell us what features are most useful, the latter gives feature ranks which provide more detailed information about differences between features. To compare the feature selection results, we examine the four kind of helpfulness models for each of the three feature sets separately, as presented below. Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. (Details of how the average-expert model performs can be found in our prior work (Xiong and Litman, 2011) .) Table 4 presents the feature selection results of computational linguistic features used in modeling the four different types of peer-review helpfulness. The first row lists the four sources of helpfulness ratings, and each column represents a corresponding model. The second row presents the most useful features in each model selected by stepwise LR, where \"# of folds\" refers to the number of trials in which the given feature appears in the resulting feature set during the 10-fold cross validation. Here we only report features that are selected by no less than five folds (half the time). The third row presents feature ranks computed using Relief, where we only report the top six features due to the space limit. Features are ordered in descending ranks, and the average merit and its standard deviation is reported for each one of the features.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that the focus of this paper is comparing feature utilities in different helpfulness models rather than predicting those types of helpfulness ratings. ", "mid_sen": "(Details of how the average-expert model performs can be found in our prior work (Xiong and Litman, 2011) .) Table 4 presents the feature selection results of computational linguistic features used in modeling the four different types of peer-review helpfulness. ", "after_sen": "The first row lists the four sources of helpfulness ratings, and each column represents a corresponding model. "}
{"citeStart": 40, "citeEnd": 49, "citeStartToken": 40, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "Gerdemann and G6tz's Troll system (see [G6Tz 1993] , [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. Loosely speaking, the resolvants of a feature structure have the same underlying finite state automaton as the feature structure, and differ only in their output fllnction. Troll exploits this property to represent each feature structure as a finite state automaton and a set of output flmctions. The '1¥oll unifier is closed on these representations. Thus, though RES is computationally expensive, Troll uses RES only during compilation, never during run time.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Proof, From theorem 24 and proposition 26.", "mid_sen": "Gerdemann and G6tz's Troll system (see [G6Tz 1993] , [GFRDEIVIANN AND KING 1994] and [GERDEMANN (FC)]) employs an efficient refinement of RES to test the satisfiability of feature structures. ", "after_sen": "In fact, Troll represents each feature structure as a disjunction of the resolvants of the feature structure. "}
{"citeStart": 41, "citeEnd": 57, "citeStartToken": 41, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past few years there has been a flourishing of interest in the study of word collocations. A common method to extract collocations is using windowing techniques for the extraction of word associations. In (Zernik 1990; Calzolari and Bindi 1990; Smadja 1989; Church and Hanks 1990) associations are detected in a ±5 window. A wider window (± tO0 words) is used in (Gale et al. 1992) . Windowing techniques are also used in (Jelinek et al, 1990) , where it is proposed a trigram model to automatically derive, and refine, context-free rules of the grammar (Fujisaki et al, 1991) .", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (Zernik 1990; Calzolari and Bindi 1990; Smadja 1989; Church and Hanks 1990) associations are detected in a ±5 window. ", "mid_sen": "A wider window (± tO0 words) is used in (Gale et al. 1992) . ", "after_sen": "Windowing techniques are also used in (Jelinek et al, 1990) , where it is proposed a trigram model to automatically derive, and refine, context-free rules of the grammar (Fujisaki et al, 1991) ."}
{"citeStart": 158, "citeEnd": 181, "citeStartToken": 158, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005) . For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation. An +LM item of node v has the form (v a⋆b ), where a and b are the target-language boundary words. For example, (VP held ⋆ Sharon", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. ", "mid_sen": "Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation. ", "after_sen": "An +LM item of node v has the form (v a⋆b ), where a and b are the target-language boundary words. "}
{"citeStart": 177, "citeEnd": 204, "citeStartToken": 177, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "The study and the first results we have here presented cover from lexical semantics to discourse structures, with strong interactions between these two ends. Indeed, lexical informations can be used to disambiguate the structure of the discours, as well as discourse relations can be used to disambiguate lexical entries, as shown in (Asher & Sablayrolles, 1994b) . Our work is based on systematic and very detailed linguistic studies which lead to rather complex computations for calculat-ing the spatiotemporal semantics of a motion complex. But we have seen that this level of detail and complexity is necessary if one want to understand, to formalize and to compute a right spatiotemporal semantics for motion complexes. We continue our investigations on two directions: 1. we compare our results with similar works in course of realization on the Basquian language (by Michel Aurnague) and on the Japanese language (by Junichi Saito);", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The study and the first results we have here presented cover from lexical semantics to discourse structures, with strong interactions between these two ends. ", "mid_sen": "Indeed, lexical informations can be used to disambiguate the structure of the discours, as well as discourse relations can be used to disambiguate lexical entries, as shown in (Asher & Sablayrolles, 1994b) . ", "after_sen": "Our work is based on systematic and very detailed linguistic studies which lead to rather complex computations for calculat-ing the spatiotemporal semantics of a motion complex. "}
{"citeStart": 175, "citeEnd": 180, "citeStartToken": 175, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "The BFP algorithm added linguistic constraints on CONTRA-INDEXING to the centering framework. These constraints are exemplified by the fact that, in the sentence he Hkes him, the entity cospecified by he cannot be the same as that cospecified by him. We say that he and him are CONTRA-INDEXED. The BFP algorithm depends on semantic processing to precompute these constraints, since they are derived from the syntactic structure, and depend on some notion of c-command [Rei76] . The other assumption that is dependent on syntax is that the the representations of discourse entities can be marked with the grammatical function through which they were realized, e.g. subject.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We say that he and him are CONTRA-INDEXED. ", "mid_sen": "The BFP algorithm depends on semantic processing to precompute these constraints, since they are derived from the syntactic structure, and depend on some notion of c-command [Rei76] . ", "after_sen": "The other assumption that is dependent on syntax is that the the representations of discourse entities can be marked with the grammatical function through which they were realized, e.g. subject."}
{"citeStart": 5, "citeEnd": 17, "citeStartToken": 5, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "When (2) is applied to the predicate, (15) will result after 13-reduction. However, under first-order unification, this needs to simulated by having the variable z in Az.run(z) unify both with Bill and John, and this is not possible. See (Jowsey, 1990) and (Moore, 1989 ) for a thorough discussion. (Moore, 1989) suggests that the way to overcome this problem is to use explicit A-terms and encode /~-reduction to perform the needed reduction. For example, the logical form in (3) would be produced, where X\\rtm(X) is the representation of Az.run (z).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, under first-order unification, this needs to simulated by having the variable z in Az.run(z) unify both with Bill and John, and this is not possible. ", "mid_sen": "See (Jowsey, 1990) and (Moore, 1989 ) for a thorough discussion. ", "after_sen": "(Moore, 1989) suggests that the way to overcome this problem is to use explicit A-terms and encode /~-reduction to perform the needed reduction. "}
{"citeStart": 0, "citeEnd": 16, "citeStartToken": 0, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. Yi et al. (2007) has made the first attempt working on the single semantic role level to make further improvement. However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level? Would that help the improvement of SRC?", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. ", "mid_sen": "Yi et al. (2007) has made the first attempt working on the single semantic role level to make further improvement. ", "after_sen": "However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. "}
{"citeStart": 42, "citeEnd": 59, "citeStartToken": 42, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "original: In [N early trading N] in [N Hong Kong N] [N Monday N] , [N gold N] Other representations for NP chunking can be used as well. An example is the representation used in (Ratnaparkhi, 1998) where all the chunkinitial words receive the same start tag (analogous to the B tag) while the remainder of the words in the chunk are paired with a different tag. This removes tagging ambiguities. In the Ratnaparkhi representation equal noun phrases receive the same tag sequence regardless of the context in which they appear.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In [N early trading N] in [N Hong Kong N] [N Monday N] , [N gold N] Other representations for NP chunking can be used as well. ", "mid_sen": "An example is the representation used in (Ratnaparkhi, 1998) where all the chunkinitial words receive the same start tag (analogous to the B tag) while the remainder of the words in the chunk are paired with a different tag. ", "after_sen": "This removes tagging ambiguities. "}
{"citeStart": 73, "citeEnd": 91, "citeStartToken": 73, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Early experiments also revealed that an ensemble based on a majority vote of all 81 classifiers performed rather poorly. The accuracy for interest was approximately 81% and line was disambiguated with slightly less than 80% accuracy. The lesson taken from these results was that an ensemble should consist of classifiers that represent as differently sized windows of context as possible; this reduces the impact of redundant errors made by classifiers that represent very similarly sized windows of context. The ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors. This is discussed in the context of combining part-of-speech taggers in (Brill and Wu, 1998) . They provide a measure for assessing the comple-mentarity of errors between two taggers that could be adapted for use with larger ensembles such as the one discussed here, which has nine members.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors. ", "mid_sen": "This is discussed in the context of combining part-of-speech taggers in (Brill and Wu, 1998) . ", "after_sen": "They provide a measure for assessing the comple-mentarity of errors between two taggers that could be adapted for use with larger ensembles such as the one discussed here, which has nine members."}
{"citeStart": 76, "citeEnd": 101, "citeStartToken": 76, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "Crucially, however, our approach describes the dynamics of interpretation explicitly and declaratively. We do not need to assume extra machinery beyond the encoding of words as PDDL planning operators; for example, our planning operators give a self-contained description of how each individual word contributes to resolving references. This makes our encoding more direct and transparent than those in work like Thomason and Hobbs (1997) and Stone et al. (2003) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do not need to assume extra machinery beyond the encoding of words as PDDL planning operators; for example, our planning operators give a self-contained description of how each individual word contributes to resolving references. ", "mid_sen": "This makes our encoding more direct and transparent than those in work like Thomason and Hobbs (1997) and Stone et al. (2003) .", "after_sen": "We present our encoding in a sequence of steps, each of which adds more linguistic information to the planning operators. "}
{"citeStart": 77, "citeEnd": 95, "citeStartToken": 77, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "We used two datasets, customer reviews (Hu and Liu, 2004) and movie reviews (Pang and Lee, 2005) to evaluate sentiment classification of sentences. Both of these two datasets are often used for evaluation in sentiment analysis researches. The number of examples and other statistics of the datasets are shown in Table 1 .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We used two datasets, customer reviews (Hu and Liu, 2004) and movie reviews (Pang and Lee, 2005) to evaluate sentiment classification of sentences. ", "after_sen": "Both of these two datasets are often used for evaluation in sentiment analysis researches. "}
{"citeStart": 153, "citeEnd": 162, "citeStartToken": 153, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "We work with parallel training corpora of 3.0 M Chinese-English sentence pairs (77.5 M Chinese / 81.0 M English running words after preprocessing) and 2.5 M Arabic-English sentence pairs (54.3 M Arabic / 55.3 M English running words after preprocessing), respectively. Word alignments are created by aligning the data in both directions with GIZA++ and symmetrizing the two trained alignments (Och and Ney, 2003) . When extracting phrases, we apply several restrictions, in particular a maximum length of ten on source and target side for lexical phrases, a length limit of five on source and ten on target side for hierarchical phrases (including non-terminal symbols), and no more than two gaps per phrase. The decoder loads only the best translation options per distinct source side with respect to the weighted phrase-level model scores (100 for Chinese, 50 for Arabic). The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002) . During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S . Model weights are optimized with MERT (Och, 2003) on 100-best lists. The optimized weights are obtained (separately for deep and for shallow-1 grammars) with a k-best generation size of 1 000 for Chinese→English and of 500 for Arabic→English and kept for all setups. We employ MT06 as development sets. Translation quality is measured in truecase with BLEU (Papineni et al., 2002) on the MT08 test sets. Data statistics for the preprocessed source sides of both the Chinese→English MT08 test set and the Arabic→English MT08 test set are given in Table 1 .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The language models are 4-grams with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) which have been trained with the SRILM toolkit (Stolcke, 2002) . ", "mid_sen": "During decoding, a maximum length constraint of ten is applied to all non-terminals except the initial symbol S . Model weights are optimized with MERT (Och, 2003) on 100-best lists. ", "after_sen": "The optimized weights are obtained (separately for deep and for shallow-1 grammars) with a k-best generation size of 1 000 for Chinese→English and of 500 for Arabic→English and kept for all setups. "}
{"citeStart": 83, "citeEnd": 100, "citeStartToken": 83, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system. While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach. Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU. The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation. This approach used the same set of features as the alignment template approach in (Och and Ney, 2002) .", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation. ", "mid_sen": "This approach used the same set of features as the alignment template approach in (Och and Ney, 2002) .", "after_sen": "SMT Team (2003) also used minimum error training as in Och (2003) , but used a large number of feature functions. "}
{"citeStart": 93, "citeEnd": 108, "citeStartToken": 93, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "A generic noun phrase (with a countable head noun) can generally be expressed in three ways (Huddleston 1984) . We call these GEN 'a', where the noun phrase is indefinite: A mammoth is a mammal; GEN 'the', where the noun phrase is definite: The mammoth is a mammal; and GEN ~/5, where there is no article: Mammoths are mammals. Uncountable nouns and pluralia tanta can only he expressed by GEN </~ (eg: Furniture is expensive). They cannot take GEN 'a' because they cannot be moditied by a. They do not take GEN 'the', because then the noun phrase woukl normally be interpreted as having detinite reference. Nouns that can be either countable or uncountable also only take GEN (,: Cake is delicious/Cakes are delicious. These combinations are shown in Table 2 , noun phrases that can not be used to show generic reference are marked *. The use all three kinds of generic noun phrases is not acceptable in some cofltexts, for example * a mammoth evolved. Sometimes a noun phrase can be ambiguous, for example 1 like the elel~hant, where the speaker could like a particular elephant, or all elephants. Because the use of GEN (~ is acceptable in all Contexts, AI,T-J/E generates all generic noun phrases as such, that is as bare noun phrases. The number of the noun phrase is then determined by the countability preference of the noun phrase heading it. Fully countable nouns and pluralia tanta will be plural, all others are singular.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A generic noun phrase (with a countable head noun) can generally be expressed in three ways (Huddleston 1984) . ", "after_sen": "We call these GEN 'a', where the noun phrase is indefinite: A mammoth is a mammal; GEN 'the', where the noun phrase is definite: "}
{"citeStart": 141, "citeEnd": 165, "citeStartToken": 141, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "word sense disambiguation, information retrieval, natural language generation and so on. Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "word sense disambiguation, information retrieval, natural language generation and so on. ", "mid_sen": "Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). ", "after_sen": "However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) )."}
{"citeStart": 54, "citeEnd": 72, "citeStartToken": 54, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Other potential applications apply the hypothesised relationship (Harris, 1968 ) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other potential applications apply the hypothesised relationship (Harris, 1968 ) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. ", "mid_sen": "One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain.", "after_sen": "However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). "}
{"citeStart": 148, "citeEnd": 153, "citeStartToken": 148, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "Why should we, as computational linguists, be interested in factors that contribute to the interactivity of a discourse? There are both theoretical and practical motivations. First, we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant, multi-utterance discourses [Po186, CP86] . Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . Since conversation is a collaborative process [CWG86, SSJ74] , models of conversation can provide the basis for extending planning theories [GS90, CLNO90] . When the situation requires the negotiation of a collaborative plan, these theories must account for the interacting beliefs and intentions of multiple participants. ~,From a practical perspective, there is ample evidence that limited mixed-initiative has contributed to lack of system usability. Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems: They don't allow users to participate in the reasoning process, or to ask the questions they want answered [PHW82, Kid85, FL89] . In addition, question answering systems often fail to take account of the system's role as a conversational partner.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are both theoretical and practical motivations. ", "mid_sen": "First, we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant, multi-utterance discourses [Po186, CP86] . ", "after_sen": "Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . "}
{"citeStart": 4, "citeEnd": 19, "citeStartToken": 4, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "The major topic in the development of word-Pos guessers is the strategy which is to be used for the acquisition of the guessing rules. A rule-based tagger described in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition. A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources. In (Zhang&Kim, 1990) a system for the automated learning of morphological word-formation rules is described. This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features. Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus. A statistical-based suffix learner is presented in (Schmid, 1994) . From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure. Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art, they require specially prepared training data --a pretagged training corpus, training examples, etc.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources. ", "mid_sen": "In (Zhang&Kim, 1990) a system for the automated learning of morphological word-formation rules is described. ", "after_sen": "This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features. "}
{"citeStart": 95, "citeEnd": 119, "citeStartToken": 95, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++ 2 Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Then these alignments are used to extract the phrase pairs.", "mid_sen": "We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. ", "after_sen": "This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++ 2 Toolkit and POS information. "}
{"citeStart": 150, "citeEnd": 168, "citeStartToken": 150, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "Natural language involves statements that do not contain complete, exact, and unbiased information. Many of these are subjective, which share the common property described in narrative theory (Banfield, 1982) as \"(subjective statements) must all be referred to the speaking subject for interpretation\". Wiebe (1990) further adapted this definition of subjectivity to be \"the linguistic expression of private states (Quirk et al., 1985) \". So far, linguistic cues have played an important role in research of subjectivity recognition (e.g. (Wilson et al., 2006) ), sentiment analysis (e.g. Pang and Lee, 2004) ), and emotion studies (e.g. (Pennebaker et al., 2001) ). While most linguistic cues are grouped under the general rubric of subjectivity, they are usually originated from different dimensions, including:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Wiebe (1990) further adapted this definition of subjectivity to be \"the linguistic expression of private states (Quirk et al., 1985) \". ", "mid_sen": "So far, linguistic cues have played an important role in research of subjectivity recognition (e.g. (Wilson et al., 2006) ), sentiment analysis (e.g. Pang and Lee, 2004) ), and emotion studies (e.g. (Pennebaker et al., 2001) ). ", "after_sen": "While most linguistic cues are grouped under the general rubric of subjectivity, they are usually originated from different dimensions, including:"}
{"citeStart": 113, "citeEnd": 127, "citeStartToken": 113, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "The Model Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992) , (Church, 1988) , (HajiS, HladkA, 1997) ) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a \"direct\" approach to modeling, for which we have chosen an exponential probabilistic model. Such model (when predicting an event ~ y E Y in a context x) has the general form", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The Model Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992) , (Church, 1988) , (HajiS, HladkA, 1997) ) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a \"direct\" approach to modeling, for which we have chosen an exponential probabilistic model. ", "after_sen": "Such model (when predicting an event ~ y E Y in a context x) has the general form"}
{"citeStart": 240, "citeEnd": 252, "citeStartToken": 240, "citeEndToken": 252, "sectionName": "UNKNOWN SECTION NAME", "string": "The results of the evaluation are exlremely encouraging, especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs (Hearst, 1991; Cowie et al., 1992) . A note worth adding: it is not clear that the \"exact match\" criterion --that is, evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline --is the right task. In particular, in many tasks it is at least as important to avoid inappropriate senses than to select exactly the right one. This would be the case in query expansion for information retrieval, for example, where indiscriminately adding inappropriate words to a query can degrade performance (Voorhees, 1994) . The examples presented in Section 3 are encouraging in this regard: in addition to performing well at the task of assigning a high score to the best sense, it does a good job of assigning low scores to senses that are clearly inappropriate.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The results of the evaluation are exlremely encouraging, especially considering that disambiguating word senses to the level of fine-grainedness found in WordNet is quite a bit more difficult than disambiguation to the level of homographs (Hearst, 1991; Cowie et al., 1992) . ", "after_sen": "A note worth adding: it is not clear that the \"exact match\" criterion --that is, evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline --is the right task. "}
{"citeStart": 163, "citeEnd": 177, "citeStartToken": 163, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "As our second experimental goal, we compared the models SL-DOP and LS-DOP explained in Section 3.2. Recall that for n=1, SL-DOP is equal to the PCFG-reduction of Bod (2001) (which we also called Likelihood-DOP) while LS-DOP is equal to Simplicity-DOP. Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000) , who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002) , especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. ", "mid_sen": "This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000) , who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. ", "after_sen": "While SL-DOP and LS-DOP have been compared before in Bod (2002) , especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. "}
{"citeStart": 105, "citeEnd": 118, "citeStartToken": 105, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "The formalism described above has been implemented as part of ISSCO's ELU l°, an enhanced PATR-II style (Shieber, 1986) unification grammar environment, based on the UD system presented by Johnson and Rosner (1989) . ELU incorporates a parser and generatot, and is primarily intended for use as a tool for research in machine translation. Use of transfer rules in translation has not so far brought to light instances where the serial rule invocation regime described in section 3.2 proves necessary. ELU grammars permit the use of typed feature structures (cf. Johnson and Rosner, op. cit., Moens et al., 1989) in grammars; although the present transfer rule format does not, they are clearly a desirable addition, since they would provide a means of exerting control over rule interactions.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "9", "mid_sen": "The formalism described above has been implemented as part of ISSCO's ELU l°, an enhanced PATR-II style (Shieber, 1986) unification grammar environment, based on the UD system presented by Johnson and Rosner (1989) . ELU incorporates a parser and generatot, and is primarily intended for use as a tool for research in machine translation. ", "after_sen": "Use of transfer rules in translation has not so far brought to light instances where the serial rule invocation regime described in section 3.2 proves necessary. "}
{"citeStart": 35, "citeEnd": 55, "citeStartToken": 35, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996) , which is based on weighted finite-state transducers (FSTs). Our approach is similarly motivated but is based on a different mechanism: linear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat's system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001) . Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004) . They also use a unified approach to word breaking and OOV identification.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We propose a unified approach that solves both problems simultaneously. ", "mid_sen": "A previous work along this line is Sproat et al. (1996) , which is based on weighted finite-state transducers (FSTs). ", "after_sen": "Our approach is similarly motivated but is based on a different mechanism: linear mixture models. "}
{"citeStart": 73, "citeEnd": 88, "citeStartToken": 73, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000) . Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight. In our case the probabilities have been estimated using the maximum likelihood estimate, smoothed adding a small constant (0.1) when probabilities are zero. Decisions taken with negative values were discarded (Agirre & Martinez, 2001b) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "At present we have chosen one algorithm which does not combine features (Decision Lists) and another which does combine features (AdaBoost).", "mid_sen": "Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000) . ", "after_sen": "Features are weighted with a log-likelihood measure, and arranged in an ordered list according to their weight. "}
{"citeStart": 101, "citeEnd": 120, "citeStartToken": 101, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "c5.0 3 , a commercial version of c4.5 Quinlan, 1993, performs top-down induction of decision trees tdidt. On the basis of an instance base of examples, c5.0 constructs a decision tree which compresses the classi cation information in the instance base by exploiting differences in relative importance of di erent features. Instances are stored in the tree as paths A demo of the NP and VP chunker is available at http: www.sfb441.unituebingen.de ~dejean chunker.h tml 3 Available from http: www.rulequest.com of connected nodes ending in leaves which contain classi cation information. Nodes are connected via arcs denoting feature values. Feature information gain mutual information between features and class is used to determine the order in which features are employed as tests at all levels of the tree Quinlan, 1993 . With the full input representation words and POS tags, we were not able to run complete experiments. We therefore experimented only with the POS tags with a context of two left and right. We h a ve used the default parameter setting with decision trees combined with value grouping. We have used a nearest neighbor algorithm ib1-ig, here listed as MBL and a decision tree algorithm IGTree from the TiMBL learning package Daelemans et al., 1999b . Both algorithms store the training data and classify new items by choosing the most frequent classi cation among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. Each feature receives a weight which is based on the amount of information which it provides for computing the classi cation of the items in the training data. ib1-ig uses these weights for computing the distance between a pair of data items and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree Daelemans et al., 1999b . We will use their default parameters except for the ib1-ig parameter for the numberof examined nearest neighbors k which we have set to 3 Daelemans et al., 1999a . The classi ers use a left and right context of four words and partof-speech tags. For the four IO representations we have used a second processing stage which used a smaller context but which included information about the IO tags predicted by the rst processing phase Tjong Kim Sang, 2000. When building a classi er, one must gather evidence for predicting the correct class of an item from its context. The Maximum Entropy MaxEnt framework is especially suited for integrating evidence from various information sources. Frequencies of evidence class combinations called features are extracted from a sample corpus and considered to beproperties of the classi cation process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active i.e. a feature is active when the context meets the requirements given by the feature. For every class the weights of the active features are combined and the best scoring class is chosen Berger et al., 1996 . For the classier built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features consisting of one of the mentioned information sources and complex features combinations thereof were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software Dehaspe, 1997 . MBSL Argamon et al., 1999 uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP including complete phrases. These sequences may include a few context tags, up to a prespeci ed max context. During inference, MBSL tries to 'tile' each POS string with parts of noun-phrases from the memory. If the string could befully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm. Adding a context extends the possibilities for tiling, thereby giving more opportunities to better candidates. The approach of MBSL to the problem of identifying baseNPs is sequence-based rather than word-based, that is, decisions are taken per POS sequence, or per candidate, but not for a single word. In addition, the tiling process gives no preference to any direction in the sentence. The tiles may b e of any length, up to the maximal length of a phrase in the training data, which gives MBSL a generalization power that compensates for the setup of using only POS tags. The results presented here were obtained by optimizing MBSL parameters based on 5-fold CV on the training data.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "During evaluation it is tested which features are active i.e. a feature is active when the context meets the requirements given by the feature. ", "mid_sen": "For every class the weights of the active features are combined and the best scoring class is chosen Berger et al., 1996 . ", "after_sen": "For the classier built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. "}
{"citeStart": 75, "citeEnd": 88, "citeStartToken": 75, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories, (see e.g. (Katz and Fodor, 1963; Wilks, 1975) ). The induction of selectional preferences from corpus data was pioneered by Resnik (1996) . All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words. Resnik uses the WordNet noun hierarchy for generalization. His information-theoretic approach models the selectional preference strength of an argument position 1 r p of a predicate p as", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories, (see e.g. (Katz and Fodor, 1963; Wilks, 1975) ). ", "mid_sen": "The induction of selectional preferences from corpus data was pioneered by Resnik (1996) . ", "after_sen": "All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words. "}
{"citeStart": 94, "citeEnd": 121, "citeStartToken": 94, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "The second stage of our dependency parser is a reranker that operates on the output of the k-best MST parser. Features in this model are not constrained as in the edge-factored model. Many of the model features have been inspired by the constituency-based features presented in Charniak and Johnson (2005) . We have also included features that exploit non-projectivity where possible. The node-type is the same as defined for the MST model.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Features in this model are not constrained as in the edge-factored model. ", "mid_sen": "Many of the model features have been inspired by the constituency-based features presented in Charniak and Johnson (2005) . ", "after_sen": "We have also included features that exploit non-projectivity where possible. "}
{"citeStart": 16, "citeEnd": 32, "citeStartToken": 16, "citeEndToken": 32, "sectionName": "UNKNOWN SECTION NAME", "string": "Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998) . In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004) , where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002) . (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002) , keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech -meetings in particular. Thus our goal in this paper is to investi-gate whether we can successfully apply some existing techniques, as well as propose new approaches to extract keywords for the meeting domain. The aim of this study is to set up some starting points for research in this area.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998) . ", "mid_sen": "In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. ", "after_sen": "We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. "}
{"citeStart": 201, "citeEnd": 216, "citeStartToken": 201, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "What we have called a context-free λ-term grammar is nothing but an alternative notation for an abstract categorial grammar (de Groote, 2001) whose abstract vocabulary is second-order, with the restriction to linear λ-terms removed. In the linear case, Salvati (2005) has shown the recognition/parsing complexity to be PTIME, and exhibited an algorithm similar to Earley parsing for TAGs. Second-order linear ACGs are known to be expressive enough to encode well-known mildly context-sensitive grammar formalisms in a straightforward way, including TAGs and multiple context-free grammars (de Groote, 2002; de Groote and Pogodalla, 2004) .", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the linear case, Salvati (2005) has shown the recognition/parsing complexity to be PTIME, and exhibited an algorithm similar to Earley parsing for TAGs. ", "mid_sen": "Second-order linear ACGs are known to be expressive enough to encode well-known mildly context-sensitive grammar formalisms in a straightforward way, including TAGs and multiple context-free grammars (de Groote, 2002; de Groote and Pogodalla, 2004) .", "after_sen": "S(λy.X 1 (λz.z)y) :− A(X 1 ). A(λxy.a o→o (X 1 (λz.b o→o (x(c o→o z)))(d o→o y))) :− A(X 1 ). A(λxy.xy)."}
{"citeStart": 159, "citeEnd": 178, "citeStartToken": 159, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "The results are notable in demonstrating that very simple preprocessing of the parser input facilitates significant improvements in parser performance. We provide the first definitive results that word sense information can enhance Penn Treebank parser performance, building on earlier results of Bikel 2000and Xiong et al. (2005) . Given our simple procedure for incorporating lexical semantics into the parsing process, our hope is that this research will open the door to further gains using more sophisticated parsing models and richer semantic options.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results are notable in demonstrating that very simple preprocessing of the parser input facilitates significant improvements in parser performance. ", "mid_sen": "We provide the first definitive results that word sense information can enhance Penn Treebank parser performance, building on earlier results of Bikel 2000and Xiong et al. (2005) . ", "after_sen": "Given our simple procedure for incorporating lexical semantics into the parsing process, our hope is that this research will open the door to further gains using more sophisticated parsing models and richer semantic options."}
{"citeStart": 47, "citeEnd": 71, "citeStartToken": 47, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Noun phrase recognition can be divided in two tasks: recognizing base noun phrases and recognizing arbitrary noun phrases. Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995) . The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material 1. The baseNPs in this data are slightly different from the ones that can be derived from the Treebank, most notably in the attachment of genitive markers.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, it is not a baseNP since it contains two other noun phrases. ", "mid_sen": "Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995) . ", "after_sen": "The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material 1. "}
{"citeStart": 44, "citeEnd": 57, "citeStartToken": 44, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "The above methods can also be applied to other tasks in natural language processing such as chunking and POS tagging because the quadratic kernels give good results. Utsuro et al. (2001) report that a combination of two NE recognizers attained F = 84.07%, but wrong word boundary cases are excluded. Our system attained 85.04% and word boundaries are automatically adjusted. Yamada also reports that p B is best. Although his system attained F = 83.7% for 5-fold cross-validation of the CRL data , our system attained 86.8%. Since we followed Isozaki's implementation (Isozaki, 2001) , our system is different from Yamada's system in the following points: 1) adjustment of word boundaries, 2) ChaSen's parameters for unknown words, 3) character types, 4) use of the Viterbi search.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although his system attained F = 83.7% for 5-fold cross-validation of the CRL data , our system attained 86.8%. ", "mid_sen": "Since we followed Isozaki's implementation (Isozaki, 2001) , our system is different from Yamada's system in the following points: 1) adjustment of word boundaries, 2) ChaSen's parameters for unknown words, 3) character types, 4) use of the Viterbi search.", "after_sen": "For efficient classification, Burges and Schölkopf (1997) propose an approximation method that uses \"reduced set vectors\" instead of support vectors. "}
{"citeStart": 118, "citeEnd": 131, "citeStartToken": 118, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005 ). The TC-STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions (EPPS). In our experiments, we used the translations from Span- ish to English. The MT engines for this task had been trained on 1.2M sentence pairs (32M running words). Table 1 gives an overview of the test corpora, on which the enhanced hypotheses alignment was computed, and for which the consensus translations were determined. The official IWSLT04 test corpus was used for the IWSLT 04 tasks; the CSTAR03 test corpus was used for the speech translation task. The March 2005 test corpus of the TC-STAR evaluation (verbatim condition) was used for the EPPS task. In Table 1 , the number of running words in English is the average number of running words in the hypotheses, from which the consensus translation was computed; the vocabulary of English is the merged vocabulary of these hypotheses. For the BTEC IWSLT04 corpus, the statistics for English is given for the experiments described in Sections 3.3 and 3.5, respectively.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training.", "mid_sen": "Finally, we also computed consensus translation from some of the submissions to the TC-STAR 2005 evaluation campaign (TC-STAR, 2005 ). ", "after_sen": "The TC-STAR participants had submitted translations of manually transcribed speeches from the European Parliament Plenary Sessions (EPPS). "}
{"citeStart": 164, "citeEnd": 176, "citeStartToken": 164, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog, but instead needs to be implemented by additional programming. While theoretically possible, it becomes quite problematic to actually implement. The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While theoretically possible, it becomes quite problematic to actually implement. ", "mid_sen": "The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . ", "after_sen": "This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. "}
{"citeStart": 59, "citeEnd": 84, "citeStartToken": 59, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Applicative Categorial Grammar is the most basic form of Categorial Grammar, with just a single combination rule corresponding to function application. It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994) , it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994) , and by Lambek Categorial Grammars (Lambek 1958) . It is therefore worth giving some brief indications of how it fits in with these developments.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It was first applied to linguistic description by Adjukiewicz and Bar-Hillel in the 1950s. ", "mid_sen": "Although it is still used for linguistic description (e.g. Bouma and van Noord, 1994) , it has been somewhat overshadowed in recent years by HPSG (Pollard and Sag 1994) , and by Lambek Categorial Grammars (Lambek 1958) . ", "after_sen": "It is therefore worth giving some brief indications of how it fits in with these developments."}
{"citeStart": 146, "citeEnd": 169, "citeStartToken": 146, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991) , Resnik (1993b) , and Brill and Resnik (1994) , referred to respectively as LA, SA, and TEL.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the former exceeds the latter (by a certain margin) we attach it to verb, else if the latter exceeds the former (by the same margin) we attach it to noun1.", "mid_sen": "In our experiments, described below, we compare the performance of our proposed method, which we refer to as MDL, against the methods proposed by Hindle and Rooth (1991) , Resnik (1993b) , and Brill and Resnik (1994) , referred to respectively as LA, SA, and TEL.", "after_sen": "Data Set. "}
{"citeStart": 250, "citeEnd": 275, "citeStartToken": 250, "citeEndToken": 275, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ([Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes \"these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 45, "citeEnd": 60, "citeStartToken": 45, "citeEndToken": 60, "sectionName": "UNKNOWN SECTION NAME", "string": "Abney 2002compares two major kinds of unsupervised approach to classification (co-training and the Yarowsky algorithm). As we do not use multiple classifiers our approach is quite far from cotraining. But it is close to the paradigm described by Yarowsky (1995) and Turney (2002) as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples. But our approach does not use point-wise mutual information. Instead we use relative frequencies of newly found features in a training subcorpus produced by the previous iteration of the classifier. We also use the smallest possible seed vocabulary, containing just a single word; however there are no restrictions regarding the maximum number of items in the seed vocabulary.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As we do not use multiple classifiers our approach is quite far from cotraining. ", "mid_sen": "But it is close to the paradigm described by Yarowsky (1995) and Turney (2002) as it also employs self-training based on a relatively small seed data set which is incrementally enlarged with unlabelled samples. ", "after_sen": "But our approach does not use point-wise mutual information. "}
{"citeStart": 147, "citeEnd": 161, "citeStartToken": 147, "citeEndToken": 161, "sectionName": "UNKNOWN SECTION NAME", "string": "Intuitively, compared with co-occurrence-based thesauri, hand-crafted thesauri, such as WordNet, could provide more reliable terms for query expansion. However, previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet (Voorhees, 1994; Stairmand, 1997) . Although some researchers have shown that combining terms from both types of resources is effective, the benefit of query expansion using only manually created lexical resources remains unclear. The main challenge is how to assign appropriate weights to the expanded terms.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Intuitively, compared with co-occurrence-based thesauri, hand-crafted thesauri, such as WordNet, could provide more reliable terms for query expansion. ", "mid_sen": "However, previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet (Voorhees, 1994; Stairmand, 1997) . ", "after_sen": "Although some researchers have shown that combining terms from both types of resources is effective, the benefit of query expansion using only manually created lexical resources remains unclear. "}
{"citeStart": 70, "citeEnd": 93, "citeStartToken": 70, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "We are concerned here with a particular fact about gapping noticed by Levin and Prince (1982) , namely that gapping is acceptable only with the purely conjunctive symmetric meaning of and conjoining the clauses, and not with its causal asymmetric meaning (paraphraseable by \"and as a result\"). That is, while either of sentences (1) or (2) can have the purely conjunctive reading, only sentence (2) can be understood to mean that Hillary's becoming angry was caused by or came as a result of Bill's becoming upset.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(1) Bill became upset, and Hillary angry.", "mid_sen": "We are concerned here with a particular fact about gapping noticed by Levin and Prince (1982) , namely that gapping is acceptable only with the purely conjunctive symmetric meaning of and conjoining the clauses, and not with its causal asymmetric meaning (paraphraseable by \"and as a result\"). ", "after_sen": "That is, while either of sentences (1) or (2) can have the purely conjunctive reading, only sentence (2) can be understood to mean that Hillary's becoming angry was caused by or came as a result of Bill's becoming upset."}
{"citeStart": 216, "citeEnd": 230, "citeStartToken": 216, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "A wide range of XML-based tools for NLP applications lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. In processing the HOLJ documents we have built a pipeline using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000) , (Thompson et al., 1997) and the xmlperl program (McKelvie, 1999) . The overall processing stages contained in our pipeline are shown in Figure 1 .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A wide range of XML-based tools for NLP applications lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. ", "mid_sen": "In processing the HOLJ documents we have built a pipeline using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000) , (Thompson et al., 1997) and the xmlperl program (McKelvie, 1999) . ", "after_sen": "The overall processing stages contained in our pipeline are shown in Figure 1 ."}
{"citeStart": 103, "citeEnd": 124, "citeStartToken": 103, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "Our Earley generator and the described compiler for off-line grammar optimization have been extensively tested with a large HPSG grammar. This testgrammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994 ) in the Troll system (Gerdemann and King, 1994) . Testing the developed techniques uncovered important constraints on the form of the phrase structure rules in a grammar imposed by the compiler.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our Earley generator and the described compiler for off-line grammar optimization have been extensively tested with a large HPSG grammar. ", "mid_sen": "This testgrammar is based on the implementation of an analysis of partial vP topicalization in German (Hinrichs et al., 1994 ) in the Troll system (Gerdemann and King, 1994) . ", "after_sen": "Testing the developed techniques uncovered important constraints on the form of the phrase structure rules in a grammar imposed by the compiler."}
{"citeStart": 117, "citeEnd": 136, "citeStartToken": 117, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005) . the topics discussed and often had trouble deciding the important sentences or keywords. In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. Sometimes there are more possible keywords and the annotators felt it is hard to decide which five are the most topic indicative. Among the three annotators, we notice that in general the quality of annotator I is the poorest. This is based on the authors' judgment, and is also confirmed later by an independent human evaluation (in Section 6).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that these meetings are research discussions, and that the annotators may not be very familiar with", "mid_sen": "We selected these 27 meetings because they have been used in previous work for topic segmentation and summarization (Galley et al., 2003; Murray et al., 2005) . the topics discussed and often had trouble deciding the important sentences or keywords. ", "after_sen": "In addition, limiting the number of keywords that an annotator can select for a topic also created some difficulty. "}
{"citeStart": 42, "citeEnd": 62, "citeStartToken": 42, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "The WordNet Domains resource (Magnini and Cavaglia, 2000) assigns domain labels to synsets in WordNet. Since the focus of the WSJ corpus is on business and financial news, we can make use of WordNet Domains to select the set of nouns having at least one synset labeled with a business or finance related domain label. This is similar to the approach taken in (Koeling et al., 2005) where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domains. Hence, we select the subset of DSO nouns that have at least one synset labeled with any of these domain labels: commerce, enterprise, money, finance, banking, and economy. This gives a set of 21 nouns: book, business, center, community, condition, field, figure, house, interest, land, line, money, need, number, order, part, power, society, term, use, value. 2 For each noun, all the BC examples are used as out-of-domain training data. One-third of the WSJ examples for each noun are set aside as evaluation Table 1 shows some information about these 21 nouns. For instance, these nouns have an average of 6.7 senses in BC and 6.8 senses in WSJ. This is slightly higher than the 5.8 senses per verb in (Chen et al., 2006) , where the experiments were conducted using coarse-grained evaluation. Assuming we have access to an \"oracle\" which determines the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data perfectly, and we assign this most frequent sense to each noun in the test data, we will have achieved an accuracy of 61.1% as shown in the column MFS accuracy of Table 1. Finally, we note that we have an average of BC training examples and 406 WSJ adaptation examples per noun.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the focus of the WSJ corpus is on business and financial news, we can make use of WordNet Domains to select the set of nouns having at least one synset labeled with a business or finance related domain label. ", "mid_sen": "This is similar to the approach taken in (Koeling et al., 2005) where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domains. ", "after_sen": "Hence, we select the subset of DSO nouns that have at least one synset labeled with any of these domain labels: commerce, enterprise, money, finance, banking, and economy. "}
{"citeStart": 82, "citeEnd": 94, "citeStartToken": 82, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "An example is shown in Figure 4 where (w, k) = (3, 4) for POS tags, and (w, k) = (1, 2) for words. In this example the word \"how\" is the designated word with POS tag \"WRB\". \"0\" marks the position of the current word (tag) if it is not part of the feature, and \"(how)\" or \"(WI:tB)\" marks the position of the current word (tag) if it is part of the current feature. The distance of a conjunction from the current word (tag) can be induced by the placement of the special character %\" in the feature. We do not consider mixed features between words and POS tags as in (l:tamshaw and Marcus, 1995) , that is, a single feature consists of either words or tags.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The distance of a conjunction from the current word (tag) can be induced by the placement of the special character %\" in the feature. ", "mid_sen": "We do not consider mixed features between words and POS tags as in (l:tamshaw and Marcus, 1995) , that is, a single feature consists of either words or tags.", "after_sen": "Additionally, in the Inside/Outside model, the second predictor incorporates as features the OIB status of the w words before and after the designated word, and the conjunctions of size 2 of the words surrounding it. "}
{"citeStart": 51, "citeEnd": 71, "citeStartToken": 51, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we show that the strongly equivalent grammars enable the sharing of \"parsing techniques\", which are dependent on each computational framework and have never been shared among HPSG and LTAG communities. We apply our system to the latest version of the XTAG English grammar (The XTAG Research Group, 2001 ), which is a large-scale FB-LTAG grammar. A parsing experiment shows that an efficient HPSG parser with the obtained grammar achieved a significant speed-up against an existing LTAG parser ). This result implies that parsing techniques for HPSG are also beneficial for LTAG parsing. We can say that the grammar conversion enables us to share HPSG parsing techniques in LTAG parsing. Figure 1 depicts a brief sketch of the RenTAL system. The system consists of the following four modules: Tree converter, Type hierarchy extractor, Lexicon converter and Derivation translator. The tree converter module is a core module of the system, which is an implementation of the grammar conversion algorithm given in Section 3. The type hierarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse (Tateisi et al., 1998) . However, their method depended on translator's intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. ", "mid_sen": "The derivation translator module takes HPSG parse (Tateisi et al., 1998) . ", "after_sen": "However, their method depended on translator's intuitive analysis of the original grammar. "}
{"citeStart": 99, "citeEnd": 121, "citeStartToken": 99, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section we describe the stages of the cascade. The very first stage consists of a Memory-Based Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996) . The next three stages involve determining boundaries and labels of chunks. Chunks are nonrecursive, non-overlapping constituent parts of sentences (see (Abney, 1991) ). First, we simultaneously chunk sentences into: NP-, VP-, Prep-, ADJP-and APVP-chunks. As these chunks are non-overlapping, no words can belong to more than one chunk, and thus no conflicts can arise. Prep-chunks are the prepositional part of PPs, thus excluding the nominal part. Then we join a Prep-chunk and one -or more coordinated --NP-chunks into a PPchunk. Finally, we assign adverbial function (ADVFUNC) labels (e.g. locative or temporal) to all chunks.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section we describe the stages of the cascade. ", "mid_sen": "The very first stage consists of a Memory-Based Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996) . ", "after_sen": "The next three stages involve determining boundaries and labels of chunks. "}
{"citeStart": 255, "citeEnd": 279, "citeStartToken": 255, "citeEndToken": 279, "sectionName": "UNKNOWN SECTION NAME", "string": "Syntax The allowed sequences of morphemes, and the syntactic and semantic properties of morphemes and of the words derived by combining them, are specified by morphosyntactic production rules (dimension (b)) and lexical entries both for affixes (dimension (b)) and for roots (dimension (c)), essentially as described by Alshawi (1992) (where the production rules are referred to as \"morphology rules\"). Affixes may appear explicitly in production rules or, like roots, they may be assigned complex feature-valued categories. Information, including the creation of logical forms, is passed between constituents in a rule by the sharing of variables. These feature-augmented production rules are just the same device as those used in the CLE's syntactico-semantic descriptions, and are a much more natural way to express morphotactic information than finite-state devices such as continuation classes (see Trost and Matiasek, 1994 , for a related approach).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Information, including the creation of logical forms, is passed between constituents in a rule by the sharing of variables. ", "mid_sen": "These feature-augmented production rules are just the same device as those used in the CLE's syntactico-semantic descriptions, and are a much more natural way to express morphotactic information than finite-state devices such as continuation classes (see Trost and Matiasek, 1994 , for a related approach).", "after_sen": "The syntactic and semantic production rules for deriving the feminine singular of a French adjective by suffixation with \"e\" are given, with some details omitted, in Figure 3 . "}
{"citeStart": 87, "citeEnd": 108, "citeStartToken": 87, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "A recent study by also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006) , with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an objective function which allows them to do so; whereas, we are interested in allowing for multiple objective functions in order to adapt the parser model parameters to downstream tasks or alternative intrinsic (parsing) objectives.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. ", "mid_sen": "The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006) , with the exception that the new parse data is targeted to produce accurate word reorderings. ", "after_sen": "Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. "}
{"citeStart": 111, "citeEnd": 124, "citeStartToken": 111, "citeEndToken": 124, "sectionName": "UNKNOWN SECTION NAME", "string": "This account also explains similar differences in felicity for other coordinating conjunctions as discussed in Kehler (1994) , as well as why gapping is infelicitous in constructions with subordinating conjunctions indicating Coherent Situation relations, as exemplified in (16).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since a gapped clause in and of itself has no sentence-level semantics, the gapping fails to be felicitous in these cases.", "mid_sen": "This account also explains similar differences in felicity for other coordinating conjunctions as discussed in Kehler (1994) , as well as why gapping is infelicitous in constructions with subordinating conjunctions indicating Coherent Situation relations, as exemplified in (16).", "after_sen": "(16) # Bill became upset, { because } even though Hillary angry. "}
{"citeStart": 199, "citeEnd": 214, "citeStartToken": 199, "citeEndToken": 214, "sectionName": "UNKNOWN SECTION NAME", "string": "murder (x (y)) Agent Theme fear (x (y)) Exp Theme To abstract away from language-particular case systems and mapping of thematic roles to grammatical functions, I assume the Applicative Hierarchy of Shaumyan (1987) for the definition of prominence:", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As an illustration, the predicate-argument structures of the agentive verb murder and the psychological verb fear are (Grimshaw, 1990, p.8) :", "mid_sen": "murder (x (y)) Agent Theme fear (x (y)) Exp Theme To abstract away from language-particular case systems and mapping of thematic roles to grammatical functions, I assume the Applicative Hierarchy of Shaumyan (1987) for the definition of prominence:", "after_sen": "Primary Term > Secondary Term > Tertiary Term > Oblique Term. "}
{"citeStart": 46, "citeEnd": 64, "citeStartToken": 46, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper we presented a novel semantic space model that enriches traditional vector-based models with syntactic information. The model is highly general and can be optimised for different tasks. It extends prior work on syntax-based models (Grefenstette, 1994; Lin, 1998) , by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The model is highly general and can be optimised for different tasks. ", "mid_sen": "It extends prior work on syntax-based models (Grefenstette, 1994; Lin, 1998) , by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space.", "after_sen": "Our approach differs from Lin (1998) in three important ways: (a) by introducing dependency paths we can capture non-immediate relationships between words (i.e., between subjects and objects), whereas Lin considers only local context (dependency edges in our terminology); the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; (b) Lin creates the semantic space from the set of dependency edges that are relevant for a given word; by introducing dependency labels and the path value function we can selectively weight the importance of different labels (e.g., subject, object, modifier) and parametrize the space accordingly for different tasks; (c) considerable flexibility is allowed in our formulation for selecting the dimensions of the semantic space; the latter can be words (see the leaves in Figure 1 ), parts of speech or dependency edges; in Lin's approach, it is only dependency edges (features in his terminology) that form the dimensions of the semantic space."}
{"citeStart": 96, "citeEnd": 118, "citeStartToken": 96, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Another line of research that is correlated with ours is recognition of agreement/disagreement (Misra and Walker, 2013; Yin et al., 2012; Abbott et al., 2011; Andreas et al., 2012; Galley et al., 2004; Hillard et al., 2003) and classification of stances (Walker et al., 2012; Somasundaran and Wiebe, 2010) in online forums. For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013) , and thread structure (Murakami and Raymond, 2010; Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The corpus and guidelines will also be shared with the research community.", "mid_sen": "Another line of research that is correlated with ours is recognition of agreement/disagreement (Misra and Walker, 2013; Yin et al., 2012; Abbott et al., 2011; Andreas et al., 2012; Galley et al., 2004; Hillard et al., 2003) and classification of stances (Walker et al., 2012; Somasundaran and Wiebe, 2010) in online forums. ", "after_sen": "For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013) , and thread structure (Murakami and Raymond, 2010; Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy."}
{"citeStart": 170, "citeEnd": 185, "citeStartToken": 170, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Different concepts have been used in the literature as primitives. ", "mid_sen": "These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. ", "after_sen": "An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. "}
{"citeStart": 147, "citeEnd": 165, "citeStartToken": 147, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al. 1983) . One further possibility is to choose a single syntax tree, and to use destructive tree operations later in the parse a. The approach which we will adopt here is based on Milward (1992 Milward ( , 1994 . Partial syntax trees can be regarded as performing two main roles. The first is to provide syntactic information which guides how the rest of the sentence can be integrated into the tree. The second is to provide a basis for a semantic representation. The first role can be captured using syntactic types, where each type corresponds to a potentially infinite number of partial syntax trees. The second role can be captured by the parser constructing semantic representations directly. The general processing model therefore consists of transitions of the form:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "S /\\ np vp Mary / \\ V S thinks / \\ np vp^ John", "mid_sen": "M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al. 1983) . ", "after_sen": "One further possibility is to choose a single syntax tree, and to use destructive tree operations later in the parse a. "}
{"citeStart": 193, "citeEnd": 203, "citeStartToken": 193, "citeEndToken": 203, "sectionName": "UNKNOWN SECTION NAME", "string": "Our search procedure is to optimize one weight at a time, holding all others fixed, and iterating through the set of weights to be set. The objective function describing the total dependency length of the corpus is piecewise constant, as the dependency length will not change until one weight crosses another, causing two dependents to reverse order, at which point the total length will discontinuously jump. Nondifferentiability implies that methods based on gradient ascent will not apply. This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs, and we employ an optimization technique similar to that used by Och (2003) for machine translation. Because the objective function only changes at points where one weight crosses another's value, the set of segments of weight values with different values of the objective function can be exhaustively enumerated. In fact, the only significant points are the values of other weights for dependency types which occur in the corpus attached to the same head", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Nondifferentiability implies that methods based on gradient ascent will not apply. ", "mid_sen": "This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs, and we employ an optimization technique similar to that used by Och (2003) for machine translation. ", "after_sen": "Because the objective function only changes at points where one weight crosses another's value, the set of segments of weight values with different values of the objective function can be exhaustively enumerated. "}
{"citeStart": 46, "citeEnd": 84, "citeStartToken": 46, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "The delicacy of testsuite construction is acknowledged in EAGLES, 1996, p.37 . Although there are a number of e orts to construct reusable testsuites, none has to my knowledge explored how existing grammars can be exploited. Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Starting with Flickinger et al., 1987 , testsuites have been drawn up from a linguistic viewpoint, informed by the study of linguistics and re ecting the grammatical issues that linguists have concerned themselves with Flickinger et al., 1987, p.4 . Al-though the question is not explicitly addressed in Balkan, 1994 , all the testsuites reviewed there also seem to follow the same methodology. ", "mid_sen": "The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "after_sen": "The use of corpora with various levels of annotation has been studied, but the recommendations are that much manual work is required to turn corpus examples into test cases e.g., Balkan and Fouvry, 1995 . "}
{"citeStart": 333, "citeEnd": 350, "citeStartToken": 333, "citeEndToken": 350, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been successful attempts at using machine learning in search of a solution for linguistic tasks, e.g. discriminating between discourse and sentential senses of cues ([Litman 1996]) or resolution of coreferences in texts ([McCarthy & Lehnert 1995] ). Like our work, these problems are cast as classification problems, and then machine learning (mainly C4.5) techniques are used to induce classifiers for each class. What makes \"these applications different from ours is that they have worked on surface linguistic or mixed surface linguistic and intonational representation, and that the classes are relatively balanced, while in our case the class of compound sentences is much less numerous than the class of non-composite sentences. Such unbalanced classes create problems for the majority of inductive learning systems. A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. Other researchers, such as [Lawrence et al. 1996] , have compared neural networks and machine learning methods at the task of sentence classification. In this task, the system must classify a string as either grammatical or not. We do not content ourselves with results based on a grammatical/ungrammatical dichotomy. We are looking for heuristics, using relevant features, that will do better than the current ones and improve the overall performance of a natural language processor: this is a very difficult problem (see, e.g., [Huyck & Lytinen 1993] ). One could also look at this problem as one of optimisation of a rule-based system.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A distinctive feature of our work is the fact that we used machine learning techniques to improve an existing rule-based natural language processor from the inside. ", "mid_sen": "This contrasts with approaches where there are essentially no explicit rules, such as neural networks (e.g. [Buo 1996]) , or approaches where the machine learning algorithms attempt to infer--via deduction (e.g. [Samuelsson 1994] ), induction (e.g. [Theeramunkong et al. 1997] ; [Zelle & Mooney 1994] ) under user cooperation (e.g. [Simmons & Yu 1992] ; [Hermjakob & Mooney 1997] ), transformation-based error-driven learning (e.g. [Brill 1993]) , or even decision trees (e.g. [Magerman 1995])--a grammar from raw or preprocessed data. ", "after_sen": "In our work, we do not wish to acquire a grammar: we have one and want to devise a mechanism to make some of its parts adaptable to the corpus at hand or, to improve some aspect of its performance. "}
{"citeStart": 39, "citeEnd": 65, "citeStartToken": 39, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984) , which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing. Figure 8 shows the phonological phrase tree that is built from the syntactic structure of Figure 7 . The rules for building this tree apply from left to right, following the analysis we described in the preceding section. Figures  9-11 show the prosodic phrase derivation. Numbered nodes refer to salience values, with higher numbers indicating greater salience. The index is assigned according to phonological word count, with one point added for the node itself. Figure 11 is the final prosodic phrase tree; in the notation we have been using, the phrasing represented by Figure 11 is He told me I last night I[ he was coming to London II for several days. Figure 9 shows the effect of two applications of verb balancing. Applying from left to right, the rule first looks at the phonological verb he + told + me. Since the material to the left of the verb is null, the rule must group this verb with the constituent on its right to form the node labeled ®. On its second application, the rule balances the prosodic phrase it has just formed against the single phonological word to + London, and groups the verb rightward to form node ®.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following G&G, we require that the prosody rules build a binary tree whose terminals are phonological words and whose node labels are indices that mark boundary salience. ", "mid_sen": "An alternative representation based on Liberman and Prince (1977) is presented in Selkirk (1984) , which contends that prosody, including prosodic phrasing, is more properly represented as a grid instead of a tree. ", "after_sen": "Although a grid may be more descriptively suitable for some aspects of prosody (for example, Sproat and Liberman (1987) use the grid representation for their implementation of stress assignment in compound nominals), we are not aware of any evidence for or against a grid representation of discourseneutral phrasing. "}
{"citeStart": 241, "citeEnd": 255, "citeStartToken": 241, "citeEndToken": 255, "sectionName": "UNKNOWN SECTION NAME", "string": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . How to evaluate the different feature types' effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types' or feature types combination's predictive power for syntactic structure.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996] . ", "after_sen": "How to evaluate the different feature types' effects for syntactic parsing? "}
{"citeStart": 3, "citeEnd": 17, "citeStartToken": 3, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "In Kehler (1993b) , we presented an analysis of VPellipsis that distinguished between two types of relationship between clauses, parallel and non-parallel. An architecture was presented whereby utterances were parsed into propositional representations which were subsequently integrated into a discourse model. It was posited that VP-ellipsis could access either propositional or discourse model representations: in the case of parallel constructions, the source resided in the propositional representation; in the case of non-parallel constructions, the source had been integrated into the discourse model. In Kehler (1994) , we showed how this architecture also accounted for the facts that Levin and Prince noted about gapping.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "resolution to an account of discourse structure and coherence, namely our previous account (Kehler, 1993b) and the accounts of Priist (1992) and Asher (1993) .", "mid_sen": "In Kehler (1993b) , we presented an analysis of VPellipsis that distinguished between two types of relationship between clauses, parallel and non-parallel. ", "after_sen": "An architecture was presented whereby utterances were parsed into propositional representations which were subsequently integrated into a discourse model. "}
{"citeStart": 125, "citeEnd": 143, "citeStartToken": 125, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary disambiguation (Palmer et al. 1997) , parsing (Magerman 1995) and word segmentation (Meknavin et al. 1997) . We employ the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm for word extraction.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary disambiguation (Palmer et al. 1997) , parsing (Magerman 1995) and word segmentation (Meknavin et al. 1997) . ", "after_sen": "We employ the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm for word extraction."}
{"citeStart": 112, "citeEnd": 126, "citeStartToken": 112, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "If an ordered representation of the PAS is assumed as many theories do nowadays, its derivation from the surface string requires that the category assignment for case cues be rich enough in word order and grammatical function information to correctly place the arguments in the PAS. This work shows that these categories and their types can be uniquely characterized in the lexicon and tightly controlled in parsing. Spurious ambiguity problem is kept under control by normal form parsing on the syntactic side with the use of labelled categories in the grammar. Thus, the PAS of a derivation can be determined uniquely even in the presence of type shifting. The same strategy can account for deriving the PAS in unbounded constructions and non-constituent coordination (Bozsahin, 1997) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, the PAS of a derivation can be determined uniquely even in the presence of type shifting. ", "mid_sen": "The same strategy can account for deriving the PAS in unbounded constructions and non-constituent coordination (Bozsahin, 1997) .", "after_sen": "Parser's output (the combinatory form) is reduced to a PAS by normal order evaluation. "}
{"citeStart": 82, "citeEnd": 96, "citeStartToken": 82, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "The phrase structures in the Treebank (ttrees for short) are partially bracketed in the sense that arguments and modifiers are not structurally distinguished. In order to construct the etrees, which make such distinction, Lex-Tract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. Our strategy for choosing heads is similar to the one in (Collins, 1997) . An Argument Table informs LexTract what types of arguments a head can take. The Tagset Table  specifies what function tags always mark arguments (adjuncts, heads, respectively). Lex-Tract marks each sibling of a head as an argument if the sibling can be an argument of the head according to the Argument Table and none of the function tags of the sister indicates that it is an adjunct. For example, in Figure 5 , the head of the root S is the verb draft, and the verb has two siblings: the noun phrase policies is marked as an argument of the verb because from the Argument Table we know that verbs in general can take an NP object; the clause is marked as a modifier of the verb because, although verbs in general can take a sentential argument, the Tagset Table  informs LexTract that the function tag -MNR (manner) always marks a modifier.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to construct the etrees, which make such distinction, Lex-Tract requires its user to provide additional information in the form of three tables: a Head Percolation Table, an Argument Table, and a Tagset Table. ", "mid_sen": "A Head Percolation Table has previously been used in several statistical parsers (Magerman, 1995; Collins, 1997) to find heads of phrases. ", "after_sen": "Our strategy for choosing heads is similar to the one in (Collins, 1997) . "}
{"citeStart": 69, "citeEnd": 87, "citeStartToken": 69, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005) , the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes.", "mid_sen": "As has been previously observed and exploited in the NLP literature (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Barzilay and Lapata, 2005) , the above optimization function, unlike many others that have been proposed for graph or set partitioning, can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs. ", "after_sen": "In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision."}
{"citeStart": 175, "citeEnd": 196, "citeStartToken": 175, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "We have already mentioned that speech repairs constitute a good benchmark for studying the genera-tion and cancellation of pragmatic inferences along sequences of utterances (McRoy and Hirst, 1993) . Suppose, for example, that Jane has two friends --John Smith and John Pevler --and that her roommate Mary has met only John Smith, a married fellow. Assume now that Jane has a conversation with Mary in which Jane mentions only the name John because she is not aware that Mary does not know about the other John, who is a five-year-old boy. In this context, it is natural for Mary to become confused and to come to wrong conclusions. For example, Mary may reply that John is not a bachelor. Although this is true for both Johns, it is more appropriate for the married fellow than for the five-yearold boy. Mary knows that John Smith is a married male, so the utterance makes sense for her. At this point Jane realizes that Mary misunderstands her: all the time Jane was talking about John Pevler, the five-year-old boy. The utterances in (13) constitute a possible answer that Jane may give to Mary in order to clarify the problem. The first utterance in the sequence presupposes (14).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We have already mentioned that speech repairs constitute a good benchmark for studying the genera-tion and cancellation of pragmatic inferences along sequences of utterances (McRoy and Hirst, 1993) . ", "after_sen": "Suppose, for example, that Jane has two friends --John Smith and John Pevler --and that her roommate Mary has met only John Smith, a married fellow. "}
{"citeStart": 172, "citeEnd": 186, "citeStartToken": 172, "citeEndToken": 186, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. For instance, Grimshaw (1990) defines the thematic hierarchy as:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). ", "mid_sen": "PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . ", "after_sen": "PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . "}
{"citeStart": 179, "citeEnd": 190, "citeStartToken": 179, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. ", "mid_sen": "Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "after_sen": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. "}
{"citeStart": 20, "citeEnd": 38, "citeStartToken": 20, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "There have been many approaches to automatic detection of similar words from text corpora. Ours is similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There have been many approaches to automatic detection of similar words from text corpora. ", "mid_sen": "Ours is similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed.", "after_sen": "Evaluation of automatically generated lexical resources is a difficult problem. "}
{"citeStart": 56, "citeEnd": 79, "citeStartToken": 56, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "One widely used approach to evaluation is based on the notion of a reference answer (Hirschman et al., 1990 ). An agent's responses to a query are compared with a predefined key of minimum and maximum reference answers; performance is the proportion of responses that match the key. This approach has many widely acknowledged limitations (Hirschman and Pao, 1993; Danieli et al., 1992; Bates and Ayuso, 1993) , e.g., although there may be many potential dialogue strategies for carrying out a task, the key is tied to one particular dialogue strategy.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An agent's responses to a query are compared with a predefined key of minimum and maximum reference answers; performance is the proportion of responses that match the key. ", "mid_sen": "This approach has many widely acknowledged limitations (Hirschman and Pao, 1993; Danieli et al., 1992; Bates and Ayuso, 1993) , e.g., although there may be many potential dialogue strategies for carrying out a task, the key is tied to one particular dialogue strategy.", "after_sen": "In contrast, agents using different dialogue strategies can be compared with measures such as inappropriate utterance ratio, turn correction ratio, concept accuracy, implicit recovery and transaction success (Danieli LWe use the term agent to emphasize the fact that we are evaluating a speaking entity that may have a personality. "}
{"citeStart": 144, "citeEnd": 164, "citeStartToken": 144, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Although exploration of the tradeoffs between generative and discriminative machine learning techniques is one of the aims of this work, our ultimate goal, however, is to build clinical systems that provide timely access to information essential to the patient treatment process. In truth, our crossvalidation experiments do not correspond to any meaningful naturally-occurring task-structured abstracts are, after all, already appropriately labeled. The true utility of content models is to structure abstracts that have no structure to begin with. Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. Such a component would serve as the first stage of a clinical question answering system (Demner-Fushman and Lin, 2005) or summarization system (McKeown et al., 2003) . We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured. Table 3 (b) shows the effectiveness of our trained content models on abstracts that had no explicit structure to begin with. We can see that although classification accuracy is lower than that from our cross-validation experiments, performance is quite respectable. Thus, our hypothesis that unstructured abstracts are not qualitatively different from structured abstracts appears to be mostly valid.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, our exploratory experiments in applying content models trained with structured RCTs on unstructured RCTs is a closer approximation of an extrinsically-valid measure of performance. ", "mid_sen": "Such a component would serve as the first stage of a clinical question answering system (Demner-Fushman and Lin, 2005) or summarization system (McKeown et al., 2003) . ", "after_sen": "We chose to focus on randomized controlled trials because they represent the standard benchmark by which all other clinical studies are measured. "}
{"citeStart": 78, "citeEnd": 106, "citeStartToken": 78, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003) . Recent computational work either focuses on sentence 'subjectivity' (Wiebe et al. 2002; Riloff et al. 2003) , concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002) , or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003) .", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recent computational work either focuses on sentence 'subjectivity' (Wiebe et al. 2002; Riloff et al. 2003) , concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002) , or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. ", "mid_sen": "We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003) .", "after_sen": "Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. "}
{"citeStart": 161, "citeEnd": 171, "citeStartToken": 161, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "To implement the method mentioned in Section 2, we use the weights W of topics and foci, the distance D, the definiteness P, and the semantic similarity S (in R4 of Section 3.2) to determine points. The weights W oftopics and foci are given in Table 3 and  Table 4 respectively in Section 2, and represent the preferability of the desired antecedent. In this work, a topic is defined as a theme which is described, and a focus is defined as a word which is stressed by the speaker (or the writer). But we cannot detect topics and foci correctly. Therefore we approximated them as shown in Table 3 and Table 4 . The distance D is the number of the topics (foci) between the anaphor and a possible antecedent which is a topic (focus). The value P is given by the score of the definiteness in referential property analysis (Murata and Nagao, 1993) . This is because it is easier for a definite noun phrase to have an antecedent than for an indefinite noun phrase to have one. The value S is the semantic similarity between a possible antecedent and Noun X of \"Noun X no Noun Y.\" Semantic similarity is shown by level in Bunrui Goi Hyou (NLRI, 1964) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is because it is easier for a definite noun phrase to have an antecedent than for an indefinite noun phrase to have one. ", "mid_sen": "The value S is the semantic similarity between a possible antecedent and Noun X of \"Noun X no Noun Y.\" Semantic similarity is shown by level in Bunrui Goi Hyou (NLRI, 1964) .", "after_sen": ""}
{"citeStart": 48, "citeEnd": 73, "citeStartToken": 48, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Future work will involve testing our approach with higher-discrimination datasets, developing methods to pre-process review texts (e.g., improved negation tagging, and incorporating partof-speech tagging), and further addressing the problem of overfitting. To this effect we will investigate different feature selection algorithms, e.g., (Weston et al., 2003) , and their utilisation within the classifier trees. We also propose to consider aspects of reviews (Snyder and Barzilay, 2007) , and investigate other methods that measure class similarity, such as selecting typical instances (Zhang, 1992) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To this effect we will investigate different feature selection algorithms, e.g., (Weston et al., 2003) , and their utilisation within the classifier trees. ", "mid_sen": "We also propose to consider aspects of reviews (Snyder and Barzilay, 2007) , and investigate other methods that measure class similarity, such as selecting typical instances (Zhang, 1992) .", "after_sen": "The root node always considers all classes and therefore considers all features across the whole training dataset."}
{"citeStart": 111, "citeEnd": 130, "citeStartToken": 111, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "Noun phrase recognition can be divided in two tasks: recognizing base noun phrases and recognizing arbitrary noun phrases. Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase. For example, the sentence contains six baseNPs (marked as phrases between square brackets). The phrase $ 366.50 an ounce is a noun phrase as well. However, it is not a baseNP since it contains two other noun phrases. Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995) . The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material 1. The baseNPs in this data are slightly different from the ones that can be derived from the Treebank, most notably in the attachment of genitive markers.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995) . ", "mid_sen": "The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material 1. ", "after_sen": "The baseNPs in this data are slightly different from the ones that can be derived from the Treebank, most notably in the attachment of genitive markers."}
{"citeStart": 96, "citeEnd": 108, "citeStartToken": 96, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to illustrate how ordinary parsers can be used to compute the intersection of a FSA and a CFG consider first the definite-clause specification of a top-down parser. This parser runs in polynomial time if implemented using Earle), deduction or XOLDT resolution (Warren, 1992) . It is assumed that the input string is represented by the trans / 3 predicate. The predicate side_effect is used to construct the parse forest grammar. The predicate always succoeds, and as a side-effect asserts that its argument is a rule of the parse forest grammar. For the senfence 'a a b b' we obtain the parse forest grammar:", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to illustrate how ordinary parsers can be used to compute the intersection of a FSA and a CFG consider first the definite-clause specification of a top-down parser. ", "mid_sen": "This parser runs in polynomial time if implemented using Earle), deduction or XOLDT resolution (Warren, 1992) . ", "after_sen": "It is assumed that the input string is represented by the trans / 3 predicate. "}
{"citeStart": 225, "citeEnd": 242, "citeStartToken": 225, "citeEndToken": 242, "sectionName": "UNKNOWN SECTION NAME", "string": "In research on multimodal interactive systems, recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances (Kaur et al., 2003) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These psycholinguistics findings have provided a foundation for our investigation.", "mid_sen": "In research on multimodal interactive systems, recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances (Kaur et al., 2003) .", "after_sen": "Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems (Campana et al., 2001) and to disambiguate speech input (Tanaka, 1999) . "}
{"citeStart": 64, "citeEnd": 88, "citeStartToken": 64, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "This undecidability result is usually circumvented by considering subsets of DCGs which can be recognized effectively. For example, we can restrict the attention to DCGs of which the contextfree skeleton does not contain cycles. Recognition for such 'off-line parsable' grammars is decidable (Pereira and Warren, 1983 ).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, we can restrict the attention to DCGs of which the contextfree skeleton does not contain cycles. ", "mid_sen": "Recognition for such 'off-line parsable' grammars is decidable (Pereira and Warren, 1983 ).", "after_sen": "Most existing constraint-based parsing algorithms will terminate for grammars that exhibit the property that for each string there is only a finite number of possible derivations. "}
{"citeStart": 72, "citeEnd": 92, "citeStartToken": 72, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "For further information on regularized model fitting, see for instance, Hastie et al. (2001) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "EQUATION", "mid_sen": "For further information on regularized model fitting, see for instance, Hastie et al. (2001) .", "after_sen": ""}
{"citeStart": 106, "citeEnd": 127, "citeStartToken": 106, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "Most of the experiments use the CoNLL 2009 data sets with the training, development and test split used in the Shared Task (Hajič et al., 2009) , but for better comparison with previous work we also report results for the standard benchmark data sets for Chinese and English. For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008) , and with the same split as in Zhang and Clark (2008) and Li et al. (2011) . 3 For English, this is the WSJ section of the Penn Treebank, converted with the head-finding rules of Yamada and Matsumoto (2003) and the labeling rules of Nivre (2006) . 4 In order to assign k-best part-of-speech tags and scores to words in the training set, we used a perceptron tagger with 10-fold jack-knifing. The same type of tagger was trained on the entire training set in order to supply tags for the development and test sets. The feature set of the tagger was optimized for English and German and provides state-of-theart accuracy for these two languages. The 1-best tagging accuracy for section 23 of the Penn Treebank is 97.28, which is on a par with Toutanova et al. (2003) . For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008) , which to our knowledge is the best tagger for German. The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank. We could not use the larger training set as it contains the test set of the CoNLL 2009 data that we use to evaluate the joint model. For Czech, the 1best tagging accuracy is 99.11 and for Chinese 92.65 on the CoNLL 2009 test set.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The 1-best tagging accuracy for section 23 of the Penn Treebank is 97.28, which is on a par with Toutanova et al. (2003) . ", "mid_sen": "For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008) , which to our knowledge is the best tagger for German. ", "after_sen": "The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank. "}
{"citeStart": 57, "citeEnd": 75, "citeStartToken": 57, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "We believe that auto parse (test) is a more realistic setting in which to test the performance of SRL on automatic parse trees. When presented with some previously unseen test data, we are forced to rely on its automatic parse trees. However, for the best results we should take advantage of gold parse trees whenever possible, including those of the labeled training data. Our maximum entropy classifier consistently outperforms (Jiang and Ng, 2006) , which also uses a maximum entropy classifier. The primary difference is that we use a later version of NomBank (September 2006 release vs. September 2005 release). In addition, we use somewhat different features and treat overlapping arguments differently.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, for the best results we should take advantage of gold parse trees whenever possible, including those of the labeled training data. ", "mid_sen": "Our maximum entropy classifier consistently outperforms (Jiang and Ng, 2006) , which also uses a maximum entropy classifier. ", "after_sen": "The primary difference is that we use a later version of NomBank (September 2006 release vs. September 2005 release). "}
{"citeStart": 296, "citeEnd": 325, "citeStartToken": 296, "citeEndToken": 325, "sectionName": "UNKNOWN SECTION NAME", "string": "In our second series of experiments, we incrementally developed a new grammar from scratch. The new grammar is basically a scaleddown and adapted version of the Core Language Engine grammar for English Pulman 1992; Rayner 1993 ; concrete development w ork and testing were organized around a speech interface to a set of functionalities o ered by a simple simulation of the Space Shuttle Rayner, Hockey and James 2000. Rules and lexical entries were added in small groups, typically 2 3 rules or 5 10 lexical entries in one increment. After each round of expansion, we tested to make sure that the grammar could still be compiled into a usable recognizer, and at several points this suggested changes in our implementation strategy. The rest of this section describes the new grammar in more detail.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our second series of experiments, we incrementally developed a new grammar from scratch. ", "mid_sen": "The new grammar is basically a scaleddown and adapted version of the Core Language Engine grammar for English Pulman 1992; Rayner 1993 ; concrete development w ork and testing were organized around a speech interface to a set of functionalities o ered by a simple simulation of the Space Shuttle Rayner, Hockey and James 2000. ", "after_sen": "Rules and lexical entries were added in small groups, typically 2 3 rules or 5 10 lexical entries in one increment. "}
{"citeStart": 58, "citeEnd": 87, "citeStartToken": 58, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "What is the role of citation contexts in the overall structure of scientific context? We assume a hierarchical, rhetorical structure not unlike RST (Mann and Thompson, 1987) , but much flatter, where the atomic units are textual blocks which carry a certain functional role in the overall scientific argument for publication (Teufel, 2010; Hyland, 2000) . Under such a general model, citation blocks are certainly a functional unit, and their recognition is a rewarding task in their own right. If citation blocks can be recognised along with their sentiment, this is even more useful, as it restricts the possibilities for which rhetorical function the segment plays. For instance, in the motivation section of a paper, before the paper contribution is introduced, we often find negative sentiment assigned to citations, as any indication can serve as a justification for the current paper. In contrast, positive sentiment is more likely to be restricted to the description of an approach which the authors include in their solution, or further develop.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "What is the role of citation contexts in the overall structure of scientific context? ", "mid_sen": "We assume a hierarchical, rhetorical structure not unlike RST (Mann and Thompson, 1987) , but much flatter, where the atomic units are textual blocks which carry a certain functional role in the overall scientific argument for publication (Teufel, 2010; Hyland, 2000) . ", "after_sen": "Under such a general model, citation blocks are certainly a functional unit, and their recognition is a rewarding task in their own right. "}
{"citeStart": 128, "citeEnd": 153, "citeStartToken": 128, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "Apart from these voting methods we have also applied two memory-based learners t;o the output of the five chunkers: IBI-IG and IGTREE, a decision tree variant of IBI-IG (Daelemans et al., 1999) . This approach is called classifier stacking. Like with the voting algorithms, we have tested these meta-classifiers with the output of the first classification stage. Unlike the voting algorithms, the classifiers do not require a uniform input. Therefore we have tested if their performance can be improved by supplying them with information about the input of the first classification stage. For this purpose we have used the part-of-speech tag of the current word as compressed representation of the first stage input (Van Halteren et al., 1998 ).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Therefore we have tested if their performance can be improved by supplying them with information about the input of the first classification stage. ", "mid_sen": "For this purpose we have used the part-of-speech tag of the current word as compressed representation of the first stage input (Van Halteren et al., 1998 ).", "after_sen": "The combination methods will generate a list of open brackets and a list of close brackets. "}
{"citeStart": 205, "citeEnd": 229, "citeStartToken": 205, "citeEndToken": 229, "sectionName": "UNKNOWN SECTION NAME", "string": "In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005) ). A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser. 1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon-ald et al., 2005) . Dependency parsing can be implemented in O(n 3 ) time using the algorithms of Eisner (2000) . In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments. 2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This section describes the relationship between our work and this previous work.", "mid_sen": "In reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994; Johnson et al., 1999; Collins, 2000; Charniak and Johnson, 2005) ). ", "after_sen": "A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser. "}
{"citeStart": 52, "citeEnd": 64, "citeStartToken": 52, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "After the cue words are identified, the next question is what portion of text each cue word suggests as the outcome, which determines the boundary of the outcome. The text was pre-processed by the Apple Pie parser (Sekine, 1997) to obtain the partof-speech and phrase information. We found that for the noun cues, the noun phrase that contains the noun will be part of the outcome. For the verb cue words, the verb and its object together constitute one portion of the outcome. For the adjective cue words, often the corresponding adjective phrase or the noun phrase belongs to the outcome. Cue words for the results of clinical trials are processed in a slightly different way. For example, for difference and superior, any immediately following prepositional phrase is also included in the results of the trial.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After the cue words are identified, the next question is what portion of text each cue word suggests as the outcome, which determines the boundary of the outcome. ", "mid_sen": "The text was pre-processed by the Apple Pie parser (Sekine, 1997) to obtain the partof-speech and phrase information. ", "after_sen": "We found that for the noun cues, the noun phrase that contains the noun will be part of the outcome. "}
{"citeStart": 0, "citeEnd": 26, "citeStartToken": 0, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lari and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. Briscoe and Waegner (1992) argued that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initialization. It attempts to seed the grammar in a favorable search space by first training it with data from an existing corpus. Section 4 discusses the induction strategies in more detail.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. ", "mid_sen": "Briscoe and Waegner (1992) argued that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. ", "after_sen": "Our idea of grammar adaptation can be seen as a form of initialization. "}
{"citeStart": 31, "citeEnd": 49, "citeStartToken": 31, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "Our opinion annotation scheme (Wiebe et al., 2005) is centered on the notion of private state, a general term that covers opinions, beliefs, thoughts, sentiments, emotions, intentions and evaluations. As Quirk et al. (1985) define it, a private state is a state that is not open to objective observation or verification. We can further view private states in terms of their functional components -as states of experiencers holding attitudes, optionally toward targets. For example, for the private state expressed in the sentence John hates Mary, the experiencer is John, the attitude is hate, and the target is Mary.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our opinion annotation scheme (Wiebe et al., 2005) is centered on the notion of private state, a general term that covers opinions, beliefs, thoughts, sentiments, emotions, intentions and evaluations. ", "after_sen": "As Quirk et al. (1985) define it, a private state is a state that is not open to objective observation or verification. "}
{"citeStart": 34, "citeEnd": 57, "citeStartToken": 34, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002) . Our task is closer to the work of Teufel and Moens (2000) , who looked at the problem of intellectual attribution in scientific texts.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002) . ", "mid_sen": "Our task is closer to the work of Teufel and Moens (2000) , who looked at the problem of intellectual attribution in scientific texts.", "after_sen": ""}
{"citeStart": 16, "citeEnd": 43, "citeStartToken": 16, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "As part of MUC (Grishman and Sundheim, 1996) , coreference resolution was evaluated as a sub-task of information extraction, which involved negotiating a definition of coreference relations that could be reliably evaluated. The final definition included only 'identity' relations between text strings: proper nouns, common nouns and pronouns. Other possible coreference relations, such as 'part-whole', and nontext strings (zero anaphora) were excluded.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As part of MUC (Grishman and Sundheim, 1996) , coreference resolution was evaluated as a sub-task of information extraction, which involved negotiating a definition of coreference relations that could be reliably evaluated. ", "after_sen": "The final definition included only 'identity' relations between text strings: proper nouns, common nouns and pronouns. "}
{"citeStart": 48, "citeEnd": 59, "citeStartToken": 48, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text. The effectiveness of such models is well known (Church 1988) and they are currently in use in parsers (e.g. de Marcken 1990) . Our work is an incremental improvement on these models in two ways: (1) We have run experiments regarding the amount of training data needed in moving to a new domain; (2) we have added probabilistic models of word features to handle unknown words effectively. We describe POST and its algorithms and then we describe our extensions, showing the results of our experiments.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We report in this paper on one application of probabilistic models to language processing, the assignment of part of speech to words in open text. ", "mid_sen": "The effectiveness of such models is well known (Church 1988) and they are currently in use in parsers (e.g. de Marcken 1990) . ", "after_sen": "Our work is an incremental improvement on these models in two ways: (1) We have run experiments regarding the amount of training data needed in moving to a new domain; (2) we have added probabilistic models of word features to handle unknown words effectively. "}
{"citeStart": 111, "citeEnd": 132, "citeStartToken": 111, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "We now turn to an extension of the system which takes proper account of re-entrancies in the structure. The essence of our approach is to define a stochastic procedure which simultaneously expands the nodes of the tree in the way outlined above and guesses the pattern of re-entrancies which relate them. It pays to stipulate that the structures which we build are fully inequated in the sense defined by Carpenter (Carpenter, 1992, p120) . The essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes, and this is in turn equivalent to the choice of a partition of the set of nodes into a set of nonempty sets. These sets of nodes are equivalence classes. The standard reeursive procedure for generating partitions of k + 1 elements is to nondeterministically add the k + lthq node to each of the equivalence classes of each of the partitions of k nodes, and also to nondeterministically consider the new node as a singleton set. The basis of the stochastic procedure for generating fullyinequated feature structures is to interleave the generation of equivalence classes with the expansion from the initial node as described above.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The essence of our approach is to define a stochastic procedure which simultaneously expands the nodes of the tree in the way outlined above and guesses the pattern of re-entrancies which relate them. ", "mid_sen": "It pays to stipulate that the structures which we build are fully inequated in the sense defined by Carpenter (Carpenter, 1992, p120) . ", "after_sen": "The essential insight is that the choice of a fully inequated feature structure involving a set of nodes is the same thing as the choice of an arbitrary equivalence relation over these nodes, and this is in turn equivalent to the choice of a partition of the set of nodes into a set of nonempty sets. "}
{"citeStart": 58, "citeEnd": 82, "citeStartToken": 58, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "The above restriction does not in any way constrain adjunction at nodes that are not in the st)ine of ass auxiliary tree. Similarly, there is no restriction on the adjunction of left or right trees at the spines of wrapping trees. Our restriction is fundamentally different from those in (Schabes and Waters, 1993; Schabes and Waters, 1995) and (Rogers, 1994) , in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times, so long as they only adjoin at one place its each others' spines. Rogers, in contrast, restricts the nesting of wrapping auxiliaries to a number of times bounded by the size of the grammar, and Sehabes and Waters forbid wrapping auxiliaries altogether, at any node in the grammar.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Similarly, there is no restriction on the adjunction of left or right trees at the spines of wrapping trees. ", "mid_sen": "Our restriction is fundamentally different from those in (Schabes and Waters, 1993; Schabes and Waters, 1995) and (Rogers, 1994) , in that we allow wrapping auxiliary trees to nest inside each other an unbounded number of times, so long as they only adjoin at one place its each others' spines. ", "after_sen": "Rogers, in contrast, restricts the nesting of wrapping auxiliaries to a number of times bounded by the size of the grammar, and Sehabes and Waters forbid wrapping auxiliaries altogether, at any node in the grammar."}
{"citeStart": 148, "citeEnd": 160, "citeStartToken": 148, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994) . Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). For the sake of critical comparison, we discuss some common syntactic constructions found in current natural language TAG analyses, that can be captured by our proposal but fall outside of the restrictions mentioned above.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• We provide evidence that the proposed subclass still captures the vast majority of TAG analyses that have been currently proposed for the syntax of English and of several other languages.", "mid_sen": "Several restrictions on the adjunction operation for TAG have been proposed in the literature (Schabes and Waters, 1993; Schabes and Waters, 1995) (Rogers, 1994) . ", "after_sen": "Differently from here, in all those works the main goal was one of characterizing, through the adjunction operation, the set of trees that can be generated by a context-free grammar (CFG). "}
{"citeStart": 219, "citeEnd": 232, "citeStartToken": 219, "citeEndToken": 232, "sectionName": "UNKNOWN SECTION NAME", "string": "Agglutinative languages could be handled ef-flciently by the current mechanism if specifications were provided for the affix combinations that were likely to occur at all often in real texts. A backup mechanism could then be provided which attempted a slower, but more complete, direct application of the rules for the rarer cases. The interaction of morphological analysis with spelling correction (Carter, 1992; Oflazer, 1994; Bowden, 1995) is another possibly fruitful area of work. Once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affixstripping, which would be amenable to exactly the technique outlined by Carter (1992) . As in that work, a discrimination net of root forms would be required; however, this could be augmented independently of spelling pattern creation, so that the flexibility resulting from not composing the lexicon with the spelling rules would not be lost.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The interaction of morphological analysis with spelling correction (Carter, 1992; Oflazer, 1994; Bowden, 1995) is another possibly fruitful area of work. ", "mid_sen": "Once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affixstripping, which would be amenable to exactly the technique outlined by Carter (1992) . ", "after_sen": "As in that work, a discrimination net of root forms would be required; however, this could be augmented independently of spelling pattern creation, so that the flexibility resulting from not composing the lexicon with the spelling rules would not be lost."}
{"citeStart": 131, "citeEnd": 145, "citeStartToken": 131, "citeEndToken": 145, "sectionName": "UNKNOWN SECTION NAME", "string": "We illustrate the relevant syntactic and semantic properties of these forms using the version of Categorial Semantics described in Pereira (1990) . In the Montagovian tradition, semantic representations are com-positionaUy generated in correspondence with the constituent modification relationships manifest in the syntax; predicates are curried. Traces are associated with assumptions which are subsequently discharged by a suitable construction. Figure 1 shows the representations for the sentence Bill became upset; this will serve as the initial source clause representation for the examples that follow. 3 For our analysis of gapping, we follow Sag (1976) in hypothesizing that a post-surface-structure level of syntactic representation is used as the basis for interpretation. In source clauses of gapping constructions, constituents in the source that are parallel to the overt constituents in the target are abstracted out of the clause representation. 4 For simplicity, we will assume that 3We will ignore the tense of the predicates for ease of exposition.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In subsequent sections, we show how the distinct mechanisms for recovering these types of missing information interact with two types of discourse inference to predict the phenomena noted in the previous section.", "mid_sen": "We illustrate the relevant syntactic and semantic properties of these forms using the version of Categorial Semantics described in Pereira (1990) . ", "after_sen": "In the Montagovian tradition, semantic representations are com-positionaUy generated in correspondence with the constituent modification relationships manifest in the syntax; predicates are curried. "}
{"citeStart": 65, "citeEnd": 93, "citeStartToken": 65, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the Wall Street Journal (WSJ) of the years 88-89. The size of our corpus is about 1,000,000 words. The corpus was divided into 80% training and 20% test. The training and the test data were processed by the FDG parser (Tapanainen and Jrvinen, 1997) . Only verbs that occur at least 50 times in the corpus were chosen. This resulted in 278 verbs that we split into 139 confusion sets as above. After filtering the examples of verbs which were not in any of the sets we use 73, 184 training examples and 19,852 test examples.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The corpus was divided into 80% training and 20% test. ", "mid_sen": "The training and the test data were processed by the FDG parser (Tapanainen and Jrvinen, 1997) . ", "after_sen": "Only verbs that occur at least 50 times in the corpus were chosen. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "fact, it appears that gapping is felicitous in those constructions where VP-ellipsis requires a syntactic antecedent, whereas gapping is infelicitous in cases where VP-ellipsis requires only a suitable semantic antecedent. Past approaches to VP-ellipsis that operate within a single module of language processing fail to make the distinctions necessary to account for these differences. Sag and Hankamer (1984) note that while elliptical sentences such as (6) are unacceptable because of a voice mismatch, similar examples with non-elided event referential forms such as do it are much more acceptable:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Past approaches to VP-ellipsis that operate within a single module of language processing fail to make the distinctions necessary to account for these differences. ", "mid_sen": "Sag and Hankamer (1984) note that while elliptical sentences such as (6) are unacceptable because of a voice mismatch, similar examples with non-elided event referential forms such as do it are much more acceptable:", "after_sen": "(10) The decision was reversed by the FBI, and the ICC did it too. "}
{"citeStart": 78, "citeEnd": 88, "citeStartToken": 78, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Their evaluation method is almost the same as pertbrmed in our experinmnts. One difference is that they use the \"OpenText\" search engine 7, and thus the performance tbr Jal)anese-Japanese IR is higher than obtained in out\" evaluation. However, the performance of their Japanese-English CLIR systems, which is roughly 50-60% of that for their Japanese-Japanese IR system, is comparable with our CLIR system performance. It is expected that using a more sophisticated search engine, our CLIR system will achieve a higher performance than that obtained by Kando and Aizawa. In this paper, we proposed a Japanese/English cross-language information retrieval system, targeting technical documents. We combined a query translation module, which performs 7Devcloped by OpenText Corp. compound wor(1 translation and transliteration, with an existing monolingual retrieval method. Our experimental results showed that compound word translation and transliteration methods individually improve on the baseline performance, and when used together the improvement is even greater. Future work will inelude the application of automatic word alignment methods (Fung, 1995; Smadja et al., 1996) to enhance the dictionary.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our experimental results showed that compound word translation and transliteration methods individually improve on the baseline performance, and when used together the improvement is even greater. ", "mid_sen": "Future work will inelude the application of automatic word alignment methods (Fung, 1995; Smadja et al., 1996) to enhance the dictionary.", "after_sen": ""}
{"citeStart": 146, "citeEnd": 160, "citeStartToken": 146, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Grammar extraction algorithm Systemic Functional Grammar (SFG) (Halliday, 1985) is based on the assumption that the differentiation of syntactic phenomena is always deter-mined by its function in the communicative context. This functional orientation has lead to the creation of detailed linguistic resources that are characterized by an integrated treatment of content-related, textual and pragmatic aspects. Computational instances of systemic grammar are successfully employed in some of the largest and most influential text generation projects--such as, for example, PENMAN (Mann, 1983) , COMMUNAL (Fawcett and Tucker, 1990) , TECHDOC (KSsner and Stede, 1994) , Drafter (Paris and Vander Linden , 1996) , and Gist (Not and Stock, 1994) . For our present purposes, however, it is the formal characteristics of systemic grammar and its implementations that are more important. Systemic grammar assumes multifunctional constituent structuresrepresentable as feature structures with coreferences. As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As shown in the following function structure example for the sentence \"The people that buy silver love it.\", different functions can be filled by one and the same constituent: ", "mid_sen": "Given the notational equivalence of HPSG and systemic grammar first mentioned by (Carpenter, 1992) and (Zajac, 1992) , and further elaborated in (Henschel, 1995) , one can characterize a systemic grammar as a large type hierarchy with multiple (conjunctive and disjunctive) and multi-dimensional inheritance with an open-world semantics. ", "after_sen": "The basic element of a systemic grammar--a so-called system--is a type axiom of the form (adopting the notation of CUF (DSrre et al., 1996) ):"}
{"citeStart": 114, "citeEnd": 138, "citeStartToken": 114, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "• Negation Voting proposed by Hu and Liu (2004) is the majority voting that takes negations into account. As negations, we employed not, no, yet, never, none, nobody, nowhere, nothing, and neither, which are taken from (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006; Hu and Liu, 2004 ) (Section 3).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "• Negation Voting proposed by Hu and Liu (2004) is the majority voting that takes negations into account. ", "mid_sen": "As negations, we employed not, no, yet, never, none, nobody, nowhere, nothing, and neither, which are taken from (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006; Hu and Liu, 2004 ) (Section 3).", "after_sen": "• Word-wise was described in Section 4.1."}
{"citeStart": 190, "citeEnd": 212, "citeStartToken": 190, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009) . We suspect that these features may play a significant role in determining the type of referenced used, the prediction of which acts as a 'bottleneck' in generating exact REs.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009) . ", "after_sen": "We suspect that these features may play a significant role in determining the type of referenced used, the prediction of which acts as a 'bottleneck' in generating exact REs."}
{"citeStart": 203, "citeEnd": 218, "citeStartToken": 203, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. This means that natural language expressions such as \"A is B,\" \"A is the same as B,\" etc. are not directly represented by logical equality; similarly, \"not\" is often not treated as logical negation; cf. Hintikka (1985) .", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unless explicitly stated otherwise, we assume that formulas are expressed in a certain (formal) language L without equality; the extension L(=) of L is going to be used only in Section 5 for dealing with noun phrase references. ", "mid_sen": "This means that natural language expressions such as \"A is B,\" \"A is the same as B,\" etc. are not directly represented by logical equality; similarly, \"not\" is often not treated as logical negation; cf. Hintikka (1985) .", "after_sen": "All logical notions that we are going to consider, such as theory or model, will be finitary. "}
{"citeStart": 123, "citeEnd": 148, "citeStartToken": 123, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. (2004) , in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible, context-sensitive category structures. As such, our goal is primarily ontological rather than lexicographic, though both approaches are complementary since each views syntagmatic evidence as the key to understanding the use of lexical concepts in context. By defining category membership in terms of syntagmatic expectations, we establish a functional and gradable basis for determining whether one lexical concept (or synset) in WordNet deserves to be seen as a descendant of another in a particular corpus and context. Augmented with ontological constraints derived from the usage of \"X-like Y\" patterns on the web, we also show how these membership functions can implement Glucksberg's (2001) theory of category inclusion.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A resource like WordNet, in which is-a links are reserved for category relationships that are always true, in any context, is going to be inherently limited when dealing with real text.", "mid_sen": "We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. (2004) , in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible, context-sensitive category structures. ", "after_sen": "As such, our goal is primarily ontological rather than lexicographic, though both approaches are complementary since each views syntagmatic evidence as the key to understanding the use of lexical concepts in context. "}
{"citeStart": 70, "citeEnd": 88, "citeStartToken": 70, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "The impact of the existence of lazy implementations for enrichment operations is twofold: we can (a) now maintain minimized base lexicons for storage efficiency and add enrichments lazily to the currently pursued string hypothesis only, possibly modulated by exception diacritics that control when enrichment should or should not happen. 4 And (b), laziness suffices to make the proposed reduplication method reasonably time-efficient, despite the larger number of online operations. Actual benchmarks from a pilot implementation are reported elsewhere (Walther, submitted) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "4 And (b), laziness suffices to make the proposed reduplication method reasonably time-efficient, despite the larger number of online operations. ", "mid_sen": "Actual benchmarks from a pilot implementation are reported elsewhere (Walther, submitted) .", "after_sen": ""}
{"citeStart": 3, "citeEnd": 17, "citeStartToken": 3, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. Gerdemann's generator follows a head-driven strategy in order to avoid inefficient evaluation orders. More specifically, the head of the right-hand side of each grammar rule is distinguished, and distinguished categories are scanned or predicted upon first. The resulting evaluation strategy is similar to that of the head-corner approach (Shieber et al., 1990 ; Gerdemann and IIinrichs, in press): prediction follows the main flow of semantic information until a lexical pivot is reached, and only then are the headdependent subparts of the construction built up in a bottom-up fashion. This mixture of top-down and bottom-up information flow is crucial since the topdown semantic information from the goal category must be integrated with the bottom-up subcategorization information from the lexicon. A strict topdown evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recursion in parsing. Shieber et al. (1990) show that a top-down evaluation strategy will fail for rules such as vP --* vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. By combining the off-line optimization process with a mixed bottom-up/top-down evaluation strategy, we can refrain from a complete reformulation of the grammar as, for example, in Minnen et al. (1995) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. ", "after_sen": "Gerdemann (1991) tackled this shortcoming by modifying the restriction function to make top-down information available for the bottom-up completion step. "}
{"citeStart": 26, "citeEnd": 51, "citeStartToken": 26, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "final (q0) . ([l,0,1,1,11A] whether some PCP has a solution or not is not decidable. This result is proved by (Hopcroft and Ullman, 1979) by showing that the halting problem for Turing Machines can be encoded as an instance of Post's Correspondence Problem. First I give a simple algorithm to encode any instance of a PCP as a pair, consisting of a FSA and an off-line parsable DCG, in such a way that the question whether there is a solution to this PCP is equivalent to the question whether the intersection of this FSA and DCG is empty.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "final (q0) . ([l,0,1,1,11A] whether some PCP has a solution or not is not decidable. ", "mid_sen": "This result is proved by (Hopcroft and Ullman, 1979) by showing that the halting problem for Turing Machines can be encoded as an instance of Post's Correspondence Problem. ", "after_sen": "First I give a simple algorithm to encode any instance of a PCP as a pair, consisting of a FSA and an off-line parsable DCG, in such a way that the question whether there is a solution to this PCP is equivalent to the question whether the intersection of this FSA and DCG is empty."}
{"citeStart": 90, "citeEnd": 116, "citeStartToken": 90, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Deerwester , et al. (1990) introduced an algorithm called Latent Semantic Analysis (LSA) which showed that valid semantic relationships between words and documents in a corpus can be induced with virtually no human intervention. To do this, one typically begins by applying singular value decomposition (SVD) to a matrix, M, whose entries M(i,j) contains the frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999) . In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000) ). Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993) , who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w's row, the first N columns denote words that", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999) . ", "mid_sen": "In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000) ). ", "after_sen": "Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993) , who performed SVD on a Nx2N term-term matrix. "}
{"citeStart": 126, "citeEnd": 136, "citeStartToken": 126, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "Much research has been done on knowledge acquisition from large-scale annotated corpora as a rich source of linguistic knowledge. Major works done to create English POS taggers (henceforth, \"taggers\"), for example, include (Church 1988) , (Kupiec 1992) , (Brill 1992) and (Voutilainen et al. 1992) . The problem with this framework, however, is that such reliable corpora are hardly available due to a huge amount of the labor-intensive work required. In case of the acquisition of non-core knowledge, such as specific, lexically or domain dependent knowledge, preparation of annotated corpora becomes more serious problem. One viable approach then is to utilize plain text corpora instead, as in (Mikheev 1996) . But The method proposed by (Mikheev 1996) has its own weaknesses, in that it is restricted in scope. That is, it aims to acquire rules for unknown words in corpora from their ending characters without looking at the context. In the meantime, (Brill 1995a ) (Brill 1995b) proposed a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning. The weakness of his method is that the effect of unsupervised learning decreases as the training corpus size increases. The problem in using plain text corpora for knowledge acquisition is that we need a human supervisor who can evaluate and sift the obtained knowledge. An alternative to this would be to use a number of modules of a welldeveloped NLP system which stores most of the highly reliable general rules. Here, one module functions as a supervisor for other modules, since all these modules are designed to work cooperatively and the knowledges stored in each module are correlated. Keeping this idea in mind, we propose a new unsupervised learning method for obtaining linguistic rules from plain text corpora using the existing linguistic knowledge. This method has been implemented in the rule extraction system APRAS (Automatic POS Rule Acquisition System), which automatically acquires rules for refining the morphological analyzer (tagger) in our English-Japanese MT system ASTRANSAC (Hirakawa et al. 1991) through the interaction between the system's tagger and parser on the assumption that they are considerably accurate. This paper is organized as follows: Section 2 illustrates the basic idea of our method; Section 3 gives the outline of APRAS; Sections 4 and 5 describe our experiments.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much research has been done on knowledge acquisition from large-scale annotated corpora as a rich source of linguistic knowledge. ", "mid_sen": "Major works done to create English POS taggers (henceforth, \"taggers\"), for example, include (Church 1988) , (Kupiec 1992) , (Brill 1992) and (Voutilainen et al. 1992) . ", "after_sen": "The problem with this framework, however, is that such reliable corpora are hardly available due to a huge amount of the labor-intensive work required. "}
{"citeStart": 9, "citeEnd": 22, "citeStartToken": 9, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "However, Sproat (1992) observes that, despite the existence of \"working systems that are capable of doing a great deal of morphological analysis\", \"there are still outstanding problems and areas which have not received much serious attention\" (ibid., 123) . Problem areas in his view include subtractive morphology, infixation, the proper inclusion of prosodic structure and, in particular, reduplication: \"From a computational point of view, one point cannot be overstressed: the copying required in reduplication places reduplication in a class apart from all other morphology.\" (ibid., 60) . Productive reduplication is so troublesome for a formal account based on regular languages (or regular relations) because unbounded total instances like Indonesian noun plural (orang-orang 'men') are isomorphic to the copy language ww, which is context-sensitive.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using finite-state methods, it has been possible to describe both word formation and the concomitant phonological modifications in many languages, ranging from straightforward concatenative combination (Koskenniemi, 1983) over Semitic-style nonconcatenative intercalation (Beesley (1996) , Kiraz (1994)) to circumfixional long-distance dependencies (Beesley, 1998) .", "mid_sen": "However, Sproat (1992) observes that, despite the existence of \"working systems that are capable of doing a great deal of morphological analysis\", \"there are still outstanding problems and areas which have not received much serious attention\" (ibid., 123) . ", "after_sen": "Problem areas in his view include subtractive morphology, infixation, the proper inclusion of prosodic structure and, in particular, reduplication: \"From a computational point of view, one point cannot be overstressed: the copying required in reduplication places reduplication in a class apart from all other morphology.\" (ibid., 60) . "}
{"citeStart": 48, "citeEnd": 67, "citeStartToken": 48, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "There are many variations of the longest match method, possibly augmented with further heuristics. We used a simple greedy algorithm described in [Sproat et al., 1996] . It starts at the beg6nning of the sentence, finds the longest word starting at that point, and then repeats the process starting at the next character until the end of the sentence is reached. We chose the greedy algorithm because it is easy to implement and guaranteed to produce only one segmentation. [Sproat et al., 1996 ] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus. It derives the initial estimates from the frequencies in the corpus of the strings of character making up each word in the dictionary whether or not each string is actually an instance of the word in question. The total number of words in the corpus is derived simply by summing the string frequency of each word in the dictionary. Finding (and counting) all instances of a string W in a large text T can be efficiently accomplished by making a data structure known as a sUtrLX array, which is basically a sorted list of all the su~ixes of T [Manber and Myers, 1993] .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There are many variations of the longest match method, possibly augmented with further heuristics. ", "mid_sen": "We used a simple greedy algorithm described in [Sproat et al., 1996] . ", "after_sen": "It starts at the beg6nning of the sentence, finds the longest word starting at that point, and then repeats the process starting at the next character until the end of the sentence is reached. "}
{"citeStart": 204, "citeEnd": 227, "citeStartToken": 204, "citeEndToken": 227, "sectionName": "UNKNOWN SECTION NAME", "string": "We focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions. Identifying opinion sources will be especially critical for opinion-oriented questionanswering systems (e.g., systems that answer questions of the form \"How does [X] feel about [Y] ?\") and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another. 1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text. To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003) , Wiebe and Riloff (2005) , Wilson et al. (2005) ) and the nesting structure of sources (e.g., Breck and Cardie (2004) ). The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 The goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text. ", "mid_sen": "To illustrate the nature of this problem, consider the examples below: S1: Taiwan-born voters favoring independence... 1 In related work, we investigate methods to identify the opinion expressions (e.g., Riloff and Wiebe (2003) , Wiebe and Riloff (2005) , Wilson et al. (2005) ) and the nesting structure of sources (e.g., Breck and Cardie (2004) ). ", "after_sen": "The target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus."}
{"citeStart": 68, "citeEnd": 84, "citeStartToken": 68, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Context sensitive rewrite rules have been widely used in several areas of natural language processing. Johnson (1972) has shown that such rewrite rules are equivalent to finite state transducers in the special case that they are not allowed to rewrite their own output. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994) . Improvements and extensions to this algorithm have been provided by Karttunen (1995) , Karttunen (1997) , Karttunen (1996) and Mohri and Sproat (1996) . In this paper, the algorithm will be extended to provide a limited form of backreferencing.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An algorithm for compilation into transducers was provided by Kaplan and Kay (1994) . ", "mid_sen": "Improvements and extensions to this algorithm have been provided by Karttunen (1995) , Karttunen (1997) , Karttunen (1996) and Mohri and Sproat (1996) . ", "after_sen": "In this paper, the algorithm will be extended to provide a limited form of backreferencing."}
{"citeStart": 95, "citeEnd": 96, "citeStartToken": 95, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "I adopt the assumption that the participants in a dialogue are trying to achieve some purpose [7] . Some aspects of the structure of dialogue arises from the structure of these purposes and their relation to one another. The minimal purpose of any dialogue is that an utterance be understood, and this goal is a prerequisite to achieving other goals in dialogue, such as commitment to future action. Thus achieving mutual belief of understanding is an instance of the type of activity that agents must perform as they collaborate to achieve the purposes of the dialogue. I claim that a model of the achievement of mutual belief of understanding can he extended to the achievement of other goals in dialogue.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This distinguishes this account from others that assume either that utterance actions always succeed or that they succeed unless the addressee previously believed othecwise [11, 8] .", "mid_sen": "I adopt the assumption that the participants in a dialogue are trying to achieve some purpose [7] . ", "after_sen": "Some aspects of the structure of dialogue arises from the structure of these purposes and their relation to one another. "}
{"citeStart": 185, "citeEnd": 209, "citeStartToken": 185, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "In the past few years, a \"standard model\" of scope underspecification has emerged: A range of formalisms from Underspecified DRT (Reyle, 1993) to dominance graphs (Althaus et al., 2003) have offered mechanisms to specify the \"semantic material\" of which the semantic representations are built up, plus dominance or outscoping relations between these building blocks. This has been a very successful approach, but recent algorithms for eliminating subsets of readings have pushed the expres-sive power of these formalisms to their limits; for instance, Koller and Thater (2006) speculate that further improvements over their (incomplete) redundancy elimination algorithm require a more expressive formalism than dominance graphs. On the theoretical side, Ebert (2005) has shown that none of the major underspecification formalisms are expressively complete, i.e. supports the description of an arbitrary subset of readings. Furthermore, the somewhat implicit nature of dominance-based descriptions makes it difficult to systematically associate readings with probabilities or costs and then compute a best reading.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A range of formalisms from Underspecified DRT (Reyle, 1993) to dominance graphs (Althaus et al., 2003) have offered mechanisms to specify the \"semantic material\" of which the semantic representations are built up, plus dominance or outscoping relations between these building blocks. ", "mid_sen": "This has been a very successful approach, but recent algorithms for eliminating subsets of readings have pushed the expres-sive power of these formalisms to their limits; for instance, Koller and Thater (2006) speculate that further improvements over their (incomplete) redundancy elimination algorithm require a more expressive formalism than dominance graphs. ", "after_sen": "On the theoretical side, Ebert (2005) has shown that none of the major underspecification formalisms are expressively complete, i.e. supports the description of an arbitrary subset of readings. "}
{"citeStart": 102, "citeEnd": 120, "citeStartToken": 102, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "An analysis of the mechanism of temporal anaphoric reference hinges upon an understanding of the ontological and logical foundations of temporal reference. Different concepts have been used in the literature as primitives. These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. In this paper, we only consider events and states, together termed eventualities in (Bach, 1981) . In narrative sequences, event clauses seem to advance the narrative time, while states block its progression. The mechanism used to account for this phenomena in (Hinrichs, 1986) and (Partee, 1984) , is based on the notion of reference time, originally proposed by Reichenbach (1947) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Different concepts have been used in the literature as primitives. ", "mid_sen": "These range from temporal instants in Tense logic (Prior, 67) , through intervals of time (Bennet and Partee, 1978(1972) ) as in the analysis of temporal connectives in (Hein~mKki, 1978) , to event structures (Kamp, 1979) as in Hinrichs' (1986) analysis of temporal anaphora. ", "after_sen": "An important factor in the interpretation of temporal expressions is the classification of situations ihto different aspectual classes (or Aktionsarten), which is based on distributional and semantic properties. "}
{"citeStart": 37, "citeEnd": 57, "citeStartToken": 37, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "Lexicalized Tree Adjoining Grammars (Schabes et al., 1988 ) are a refinement of TAGs such that each elementary tree is associated with a lexieal item, called the anchor of the tree. Therefore, Lexicalized TAGs conform to a common tendency in modem theories of grammar, namely the attempt to embed grammatical information within lexical items. Notably, the association between elementary trees and anchors improves also parsing performance, as will be discussed below.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (Kroch and Joshi, 1985) a detailed discussion of the linguistic relevance of TAGs can be found.", "mid_sen": "Lexicalized Tree Adjoining Grammars (Schabes et al., 1988 ) are a refinement of TAGs such that each elementary tree is associated with a lexieal item, called the anchor of the tree. ", "after_sen": "Therefore, Lexicalized TAGs conform to a common tendency in modem theories of grammar, namely the attempt to embed grammatical information within lexical items. "}
{"citeStart": 16, "citeEnd": 20, "citeStartToken": 16, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "\"Church et al. (1989 ), Wettler & Rapp (1989 and Church & Hanks (1990) describe algorithms which do this. However, the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects.\"", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example:", "mid_sen": "\"Church et al. (1989 ), Wettler & Rapp (1989 and Church & Hanks (1990) describe algorithms which do this. ", "after_sen": "However, the validity of these algorithms has not been tested by systematic comparisons with associations of human subjects.\""}
{"citeStart": 85, "citeEnd": 110, "citeStartToken": 85, "citeEndToken": 110, "sectionName": "UNKNOWN SECTION NAME", "string": "The temporal ontology is based on a recent theory of temporal semantics developed by Moens and Steedman (1988) . This allows a modular representation of the semantics of temporal adverbials like \"until\" and \"by\", and also aids in the generation of tense and aspect.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The alternatives arise when more than one event can be used.", "mid_sen": "The temporal ontology is based on a recent theory of temporal semantics developed by Moens and Steedman (1988) . ", "after_sen": "This allows a modular representation of the semantics of temporal adverbials like \"until\" and \"by\", and also aids in the generation of tense and aspect."}
{"citeStart": 6, "citeEnd": 31, "citeStartToken": 6, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Apart from these five voting methods we have also processed the output streams with two classifiers: IBI-IG (memory-based) and IGTREE (decision tree). This approach is called classifier stacking. Like (Van Halteren et al., 1998) , we have used different input versions: one containing only the classifier output and another containing both classifier output and a compressed representation of the classifier input. For the latter purpose we have used the part-ofspeech tag of the current word.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This approach is called classifier stacking. ", "mid_sen": "Like (Van Halteren et al., 1998) , we have used different input versions: one containing only the classifier output and another containing both classifier output and a compressed representation of the classifier input. ", "after_sen": "For the latter purpose we have used the part-ofspeech tag of the current word."}
{"citeStart": 165, "citeEnd": 179, "citeStartToken": 165, "citeEndToken": 179, "sectionName": "UNKNOWN SECTION NAME", "string": "The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995) , extending it to operate across related documents. Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006) .", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995) , extending it to operate across related documents. ", "after_sen": "Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006) ."}
{"citeStart": 172, "citeEnd": 193, "citeStartToken": 172, "citeEndToken": 193, "sectionName": "UNKNOWN SECTION NAME", "string": "An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French. These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals. Furthermore the use of the infinitive-form in instructions in general as observed by (Kocourek, 1982) is declining, as some of the conventions already common in English technical writing are being adopted by French technical writers, e.g., (Timbal-Duclaux, 1990) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our analysis goes beyond previous work by identifying within the discourse context the means for exercising explicit control over a text generator.", "mid_sen": "An interesting difference with respect to previous descriptions is the use of the true (or direct) imperative to express an action in the procedure genre, as results from (Paris and Scott, 1994) seem to indicate that the infinitive-form of the imperative is preferred in French. ", "after_sen": "These results, however, were obtained from a corpus of instructions mostly for domestic appliances as opposed to software manuals. "}
{"citeStart": 239, "citeEnd": 269, "citeStartToken": 239, "citeEndToken": 269, "sectionName": "UNKNOWN SECTION NAME", "string": "Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief. For example, the following opinions contain Claims but no Sentiments: \"I believe the world is flat\" \"The Gap is likely to go bankrupt\" \"Bin Laden is hiding in Pakistan\" \"Water always flushes anti-clockwise in the southern hemisphere\" Like Yu and Hatzivassiloglou (2003) , we want to automatically identify Sentiments, which in this work we define as an explicit or implicit expression in text of the Holder's positive, negative, or neutral regard toward the Claim about the Topic. (Other sentiments we plan to study later.) Sentiments always involve the Holder's emotions or desires, and may be present explicitly or only implicitly: \"I think that attacking Iraq would put the US in a difficult position\" (implicit) \"The US attack on Iraq is wrong\" (explicit) \"I like Ike\" (explicit) \"We should decrease our dependence on oil\" (implicit) \"Reps. Tom Petri and William F. Goodling asserted that counting illegal aliens violates citizens' basic right to equal representation\" (implicit)", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief. ", "mid_sen": "For example, the following opinions contain Claims but no Sentiments: \"I believe the world is flat\" \"The Gap is likely to go bankrupt\" \"Bin Laden is hiding in Pakistan\" \"Water always flushes anti-clockwise in the southern hemisphere\" Like Yu and Hatzivassiloglou (2003) , we want to automatically identify Sentiments, which in this work we define as an explicit or implicit expression in text of the Holder's positive, negative, or neutral regard toward the Claim about the Topic. ", "after_sen": "(Other sentiments we plan to study later.) Sentiments always involve the Holder's emotions or desires, and may be present explicitly or only implicitly: \"I think that attacking Iraq would put the US in a difficult position\" (implicit) \"The US attack on Iraq is wrong\" (explicit) \"I like Ike\" (explicit) \"We should decrease our dependence on oil\" (implicit) \"Reps. "}
{"citeStart": 45, "citeEnd": 57, "citeStartToken": 45, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "Following annotation schemes like the one of Stede (2004) , we model discourse structures by binary trees. Fig. (1b-f) represent the potential structures of (1). We write each elementary discourse unit (EDU) in square brackets.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following annotation schemes like the one of Stede (2004) , we model discourse structures by binary trees. ", "after_sen": "Fig. (1b-f) represent the potential structures of (1). "}
{"citeStart": 24, "citeEnd": 36, "citeStartToken": 24, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "1. We provide a clear description of the problem of hedge classification and offer an improved and expanded set of annotation guidelines, which as we demonstrate experimentally are sufficient to induce a high level of agreement between independent annotators. 2. We discuss the specificities of hedge classification as a weakly supervised ML task. 3. We derive a probabilistic weakly supervised learning model and use it to motivate our approach. 4. We analyze our learning model experimentally and report promising results for the task on a new publicly-available dataset. 1 scientific text, eg. (Hyland, 1994) , there is little of direct relevance to the task of classifying speculative language from an NLP/ML perspective. The most clearly relevant study is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. We will draw on this work throughout our presentation of the task.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "4. We analyze our learning model experimentally and report promising results for the task on a new publicly-available dataset. ", "mid_sen": "1 scientific text, eg. (Hyland, 1994) , there is little of direct relevance to the task of classifying speculative language from an NLP/ML perspective. ", "after_sen": "The most clearly relevant study is Light et al. (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. "}
{"citeStart": 172, "citeEnd": 184, "citeStartToken": 172, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "The GLR* Parser GLR* is a recently developed robust version of the Generalized LR Parser, that allows the skipping of unrecognizable parts of the input sentence [Lavie and Tomita, 1993] . It is designed to enhance the parsability of domains such as spontaneous speech, where the input is likely to contain deviations from the grammar, due to either extra-grammaticalities or limited grammar coverage. In cases where the complete input sentence is not covered by the grammar, the parser attempts to find a maximal subset of the input that is parsable. In many cases, such a parse can serve as a good approximation to the true parse of the sentence.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The result of parsing an input sentence consists of both a parse tree and the computed feature structure associated with the non-terminal at the root of the tree.", "mid_sen": "The GLR* Parser GLR* is a recently developed robust version of the Generalized LR Parser, that allows the skipping of unrecognizable parts of the input sentence [Lavie and Tomita, 1993] . ", "after_sen": "It is designed to enhance the parsability of domains such as spontaneous speech, where the input is likely to contain deviations from the grammar, due to either extra-grammaticalities or limited grammar coverage. "}
{"citeStart": 130, "citeEnd": 149, "citeStartToken": 130, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "We view both review identification and polarity classification as a classification task. For review identification, we train a classifier to distinguish movie reviews and movie-related nonreviews (e.g., movie ads, plot summaries) using only unigrams as features, obtaining an accuracy of over 99% via 10-fold cross-validation. Similar experiments using documents from the book domain also yield an accuracy as high as 97%. An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions. Next, we learn our polarity classifier using positive and negative reviews taken from two movie review datasets, one assembled by Pang and Lee (2004) and the other by ourselves. The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.'s dataset and ours, respectively. To our knowledge, our result on Pang et al.'s dataset is one of the best reported to date. Perhaps more importantly, an analysis of these results show that the various types of features interact in an interesting manner, allowing us to draw conclusions that provide new insights into polarity classification.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An analysis of the results reveals that the high accuracy can be attributed to the difference in the vocabulary employed in reviews and non-reviews: while reviews can be composed of a mixture of subjective and objective language, our non-review documents rarely contain subjective expressions. ", "mid_sen": "Next, we learn our polarity classifier using positive and negative reviews taken from two movie review datasets, one assembled by Pang and Lee (2004) and the other by ourselves. ", "after_sen": "The resulting classifier, when trained on a feature set derived from the four types of linguistic knowledge sources mentioned above, achieves a 10-fold cross-validation accuracy of 90.5% and 86.1% on Pang et al.'s dataset and ours, respectively. "}
{"citeStart": 82, "citeEnd": 101, "citeStartToken": 82, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "We compared four different state-of-the-art unsupervised systems for morphological decomposition (cf. (Demberg, 2006; Demberg, 2007) ). The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The algorithms were trained on a German newspaper corpus (taz), containing about 240 million words. ", "mid_sen": "The same algorithms have previously been shown to help a speech recognition task (Kurimo et al., 2006) .", "after_sen": ""}
{"citeStart": 145, "citeEnd": 155, "citeStartToken": 145, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "A well-known case from the context-sensitivity debate of the eighties is the N-o-N reduplicative construction from Bambara (Northwestern Mande, (Culy, 1985) ):", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A well-known case from the context-sensitivity debate of the eighties is the N-o-N reduplicative construction from Bambara (Northwestern Mande, (Culy, 1985) ):", "after_sen": "(1) a. wulu-o-wulu 'whichever dog' b. wulunyinina-o-wulunyinina 'whichever dog searcher' c. wulunyininafil~la-o-wulunyininafil~la 'whoever watches dog searchers'"}
{"citeStart": 84, "citeEnd": 100, "citeStartToken": 84, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "While keystroke dynamics is concerned with a number of timing metrics, such as key holds (h in Figure 1 ) and pauses between every keystroke (p in Figure 1 ), the current study looked only at the pause preceding a word (the second p in Figure 1 ). This interval consists of the time between the spacebar being released and the first key of the word being pressed. We also did not remove any outliers, although this is common in keystroke dynamics (Epp et al., 2011; Zhong et al., 2012) . We feel it is difficult-toimpossible to discriminate between a \"true\" pause that is indicative of a subject's increased cognitive effort and any other type of pause, such as those caused by distraction or physical fatigue. As such we include any idiosyncrasies, such as long pauses, in our analyses rather than dismiss them as noise.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This interval consists of the time between the spacebar being released and the first key of the word being pressed. ", "mid_sen": "We also did not remove any outliers, although this is common in keystroke dynamics (Epp et al., 2011; Zhong et al., 2012) . ", "after_sen": "We feel it is difficult-toimpossible to discriminate between a \"true\" pause that is indicative of a subject's increased cognitive effort and any other type of pause, such as those caused by distraction or physical fatigue. "}
{"citeStart": 123, "citeEnd": 144, "citeStartToken": 123, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007) . Each committee member's training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson & Dagan (1996) in the implementation of vote entropy for sentence selection using these models.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We label this scheme QBUOMM (OMM = \"One Minus Max\").", "mid_sen": "Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007) . ", "after_sen": "Each committee member's training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. "}
{"citeStart": 106, "citeEnd": 119, "citeStartToken": 106, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "Underspecification of the head of the rule allows it to unify with both finite auxiliaries and finite ditransitive main verbs. In combination with the underspecification of the complements, this allows the rule not only to be used for argument composition constructions, as discussed above, but also for constructions in which a finite main verb becomes saturated. This means that the logical form of the nonverbal complements (if] and [~) becomes available either upon the evaluation of the complement tagged [] (in case of argument composition), or upon the evaluation of the finite verb (in case the head of the rule is a ditransitive main verb). As a result, the use of generalization does not suffice to mimic the evaluation of the respective right-hand side categories. Because both verbal categories have defining lexical entries which do not instantiate the logical form of the nonverbal arguments, the dataflow analysis leads to the conclusion that the logical form of the nonverbal complements never becomes instantiated. This causes the rejection of all possible evaluation orders for this rule, as the evaluation of an unrestricted nonverbal complement clearly exceeds the allowed maximal degree of nondeterminacy of the grammar. We are therefore forced to split this schematic phrase structure rule into two more specific rules at least during the optimization process. It is important to note that this is a consequence of a general limitation of dataflow analysis (see also Mellish, 1981) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We are therefore forced to split this schematic phrase structure rule into two more specific rules at least during the optimization process. ", "mid_sen": "It is important to note that this is a consequence of a general limitation of dataflow analysis (see also Mellish, 1981) .", "after_sen": ""}
{"citeStart": 190, "citeEnd": 212, "citeStartToken": 190, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "Theorem 4.1 A probabilistic grammar is consistent if the spectral radius p(A4) < 1, where ,h,4 is the stochastic expectation matrix computed from the grammar. (Booth and Thompson, 1973; Soule, 1974) This theorem provides a way to determine whether a grammar is consistent. All we need to do is compute the spectral radius of the square matrix A4 which is equal to the modulus of the largest eigenvalue of •. If this value is less than one then the grammar is consistent 4. Computing consistency can bypass the computation of the eigenvalues for A4 by using the following theorem by Ger~gorin (see (Horn and Johnson, 1985; Wetherell, 1980) ).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All we need to do is compute the spectral radius of the square matrix A4 which is equal to the modulus of the largest eigenvalue of •. ", "mid_sen": "If this value is less than one then the grammar is consistent 4. Computing consistency can bypass the computation of the eigenvalues for A4 by using the following theorem by Ger~gorin (see (Horn and Johnson, 1985; Wetherell, 1980) ).", "after_sen": "Theorem 4.2 For any square matrix .h4, p(.M) < 1 if and only if there is an n > 1 such that the sum of the absolute values of the elements of each row of ."}
{"citeStart": 46, "citeEnd": 84, "citeStartToken": 46, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "within the fi-amework of VERBMOBIL, a speech-to-speech translation system. Its usefulness for simultaneous interpretation results from its incremental and pa'rallel style of processing. VM-GEN is able to consume input increinents of varying size. q'hese, increments describe lexical items or semantic relations between them. Single input increments are handed over to objects of a distril)uted parallel system, each of which tries to verbalize the structure that results from the corresponding input increment. VM-O,]'~N uses ml extension of Tree Adjoining Gra'm'mct'rs (TACs, c['. (aoshi, 1985) ) as its syntactic representation formalism that is not only adequate for Ihe description of natura.1 language hut also supports incremental genera.lion (I(ilger aml Finkler, 1994).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Single input increments are handed over to objects of a distril)uted parallel system, each of which tries to verbalize the structure that results from the corresponding input increment. ", "mid_sen": "VM-O,]'~N uses ml extension of Tree Adjoining Gra'm'mct'rs (TACs, c['. (aoshi, 1985) ) as its syntactic representation formalism that is not only adequate for Ihe description of natura.", "after_sen": "1 language hut also supports incremental genera.lion (I(ilger aml Finkler, 1994)."}
{"citeStart": 76, "citeEnd": 93, "citeStartToken": 76, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "To address the above problems, we build a decision tree of SVMs that reduces the set of possible classes at each decision node, and takes relative class similarity into account during the tree construction process. We construct the decision tree as a Minimum Cost Spanning Tree (MCST), denoted MCST-SVM, based on inter-class similarity measured from feature values (Lorena and de Carvalho, 2005) . Each of the decision tree leaves corresponds to a target class, and the interior nodes group classes into disjoint sets. For each internal node in the MCST, an SVM is trained to separate all the samples belonging to classes in its left subtree from those in its right subtree. We use linear SVMs, which have been shown to be effective text classifiers (Pang et al., 2002; Pang and Lee, 2005) , and set the SVM parameters to match those used in (Pang and Lee, 2005) . 1 Figure 1 contrasts SVMs are implemented using the C/C++ library liblinear, a variant of libsvm (Chang and Lin, 2001 ). The MCST is constructed using Kruskal's algorithm (1956) , which works in polynomial time (Algorithm 1). This algorithm requires a measure of the similarity between every pair of classes, which is calculated using the distance between a representative vector for each class (Section 3.2). The MCST is iteratively built in a bottom-up fashion, beginning with all classes as singleton nodes. In each iteration, the algorithm constructs a node comprising the most similar sets of classes from two previously generated nodes. The similarity between two sets of classes is the shortest distance between the representative vectors of the classes in each set. For instance, the shortest distance between the sets of classes {*/**} and {***/****} is min{dist(*,***), dist(*,****), dist(**,***), dist(**,****)}. An SVM is then trained to discriminate between the children of the constructed nodes.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each internal node in the MCST, an SVM is trained to separate all the samples belonging to classes in its left subtree from those in its right subtree. ", "mid_sen": "We use linear SVMs, which have been shown to be effective text classifiers (Pang et al., 2002; Pang and Lee, 2005) , and set the SVM parameters to match those used in (Pang and Lee, 2005) . ", "after_sen": "1 Figure 1 contrasts SVMs are implemented using the C/C++ library liblinear, a variant of libsvm (Chang and Lin, 2001 ). "}
{"citeStart": 160, "citeEnd": 194, "citeStartToken": 160, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "Each of these subtasks is cast as an unsupervised collective classification problem and solved using the same mechanism. In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of labels to objects. In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of labels to objects. ", "mid_sen": "In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)) .", "after_sen": "A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tuples while trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computer vision (e.g., model-based matching). "}
{"citeStart": 184, "citeEnd": 196, "citeStartToken": 184, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "The description of Sussna's algorithm for disambiguating noun groupings like this one is similar to the one proposed here, in a number of ways: relatedness is characterized in terms of a semantic network (specifically WordNet); the focus is on nouns only; and evaluations of semantic similarity (or, in Sussna's case, semantic distance) are the basis for sense selection. However, there are some important differences, as well. First, unlike Sussna's proposal, this algorithm aims to disambiguate groupings of nouns already established (e.g. by clustering, or by manual effort) to be related, as opposed to groupings of nouns that happen to appear near each other in running text (which may or may not reflect relatedness based on meaning). This provides some justification for restricting attention to similarity (reflected by the scaffolding of IS-A links in the taxonomy), as opposed to the more general notion of association. Second, this difference is reflected algonthmically by the fact that Sussna uses not only IS-A links but also other WordNet links such as PART-OF. Third, unlike Sussna's algorithm, the semantic similarity/distance computation here is not based on path length, but on information content, a choice that I have argued for elsewhere (Resnik, 1993; Resnik, 1995) . Fourth, the combinatorics are handled differently: Sussna explores analyzing all sense combinations (and living with the exponential complexity), as well as the alternative of sequentially \"freezing\" a single sense for each of Wl,..., W~_l and using those choices, assumed to be correct, as the basis for disambiguating wi. The algorithm presented here falls between those two alternatives.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, this difference is reflected algonthmically by the fact that Sussna uses not only IS-A links but also other WordNet links such as PART-OF. ", "mid_sen": "Third, unlike Sussna's algorithm, the semantic similarity/distance computation here is not based on path length, but on information content, a choice that I have argued for elsewhere (Resnik, 1993; Resnik, 1995) . ", "after_sen": "Fourth, the combinatorics are handled differently: Sussna explores analyzing all sense combinations (and living with the exponential complexity), as well as the alternative of sequentially \"freezing\" a single sense for each of Wl,..., W~_l and using those choices, assumed to be correct, as the basis for disambiguating wi. "}
{"citeStart": 310, "citeEnd": 322, "citeStartToken": 310, "citeEndToken": 322, "sectionName": "UNKNOWN SECTION NAME", "string": "C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text-and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\" While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991) . 2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicatorsaverage sentence length and type-token ratio -obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool. Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text-and word-specific factors. ", "mid_sen": "The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\" While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991) . 2 In our model, we aim at combining features touching all levels of language. ", "after_sen": "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. "}
{"citeStart": 198, "citeEnd": 210, "citeStartToken": 198, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems (Campana et al., 2001) and to disambiguate speech input (Tanaka, 1999) . In contrast to these earlier studies, our work focuses on a different goal of using eye gaze for automated vocabulary acquisition and interpretation.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In research on multimodal interactive systems, recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances (Kaur et al., 2003) .", "mid_sen": "Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems (Campana et al., 2001) and to disambiguate speech input (Tanaka, 1999) . ", "after_sen": "In contrast to these earlier studies, our work focuses on a different goal of using eye gaze for automated vocabulary acquisition and interpretation."}
{"citeStart": 39, "citeEnd": 62, "citeStartToken": 39, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "The optimisations improve throughput by a factor of more than three. describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF 'backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the 'residue' of features in the unification grammar that are not encoded in the backbone. In this parser, the LALR(1) technique (Aho, Sethi Ullman, 1986 ) is used, in conjunction with a graph-structured stack (Tomita, 1987) , adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "describe a methodology for constructing an LR parser for a unificationbased grammar, in which a CF 'backbone' grammar is automatically constructed from the unification grammar, a parse table is constructed from the backbone grammar, and a parser is driven by the table and further controlled by unification of the 'residue' of features in the unification grammar that are not encoded in the backbone. ", "mid_sen": "In this parser, the LALR(1) technique (Aho, Sethi Ullman, 1986 ) is used, in conjunction with a graph-structured stack (Tomita, 1987) , adapting for unification-based parsing Kipps' (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching.", "after_sen": "On each reduction the parser performs the unifications specified by the unification grammar version of the CF backbone rule being applied. "}
{"citeStart": 129, "citeEnd": 141, "citeStartToken": 129, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand if the text unit is the sub-sentence we lace one major problem, that is the possibility that the resulting translation of the whole sentence will be of low quality, due to Ixmndary friction and incorrect chunking. In practice, EBMT systems that operate at sub-sentence level involve the dynamic derivation of the optimum length of segments of the input sentence by analysing the available parallel corpora. This rexluires a procedure for determining the best \"cover\" of an input text by segments of sentences contained in the database [Nirenburg 93 ]. It is assumed that the translation of the segments of the database that cover the input sentence is known. What is needed, therefore, is a procedure lbr aligning parallel texts at sub-sentence level [Kaji 921, [Sadler 901 . If sub-sentence alignment is available, the approach is fully automated but is quite vulnerable to the problem of luw quality as mentioned above, as well as to ambiguity problems when the produced segments are rather small.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In practice, EBMT systems that operate at sub-sentence level involve the dynamic derivation of the optimum length of segments of the input sentence by analysing the available parallel corpora. ", "mid_sen": "This rexluires a procedure for determining the best \"cover\" of an input text by segments of sentences contained in the database [Nirenburg 93 ]. ", "after_sen": "It is assumed that the translation of the segments of the database that cover the input sentence is known. "}
{"citeStart": 11, "citeEnd": 29, "citeStartToken": 11, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "Unlike in (Jiang and Ng, 2006) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data. Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al., 2005) . After the features of every constituent are extracted, each constituent is simply classified independently as either argument or non-argument.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These features are reproduced in Table 1 for easy reference.", "mid_sen": "Unlike in (Jiang and Ng, 2006) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data. ", "after_sen": "Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al., 2005) . "}
{"citeStart": 206, "citeEnd": 231, "citeStartToken": 206, "citeEndToken": 231, "sectionName": "UNKNOWN SECTION NAME", "string": "• Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996) , for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra struc-tural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967) . One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979) , which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992) . Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. ", "mid_sen": "For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992) . ", "after_sen": "Part of our work explores the in-between case, when only some constituent labels are available. "}
{"citeStart": 53, "citeEnd": 66, "citeStartToken": 53, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "Though theoretically very attractive, codescription has its price: (i) the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and (ii) there is a computational overhead when parsers use the complete descriptions. Problems of these kinds which were already noted by (Shieber, 1985) motivated tile research described here. The goal was to develop more flexible ways of using codescriptive grammars than having them applied by a parser with full informational power. The underlying observation is that constraints in such grammars can play different roles:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Though theoretically very attractive, codescription has its price: (i) the grammar is difficult to modularize due to the fact that the levels constrain each other mutually and (ii) there is a computational overhead when parsers use the complete descriptions. ", "mid_sen": "Problems of these kinds which were already noted by (Shieber, 1985) motivated tile research described here. ", "after_sen": "The goal was to develop more flexible ways of using codescriptive grammars than having them applied by a parser with full informational power. "}
{"citeStart": 46, "citeEnd": 66, "citeStartToken": 46, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003) . They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. These results imply the superiority of deep linguistic analysis for this task.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Interestingly, several studies suggested that the identification of PropBank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (Gildea and Hockenmaier, 2003; Chen and Rambow, 2003) . ", "mid_sen": "They employed a CCG (Steedman, 2000) or LTAG (Schabes et al., 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. ", "after_sen": "That is, they used deep analysis as a preprocessor to obtain useful features for training a probabilistic model or statistical classifier of a semantic argument identifier. "}
{"citeStart": 251, "citeEnd": 277, "citeStartToken": 251, "citeEndToken": 277, "sectionName": "UNKNOWN SECTION NAME", "string": "Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory. In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also \"remembers\" exceptional, low-frequency cases which are useful to extrapolate from. Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997) . We have used the following MBL algorithms1: IB1 : A variant of the k-nearest neighbor (k-NN) algorithm. The distance between a test item and each memory item is defined as the number of features for which they have a different value (overlap metric).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In recent work Daelemans et al. (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also \"remembers\" exceptional, low-frequency cases which are useful to extrapolate from. ", "mid_sen": "Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997) . ", "after_sen": "We have used the following MBL algorithms1: IB1 : A variant of the k-nearest neighbor (k-NN) algorithm. "}
{"citeStart": 86, "citeEnd": 107, "citeStartToken": 86, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "(III) The way in which spans are annotated as ar-guments to connectives also raises a challenge. First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003) , a span can serve as argument to >1 connective. Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005) , discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them. Instead, in annotating arguments, PDTB annotators have selected the minimal clausal text span needed to interpret the relation. This could comprise an embedded, subordinate or coordinate clause, an entire sentence, or a (possibly disjoint) sequence of sentences. As a result, there are fairly complex patterns of spans within and across sentences that serve as arguments to different connectives, and there are parts of sentences that don't appear within the span of any connective, explicit or implicit. The result is that the PDTB provides only a partial but complexly-patterned cover of the corpus. Understanding what's going on and what it implies for discourse structure (and possibly syntactic structure as well) is a challenge we're currently trying to address (Lee et al., 2006) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003) , a span can serve as argument to >1 connective. ", "mid_sen": "Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005) , discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them. ", "after_sen": "Instead, in annotating arguments, PDTB annotators have selected the minimal clausal text span needed to interpret the relation. "}
{"citeStart": 87, "citeEnd": 105, "citeStartToken": 87, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Statistical parsing using combined systems of handcoded linguistically fine-grained grammars and stochastic disambiguation components has seen considerable progress in recent years. However, such attempts have so far been confined to a relatively small scale for various reasons. Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems. Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000) , or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999) . Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text. The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994) .", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems. ", "mid_sen": "Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000) , or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999) . ", "after_sen": "Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text. "}
{"citeStart": 212, "citeEnd": 242, "citeStartToken": 212, "citeEndToken": 242, "sectionName": "UNKNOWN SECTION NAME", "string": "As the effort to engineer a grammar suitable for realization from the CCGbank proceeds in parallel to our work on hypertagging, we expect the hypertagger-seeded realizer to continue to improve, since a more complete and precise extracted grammar should enable more complete realizations to be found, and richer semantic representations should simplify the hypertagging task. Even with the current incomplete set of semantic templates, the hypertagger brings realizer performance roughly up to state-of-the-art levels, as our overall test set BLEU score (0.6701) slightly exceeds that of Cahill and van Genabith (2006) , though at a coverage of 96% instead of 98%. We caution, however, that it remains unclear how meaningful it is to directly compare these scores when the realizer inputs vary considerably in their specificity, as Langkilde-Geary's (2002) experiments dramatically illustrate.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As the effort to engineer a grammar suitable for realization from the CCGbank proceeds in parallel to our work on hypertagging, we expect the hypertagger-seeded realizer to continue to improve, since a more complete and precise extracted grammar should enable more complete realizations to be found, and richer semantic representations should simplify the hypertagging task. ", "mid_sen": "Even with the current incomplete set of semantic templates, the hypertagger brings realizer performance roughly up to state-of-the-art levels, as our overall test set BLEU score (0.6701) slightly exceeds that of Cahill and van Genabith (2006) , though at a coverage of 96% instead of 98%. ", "after_sen": "We caution, however, that it remains unclear how meaningful it is to directly compare these scores when the realizer inputs vary considerably in their specificity, as Langkilde-Geary's (2002) experiments dramatically illustrate."}
{"citeStart": 62, "citeEnd": 82, "citeStartToken": 62, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences). This is comparable to the better implementations presented in Dunlop et al. (2011) . As can be seen in Figure  11 , the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4× speedup against the sequential parser. On the GTX480, Block+PR is the fastest, showing a 25.8× speedup. Their runtimes were 0.32 seconds/sentence and 0.21 seconds/sentence, respectively. It is noteworthy that the fastest configuration differs for the two devices. We provide an explanation later in this section.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The exhaustive sequential CKY parser was written in C and is reasonably optimized, taking 5.5 seconds per sentence (or 5,505 seconds for the 1000 benchmark sentences). ", "mid_sen": "This is comparable to the better implementations presented in Dunlop et al. (2011) . ", "after_sen": "As can be seen in Figure  11 , the fastest configuration on the GTX285 is Block+PR+SS+tex:scores, which shows a 17.4× speedup against the sequential parser. "}
{"citeStart": 16, "citeEnd": 33, "citeStartToken": 16, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea is similar to Agrawal and Srikant's (1995) notion of generalized association rules. We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product). We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences. Let b, a 1 , a 2 , . . . a n or b, A denote the contents of an itemset, and c ( b, A ) denote the support for this itemset. For a given item b, π(b) denotes its immediate parent its value taxonomy, or 'root' for flat sets. For each item set, we collect rules b, A and compute their interestingness relative to the itemset π(b), A . Interestingness is defined as follows:", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We treat each appraisal expression as a transaction, with the attributes of attitude type, orientation, polarity, force, and thing type, as well as the document attributes product name, product type, and document classification (based on the number of stars the reviewer gave the product). ", "mid_sen": "We use CLOSET+ (Wang et al., 2003) to find all of the frequent closed itemsets in the data, with a support greater than or equal to 20 occurrences. ", "after_sen": "Let b, a 1 , a 2 , . . . a n or b, A denote the contents of an itemset, and c ( b, A ) denote the support for this itemset. "}
{"citeStart": 0, "citeEnd": 22, "citeStartToken": 0, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "A second possibility is to factor out recursive structures from a grammar. Thompson et al. (1991) show how this can be done for a phrase structure grammar (creating an equivalent 'Pree Adjoining (;rammar (,Ioshi I987)). The parser for the resulting grammar allows linear parsing tbr an (infinitely) parallel system, with Cite absorption of each word performed in constant time. At each choice point, there are only a finite number of possible new partial TAG trees (the TAG trees represents the possibly inlinite nmnbet of trees which can be forlned using adjunct|on). It should agMn be possible to extract 'default' semantic values, by taking the semantics from the TA(I tree (i.e. by assuming that there are to be ,to adj unctions). A somewhat similar system has recently been proposed by Shieber and Johnson (191t3) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A second possibility is to factor out recursive structures from a grammar. ", "mid_sen": "Thompson et al. (1991) show how this can be done for a phrase structure grammar (creating an equivalent 'Pree Adjoining (;rammar (,Ioshi I987)). ", "after_sen": "The parser for the resulting grammar allows linear parsing tbr an (infinitely) parallel system, with Cite absorption of each word performed in constant time. "}
{"citeStart": 54, "citeEnd": 73, "citeStartToken": 54, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but even here the recommendations are that much manual work is required to turn corpus examples into test cases (e.g., (Balkan and Fouvry, 1995) ). The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration. (Oepen and Flickinger, 1998) stress the interdependence between application and testsuite, but don't comment on the relation between grammar and testsuite.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Starting with (Flickinger et al., 1987) , testsuites have been drawn up from a linguistic viewpoint, \"in-]ormed by [the] study of linguistics and [reflecting] the grammatical issues that linguists have concerned themselves with\" (Flickinger et al., 1987, , p.4) . ", "mid_sen": "Although the question is not explicitly addressed in (Balkan et al., 1994) , all the testsuites reviewed there also seem to follow the same methodology. ", "after_sen": "The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998) , which built large multilingual testsuites, likewise fall into this category. "}
{"citeStart": 103, "citeEnd": 127, "citeStartToken": 103, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs. Combining statistical and parsing methods has been done by (Hindle, 1990; Hindle and Rooths,1991) and (Smadja and McKewon, 1990; Smadja,1991) . The novel aspect of our study is that we collect not only operational pairs, but triples, such as N_prep N, V_prep_N etc. In fact, the preposition convey important information on the nature of the semantic link between syntactically related content words. By looking at the preposition, it is possible to restrict the set of semantic relations underlying a syntactic relation (e.g. for=purpose,beneficiary). To extract syntactic associations two methods have been adopted in the literature. Smadja attempts to apply syntactic information to a set of automatically collected collocations (statistics-first). Hindle performs syntactic parsing before collocational analysis (syntax-first). In our study, we decided to adopt the syntax-first approach, because:", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Clustered association data are collected by first extracting from the corpus all the syntactically related word pairs. ", "mid_sen": "Combining statistical and parsing methods has been done by (Hindle, 1990; Hindle and Rooths,1991) and (Smadja and McKewon, 1990; Smadja,1991) . ", "after_sen": "The novel aspect of our study is that we collect not only operational pairs, but triples, such as N_prep N, V_prep_N etc. "}
{"citeStart": 316, "citeEnd": 327, "citeStartToken": 316, "citeEndToken": 327, "sectionName": "UNKNOWN SECTION NAME", "string": "The datasets and software to replicate our experiments are available from http://web.science.mq.edu.au/ bborschi/ tion of its effectiveness in adult speech processing (Cutler et al., 1986) . Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012) . Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007) , however, have until recently completely ignored stress. The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model , demonstrating that this leads to an improvement in segmentation performance. In this paper, we extend their work and show how to integrate stress cues into the flexible Adaptor Grammar framework (Johnson et al., 2007) . This allows us to both start from a stronger baseline model and to investigate how the role of stress cues interacts with other aspects of the model. In particular, we find that phonotactic cues to word-boundaries interact with stress cues, indicating synergistic effects for small inputs and partial redundancy for larger inputs. Overall, we find that stress cues add roughly 6% token f-score to a model that does not account for phonotactics and 4% to a model that already incorporates phonotactics. Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a) , we observe that phonotactic cues require more input than stress cues to be used efficiently. A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead of having to be pre-specified; and that our models correctly identify the predominant stress pattern of the input but underestimate the frequency of iambic words, which have been found to be missegmented by infant learners. The outline of the paper is as follows. In Section 2 we review prior work. In Section 3 we introduce our own models. In Section 4 we explain our experimental evaluation and its results. Section 5 discusses our findings, and Section 6 concludes and provides some suggestions for future research.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The datasets and software to replicate our experiments are available from http://web.science.mq.edu.au/ bborschi/ tion of its effectiveness in adult speech processing (Cutler et al., 1986) . ", "mid_sen": "Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012) . Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007) , however, have until recently completely ignored stress. ", "after_sen": "The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model , demonstrating that this leads to an improvement in segmentation performance. "}
{"citeStart": 186, "citeEnd": 199, "citeStartToken": 186, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "The Core Language Engine (CLE) is an application independent, unification based \"general purpose device for mapping between natural language sentences and logical form representations\" (Alshawi, 1992) . Its intermediate syntactic stages involve phrasal parsing followed by full syntactic analysis (top-down, left-corner). If the latter stage fails, CLE invokes partial parsing.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Tag checks though the current set of categories stop when one category passes, but backtrack and continue if parsing then fails.", "mid_sen": "The Core Language Engine (CLE) is an application independent, unification based \"general purpose device for mapping between natural language sentences and logical form representations\" (Alshawi, 1992) . ", "after_sen": "Its intermediate syntactic stages involve phrasal parsing followed by full syntactic analysis (top-down, left-corner). "}
{"citeStart": 47, "citeEnd": 65, "citeStartToken": 47, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "To solve these problems, we translate the whole sentences but with reordering constraints ensuring that the opinionated segments are preserved during translation. That is, the text between the relevant segment boundaries is not reordered nor mixed with the text outside these boundaries. 3 Thus the text in the target language segment comes only from the corresponding source language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation. In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3 . Moses also allows mark-up to be directly passed to the translation, via the x tag. We use this functionality to keep track, via the tags <ou[id][-label]> and </ou[id]>, of the segment boundaries (ou stands for Opinionated Unit), of the opinionated segment identifier ([id] ) and, for training and evaluation purposes, of the polarity label ([-label] ). In the example of Figure 3 , the id is 1 and the label is P.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "3 Thus the text in the target language segment comes only from the corresponding source language segment. ", "mid_sen": "We use the Moses statistical MT (SMT) toolkit (Koehn et al., 2007) to perform the translation. ", "after_sen": "In Moses, these reordering constraints are implemented with the zone and wall tags, as indicated in Figure 3 . "}
{"citeStart": 166, "citeEnd": 176, "citeStartToken": 166, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "Once a set of beliefs forming justification chains is identified, the system must then select from this set those belief chains which, when presented to the user, are predicted to convince the user of .bel. Our system will first construct a singleton set for each such justification chain and select the sets containing justification which, when presented, is predicted to convince the user of _bel. If no single justification chain is predicted to be sufficient to change the nser's beliefs, new sets will be constructed by combining the single justification chains, and the selection ~ is repeated. This will produce a set of possible candidate justification chains, and three heuristics will then be applied to select from among them. The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form (Luchok and McCroskey, 1978) . Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. The second heuristic prefers evidence that is novel to the user, since studies have shown that evidence is most persuasive ff it is previously unknown to the hearer (Wyer, 1970; Morley, 1987) . The third heuristic is based on C.nice's maxim of quantity and prefers justification chains that contain the fewest beliefs.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. ", "mid_sen": "The second heuristic prefers evidence that is novel to the user, since studies have shown that evidence is most persuasive ff it is previously unknown to the hearer (Wyer, 1970; Morley, 1987) . ", "after_sen": "The third heuristic is based on C.nice's maxim of quantity and prefers justification chains that contain the fewest beliefs."}
{"citeStart": 93, "citeEnd": 101, "citeStartToken": 93, "citeEndToken": 101, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand if the text unit is the sub-sentence we lace one major problem, that is the possibility that the resulting translation of the whole sentence will be of low quality, due to Ixmndary friction and incorrect chunking. In practice, EBMT systems that operate at sub-sentence level involve the dynamic derivation of the optimum length of segments of the input sentence by analysing the available parallel corpora. This rexluires a procedure for determining the best \"cover\" of an input text by segments of sentences contained in the database [Nirenburg 93 ]. It is assumed that the translation of the segments of the database that cover the input sentence is known. What is needed, therefore, is a procedure lbr aligning parallel texts at sub-sentence level [Kaji 921, [Sadler 901 . If sub-sentence alignment is available, the approach is fully automated but is quite vulnerable to the problem of luw quality as mentioned above, as well as to ambiguity problems when the produced segments are rather small.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is assumed that the translation of the segments of the database that cover the input sentence is known. ", "mid_sen": "What is needed, therefore, is a procedure lbr aligning parallel texts at sub-sentence level [Kaji 921, [Sadler 901 . ", "after_sen": "If sub-sentence alignment is available, the approach is fully automated but is quite vulnerable to the problem of luw quality as mentioned above, as well as to ambiguity problems when the produced segments are rather small."}
{"citeStart": 113, "citeEnd": 114, "citeStartToken": 113, "citeEndToken": 114, "sectionName": "UNKNOWN SECTION NAME", "string": "The situation ~q, used above in the mutual belief induction schema, is the context of what has been said. This schema supports a weak model of mutual beliefs, that is more akin to mutual assumptions or mutual suppositions [13] . Mutual beliefs can be inferred based on some evidence, but these beliefs may depend on underlying assumptions that are easily defensible. This model can be implemented using Gallier's theory of autonomous belief revision and the corresponding system [4] .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mutual beliefs can be inferred based on some evidence, but these beliefs may depend on underlying assumptions that are easily defensible. ", "mid_sen": "This model can be implemented using Gallier's theory of autonomous belief revision and the corresponding system [4] .", "after_sen": "A key part of this model is that some types of evidence provide better support for beliefs than other types. "}
{"citeStart": 118, "citeEnd": 136, "citeStartToken": 118, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluate this hypertagger in two ways: first, we evaluate it as a tagger, where the hypertagger achieves high single-best (93.6%) and multitagging labelling accuracies (95.8-99.4% with category per lexical predication ratios ranging from 1.1 to 3.9). 2 Second, we compare a hypertagger-augmented version of OpenCCG's chart realizer with the preexisting chart realizer (White et al., 2007) that simply instantiates the chart with all possible CCG categories (subject to frequency cutoffs) for each input LF predicate. The hypertagger-seeded realizer runs approximately twice as fast as the pre-existing OpenCCG realizer and finds a larger number of complete realizations, resorting less to chart fragment assembly in order to produce an output within a 15 second per-sentence time limit. Moreover, the overall BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCGbank) are higher for the hypertagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in OpenCCG using a corpus-derived grammar. Section 3 describes our hypertagging approach and how it is integrated into the realizer. Section 4 describes our results, followed by related work in Section 5 and our conclusions in Section 6.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We evaluate this hypertagger in two ways: first, we evaluate it as a tagger, where the hypertagger achieves high single-best (93.6%) and multitagging labelling accuracies (95.8-99.4% with category per lexical predication ratios ranging from 1.1 to 3.9). ", "mid_sen": "2 Second, we compare a hypertagger-augmented version of OpenCCG's chart realizer with the preexisting chart realizer (White et al., 2007) that simply instantiates the chart with all possible CCG categories (subject to frequency cutoffs) for each input LF predicate. ", "after_sen": "The hypertagger-seeded realizer runs approximately twice as fast as the pre-existing OpenCCG realizer and finds a larger number of complete realizations, resorting less to chart fragment assembly in order to produce an output within a 15 second per-sentence time limit. "}
{"citeStart": 51, "citeEnd": 73, "citeStartToken": 51, "citeEndToken": 73, "sectionName": "UNKNOWN SECTION NAME", "string": "The support/oppose classification problem can be approached through the use of standard classifiers such as support vector machines (SVMs), which consider each text unit in isolation. As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. Our classification framework, directly inspired by Blum and Chawla (2001) , integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label. In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As discussed in Section 1, however, the conversational nature of our data implies the existence of various relationships that can be exploited to improve cumulative classification accuracy for speech segments belonging to the same debate. ", "mid_sen": "Our classification framework, directly inspired by Blum and Chawla (2001) , integrates both perspectives, optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label. ", "after_sen": "In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships."}
{"citeStart": 271, "citeEnd": 297, "citeStartToken": 271, "citeEndToken": 297, "sectionName": "UNKNOWN SECTION NAME", "string": "We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000) . To our knowledge, we are the first to report tagging results in the semantic-to-syntactic direction. We have also shown that, by integrating this hypertagger with a broad-coverage CCG chart realizer, considerably faster realization times are possible (approximately twice as fast as compared with a realizer that performs simple lexical look-ups) with higher BLEU, METEOR and exact string match scores. Moreover, the hypertagger-augmented realizer finds more than twice the number of complete realizations, and further analysis revealed that the realization quality (as per modified BLEU and ME-TEOR) is higher in the cases when the realizer finds a complete realization. This suggests that further improvements to the hypertagger will lead to more complete realizations, hence more high-quality realizations. Finally, further efforts to engineer a grammar suitable for realization from the CCGbank should provide richer feature sets, which, as our feature ablation study suggests, are useful for boosting hypertagging performance, hence for finding better and more complete realizations.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000) . ", "after_sen": "To our knowledge, we are the first to report tagging results in the semantic-to-syntactic direction. "}
{"citeStart": 197, "citeEnd": 215, "citeStartToken": 197, "citeEndToken": 215, "sectionName": "UNKNOWN SECTION NAME", "string": "A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994) , word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004) , collocation extraction (Schone and Jurafsky, 2001 ), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975) . In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997 ) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998) . Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Examples include automatic thesaurus extraction (Grefenstette, 1994) , word sense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004) , collocation extraction (Schone and Jurafsky, 2001 ), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975) . ", "mid_sen": "In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997 ) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998) . ", "after_sen": "Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004) ."}
{"citeStart": 47, "citeEnd": 52, "citeStartToken": 47, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "Why should we, as computational linguists, be interested in factors that contribute to the interactivity of a discourse? There are both theoretical and practical motivations. First, we wish to extend formal accounts of single utterances produced by single speakers to explain multi-participant, multi-utterance discourses [Po186, CP86] . Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . Since conversation is a collaborative process [CWG86, SSJ74] , models of conversation can provide the basis for extending planning theories [GS90, CLNO90] . When the situation requires the negotiation of a collaborative plan, these theories must account for the interacting beliefs and intentions of multiple participants. ~,From a practical perspective, there is ample evidence that limited mixed-initiative has contributed to lack of system usability. Many researchers have noted that the absence of mixed-initiative gives rise to two problems with expert systems: They don't allow users to participate in the reasoning process, or to ask the questions they want answered [PHW82, Kid85, FL89] . In addition, question answering systems often fail to take account of the system's role as a conversational partner.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Previous studies of the discourse structure of multiparticipant dialogues have often factored out the role of MIXED-INITIATIVE, by allocating control to one participant [Gro77, Coh84] , or by assuming a passive listener [McK85, Coh87] . ", "mid_sen": "Since conversation is a collaborative process [CWG86, SSJ74] , models of conversation can provide the basis for extending planning theories [GS90, CLNO90] . ", "after_sen": "When the situation requires the negotiation of a collaborative plan, these theories must account for the interacting beliefs and intentions of multiple participants. "}
{"citeStart": 134, "citeEnd": 136, "citeStartToken": 134, "citeEndToken": 136, "sectionName": "UNKNOWN SECTION NAME", "string": "It seems a perfectly valid rule of conversation not to tell people what they already know. Indeed, Grice's QUANTITY lllaxim has often been interpreted this way: Do not make your contribution more informative than is required [f] . Stalnaker, as well, suggests that to assert something that is already presupposed is to attempt to do something that is already done [14] . Thus, the notion of what is informative is judged against a background of what is presupposed, i.e. propositions that all conversants assume are mutually known or believed. These propositions are known as the COMMON GROUND [10, 5] .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Do not make your contribution more informative than is required [f] . ", "mid_sen": "Stalnaker, as well, suggests that to assert something that is already presupposed is to attempt to do something that is already done [14] . ", "after_sen": "Thus, the notion of what is informative is judged against a background of what is presupposed, i.e. propositions that all conversants assume are mutually known or believed. "}
{"citeStart": 74, "citeEnd": 88, "citeStartToken": 74, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods. The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance. Similar methods have also been used in the field of intrinsic plagiarism detection, which involves segmenting a text and then identifying outlier segments (Stamatatos, 2009; .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Subsequent work, such as the Stahel -Donoho Estimator (Stahel, 1981; Donoho, 1982) , PCout (Filzmoser et al., 2008) , LOF (Breunig and Kriegel, 2000) and ABOD (Kriegel et al., 2008) have generalized univariate methods to highdimensional data points.", "mid_sen": "In his comprehensive review of outlier detection methods in textual data, Guthrie (2008) compares a variety of vectorization methods along with a variety of generic outlier methods. ", "after_sen": "The vectorization methods employ a variety of lexical and syntactic stylistic features, while the outlier detection methods use a variety of similarity/distance measures such as cosine and Euclidean distance. "}
{"citeStart": 91, "citeEnd": 117, "citeStartToken": 91, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words. The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004) . Both identify product features from reviews, but OPINE significantly improves on both. (Hu and Liu, 2004) doesn't assess candidate features, so its precision is lower than OPINE's. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE's focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. ", "mid_sen": "OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . ", "after_sen": "Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . "}
{"citeStart": 123, "citeEnd": 146, "citeStartToken": 123, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "Events are occurrences that have a structure to them; in particular, their result, or their coming to an end is included in them: to destroy a building, to write a book. As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished. While Bach (1986) did not investigate the internal structure of events, others suggested that this needs to be done (e.g., Moens and Steedman 1988; Parsons 1990) . Pustejovsky (1991) treated Vendlerian accomplishments and achievements as transitions from a state Q(y) to NOT-Q(y), and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished. ", "mid_sen": "While Bach (1986) did not investigate the internal structure of events, others suggested that this needs to be done (e.g., Moens and Steedman 1988; Parsons 1990) . ", "after_sen": "Pustejovsky (1991) treated Vendlerian accomplishments and achievements as transitions from a state Q(y) to NOT-Q(y), and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state."}
{"citeStart": 134, "citeEnd": 153, "citeStartToken": 134, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "Participants were asked to schedule a health care appointment with each of the 9 systems, yielding a total of 9 dialogues per participant. System utterances were generated using a simple template-based algorithm and synthesised using the speech synthesis system Cerevoice (Aylett et al., 2006) , which has been shown to be intelligible to older users (Wolters et al., 2007) . The human wizard took over the function of the speech recognition, language understanding, and dialogue management components.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Participants were asked to schedule a health care appointment with each of the 9 systems, yielding a total of 9 dialogues per participant. ", "mid_sen": "System utterances were generated using a simple template-based algorithm and synthesised using the speech synthesis system Cerevoice (Aylett et al., 2006) , which has been shown to be intelligible to older users (Wolters et al., 2007) . ", "after_sen": "The human wizard took over the function of the speech recognition, language understanding, and dialogue management components."}
{"citeStart": 198, "citeEnd": 210, "citeStartToken": 198, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "Language reuse involves two components: a source text written by a human and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993) , the use of entire factual sentences extracted from corpora (e.g., \"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995) . In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. ", "mid_sen": "Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993) .", "after_sen": "Stydying the concept of language reuse is rewarding because it allows generation systems to leverage on texts written by humans and their deliberate choice of words, facts, structure."}
{"citeStart": 225, "citeEnd": 259, "citeStartToken": 225, "citeEndToken": 259, "sectionName": "UNKNOWN SECTION NAME", "string": "Left-corner parsing Left-corner (LC) parsing (Rosenkrantz and Lewis II, 1970) is a well-known strategy that uses both bottom-up evidence (from the left corner of a rule) and top-down prediction (of the rest of the rule). Rosenkrantz and Lewis showed how to transform a context-free grammar into a grammar that, when used by a topdown parser, follows the same search path as an LC parser. These LC grammars allow us to use exactly the same predictive parser to evaluate top-down versus LC parsing. Naturally, an LC grammar performs best with our parser when right binarized, for the same reasons outlined above. We use transform composition to apply first one transform, then another to the output of the first. We denote this A o B where (A o B) (t) = B (A (t)). After applying the left-corner transform, we then binarize the resulting grammar 5, i.e. LC o RB. Another probabilistic LC parser investigated (Manning and Carpenter, 1997), which utilized an LC parsing architecture (not a transformed grammar), also got a performance boost 4The very efficient bottom-up statistical parser detailed in Charniak et al. (1998) measured efficiency in terms of total edges popped. An edge (or, in our case, a parser state) is considered when a probability is calculated for it, and we felt that this was a better efficiency measure than simply those popped. As a baseline, their parser considered an average of 2216 edges per sentence in section 22 of the WSJ corpus (p.c.).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After applying the left-corner transform, we then binarize the resulting grammar 5, i.e. LC o RB. ", "mid_sen": "Another probabilistic LC parser investigated (Manning and Carpenter, 1997), which utilized an LC parsing architecture (not a transformed grammar), also got a performance boost 4The very efficient bottom-up statistical parser detailed in Charniak et al. (1998) measured efficiency in terms of total edges popped. ", "after_sen": "An edge (or, in our case, a parser state) is considered when a probability is calculated for it, and we felt that this was a better efficiency measure than simply those popped. "}
{"citeStart": 32, "citeEnd": 44, "citeStartToken": 32, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand, in the context of automatic lexicon construction, the emphasis is mainly on the extraction of lexical/semantic collocational knowledge of specific words rather than its use in sentence parsing. For example, Haruno (1995) applied an information-theoretic data compression technique to corpus-based case frame learning, and proposed a method of finding case frames of verbs as compressed representation of verb-noun collocational data in corpus. The work concentrated on the extraction of declarative representation of case frames and did not consider their performance in sentence parsing. As in the case of the models of Black (1993) , Magerman (1995) , and Collins (1996) , this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. However, unlike the models of Black (1993) , Magerman (1995) , and Collins (1996) , we put an assumption that syntactic and lexical/semantie features are independent. Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The work concentrated on the extraction of declarative representation of case frames and did not consider their performance in sentence parsing. ", "mid_sen": "As in the case of the models of Black (1993) , Magerman (1995) , and Collins (1996) , this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis. ", "after_sen": "However, unlike the models of Black (1993) , Magerman (1995) , and Collins (1996) , we put an assumption that syntactic and lexical/semantie features are independent. "}
{"citeStart": 166, "citeEnd": 189, "citeStartToken": 166, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "Aspectual coercion, a second type of aspectual transformation, can take place when a clause is modified by an aspectual marker that violates an aspectual constraint (Moens and Steedman 1988; Pustejovsky 1991) . In this case, an alternative interpretation of the clause is inferred which satisfies the aspectual constraint. For example, the progressive marker is constrained to appear with an extended event. Therefore, if it appears with an atomic event, e.g., 12He hiccupped (point), the event is transformed to an extended event, e.g.,", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "I finished staring at it (culminated process).", "mid_sen": "Aspectual coercion, a second type of aspectual transformation, can take place when a clause is modified by an aspectual marker that violates an aspectual constraint (Moens and Steedman 1988; Pustejovsky 1991) . ", "after_sen": "In this case, an alternative interpretation of the clause is inferred which satisfies the aspectual constraint. "}
{"citeStart": 166, "citeEnd": 184, "citeStartToken": 166, "citeEndToken": 184, "sectionName": "UNKNOWN SECTION NAME", "string": "Here we test how well the systems perform using the same small annotated training set, the 3299 words of elementary school reading comprehension test bodies used in Ferro et al. (1999) . 2 We are mainly interested in comparing the parts of the system that takes in syntax (noun, verb, etc.) chunks (also known as groups) and finds the GRs between those chunks. So for the experiment, we used the general TiMBL system to just reconstruct the part of the MB system that takes in chunks and finds GRs. The input to both this reconstructed part and the TR system is data that has been manually annotated for syntax chunks and GRs, along with automatic lexeme and sentence segmentation and part-ofspeech tagging. In addition, the TR system has manual named-entity annotation, and automatic estimations for verb properties and preposition and subordinate conjunction attachments (Ferro et al., 1999) . Because the MB system was originally designed to handle GRs attached to verbs (and not noun to noun GRs, etc.), we ran the reconstructed part to only find GRs to verbs, and ignored other types of GRs when comparing the reconstructed part with the TR system. The test set is the 1151 word test set used in Ferro et al. (1999) . Only GRs to verbs were examined, so the effective training set GR count fell from 1963 to 1298 and test set GR count from 748 to 500.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The input to both this reconstructed part and the TR system is data that has been manually annotated for syntax chunks and GRs, along with automatic lexeme and sentence segmentation and part-ofspeech tagging. ", "mid_sen": "In addition, the TR system has manual named-entity annotation, and automatic estimations for verb properties and preposition and subordinate conjunction attachments (Ferro et al., 1999) . ", "after_sen": "Because the MB system was originally designed to handle GRs attached to verbs (and not noun to noun GRs, etc.), we ran the reconstructed part to only find GRs to verbs, and ignored other types of GRs when comparing the reconstructed part with the TR system. "}
{"citeStart": 25, "citeEnd": 48, "citeStartToken": 25, "citeEndToken": 48, "sectionName": "UNKNOWN SECTION NAME", "string": "We assign the score of the lowest-scoring bigram because we are interested in anomalous sequences. This is in the spirit of Květon and Oliva (2002) , who define invalid bigrams for POS annotation sequences in order to detect annotation errors.. As one example, consider (6), where the reduced rule NP → NP DT NNP is composed of the bigrams START NP, NP DT, DT NNP, and NNP END. All of these are relatively common (more than a hundred occurrences each), except for NP DT, which appears in only two other rule types. Indeed, DT is an incorrect tag (NNP is correct): when NP is the first daughter of NP, it is generally a possessive, precluding the use of a determiner.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We assign the score of the lowest-scoring bigram because we are interested in anomalous sequences. ", "mid_sen": "This is in the spirit of Květon and Oliva (2002) , who define invalid bigrams for POS annotation sequences in order to detect annotation errors.. As one example, consider (6), where the reduced rule NP → NP DT NNP is composed of the bigrams START NP, NP DT, DT NNP, and NNP END. ", "after_sen": "All of these are relatively common (more than a hundred occurrences each), except for NP DT, which appears in only two other rule types. "}
{"citeStart": 131, "citeEnd": 155, "citeStartToken": 131, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many of the NLU systems developed in the 70's indu(le(l a kind of error recovery Inechanisln ranging flom the treatment only of spelling e.rrors, PARRY (1)arkinson c 't al., 1977) , to tile inclusion also of incomplete int)ut containing some kind of ellipsis, LAD-DEll,/LIFEll (Hendrix et al., 1977) .", "mid_sen": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . ", "after_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. "}
{"citeStart": 305, "citeEnd": 314, "citeStartToken": 305, "citeEndToken": 314, "sectionName": "UNKNOWN SECTION NAME", "string": "During the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems. Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models Magerman, 1995) , training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993) , or other techniques (Bod, 1993) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "During the last few years large treebanks have become available to many researchers, which has resulted in researches applying a range of new techniques for parsing systems. ", "mid_sen": "Most of the methods that are being suggested include some kind of Machine Learning, such as history based grammars and decision tree models Magerman, 1995) , training or inducing statistical grammars (Black, Garside and Leech, 1993; Pereira and Schabes, 1992; Schabes et al., 1993) , or other techniques (Bod, 1993) .", "after_sen": "Consequently, syntactical analysis has become an area with a wide variety of (a) algorithms and methods for learning and parsing, and (b) type of information used for learning and parsing (sometimes referred to as feature set). "}
{"citeStart": 137, "citeEnd": 149, "citeStartToken": 137, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967) , although the latter is often augmented with top-down predic-tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975) . Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. Although incorporating unification increases their complexity to exponential on grammar size and input length (section 3), this appears to have little impact on practical performance (section 4). Sections 5 and 6 discuss these findings and present conclusions.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, parsing algorithms assume more importance for grammars having more substantial phrase structure components, such as CLARE (which although employing some HPSGlike analyses still contains several tens of rules) and the ANLT (which uses a formalism derived from GPSG; Gazdar et al., 1985) , sincethe more specific rule set can be used to control which unifications are performed.", "mid_sen": "In NL analysis, the syntactic information associated with lexical items makes top-down parsing less attractive than bottom-up (e.g. CKY; Kasami, 1965; Younger, 1967) , although the latter is often augmented with top-down predic-tion to improve performance (e.g. Earley, 1970; Lang, 1974; Pratt, 1975) . ", "after_sen": "Section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up CF parsing algorithms. "}
{"citeStart": 152, "citeEnd": 166, "citeStartToken": 152, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) . The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures ( feature sets, but then efficiency becomes a problem. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on re-ranking using a finite set of features (Collins, 2000) . In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000) . The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999) . Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000) . The difference from previous approaches is in the nature of the input to the log-linear model. We do not use handcrafted features, but instead we use a finite vector of real-valued features which are induced as part of the neural network training process. These induced features represent the information about the derivation history which the training process has decided is relevant to estimating the output probabilities. In neural networks these feature vectors are called the hidden layer activations, but for continuity with the previous discussion we will refer to them as the history features.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The outputs of this network are probability estimates computed with a log-linear model (also known as a maximum entropy model), as is done in (Ratnaparkhi, 1999) . ", "mid_sen": "Log-linear models have proved successful in a wide variety of applications, and are the inspiration behind one of the best current statistical parsers (Charniak, 2000) . ", "after_sen": "The difference from previous approaches is in the nature of the input to the log-linear model. "}
{"citeStart": 138, "citeEnd": 148, "citeStartToken": 138, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "(1) Before John makes a phone call, he always lights up a cigarette. (Partee, 1984) (2) Often, when Anne came home late, Paul had already prepared dinner. (de Swart, 1991) (3) When he came home, he always switched on the tv. He took a beer and sat down in his armchair to forget the day. (de Swart, 1991) (4) When John is at the beach, he always squints when the sun is shining. (de Swart, 1991) The analysis of sentences such as (1) in (Partee, 1984) , within the framework of Discourse Representation Theory (DRT) (Kamp, 1981) gives the wrong truth-conditions, when the temporal connective in the sentence is before or after. In DRT, such sentences trigger box-splitting with the eventuality of the subordinate clause and an updated reference time in the antecedent box, and the eventuality of the main clause in the consequent box, causing undesirable universal quantification over the reference time.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(de Swart, 1991) (4) When John is at the beach, he always squints when the sun is shining. ", "mid_sen": "(de Swart, 1991) The analysis of sentences such as (1) in (Partee, 1984) , within the framework of Discourse Representation Theory (DRT) (Kamp, 1981) gives the wrong truth-conditions, when the temporal connective in the sentence is before or after. ", "after_sen": "In DRT, such sentences trigger box-splitting with the eventuality of the subordinate clause and an updated reference time in the antecedent box, and the eventuality of the main clause in the consequent box, causing undesirable universal quantification over the reference time."}
{"citeStart": 121, "citeEnd": 132, "citeStartToken": 121, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We are not aware of other work that uses such collocations as we do.", "mid_sen": "Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999) .", "after_sen": "We are not aware of other work identifying and using density parameters as described in this article."}
{"citeStart": 98, "citeEnd": 111, "citeStartToken": 98, "citeEndToken": 111, "sectionName": "UNKNOWN SECTION NAME", "string": "In recent years there has been a resurgence of interest in statistical approaches to natural language processing. Such approaches are not new, witness the statistical approach to machine translation suggested by Weaver (1955) , but the current level of interest is largely due to the success of applying hidden Markov models and N-gram language models in speech recognition. This success was directly measurable in terms of word recognition error rates, prompting language processing researchers to seek corresponding improvements in performance and robustness. A speech translation system, which by necessity combines speech and language technology, is a natural place to consider combining the statistical and conventional approaches and much of this paper describes probabilistic models of structural language analysis and translation. Our aim will be to provide an overall model for translation with the best of both worlds. Various factors will lead us to conclude that a lexicalist statistical model with dependency relations is well suited to this goal.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In recent years there has been a resurgence of interest in statistical approaches to natural language processing. ", "mid_sen": "Such approaches are not new, witness the statistical approach to machine translation suggested by Weaver (1955) , but the current level of interest is largely due to the success of applying hidden Markov models and N-gram language models in speech recognition. ", "after_sen": "This success was directly measurable in terms of word recognition error rates, prompting language processing researchers to seek corresponding improvements in performance and robustness. "}
{"citeStart": 77, "citeEnd": 97, "citeStartToken": 77, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "The task of identifying sentence boundaries in text has not received as much attention as it deserves. Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996) ). Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992) ).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g. (Brill, 1994; Collins, 1996) ). ", "mid_sen": "Others perform the division implicitly without discussing performance (e.g. (Cutting et al., 1992) ).", "after_sen": "On first glance, it may appear that using a short list of sentence-final punctuation marks, such as., ?, and /, is sufficient. "}
{"citeStart": 150, "citeEnd": 173, "citeStartToken": 150, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. (Quinlan 1986; Bahl et al. 1989 ) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994) . We tested the Satz system using the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. These results are discussed in Section 4.10.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Sections 4.1-4.9 we present results of testing the Satz system with a neural network, including investigations of the effects of varying network parameters such as hidden layer size, threshold values, and amount of training data. ", "mid_sen": "(Quinlan 1986; Bahl et al. 1989 ) have been successfully applied to NLP problems such as parsing (Resnik 1993; Magerman 1995) and discourse analysis (Siegel and McKeown 1994; Soderland and Lehnert 1994) . ", "after_sen": "We tested the Satz system using the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm and compared the results to those obtained previously with the neural network. "}
{"citeStart": 150, "citeEnd": 160, "citeStartToken": 150, "citeEndToken": 160, "sectionName": "UNKNOWN SECTION NAME", "string": "Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily. Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. Luhn, 1958; Edmundson, 1969) or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g. Kupiec et al., 1995; Mani and Bloedorn, 1998) . Neither approach is very satisfactory. Relying only on your own intuitions inevitably creates a biased resource; indeed, Rath et al. (1961) report low agreement between human judges carrying out this kind of task. On the other hand, using abstracts as targets is not necessarily a good gold standard for comparison of the systems' results, although abstracts are the only kind of gold standard that comes for free with the papers. Even if the abstracts are written by professional abstractors, there are considerable differences in length, structure, and information content. This is due to differences in the common abstract presentation style in different disciplines and to the projected use of the abstracts (cf. Liddy, 1991) . In the case of our corpus, an additional problem was the fact that the abstracts are written by the authors themselves and thus susceptible to differences in individual writing style.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, the question of what constitutes a useful gold standard has not yet been solved satisfactorily. ", "mid_sen": "Researchers developing corpus resources for summarisation work have often defined their own gold standard, relying on their own intuitions (see, e.g. Luhn, 1958; Edmundson, 1969) or have used abstracts supplied by authors or by professional abstractors as their gold standard (e.g. Kupiec et al., 1995; Mani and Bloedorn, 1998) . ", "after_sen": "Neither approach is very satisfactory. "}
{"citeStart": 29, "citeEnd": 48, "citeStartToken": 29, "citeEndToken": 48, "sectionName": "UNKNOWN SECTION NAME", "string": "We implemented Cubit, a Python clone of the Pharaoh decoder (Koehn, 2004) , and adapted cube pruning to it as follows. As in Pharaoh, each bin i contains hypotheses (i.e., +LM items) covering i words on the source-side. But at each bin (see Figure 5) , all +LM items from previous bins are first partitioned into −LM items; then the hyperedges leading from those −LM items are further grouped into hyperedge bundles ( Figure 6 ), which are placed into the priority queue of the current bin. Our data preparation follows Huang et al. (2006) : the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side. We use the same test set as (Huang et al., 2006) , which is a 140-sentence subset of the NIST 2003 test set with 9-36 words on the English side. The weights for the log-linear model are tuned on a separate development set. We set the decoder phrase-table limit to 100 as suggested in (Koehn, 2004) and the distortion limit to 4. Figure 7 (a) compares cube pruning against fullintegration in terms of search quality vs. search efficiency, under various pruning settings (threshold beam set to 0.0001, stack size varying from 1 to 200). Search quality is measured by average model cost per sentence (lower is better), and search efficiency is measured by the average number of hypotheses generated (smaller is faster). At each level of search quality, the speed-up is always better than a factor of 10. The speed-up at the lowest searcherror level is a factor of 32. Figure 7 (b) makes a similar comparison but measures search quality by BLEU, which shows an even larger relative speed-up for a given BLEU score, because translations with very different model costs might have similar BLEU scores. It also shows that our full-integration implementation in Cubit faithfully reproduces Pharaoh's performance. Fixing the stack size to 100 and varying the threshold yielded a similar result.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But at each bin (see Figure 5) , all +LM items from previous bins are first partitioned into −LM items; then the hyperedges leading from those −LM items are further grouped into hyperedge bundles ( Figure 6 ), which are placed into the priority queue of the current bin. ", "mid_sen": "Our data preparation follows Huang et al. (2006) : ", "after_sen": "the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side. "}
{"citeStart": 61, "citeEnd": 76, "citeStartToken": 61, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "As our second experimental goal, we compared the models SL-DOP and LS-DOP explained in Section 3.2. Recall that for n=1, SL-DOP is equal to the PCFG-reduction of Bod (2001) (which we also called Likelihood-DOP) while LS-DOP is equal to Simplicity-DOP. Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12. But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity -DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP. The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000) , who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. While SL-DOP and LS-DOP have been compared before in Bod (2002) , especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%. ", "mid_sen": "This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bod's PCFG-reduction reported in Table 1 . Compared to the reranking technique in Collins (2000) , who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction. ", "after_sen": "While SL-DOP and LS-DOP have been compared before in Bod (2002) , especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones. "}
{"citeStart": 222, "citeEnd": 236, "citeStartToken": 222, "citeEndToken": 236, "sectionName": "UNKNOWN SECTION NAME", "string": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . That is, the current system learns procedures rather than data structures. There is some literature on procedure acquisition such as the LISP synthesis work described in Biermann et al. (1984) and the PROLOG synthesis method of Shapiro (1982) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The VNLCE processor may be considered to be a learning system of the tradition described, for example, in Michalski et al. (1984) . ", "mid_sen": "The current system learns finite state flowcharts whereas typical learning systems usually acquire coefficient values as in Minsky and Papert (1969) , assertional statements as in Michalski (1980) , or semantic nets as in Winston (1975) . ", "after_sen": "That is, the current system learns procedures rather than data structures. "}
{"citeStart": 55, "citeEnd": 72, "citeStartToken": 55, "citeEndToken": 72, "sectionName": "UNKNOWN SECTION NAME", "string": "Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones. We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima'an (2007) . In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go. Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (Levinger et al., 1995; ) will make the parser more robust and suitable for use in more realistic scenarios.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go. ", "mid_sen": "Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. (Levinger et al., 1995; ) will make the parser more robust and suitable for use in more realistic scenarios.", "after_sen": "An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc."}
{"citeStart": 38, "citeEnd": 58, "citeStartToken": 38, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "Some researchers, e.g., [Mann, 1988; KowtkoetaL, 1991] , assume a library of discourse level actions, sometimes called dialogue games, which encode common communicative interactions. To be co-operative, an agent must always be participating in one of these games. So if a question is asked, only a fixed number of activities, namely those introduced by a question, are cooperative responses. Games provide a better explanation of coherence, but still require the agent's to recognize each other's intentions to perform the dialogue game. As a result, this work can be viewed as a special case of the intentional view. An interesting model is described by [Airenti et al., 1993] , which separates out the conversational games from the task-related games in a way similar way to [Litman and Allen, 1987] . Because of this separation, they do not have to assume co-operation on the tasks each agent is performing, but still require recognition of intention and cooperation at the conversational level. It is left unexplained what goals motivate conversational co-operation.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a result, this work can be viewed as a special case of the intentional view. ", "mid_sen": "An interesting model is described by [Airenti et al., 1993] , which separates out the conversational games from the task-related games in a way similar way to [Litman and Allen, 1987] . ", "after_sen": "Because of this separation, they do not have to assume co-operation on the tasks each agent is performing, but still require recognition of intention and cooperation at the conversational level. "}
{"citeStart": 29, "citeEnd": 47, "citeStartToken": 29, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "Our work can be compared to (Zhang et al., 2008) which uses CRF for automatic keyword extraction from documents; they reported promising results (F-score of 51.25 percent) on a Chinese corpus.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Precision Recall F-Measure Rule-based 85.40 91.89 Machine Learning 81.8 75.00 78.26 Table 3 : Precision, Recall, F-measure of the Various Methods.", "mid_sen": "Our work can be compared to (Zhang et al., 2008) which uses CRF for automatic keyword extraction from documents; they reported promising results (F-score of 51.25 percent) on a Chinese corpus.", "after_sen": "It can also be compared to (Zhang and Wu, 2012) , which uses a multi-level termhood method to extract terminology candidates from a bilingual corpus. "}
{"citeStart": 31, "citeEnd": 45, "citeStartToken": 31, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "Polarity information is useful for several reasons. First of all, it can filter out positive outcomes if the question is about the negative aspects of a medication. Secondly, negative outcomes may be crucial even if the question does not explicitly ask about them. Finally, from the number of positive or negative descriptions of the outcome of a medication applying to a disease, clinicians can form a general idea about how \"good\" the medication is. As a first step in understanding opposing relations between scenarios in medical text, the polarity of outcomes was determined by an automatic classification process. We use support vector machines (SVMs) to distinguish positive outcomes from negative ones. SVMs have been shown to be efficient in text classification tasks (Joachims, 1998) . Given a training sample, the SVM finds a hyperplane with the maximal margin of separation between the two classes. The classification is then just to determine which side of the hyperplane the test sample lies in. We used the SVM light package (Joachims, 2002) in our experiment.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The classification is then just to determine which side of the hyperplane the test sample lies in. ", "mid_sen": "We used the SVM light package (Joachims, 2002) in our experiment.", "after_sen": ""}
{"citeStart": 191, "citeEnd": 203, "citeStartToken": 191, "citeEndToken": 203, "sectionName": "UNKNOWN SECTION NAME", "string": "The HPSG grammar developed with MicroCUF models a fragment of German. Since our focus is on the lexicon, the range of syntactic variation treated is currently limited to simplex sentences with canonical word order. We have incorporated some recent developments of HPSG, esp. the revisions of Pollard & Sag (1994, ch. 9) , Manning & Sag (1995) 's proposal for an independent level of argument structure and Bouma (1997) 's use of argument structure to eliminate procedural lexical rules in favour of relational constraints. Our elaborate ontology of semantic types -useful for non-trivial acquisition of selectional restrictions and nominal sorts -was derived from a systematic corpus study of a biological domain (Knodel 1980, 154-188) . The grammar also covers all valence classes encountered in the corpus. As for the lexicon format, we currently list full forms only. Clearly, a morphology component would supply more contextual information from known affixes but would still require the processing of unknown stems.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since our focus is on the lexicon, the range of syntactic variation treated is currently limited to simplex sentences with canonical word order. ", "mid_sen": "We have incorporated some recent developments of HPSG, esp. the revisions of Pollard & Sag (1994, ch. 9) , Manning & Sag (1995) 's proposal for an independent level of argument structure and Bouma (1997) 's use of argument structure to eliminate procedural lexical rules in favour of relational constraints. ", "after_sen": "Our elaborate ontology of semantic types -useful for non-trivial acquisition of selectional restrictions and nominal sorts -was derived from a systematic corpus study of a biological domain (Knodel 1980, 154-188) . "}
{"citeStart": 95, "citeEnd": 116, "citeStartToken": 95, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "In design and construction, the system is similar to our submission from last year's workshop (Hanneman et al., 2010) , with changes in the methods we employed for training data selection and SCFG filtering. Continuing WMT's general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French-English training data and an English language model of 1.8 billion words. Decod-ing was carried out in Joshua (Li et al., 2009) , an open-source framework for parsing-based MT. We managed our experiments with LoonyBin , an open-source tool for defining, modifying, and running complex experimental pipelines.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is our fourth yearly submission to the WMT shared translation task.", "mid_sen": "In design and construction, the system is similar to our submission from last year's workshop (Hanneman et al., 2010) , with changes in the methods we employed for training data selection and SCFG filtering. ", "after_sen": "Continuing WMT's general trend, we worked with more data than in previous years, basing our 2011 system on 13.9 million sentences of parallel French-English training data and an English language model of 1.8 billion words. "}
{"citeStart": 158, "citeEnd": 171, "citeStartToken": 158, "citeEndToken": 171, "sectionName": "UNKNOWN SECTION NAME", "string": "The incorporation of unification into the CE parser follows the methodology developed for unification-based LR parsing described in the previous section: a table is computed from a CF 'backbone', and a parser, augmented with on-line unification and feature-based subsumption opera-tions, is driven by the table. To allow meaningful comparison with the LR parser, the CE parser uses a one-word lookahead version of the table, constructed using a modified LALR technique (Carroll, 1993) 3 .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The incorporation of unification into the CE parser follows the methodology developed for unification-based LR parsing described in the previous section: a table is computed from a CF 'backbone', and a parser, augmented with on-line unification and feature-based subsumption opera-tions, is driven by the table. ", "mid_sen": "To allow meaningful comparison with the LR parser, the CE parser uses a one-word lookahead version of the table, constructed using a modified LALR technique (Carroll, 1993) 3 .", "after_sen": "To achieve the cubic time bound, the parser must be able to retrieve in unit time all items in the chart having a given state, and start and end position in the input string. "}
{"citeStart": 217, "citeEnd": 233, "citeStartToken": 217, "citeEndToken": 233, "sectionName": "UNKNOWN SECTION NAME", "string": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996) ), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. ", "mid_sen": "Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g., (Leacock et al., 1993) , (Mooney, 1996) , (Ng and Lee, 1996) , (Pealersen and ). ", "after_sen": "These studies represent the context in which an ambiguous word occurs with a wide variety of features. "}
{"citeStart": 120, "citeEnd": 144, "citeStartToken": 120, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "When constructing a l,exieal Knowledge Ilase (1,KB) useful for Natural l,anguage Processing, the source of information from which knowledge is acquired and the structuring of this information within the LKB are two key issues. Machine Readable Dictionaries (MIH)s) are a good sour(:e of lexical information and have been shown to be al)plical)le to the task of I,KII COllStruction (l)ola.n ct al., 1993; Calzolari, t992; Copestake, [990; Wilks et al., 1989; Byrd et al., 1987) . Often though, a localist approaeh is adopted whereby the words are kept in alphabetical order with some representation of their definitions in the form of a template or feature structure. F, flbrt in findlug cormections between words is seen in work on automatic extraction of sem~mtic relations Dora MRI)s (Ahlswede and Evens, 1988; Alshawi, 1989; Montemagrfi and Vandorwende, 19!32) . Additionally, effort in finding words that are close semantically is seen by the current interest in statistical techniques for word clustering, looking at (-ooccurrences of words in text corpora or dictionaries (Church and IIanks, 1989; Wilks et al., 1989; Brown et al., 11992; l'ereira et al., 11995) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Often though, a localist approaeh is adopted whereby the words are kept in alphabetical order with some representation of their definitions in the form of a template or feature structure. ", "mid_sen": "F, flbrt in findlug cormections between words is seen in work on automatic extraction of sem~mtic relations Dora MRI)s (Ahlswede and Evens, 1988; Alshawi, 1989; Montemagrfi and Vandorwende, 19!32) . ", "after_sen": "Additionally, effort in finding words that are close semantically is seen by the current interest in statistical techniques for word clustering, looking at (-ooccurrences of words in text corpora or dictionaries (Church and IIanks, 1989; Wilks et al., 1989; Brown et al., 11992; l'ereira et al., 11995) ."}
{"citeStart": 31, "citeEnd": 43, "citeStartToken": 31, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "(27) a. Clinton was introduced by John because Mary had refused to, and Gore was too. The current approach accounts for these cases. The works of Priist (1992) and Asher (1993) provide analyses of VP-ellipsis 16 in the context of an account of discourse structure and coherence. With l~This claim could be dispensed with in the treatment of VP-eUipsis, perhaps at the cost of some degree of theoretical inelegance. However, this aspect was crucial for handling the gapping data, since the infelicity of gapping in non-parallel constructions hinged on there no longer being a propositional representation available as a source.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The current approach accounts for these cases. ", "mid_sen": "The works of Priist (1992) and Asher (1993) provide analyses of VP-ellipsis 16 in the context of an account of discourse structure and coherence. ", "after_sen": "With l~This claim could be dispensed with in the treatment of VP-eUipsis, perhaps at the cost of some degree of theoretical inelegance. "}
{"citeStart": 329, "citeEnd": 354, "citeStartToken": 329, "citeEndToken": 354, "sectionName": "UNKNOWN SECTION NAME", "string": "The second part of Figure 3 shows declares how quantifiers are represented, which are required since the sentences to be processed may have determiners. forall and exists are encoded similarly to abstraction, in that they take a functional argument and so object-level binding of variables by quantifiers is handled by meta-hvel A-abstraction. >> and tt are simple constructors for implication and conjunction, to be used with forall and exists respectively, in the typical manner (Pereira and Shieber, 1987) . For example, the sentence every man found a bone has as a possible LF (8a), with the AProlog representation (8b)10:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second part of Figure 3 shows declares how quantifiers are represented, which are required since the sentences to be processed may have determiners. ", "mid_sen": "forall and exists are encoded similarly to abstraction, in that they take a functional argument and so object-level binding of variables by quantifiers is handled by meta-hvel A-abstraction. >> and tt are simple constructors for implication and conjunction, to be used with forall and exists respectively, in the typical manner (Pereira and Shieber, 1987) . ", "after_sen": "For example, the sentence every man found a bone has as a possible LF (8a), with the AProlog representation (8b)10:"}
{"citeStart": 106, "citeEnd": 116, "citeStartToken": 106, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Given the effectiveness of distributional similarity measures for numerous tasks in NLP and the interpretation of kernels as similarity functions, it seems natural to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification. As shown in Section 2.2 the standardly used linear and Gaussian kernels derive from the L 2 distance, yet Lee (1999) has shown that this distance measure is relatively poor at comparing co-occurrence distributions. Information theory provides a number of alternative distance functions on probability measures, of which the L 1 distance (also called variational distance), Kullback-Leibler divergence and Jensen-Shannon divergence are well-known in NLP and Negated nsd functions are sometimes called conditionally psd; they constitute a superset of the psd functions.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given the effectiveness of distributional similarity measures for numerous tasks in NLP and the interpretation of kernels as similarity functions, it seems natural to consider the use of kernels tailored for co-occurrence distributions when performing semantic classification. ", "mid_sen": "As shown in Section 2.2 the standardly used linear and Gaussian kernels derive from the L 2 distance, yet Lee (1999) has shown that this distance measure is relatively poor at comparing co-occurrence distributions. ", "after_sen": "Information theory provides a number of alternative distance functions on probability measures, of which the L 1 distance (also called variational distance), Kullback-Leibler divergence and Jensen-Shannon divergence are well-known in NLP and Negated nsd functions are sometimes called conditionally psd; they constitute a superset of the psd functions."}
{"citeStart": 46, "citeEnd": 69, "citeStartToken": 46, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "Cube pruning (Chiang, 2007 ) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005) , it achieves significant speed-up over full-integration on Chiang's Hiero system.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Cube pruning (Chiang, 2007 ) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. ", "mid_sen": "By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005) , it achieves significant speed-up over full-integration on Chiang's Hiero system.", "after_sen": "We push the idea behind this method further and make the following contributions in this paper:"}
{"citeStart": 139, "citeEnd": 150, "citeStartToken": 139, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006) . Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). In addition, subjects (SS) have a score above 90%. In all these cases, the dependent has a configurationally defined (but not fixed) position with respect to its head.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A more fine-grained analysis of the Swedish results reveals a high accuracy for function words, which is compatible with previous studies (Nivre, 2006) . ", "after_sen": "Thus, the labeled F-score is 100% for infinitive markers (IM) and subordinating conjunctions (UK), and above 95% for determiners (DT). "}
{"citeStart": 133, "citeEnd": 143, "citeStartToken": 133, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Inspired by research in the. areas of semantic relations, semantic distance, concept clustering, and using (,once I tual (Ji a l hs (Sowa, 1984) as our knowledge representation, we introduce (;oncept (?lustering I{nowledge Graphs (CCKGs). Each (JCKG will start as a Conceptual Graph representation of a trigger word and will expaud following a search algorit, hm to incorporate related words and ibrm a C'oncept Cn,s(,er. The concept chlstcr in itself is interesting for tasks such as word disambiguation, but the C(~K(] will give more to that cluster. It will give the relations between the words, making the graph in some aspects similar to a script (Schank and Abelson, 11975) . llowever, a CCK(I is generated automaticMly and does not rely on prin,itives but on an unlimited number of concel)ts , showing objects, persons, and actions interacting with each other. This interaction will be set, within a lmrtieular domain, and the trigger word should be a key word of the domain to represent. 11' that process would be done for the whole dictionary, we would obtain an l,l( II divided into multiple clusters of words, each represented by a CCK(]. Then during text processing fin: example, a portion of text could be analyzed using the appropriate CCK(] to lind implicit relations and hell) understanding the text.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Additionally, effort in finding words that are close semantically is seen by the current interest in statistical techniques for word clustering, looking at (-ooccurrences of words in text corpora or dictionaries (Church and IIanks, 1989; Wilks et al., 1989; Brown et al., 11992; l'ereira et al., 11995) .", "mid_sen": "Inspired by research in the. areas of semantic relations, semantic distance, concept clustering, and using (,once I tual (Ji a l hs (Sowa, 1984) as our knowledge representation, we introduce (;oncept (?lustering I{nowledge Graphs (CCKGs). ", "after_sen": "Each (JCKG will start as a Conceptual Graph representation of a trigger word and will expaud following a search algorit, hm to incorporate related words and ibrm a C'oncept Cn,s(,er. "}
{"citeStart": 130, "citeEnd": 149, "citeStartToken": 130, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "Performance of POS tagging is an important factor in our methods because they are based on word/POS sequences. Existing POS taggers might not perform well on non-native English texts because they are normally developed to analyze native English texts. Considering this, we tested CRFTagger 6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011) . It turned out that CRFTagger achieved an accuracy of 0.932 (compared to 0.970 on native texts). Although it did not perform as well as on native texts, it still achieved a fair accuracy. Accordingly, we decided to use it in our experiments.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Existing POS taggers might not perform well on non-native English texts because they are normally developed to analyze native English texts. ", "mid_sen": "Considering this, we tested CRFTagger 6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011) . ", "after_sen": "It turned out that CRFTagger achieved an accuracy of 0.932 (compared to 0.970 on native texts). "}
{"citeStart": 115, "citeEnd": 133, "citeStartToken": 115, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Our experiments were conducted on the Sentiment Scale dataset (v1.0), 4 which comprises four subcorpora of 1770, 902, 1307 and 1027 movie reviews with an associated mapping to a three and four-star rating for each review. Each sub-corpus is written by a different author (denoted Author A, B, C and D respectively), thus avoiding calibration error between individual authors and their ratings. Review texts are automatically filtered to leave only subjective sentences (motivated by the results described in (Pang and Lee, 2004) ); the mean number of words per review in each subjectivefiltered sub-corpus is 435, 374, 455 and 292 respectively. Table 1 summarizes the results for the four MCST-SVM variants (the results that are statistically significant compared to the centroid/no-culling option are boldfaced).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Each sub-corpus is written by a different author (denoted Author A, B, C and D respectively), thus avoiding calibration error between individual authors and their ratings. ", "mid_sen": "Review texts are automatically filtered to leave only subjective sentences (motivated by the results described in (Pang and Lee, 2004) ); the mean number of words per review in each subjectivefiltered sub-corpus is 435, 374, 455 and 292 respectively. ", "after_sen": "Table 1 summarizes the results for the four MCST-SVM variants (the results that are statistically significant compared to the centroid/no-culling option are boldfaced)."}
{"citeStart": 128, "citeEnd": 139, "citeStartToken": 128, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to be able to compare our results with the results obtained by other researchers, we worked with the same data sets already used by (Ramshaw and Marcus, 1995; Argamon et al., 1998) for NP and SV detection. These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al., 1993) . For NP, the training and test corpus was prepared from sections 15 to 18 and section 20, respectively; the SV corpus was prepared from sections 1 to 9 for training and section 0 for testing. Instead of using the NP bracketing information present in the tagged Treebank data, Ramshaw and Marcus modified the data so as to include bracketing information related only to the non-recursive, base NPs present in each sentence while the subject verb phrases were taken as is. The data sets include POS tag information generated by Ramshaw and Marcus using Brill's transformational part-of-speech tagger (Brill, 1995) . The sizes of the training and test data are summarized in Table 1 and Table 2 .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Instead of using the NP bracketing information present in the tagged Treebank data, Ramshaw and Marcus modified the data so as to include bracketing information related only to the non-recursive, base NPs present in each sentence while the subject verb phrases were taken as is. ", "mid_sen": "The data sets include POS tag information generated by Ramshaw and Marcus using Brill's transformational part-of-speech tagger (Brill, 1995) . ", "after_sen": "The sizes of the training and test data are summarized in Table 1 and Table 2 ."}
{"citeStart": 48, "citeEnd": 77, "citeStartToken": 48, "citeEndToken": 77, "sectionName": "UNKNOWN SECTION NAME", "string": "In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. We propose to use Markov Logic Networks (ML-N) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. It can easily incorporate rich linguistic features and global constraints by designing various logic formulas, which can also be viewed as templates or rules. Logic formulas are combined in a probabilistic framework to model soft constraints. Hence, the proposed approach can benefit a lot from this framework.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this work, we aim to identify subjective text segments and extract their corresponding explanations from product reviews in discourse level. ", "mid_sen": "We propose to use Markov Logic Networks (ML-N) (Richardson and Domingos, 2006) to learn the joint model for subjective classification and explanatory relation extraction. ", "after_sen": "MLN has been applied in several natural language processing tasks (Singla and Domingos, 2006; Poon and Domingos, 2008; Yoshikawa et al., 2009; Andrzejewski et al., 2011; Song et al., 2012) and demonstrated its advantages. "}
{"citeStart": 119, "citeEnd": 129, "citeStartToken": 119, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "As another example, certain techniques to deal with ill-formed input can be characterized as finite state transducers (Lang, 1989) ; the composition of an input string with such a finite state transducer results in a FSA that can then be input for syntactic parsing. Such an approach allows for the treatment of missing, extraneous, interchanged or misused words (Teitelbaum, 1973; Saito and Tomita, 1988; Nederhof and Bertsch, 1994) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "FSA of course generalizes such word lattices.", "mid_sen": "As another example, certain techniques to deal with ill-formed input can be characterized as finite state transducers (Lang, 1989) ; the composition of an input string with such a finite state transducer results in a FSA that can then be input for syntactic parsing. ", "after_sen": "Such an approach allows for the treatment of missing, extraneous, interchanged or misused words (Teitelbaum, 1973; Saito and Tomita, 1988; Nederhof and Bertsch, 1994) ."}
{"citeStart": 80, "citeEnd": 100, "citeStartToken": 80, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on Soon et al. (2001) . They built a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier (Denis and Baldridge, 2007) , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they did not enforce transitivity.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier's evaluation of that pair. ", "mid_sen": "Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b) , the type of classifier (Denis and Baldridge, 2007) , and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b) . ", "after_sen": "This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. "}
{"citeStart": 216, "citeEnd": 230, "citeStartToken": 216, "citeEndToken": 230, "sectionName": "UNKNOWN SECTION NAME", "string": "In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by Hofmann (2000) in terms of categorization accuracy.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We later show that the computational advantage is shared by a more general class of models.", "mid_sen": "In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by Hofmann (2000) in terms of categorization accuracy.", "after_sen": ""}
{"citeStart": 171, "citeEnd": 190, "citeStartToken": 171, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "The intrinsic quality of word segmentation is normally evaluated against a manually segmented gold-standard corpus using F-score. While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations. First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996) . Second, an increase in F-score does not necessarily imply an improvement in translation quality. It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008) . Consequently, we chose to extrinsically evaluate the performance of our approach via the Chinese-English translation task, i.e. we measure the influence of the segmentation process on the final translation output. The quality of the translation output is mainly evaluated using BLEU, with NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) as complementary metrics.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations. ", "mid_sen": "First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996) . ", "after_sen": "Second, an increase in F-score does not necessarily imply an improvement in translation quality. "}
{"citeStart": 166, "citeEnd": 181, "citeStartToken": 166, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "Adverbial phrases, whether phrasal (e.g. 'On Sunday') or clausal (e.g. 'When Bill left'), are processed before the main clause. They introduce a reference time, which overrides the current reference time, and provides an anaphoric antecedent for the tense in the main clause. This mechanism is used to explain how tense and temporal adverbials can combine to temporally locate the occurrence, without running into problems of relative scope (ttinrichs, 1988) . The tense morpheme of the main clause locates the event time with respect to the reference time, whereas temporal adverbials are used to locate the reference time.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They introduce a reference time, which overrides the current reference time, and provides an anaphoric antecedent for the tense in the main clause. ", "mid_sen": "This mechanism is used to explain how tense and temporal adverbials can combine to temporally locate the occurrence, without running into problems of relative scope (ttinrichs, 1988) . ", "after_sen": "The tense morpheme of the main clause locates the event time with respect to the reference time, whereas temporal adverbials are used to locate the reference time."}
{"citeStart": 48, "citeEnd": 61, "citeStartToken": 48, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "The next problem is how to apply these features when predicting a new case since the active features for a new case may make opposite predictions. One simple and effective strategy is employed by the decision list (Rivest, 1987) , i.e., always applying the strongest features. In a decision list, all the features are sorted in order of descending confidence. When a new target concept is classified, the classifier runs down the list and compares the features against the contexts of the target concept. The first matched feature is applied to make a predication. Obviously, how to measure the confidence of features is a very important issue for the decision list. We use the metric described in (Yarowsky, 1994; Golding, 1995) . Provided that", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Obviously, how to measure the confidence of features is a very important issue for the decision list. ", "mid_sen": "We use the metric described in (Yarowsky, 1994; Golding, 1995) . ", "after_sen": "Provided that"}
{"citeStart": 135, "citeEnd": 155, "citeStartToken": 135, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea of using the machine readable source of a published dictionary has occurred to a wide range of researchers, for spelling correction, lexical analysis, thesaurus construction, and machine translation, to name but a few applications. Most of the work on automated dictionaries has concentrated on extracting lexical or other information, essentially by batch processing (eg. Amsler, 1981 ; Walker and Amsler, 1986), or Copyright 1987 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/87/030203-218503.00 on developing dictionary servers for office automation systems (Kay, 1984b) . Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982; Byrd, 1983) ; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details) . However, as yet few results have been published concerning the utility of electronic versions of published dictionaries as sources for such lexicons. In this paper we provide an evaluation of the LDOCE grammar code system from this perspective.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Few established parsing systems have substantial lexicons and even those which employ very comprehensive grammars (eg. Robinson, 1982; Bobrow, 1978) consult relatively small lexicons, typically generated by hand. ", "mid_sen": "Two exceptions to this generalisation are the Linguistic String Project (Sager, 1981) and the IBM CRITIQUE (formerly EPISTLE) Project (Heidorn et al., 1982; Byrd, 1983) ; the former employs a dictionary of approximately 10,000 words, most of which are specialist medical terms, the latter has well over 100,000 entries, gathered from machine readable sources. ", "after_sen": "In addition, there are a number of projects under way to develop substantial lexicons from machine readable sources (see Boguraev, 1986 for details) . "}
{"citeStart": 161, "citeEnd": 192, "citeStartToken": 161, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "The Web is being used to address data sparseness for language modeling. In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al. (2003) \"balance\" their corpus using Web documents.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Web is being used to address data sparseness for language modeling. ", "mid_sen": "In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al. (2003) \"balance\" their corpus using Web documents.", "after_sen": "The information retrieval community now has a Web track as a component of its TREC evaluation initiative. "}
{"citeStart": 206, "citeEnd": 222, "citeStartToken": 206, "citeEndToken": 222, "sectionName": "UNKNOWN SECTION NAME", "string": "r A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005) , or the corpus itself was significantly smaller than ours (Feng et al. 2006; Leuski et al. 2006) . The representativeness of the sample size was not discussed in any of these studies.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "r Only an automatic evaluation was performed, which relied on having model responses .", "mid_sen": "r A user study was performed, but it was either very small compared to the corpus (Carmel, Shtalhaim, and Soffer 2000; Jijkoun and de Rijke 2005) , or the corpus itself was significantly smaller than ours (Feng et al. 2006; Leuski et al. 2006) . ", "after_sen": "The representativeness of the sample size was not discussed in any of these studies."}
{"citeStart": 109, "citeEnd": 127, "citeStartToken": 109, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "The modern Chinese language is a highly abbreviated one due to the mixed use of ancient singlecharacter words with modern multi-character words and compound words. According to Chang and Lai (2004) , approximately 20% of sentences in a typical news article have abbreviated words in them. Abbreviations have become even more popular along with the development of Internet media (e.g., online chat, weblog, newsgroup, and so on). While English words are normally abbreviated by either their first letters (i.e. acronyms) or via truncation, the formation of Chinese abbreviations is much more complex. Figure 1 shows two examples for Chinese abbreviations. Clearly, an abbreviated form of a word can be obtained by selecting one or more characters from this word, and the selected characters can be at any position in the word. In an extreme case, there are even re-ordering between a full-form phrase and its abbreviation. While the research in statistical machine translation (SMT) has made significant progress, most SMT systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006) rely on parallel corpora to extract translation entries. The richness and complexness of Chinese abbreviations imposes challenges to the SMT systems. In particular, many Chinese abbreviations may not appear in available parallel corpora, in which case current SMT systems treat them as unknown words and leave them untranslated. This affects the translation quality significantly.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In an extreme case, there are even re-ordering between a full-form phrase and its abbreviation. ", "mid_sen": "While the research in statistical machine translation (SMT) has made significant progress, most SMT systems (Koehn et al., 2003; Chiang, 2007; Galley et al., 2006) rely on parallel corpora to extract translation entries. ", "after_sen": "The richness and complexness of Chinese abbreviations imposes challenges to the SMT systems. "}
{"citeStart": 65, "citeEnd": 75, "citeStartToken": 65, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "We used \"X no Y\" (Y of X) to resolve indirect anaphora. But we would achieve get a higher accuracy rate if we could utilize a good noun case frame dictionary. Therefore we have to consider how to construct a noun case frame dictionary. A key is to get the detailed meaning of \"no (of)\" in \"X no Y.\" If it is automatically obtainable, a noun case frame dictionary could be constructed automatically. Even if the semantic analysis of \"X no Y\" is not done well, we think that it is still possible to construct the dictionary using \"X no Y.\" For example, we arrange \"noun X no noun Y\" by the meaning of \"noun Y,\" arrange them by the meaning of \"noun X\", delete those where \"noun X\" is an adjective noun, and obtain the results shown in Table 7 . In this case, we use the thesaurus dictionary \"Bunrui Goi Hyou\" (NLRI, 1964) to learn the meanings of nouns. It should not be difficult to construct a noun case frame dictionary by hand using Table 7 . We will make a noun case frame dictionary by removing aite (partner) in the line of kokumin (nation), raihin (visitor) in the line of genshu (the head of state), and noun phrases which mean characters and features. When we look over the noun phrases for kokumin (nation), we notice that almost all of them refer to countries. So we will also make the semantic constraint (or the semantic preference) that countries can be connected to kokumin (nation). When we make a noun case frame dictionary, we must remember that examples of \"X no Y\" are insufficient and we must add examples. For example, in the line of genshu (the head of state) there are few nouns that mean countries. In this case, it is good to add examples by from the arranged nouns for kokumin (nation), which is similar to genshu (the head of state). Since in this method examples are arranged by meaning in this method, it will not be very difficult to add examples.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Even if the semantic analysis of \"X no Y\" is not done well, we think that it is still possible to construct the dictionary using \"X no Y.\" For example, we arrange \"noun X no noun Y\" by the meaning of \"noun Y,\" arrange them by the meaning of \"noun X\", delete those where \"noun X\" is an adjective noun, and obtain the results shown in Table 7 . ", "mid_sen": "In this case, we use the thesaurus dictionary \"Bunrui Goi Hyou\" (NLRI, 1964) to learn the meanings of nouns. ", "after_sen": "It should not be difficult to construct a noun case frame dictionary by hand using Table 7 . "}
{"citeStart": 167, "citeEnd": 189, "citeStartToken": 167, "citeEndToken": 189, "sectionName": "UNKNOWN SECTION NAME", "string": "Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a) . A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be im-portant to provide more accurate tree structures and labels (Nivre and Scholz, 2004) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. ", "mid_sen": "On the other hand, joint learning models can benefit from edge-label information that has proven to be im-portant to provide more accurate tree structures and labels (Nivre and Scholz, 2004) .", "after_sen": "Previous studies explored the trade-off between computational costs and parsing performance. "}
{"citeStart": 182, "citeEnd": 194, "citeStartToken": 182, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990) . We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990) . ", "after_sen": "We extend this notion to suggest that words with similar meanings should be near each other in a semantic taxonomy, and in particular will likely share a hypernym as a near parent."}
{"citeStart": 141, "citeEnd": 164, "citeStartToken": 141, "citeEndToken": 164, "sectionName": "UNKNOWN SECTION NAME", "string": "Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001) . Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009) . On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. 1343", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On one side, we plan to explore alternative ways to build phrase and paraphrase tables. ", "mid_sen": "One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001) . ", "after_sen": "Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009) . "}
{"citeStart": 64, "citeEnd": 89, "citeStartToken": 64, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "Words are normalized into their base forms by the GENIA tagger (Tsuruoka and Tsujii, 2005) , which is a part-of-speech tagger trained for the biomedical Rank OBJECTIVE METHOD RESULTS CONCLUSIONS 1 # to be measure % ) suggest that 2 be to be perform ( p may be 3 to determine n = p < # these 4 study be be compare ) . should be 5 this study be determine % . these result Table 2 : Bigram features with high χ 2 values ('#' stands for a beginning of a sentence).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use features for sentence contents represented by: i) words, ii) word bigrams, and iii) mixture of words and word bigrams.", "mid_sen": "Words are normalized into their base forms by the GENIA tagger (Tsuruoka and Tsujii, 2005) , which is a part-of-speech tagger trained for the biomedical Rank OBJECTIVE METHOD RESULTS CONCLUSIONS 1 # to be measure % ) suggest that 2 be to be perform ( p may be 3 to determine n = p < # these 4 study be be compare ) . should be 5 this study be determine % . these result Table 2 : Bigram features with high χ 2 values ('#' stands for a beginning of a sentence).", "after_sen": "domain. "}
{"citeStart": 123, "citeEnd": 146, "citeStartToken": 123, "citeEndToken": 146, "sectionName": "UNKNOWN SECTION NAME", "string": "Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999) . Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. We wish to use our model on arguments of any frequency, including those that never occurred in the training corpus (and therefore have empty cooccurrence features (Section 3.3.1)). We proceed as follows: first, we exclude pairs whenever the noun occurs less than 3 times in our corpus, removing many misspellings and other noun noise. Next, we omit verb co-occurrence features for nouns that occur less than 10 times, and instead fire a low-count feature. When we move to a new corpus, previouslyunseen nouns are treated like these low-count training nouns.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We set the MI-threshold, τ , to be 0, and the negative-to-positive ratio, K, to be 2.", "mid_sen": "Numerous previous pseudodisambiguation evaluations only include arguments that occur between 30 and 3000 times (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999) . ", "after_sen": "Presumably the lower bound is to help ensure the negative argument is unobserved because it is unsuitable, not because of data sparseness. "}
{"citeStart": 26, "citeEnd": 44, "citeStartToken": 26, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "Coming right up to date, (Marcu et al., 2006 ) demonstrate that 'syntactified' target language phrases can improve translation quality for Chinese-English. They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the target parse-tree into a source sentence. The rewriting/transduction process is driven by \"xRS rules\", each consisting of a pair of a source phrase and a (possibly only partially) lexicalized syntactified target phrase. In order to extract xRS rules, the word-to-word alignment induced from the parallel training corpus is used to guide heuristic tree 'cutting' criteria.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Chiang's derived grammar does not rely on any linguistic annotations or assumptions, so that the 'syntax' induced is not linguistically motivated.", "mid_sen": "Coming right up to date, (Marcu et al., 2006 ) demonstrate that 'syntactified' target language phrases can improve translation quality for Chinese-English. ", "after_sen": "They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the target parse-tree into a source sentence. "}
{"citeStart": 121, "citeEnd": 144, "citeStartToken": 121, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "In the last decade, SVM has become one of the most studied techniques for text classification, due to the positive results it has shown. This technique uses the vector space model for the documents' representation, and assumes that documents in the same class should fall into separable spaces of the representation. Upon this, it looks for a hyperplane that separates the classes; therefore, this hyperplane should maximize the distance between it and the nearest documents, what is called the margin. The following function is used to define the hyperplane (see Figure  1 ): In order to resolve this function, all the possible values should be considered and, after that, the values of w and b that maximize the margin should be selected. This would be computationally expensive, so the following equivalent function is used to relax it (Boser et al. , 1992) (Cortes and Vapnik, 1995) :", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to resolve this function, all the possible values should be considered and, after that, the values of w and b that maximize the margin should be selected. ", "mid_sen": "This would be computationally expensive, so the following equivalent function is used to relax it (Boser et al. , 1992) (Cortes and Vapnik, 1995) :", "after_sen": "f (x) = w • x + b"}
{"citeStart": 147, "citeEnd": 169, "citeStartToken": 147, "citeEndToken": 169, "sectionName": "UNKNOWN SECTION NAME", "string": "Many methods have been proposed to measure the co-occurrence relation between two words such as χ 2 (Church and Mercer,1993) , mutual information (Church and Hanks, 1989; Pantel and Lin, 2002) , t-test (Church and Hanks, 1989) , and loglikelihood (Dunning,1993) . In this paper a revised formula of mutual information is used to measure the association since mutual information of a lowfrequency word pair tends to be very high. Table 1 gives the contingency table for two words or phrases w 1 and w 2 , where A is the number of reviews where w 1 and w 2 co-occur; B indicates the number of reviews where w 1 occurs but does not co-occur with w 2 ; C denotes the number of reviews where w 2 occurs but does not co-occur with w 1 ; D is number of reviews where neither w 1 nor w 2 occurs; N = A + B + C + D.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the absolute value of the relative distance in a sentence for a feature and an opinion word is less than Minimum-Offset, they are considered contextdependent.", "mid_sen": "Many methods have been proposed to measure the co-occurrence relation between two words such as χ 2 (Church and Mercer,1993) , mutual information (Church and Hanks, 1989; Pantel and Lin, 2002) , t-test (Church and Hanks, 1989) , and loglikelihood (Dunning,1993) . ", "after_sen": "In this paper a revised formula of mutual information is used to measure the association since mutual information of a lowfrequency word pair tends to be very high. "}
{"citeStart": 71, "citeEnd": 94, "citeStartToken": 71, "citeEndToken": 94, "sectionName": "UNKNOWN SECTION NAME", "string": "The advantage of syntactic models is that they incorporate a richer, structured notion of context. This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. It is also ableat least in principle -to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010) . At the same time, syntactic spaces are much more prone to sparsity problems, as their contexts are sparser. This leads to reliability and coverage problems.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The advantage of syntactic models is that they incorporate a richer, structured notion of context. ", "mid_sen": "This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. ", "after_sen": "It is also ableat least in principle -to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010) . "}
{"citeStart": 123, "citeEnd": 134, "citeStartToken": 123, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Fortunately, there is reason to believe that lexical disambiguation can proceed on more limited syntactic patterns. Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text. The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category (Church 1987; Garside, Leech and Sampson 1987; DeRose 1988) . These stochastic methods show impressive performance: Church reports a success rate of 95 to 99%, and shows a sample text with an error rate of less than one percent. What may seem particularly surprising is that these methods succeed essentially without reference to syntactic structure; purely surface lexical patterns are involved. In contrast to these recent stochastic methods, earlier methods based on categorical rules for surface patterns achieved only moderate success. Thus for example, Klein and Simmons (1963) and Greene and Rubin (1971) report success rates considerably below recent stochastic approaches.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Indeed, recent increased interest in the problem of disambiguating lexical category in English has led to significant progress in developing effective programs for assigning lexical category in unrestricted text. ", "mid_sen": "The most successful and comprehensive of these are based on probabilistic modeling of category sequence and word category (Church 1987; Garside, Leech and Sampson 1987; DeRose 1988) . ", "after_sen": "These stochastic methods show impressive performance: "}
{"citeStart": 245, "citeEnd": 263, "citeStartToken": 245, "citeEndToken": 263, "sectionName": "UNKNOWN SECTION NAME", "string": "A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A treebank is a body of natural language text which has been grammatically annotated by hand, in terms of some previously-established scheme of grammatical analysis. ", "mid_sen": "Treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers (Black et al., 1992; Brill, 1994; Merialdo, 1994; Weischedel et al., 1993) and for statistical parsers (Black et al., 1993; Brill, 1993; aelinek et al., 1994; Magerman, 1995; Magerman and Marcus, 1991) .", "after_sen": "In this article, we present the AT'R/Lancaster 7'reebauk of American English, a new resource tbr natural-language-, processing research, which has been prepared by Lancaster University (UK)'s Unit for Computer Research on the English Language, according to specifications provided by ATR (Japan)'s Statistical Parsing Group. "}
{"citeStart": 32, "citeEnd": 44, "citeStartToken": 32, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "2.2 Quantification over events (Partee, 1984) extends Hinrichs' treatment of temporal anaphora to the analysis of sentences, which contain a temporal adverbial and quantificatiort over eventualities. According to her analysis, these trigger box-splitting as do if or every clauses in DRT (Kamp, 1981) . Consider the following example from (Partee, 1984): (7) Whenever Mary telephoned, Sam was asleep. The subordinate clause cannot be interpreted relative to a single reference time, since Mary's telephoning is not specified to occur at some specific time. Still, the sentence needs to be interpreted relative to a reference time. This reference time can be a large interval, and should contain each of the relevant occurrences of Mary's telephoning during which Bill was asleep. This reference time is represented as r0 in the top sub-DRS.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "If the main clause is an event- Following Partee (1984), we will not construct a full DRS for this discourse, but illustrate it with a diagram in Figure 3 , with circles denoting inclusion.", "mid_sen": "2.2 Quantification over events (Partee, 1984) extends Hinrichs' treatment of temporal anaphora to the analysis of sentences, which contain a temporal adverbial and quantificatiort over eventualities. ", "after_sen": "According to her analysis, these trigger box-splitting as do if or every clauses in DRT (Kamp, 1981) . "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "Figure 4: CCGbank derivations for apposition with DT coordinate structure in biological named entities. We draw NE tags from the BBN Entity Type Corpus (Weischedel and Brunstein, 2005) , which describes 28 different entity types. These include the standard person, location and organization classes, as well person descriptions (generally occupations), NORP (National, Other, Religious or Political groups), and works of art. Some classes also have finer-grained subtypes, although we use only the coarse tags in our experiments. Clark and Curran (2007b) has a full description of the C&C parser's pre-existing features, to which we have added a number of novel NER-based features. Many of these features generalise the head words and/or POS tags that are already part of the feature set. The results of applying these features are described in Sections 5.3 and 6.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some classes also have finer-grained subtypes, although we use only the coarse tags in our experiments. ", "mid_sen": "Clark and Curran (2007b) has a full description of the C&C parser's pre-existing features, to which we have added a number of novel NER-based features. ", "after_sen": "Many of these features generalise the head words and/or POS tags that are already part of the feature set. "}
{"citeStart": 113, "citeEnd": 135, "citeStartToken": 113, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "where ∆(e, f ) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single 'best' derivation using a Viterbi or beam search algorithm. In this work we show that it is both tractable and desirable to directly account for derivational ambiguity. Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007) . These models marginalise over derivations leading to a dependency structure and splits of non-terminal categories in a PCFG, respectively.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this work we show that it is both tractable and desirable to directly account for derivational ambiguity. ", "mid_sen": "Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007) . ", "after_sen": "These models marginalise over derivations leading to a dependency structure and splits of non-terminal categories in a PCFG, respectively."}
{"citeStart": 131, "citeEnd": 143, "citeStartToken": 131, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "Immediate future work includes extending the approach to include other types of appraisal expres-sions, such as where an attitude is expressed via a noun or a verb. In this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (Turney, 2002; Esuli and Sebastiani, 2005) to the more complex task of estimating also attitude type and force. As well, a key problem is the fact that evaluative language is often context-dependent, and so proper interpretation must consider interactions between a given phrase and its larger textual context.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Immediate future work includes extending the approach to include other types of appraisal expres-sions, such as where an attitude is expressed via a noun or a verb. ", "mid_sen": "In this regard, we will be examining extension of existing methods for automatically building lexicons of positive/negative words (Turney, 2002; Esuli and Sebastiani, 2005) to the more complex task of estimating also attitude type and force. ", "after_sen": "As well, a key problem is the fact that evaluative language is often context-dependent, and so proper interpretation must consider interactions between a given phrase and its larger textual context."}
{"citeStart": 0, "citeEnd": 21, "citeStartToken": 0, "citeEndToken": 21, "sectionName": "UNKNOWN SECTION NAME", "string": "Most existing studies focus on formal texts such as news. However, exceptions exist. For instance, Radev (2010, 2008 ) study the problem of summarizing a scientific paper. They propose a clustering approach where communities in the citation summary's lexical network are formed and sentences are extracted from separate clusters. Sharifi et al. (2010) use the Phrase Reinforcement algorithm to generate one-line summary for a collection of tweets related to a topic. Though our method is also designed for tweets, there are several significant differences. Firstly, our method does not assume that the input tweets are about a topic. Secondly, our method selects representative tweets by exploiting social network features, readability and keywords. In contrast, Sharifi et al. (2010) find the most commonly used phrases that encompass the topic phrase.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They propose a clustering approach where communities in the citation summary's lexical network are formed and sentences are extracted from separate clusters. ", "mid_sen": "Sharifi et al. (2010) use the Phrase Reinforcement algorithm to generate one-line summary for a collection of tweets related to a topic. ", "after_sen": "Though our method is also designed for tweets, there are several significant differences. "}
{"citeStart": 81, "citeEnd": 92, "citeStartToken": 81, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999 ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 000 verbs and 25 000 verbal expressions. In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. ", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For shuffling paraphrases, french alternations are partially described in (Saint-Dizier, 1999 ) and a resource is available which describes alternation and the mapping verbs/alternations for roughly 1 700 verbs. ", "mid_sen": "For complementing this database and for converse constructions, the LADL tables (Gross, 1975) can furthermore be resorted to, which list detailed syntactico-semantic descriptions for 000 verbs and 25 000 verbal expressions. ", "after_sen": "In particular, (Gross, 1989) lists the converses of some 3 500 predicative nouns. "}
{"citeStart": 98, "citeEnd": 119, "citeStartToken": 98, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997) . Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998) . We assume a good algorithm is one that finds the most prominent topic boundaries.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997) . ", "after_sen": "Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. "}
{"citeStart": 244, "citeEnd": 246, "citeStartToken": 244, "citeEndToken": 246, "sectionName": "UNKNOWN SECTION NAME", "string": "1 I)CGs on I, he empty string might be dismissed as extreme, computationM properties for DCGs, it is then necessary to impose certain restrictions on their form such as o[fline-parsability (OP), a nomenclature introduced by Pereira and Warren [11] , who define an OP DCG as a grammar whose context-free skeleton CFG is not infinitely ambiguous, and show that OP DCGs lead to a decidable parsing problem. 2 Our aim in this paper is to propose a simple transformation lbr an arbitrary OP DCG putting it into a form which leads to the completeness of the direct top-down interpretation by the standard Prolog interpreter: parsing is guaranteed to enumerate all solutions to the parsing problem and terminate. The e.xistence of such a transformation is kuown: in [1, 2] , we have recently introduced a \"Generalized Greibach Normal Form\" (GGNF) for DCGs, which leads to termination of top-down interpretation in the OP case. lIowever, the awdlable presentation of the GGNF transformation is rather complex (it involves an algebraic study of the fixpoints of certain equational systems representing grammars.). Our aim here is to present a related, but much simpler, transformation, which from a theoretical viewpoint performs somewhat less than the GGNF transformation (it; involves some encoding of the initial DCG, which the (~GNF does not, and it only handles oflline-parsable grammars, while the GGNF is defined for arbitrary DCGs), a but in practice is extremely easy to implement and displays a comparable behavior when parsing with an OP grammar.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 In order to guarantee good *Thaalks to Pierre Isabelle and Frangols Perrault for their comments, and to C,[TI (Montreal) for its support during the preparation of this paper.", "mid_sen": "1 I)CGs on I, he empty string might be dismissed as extreme, computationM properties for DCGs, it is then necessary to impose certain restrictions on their form such as o[fline-parsability (OP), a nomenclature introduced by Pereira and Warren [11] , who define an OP DCG as a grammar whose context-free skeleton CFG is not infinitely ambiguous, and show that OP DCGs lead to a decidable parsing problem. ", "after_sen": "2 Our aim in this paper is to propose a simple transformation lbr an arbitrary OP DCG putting it into a form which leads to the completeness of the direct top-down interpretation by the standard Prolog interpreter: parsing is guaranteed to enumerate all solutions to the parsing problem and terminate. "}
{"citeStart": 126, "citeEnd": 143, "citeStartToken": 126, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the translation dictionary trained from the training data to further improve the alignment results. When we train the bi-directional statistical word alignment models with the training data, we get two word alignment results for the training data. By taking the intersection of the two word alignment results, we build a new alignment set. The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000) . Based on the extended alignment links, we build a translation dictionary. In order to filter the noise caused by the error alignment links, we only retain those translation pairs whose log-likelihood ratio scores (Dunning, 1993) are above a threshold.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "By taking the intersection of the two word alignment results, we build a new alignment set. ", "mid_sen": "The alignment links in this intersection set are extended by iteratively adding word alignment links into it as described in (Och and Ney, 2000) . ", "after_sen": "Based on the extended alignment links, we build a translation dictionary. "}
{"citeStart": 164, "citeEnd": 176, "citeStartToken": 164, "citeEndToken": 176, "sectionName": "UNKNOWN SECTION NAME", "string": "is zero just when p = q, and it increases as the probability decreases that p is the relative frequency distribution of a random sample drawn according to q. More formally, the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by exp-nn(p I] q) (Cover and Thomas, 1991) . Therefore, if we are trying to distinguish among hypotheses qi when p is the relative frequency distribution of observations, D(p II ql) gives the relative weight of evidence in favor of qi. Furthermore, a similar relation holds between D(p IIP') for two empirical distributions p and p' and the probability that p and p~ are drawn from the same distribution q. We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "is zero just when p = q, and it increases as the probability decreases that p is the relative frequency distribution of a random sample drawn according to q. ", "mid_sen": "More formally, the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by exp-nn(p I] q) (Cover and Thomas, 1991) . ", "after_sen": "Therefore, if we are trying to distinguish among hypotheses qi when p is the relative frequency distribution of observations, D(p II ql) gives the relative weight of evidence in favor of qi. "}
{"citeStart": 117, "citeEnd": 119, "citeStartToken": 117, "citeEndToken": 119, "sectionName": "UNKNOWN SECTION NAME", "string": "The situation ~q, used above in the mutual belief induction schema, is the context of what has been said. This schema supports a weak model of mutual beliefs, that is more akin to mutual assumptions or mutual suppositions [13] . Mutual beliefs can be inferred based on some evidence, but these beliefs may depend on underlying assumptions that are easily defensible. This model can be implemented using Gallier's theory of autonomous belief revision and the corresponding system [4] .", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The situation ~q, used above in the mutual belief induction schema, is the context of what has been said. ", "mid_sen": "This schema supports a weak model of mutual beliefs, that is more akin to mutual assumptions or mutual suppositions [13] . ", "after_sen": "Mutual beliefs can be inferred based on some evidence, but these beliefs may depend on underlying assumptions that are easily defensible. "}
{"citeStart": 90, "citeEnd": 117, "citeStartToken": 90, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "The automatic extraction of terminologies has been the focus of many studies in the past (Maynard and Ananiadou, 2000) , (Zhang and Wu, 2012) , but not many works have focussed on automatic extraction of method mentions. We believe that the extraction of method expressions from research papers can help to build lexical resources that can be used in various NLP tasks on research papers.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In some cases, the context in which the method terminologies are used in the text can contain valuable information about the method mention, such as synonyms, definition, and other relations with entities in the text.", "mid_sen": "The automatic extraction of terminologies has been the focus of many studies in the past (Maynard and Ananiadou, 2000) , (Zhang and Wu, 2012) , but not many works have focussed on automatic extraction of method mentions. ", "after_sen": "We believe that the extraction of method expressions from research papers can help to build lexical resources that can be used in various NLP tasks on research papers."}
{"citeStart": 30, "citeEnd": 50, "citeStartToken": 30, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1 . This data has since been used in studies by (Mooney, 1996) , (Towell and Voorhees, 1998) , and (Leacock et al., 1998) . In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. ", "after_sen": "These senses and their frequency distribution are shown in Table 1 . "}
{"citeStart": 434, "citeEnd": 448, "citeStartToken": 434, "citeEndToken": 448, "sectionName": "UNKNOWN SECTION NAME", "string": "C-tests follow a fixed construction pattern and are therefore easy to generate. As opposed to closed formats, the candidate space is only limited by the provided prefix and the length constraint. It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text-and word-specific factors. The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\" While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991) . 2 In our model, we aim at combining features touching all levels of language. The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. Klein-Braley (1984) performs a linear regression analysis with only two difficulty indicatorsaverage sentence length and type-token ratio -obtaining good results for her target group. Eckes (2011) intend to calibrate C-test difficulty using a Rasch model in order to compare different C-tests and build a test pool. Kamimoto (1993) was the first to perform classical item analysis on the gap level. He created a tailored C-test that only contains selected gaps in order to better discriminate between the students. However, the gap selection is based on previous test results instead of specific gap features and thus cannot be applied on new tests.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is thus harder to determine the difficulty of a C-test because it is influenced by a combination of many text-and word-specific factors. ", "mid_sen": "The search for the factors that determine the difficulty of C-tests is tightly connected to the question of construct validity: \"Which skills does the C-test measure?\" While advocates of the C-test argue that it measures general language proficiency involving all levels of language (Eckes and Grotjahn, 2006; Sigott, 1995; Klein-Braley, 1985) others reduce it to a grammar test (Babaii and Ansary, 2001) or rather a vocabulary test (Chapelle, 1994; Singleton and Little, 1991) . 2 In our model, we aim at combining features touching all levels of language. ", "after_sen": "The earliest analyses of C-test difficulty focused on the paragraph instead of the gap level. "}
{"citeStart": 206, "citeEnd": 225, "citeStartToken": 206, "citeEndToken": 225, "sectionName": "UNKNOWN SECTION NAME", "string": "Disagreement rate on the presence of word boundary between characters was only 1.7%. No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% interhuman agreement rate in (Sproat et al., 1996) . This may be explained by the relatively small number of types of strings (see Table 2 ) that are considered to be multi-character words in our corpus.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Disagreement rate on the presence of word boundary between characters was only 1.7%. ", "mid_sen": "No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% interhuman agreement rate in (Sproat et al., 1996) . ", "after_sen": "This may be explained by the relatively small number of types of strings (see Table 2 ) that are considered to be multi-character words in our corpus."}
{"citeStart": 55, "citeEnd": 59, "citeStartToken": 55, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The linguistic studies, used for the typologies of CoL verbs and spatial prepositions, have been realized on verbs considered without any adjuncts, in their atemporal form and independently of any context, on the one hand, and on prepositions considered independently of any context, on the other. This methodology, discussed in Borillo & Sablayrolles (1993) , has allowed us to extract the intrinsic semantics of these lexical items. Since natural languages put together verbs and prepositions in a sentence, we have developped a formal calculus (see (Asher & Sablayrolles, 1994b) ), based on these two typologies, which computes, in a compositional way, the spatiotemporal properties of a motion complex from the semantic properties of the verb and of the preposition. For reason of space we cannot detail our formalism here, but we intend to present it in the talk.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The linguistic studies, used for the typologies of CoL verbs and spatial prepositions, have been realized on verbs considered without any adjuncts, in their atemporal form and independently of any context, on the one hand, and on prepositions considered independently of any context, on the other. ", "mid_sen": "This methodology, discussed in Borillo & Sablayrolles (1993) , has allowed us to extract the intrinsic semantics of these lexical items. ", "after_sen": "Since natural languages put together verbs and prepositions in a sentence, we have developped a formal calculus (see (Asher & Sablayrolles, 1994b) ), based on these two typologies, which computes, in a compositional way, the spatiotemporal properties of a motion complex from the semantic properties of the verb and of the preposition. "}
{"citeStart": 145, "citeEnd": 166, "citeStartToken": 145, "citeEndToken": 166, "sectionName": "UNKNOWN SECTION NAME", "string": "However, there is often a trade-off between runtime efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compilation and the size of its output, and the independence of the morphological and lexical components. In compilation, one may compose any or all of (a) the two-level rule set, (b) the set of affixes and their allowed combinations, and (c) the lexicon; see Kaplan and Kay (1994 / for an exposition of the mathematical basis. The type of compilation appropriate for rapid development and acceptable run-time performance depends on, at least, the nature of the language being described and the number of base forms in the lexicon; that is, on the position in the three-dimensional space defined by (a), (b) and (c). For example, English inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological anMysis can be carried out at compile time, producing a list of analysed word forms which need only be looked up at run time, or a network which can be traversed very simply. Alternatively, there may be no need to provide as powerful a mechanism as two-level morphology at all; a simpler device such as affix stripping (A1shawi, 1992, pll9ff) or merely listing all inflected forms explicitly may be preferable.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, English inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological anMysis can be carried out at compile time, producing a list of analysed word forms which need only be looked up at run time, or a network which can be traversed very simply. ", "mid_sen": "Alternatively, there may be no need to provide as powerful a mechanism as two-level morphology at all; a simpler device such as affix stripping (A1shawi, 1992, pll9ff) or merely listing all inflected forms explicitly may be preferable.", "after_sen": "For agglutinative languages such as Korean, Finnish and Turkish (Kwon and Karttunen, 1994; Koskenniemi, 1983; Oflazer, 1993) , dimension (b) is very large, so creating an exhaustive word list is out of the question unless the lexicon is trivial. "}
{"citeStart": 156, "citeEnd": 170, "citeStartToken": 156, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991) . All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a) , and the first two are distributed as part of the ANLT package. The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where subanalyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The three parsers in this study are: a bottomup left-corner parser, a (non-deterministic) LR parser, and an LR-like parser based on an algorithm devised by Schabes (1991) . ", "after_sen": "All three parsers accept grammars written in the ANLT formalism (Briscoe et al., 1987a) , and the first two are distributed as part of the ANLT package. "}
{"citeStart": 241, "citeEnd": 253, "citeStartToken": 241, "citeEndToken": 253, "sectionName": "UNKNOWN SECTION NAME", "string": "Although the usefulness of on-line semantic filtering during the processing of complete sentences is debatable, filtering has a more plausible role to play in interactive, real-time environments, such as interactive spell checkers (see e.g. Wirdn (1990) I'or arguments for incremental parsing in such environnlents). IIere the choice is between whether or not to have semantic illtering at all, rather than whether to do it on-line, or at the end of the sentence. q'he concentration in early literature on using incremental interpretation for semantic filtering has perha.ps distracted f'roln SOlne other applications which provide less controversial applications. We will consider two in detail here: graphical interfaces, ~md d ialogttc,", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such cases involve reasoning with an interpretation in its immediate context, as opposed to purely .judging the likelihood of a particular linguistic expression in a given application domain (see e.g. Cooper 1993 for discussion).", "mid_sen": "Although the usefulness of on-line semantic filtering during the processing of complete sentences is debatable, filtering has a more plausible role to play in interactive, real-time environments, such as interactive spell checkers (see e.g. Wirdn (1990) I'or arguments for incremental parsing in such environnlents). ", "after_sen": "IIere the choice is between whether or not to have semantic illtering at all, rather than whether to do it on-line, or at the end of the sentence. "}
{"citeStart": 208, "citeEnd": 228, "citeStartToken": 208, "citeEndToken": 228, "sectionName": "UNKNOWN SECTION NAME", "string": "The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006) . However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. Parse tree paths features can be used to achieve high precision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006) . ", "after_sen": "However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. "}
{"citeStart": 120, "citeEnd": 133, "citeStartToken": 120, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems. The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996) . Their theoretical I finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers. The theory has also been validated empirically.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems. ", "mid_sen": "The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996) . ", "after_sen": "Their theoretical I finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers. "}
{"citeStart": 316, "citeEnd": 329, "citeStartToken": 316, "citeEndToken": 329, "sectionName": "UNKNOWN SECTION NAME", "string": "Moving further in the direction of class-based language models, we plan to consider additional distributional relations (for instance, adjectivenoun) and apply the results of clustering to the grouping of lexical associations in lexicalized grammar frameworks such as stochastic lexicalized tree-adjoining grammars (Schabes, 1992) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition to predictive power evaluations of the kind we have already carried out, it might be worth comparing automatically-derived clusters with human judge: ments in a suitable experimental setting.", "mid_sen": "Moving further in the direction of class-based language models, we plan to consider additional distributional relations (for instance, adjectivenoun) and apply the results of clustering to the grouping of lexical associations in lexicalized grammar frameworks such as stochastic lexicalized tree-adjoining grammars (Schabes, 1992) .", "after_sen": ""}
{"citeStart": 96, "citeEnd": 132, "citeStartToken": 96, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "This binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where what we will call the \"core parser\" performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver(to use Abney's terminology (1987 Abney's terminology ( , 1989 ), is required to solve the difficult cases, (such as on reaching melted in (1)). This \"core parser\" has been the subject of a number of computational implementations, including Marcus's deterministic parser (1980) , Description theory (henceforth, D-theory) (Marcus et al (1983) ), and Abney's licensing based model (1987, 1989) . It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) ).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where what we will call the \"core parser\" performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver(to use Abney's terminology (1987 Abney's terminology ( , 1989 ), is required to solve the difficult cases, (such as on reaching melted in (1)). ", "mid_sen": "This \"core parser\" has been the subject of a number of computational implementations, including Marcus's deterministic parser (1980) , Description theory (henceforth, D-theory) (Marcus et al (1983) ), and Abney's licensing based model (1987, 1989) . ", "after_sen": "It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992) , Gorrell (in press) )."}
{"citeStart": 87, "citeEnd": 107, "citeStartToken": 87, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "A POS tagger is one component in the SDT based statisticM parsing system described in (Jelinek et al., 1994 . The total word accuracy on Wall St.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "A POS tagger is one component in the SDT based statisticM parsing system described in (Jelinek et al., 1994 . ", "after_sen": "The total word accuracy on Wall St."}
{"citeStart": 43, "citeEnd": 68, "citeStartToken": 43, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Our system falls into the ensemble category, because it combines the results of the various methods into a single outcome. More specifically, it belongs to Burke's switching sub-category, where a single method is selected on a case-by-case basis. A similar approach is taken in Rotaru and Litman's (2005) reading comprehension system, but their system does not perform any learning. Instead it uses a voting mechanism to select the answer given by the majority of methods. The question answering system developed by Chu-Carroll et al. (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke's cascade sub-category). Because the results of all the methods are comparable, no learning is required: At each stage of the \"cascade of methods,\" the method that performs best is selected. In contrast to these two systems, our system employs methods that are not comparable, because they use different metrics. Therefore, we need to learn from experience when to use each method.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Instead it uses a voting mechanism to select the answer given by the majority of methods. ", "mid_sen": "The question answering system developed by Chu-Carroll et al. (2003) belongs to the merging category of approaches, where the output of an individual method can be used as input to a different method (this corresponds to Burke's cascade sub-category). ", "after_sen": "Because the results of all the methods are comparable, no learning is required: "}
{"citeStart": 72, "citeEnd": 95, "citeStartToken": 72, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "The most explicit version of this approach is the analysis presented in Gee and Grosjean (1983) (henceforth G&G). Drawing on the psycholinguistic studies mentioned above and on aspects of the grammar of prosody outlined in Selkirk (1984) , G&G propose an algorithm for mapping syntactic structure onto a hierarchical representation of phrasing; the rules they present accomplish this by integrating syntactic information (e.g. constituent structure, left-toright o:rdering) with information about constituent length. We have found that their rules, which are described in detail, provide a productive model for investigations of phrasing, and in what follows we shall frequently refer to their analysis. But, as we will show, G&G fall short of providing a comprehensive theory. Their rules are too limited and their syntax too underspecified to achieve moderate coverage for an unrestricted collection of sentences or to provide an adequate description for implementation. 5", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "4", "mid_sen": "The most explicit version of this approach is the analysis presented in Gee and Grosjean (1983) (henceforth G&G). ", "after_sen": "Drawing on the psycholinguistic studies mentioned above and on aspects of the grammar of prosody outlined in Selkirk (1984) , G&G propose an algorithm for mapping syntactic structure onto a hierarchical representation of phrasing; the rules they present accomplish this by integrating syntactic information (e.g. constituent structure, left-toright o:rdering) with information about constituent length. "}
{"citeStart": 104, "citeEnd": 121, "citeStartToken": 104, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "is the probability of producing the target tree fragment frag. To generate frag, used a geometric prior to decide how many child nodes to assign each node. Differently, we require that each multi-word non-terminal node must have two child nodes. This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al., 2007; Zhang et al., 2011a) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Differently, we require that each multi-word non-terminal node must have two child nodes. ", "mid_sen": "This is because the binary structure has been verified to be very effective for tree-based translation (Wang et al., 2007; Zhang et al., 2011a) .", "after_sen": "The generation process starts at root node N. "}
{"citeStart": 103, "citeEnd": 112, "citeStartToken": 103, "citeEndToken": 112, "sectionName": "UNKNOWN SECTION NAME", "string": "The word-based metrics are the most tx)pular, but other approaches include syntax-rule driven metrics [Sumita 88 ], character-based metrics [Sato 921 as well as some hybrids [Furuse 921 . The character-based metric has been applied to Japanese, taking advantage of certain characteristics of the Japanese. The syntax-rule driven metrics try to capture similarity of two sentences at the syntax level. This seems very promising, since similarity at the syntax level, perhaps coupled by lexical similarity in a hybrid configuration, would be the best the EBMT system could ofler as a translation propo~l. '/'he real time feasibility of such a system is, however, questionable since it involves the complex task of syntactic analysis.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Then, a similarity metric is devised, which reflects the similarity of two sentences, hy combining the individual contributions towards similarity stemming from word comparisons.", "mid_sen": "The word-based metrics are the most tx)pular, but other approaches include syntax-rule driven metrics [Sumita 88 ], character-based metrics [Sato 921 as well as some hybrids [Furuse 921 . ", "after_sen": "The character-based metric has been applied to Japanese, taking advantage of certain characteristics of the Japanese. "}
{"citeStart": 86, "citeEnd": 108, "citeStartToken": 86, "citeEndToken": 108, "sectionName": "UNKNOWN SECTION NAME", "string": "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score. The translation results in terms of error rates are shown in Table 8 . We use Model 4 in order to perform the translation experiments because Model 4 typically gives better translation results than Model 5.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The PER is guaranteed to be less than or equal to the WER.", "mid_sen": "We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score. ", "after_sen": "The translation results in terms of error rates are shown in Table 8 . "}
{"citeStart": 83, "citeEnd": 104, "citeStartToken": 83, "citeEndToken": 104, "sectionName": "UNKNOWN SECTION NAME", "string": "In Ivanova et al. (2013) , the authors run a set of experiments that provide a comparison of (1) 3 dependency schemes, (2) 3 data-driven dependency parsers and (3) 2 approaches to POS-tagging in a parsing pipeline. The comparison that is relevant here is (1). The dependency representations compared are the basic version of Stanford Dependencies (Marneffe and Manning, 2008) , and two versions of the CoNLL Syntactic Dependencies (Johansson and Nugues, 2007) . For all parsers and in most experiments (which explore several pipelines with different POS-tagging strategies), SD is easier to label (i.e., label accuracy scores are higher) and CoNLL is easier to structure (i.e., unlabeled attachment scores are higher). In terms of LAS, MaltParser performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For all parsers and in most experiments (which explore several pipelines with different POS-tagging strategies), SD is easier to label (i.e., label accuracy scores are higher) and CoNLL is easier to structure (i.e., unlabeled attachment scores are higher). ", "mid_sen": "In terms of LAS, MaltParser performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL.", "after_sen": "In Nilsson et al. (2006) , the authors investigate the effects of two types of input transformation on the performance of MaltParser. "}
{"citeStart": 139, "citeEnd": 165, "citeStartToken": 139, "citeEndToken": 165, "sectionName": "UNKNOWN SECTION NAME", "string": "Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008) , Yeniterzi and Oflazer (2010) and others. Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing.", "mid_sen": "Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008) , Yeniterzi and Oflazer (2010) and others. ", "after_sen": "Toutanova et. al.'s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. "}
{"citeStart": 164, "citeEnd": 174, "citeStartToken": 164, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (Roth, 1998; Roth, 1999) . In this view, the feature space consists of simple functions (e.g., n-grams) over the the original data so as to allow for expressive enough representations using a simple functional form (e.g., a linear function). This implies that the number of potential features that the learning stage needs to consider may be very large, and may grow rapidly when increasing the expressivity of the features. Therefore a feasible computational approach needs to be feature-efficient. It needs to tolerate a large number of potential features in the sense that the number of examples required for it to converge should depend mostly on the number features relevant to the decision, rather than on the number of potential features. This paper addresses the two issues mentioned above. It presents a rich set of features that is constructed using information readily available in the sentence along with shallow parsing and dependency information. It then presents a learning approach that can use this expressive (and potentially large) intermediate representation and shows that it yields a significant improvement in word error rate for the task of word prediction.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We believe that the main reason for that is that incorporating information sources in NLP needs to be coupled with a learning approach that is suitable for it.", "mid_sen": "Studies have shown that both machine learning and probabilistic learning methods used in NLP make decisions using a linear decision surface over the feature space (Roth, 1998; Roth, 1999) . ", "after_sen": "In this view, the feature space consists of simple functions (e.g., n-grams) over the the original data so as to allow for expressive enough representations using a simple functional form (e.g., a linear function). "}
{"citeStart": 66, "citeEnd": 93, "citeStartToken": 66, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "Before explaining our CLIR system, we classify existing CLIR into three approaches in terms of the implementation of the translation phase. The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . We prefer the first approach, the \"query translation\", to other approaches because (a) translating all the documents in a given collection is expensive, (b) the use of thesauri requires manual construction or bilingual compatable corpora, (c) interlingual vector space models also need comparable corpora, and (d) query translation can easily be combined with existing IR engines and thus the implementation cost is low. At the same time, we concede that other CLIR approaches are worth further exploration. Figure 1 depicts the overall design of our CLIR system, where most components are the same as those for monolingual IR, excluding \"translator\".", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Before explaining our CLIR system, we classify existing CLIR into three approaches in terms of the implementation of the translation phase. ", "mid_sen": "The first approach translates queries into the document language (Ballesteros and Croft, 1998; Carbonell et al., 1997; Davis and Ogden, 1997; Fujii and Ishikawa, 1999; Hull and Grefenstette, 1996; Kando and Aizawa, 1998; Okumura et al., 1998) , while the second approach translates documents into the query language (Gachot et al., 1996; Oard and Hackett, 1997) . ", "after_sen": "The third approach transfers both queries and documents into an interlingual representation: bilingual thesaurus classes (Mongar, 1969; Salton, 1970; Sheridan and Ballerini, 1996) and language-independent vector space models (Carbonell et al., 1997; Dumais et al., 1996) . "}
{"citeStart": 134, "citeEnd": 156, "citeStartToken": 134, "citeEndToken": 156, "sectionName": "UNKNOWN SECTION NAME", "string": "Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007) . Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006) .", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. ", "mid_sen": "Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006) .", "after_sen": "Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art. "}
{"citeStart": 123, "citeEnd": 135, "citeStartToken": 123, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992) , and the conditional probability of the feature given the word (within prob-abilistic-based measures) (Pereira et al., 1993) , (Lee, 1997) , (Dagan et al., 1999) . ", "mid_sen": "Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990) , (Hindle, 1990) , (Lin, 1998) , (Dagan, 2000) , defined by:", "after_sen": ") ( ) ( ) , ( log ) , ( 2 f P w P f w P f w MI ="}
{"citeStart": 62, "citeEnd": 90, "citeStartToken": 62, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "We also compute GIST vectors (Oliva and Torralba, 2001 ) for every image using LearGIST (Douze et al., 2009) . Unlike SURF descriptors, GIST produces a single vector representation for an image. The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The vector does not find points of interest in the image, but rather attempts to provide a representation for the overall \"gist\" of the whole image. ", "mid_sen": "It is frequently used in tasks like scene identification, and Deselaers and Ferrari (2011) shows that distance in GIST space correlates well with semantic distance in WordNet. ", "after_sen": "After computing the GIST vectors, each textual word is represented as the centroid GIST vector of all its images, forming the GIST modality."}
{"citeStart": 31, "citeEnd": 49, "citeStartToken": 31, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007) , the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic -the BLEU tuning seems to be better on Chinese. It also seems like METEOR should not be used in tuning due to high insertion rate and low precision. It would be interesting to know which tuning metric results in the best translations in terms of human judgment. However, this would require time consuming evaluations such as human mediated TER post-editing (Snover et al., 2006) .", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. ", "mid_sen": "Compared to the baseline from (Rosti et al., 2007) , the new method improves the BLEU scores significantly. ", "after_sen": "The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. "}
{"citeStart": 107, "citeEnd": 135, "citeStartToken": 107, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "We propose a new formalism, called \"stratified logic\", that correctly handles the pragmatic inferences, and we start by giving a very brief introduction to the main ideas that underlie it. We give the main steps of the algorithm that is defined on the backbone of stratified logic. We then show how different classes of pragmatic inferences can be captured using this formalism, and how our algorithm computes the expected results for a representative class of pragmatic inferences. The results we report here are obtained using an implementation written in Common Lisp that uses Screamer (Siskind and McAllester, 1993) , a macro package that provides nondeterministic constructs.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We then show how different classes of pragmatic inferences can be captured using this formalism, and how our algorithm computes the expected results for a representative class of pragmatic inferences. ", "mid_sen": "The results we report here are obtained using an implementation written in Common Lisp that uses Screamer (Siskind and McAllester, 1993) , a macro package that provides nondeterministic constructs.", "after_sen": ""}
{"citeStart": 68, "citeEnd": 88, "citeStartToken": 68, "citeEndToken": 88, "sectionName": "UNKNOWN SECTION NAME", "string": "Although C++ is extremely efficient, it is not suitable for rapidly gluing components together to form new tools. To overcome this problem we have implemented an interface to the infrastructure in the Python scripting language. Python has a number of advantages over other options, such as Java and Perl. Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. It has already been used to implement a framework for teaching NLP (Loper and Bird, 2002) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Python is very easy to learn, read and write, and allows commands to be entered interactively into the interpreter, making it ideal for experimentation. ", "mid_sen": "It has already been used to implement a framework for teaching NLP (Loper and Bird, 2002) .", "after_sen": "Using the Boost."}
{"citeStart": 56, "citeEnd": 78, "citeStartToken": 56, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "As a baseline, we use the best feature set from Beigman Klebanov et al. (2014) . Specifically, the baseline contains the following families of features:", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "As a baseline, we use the best feature set from Beigman Klebanov et al. (2014) . ", "after_sen": "Specifically, the baseline contains the following families of features:"}
{"citeStart": 201, "citeEnd": 217, "citeStartToken": 201, "citeEndToken": 217, "sectionName": "UNKNOWN SECTION NAME", "string": "It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997) ). In natural language processing, ensemble techniques have been successfully applied to partof-speech tagging (e.g., (Brill and Wu, 1998) ) and parsing (e.g., (Henderson and Brill, 1999) ). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, when the con-tribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996) ), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships.", "mid_sen": "It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997) ). ", "after_sen": "In natural language processing, ensemble techniques have been successfully applied to partof-speech tagging (e.g., (Brill and Wu, 1998) ) and parsing (e.g., (Henderson and Brill, 1999) ). "}
{"citeStart": 95, "citeEnd": 106, "citeStartToken": 95, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Such distributions are referred to as 'dendroid distributions' in tile literature. A dendroid distribution can be represenled by a dependency forest (i.e. a set of dependency trees), whose nodes represent the random variaMes, and whose directed arcs represent the dependencies that exist between these random w/riahles, each labeled with a number of parameters specil}'ing the probabilistic dependency. (A dendroid distribution can also be considered as a re.stricted form of the Bayesian Network (Pearl, 1988) .) It is not difficult t.o see tha.t there are 7 and only 7 such representations for the joint distribution P(X1, X,2, X3) disregarding the actual nmnerical values of t.he probability parameters. Now we turn to the problem of how to select the best dendroid distribution fi:om among all possible ones to approximate a target joint distribution based on input data generated by it. This problem has been inw?stiga.ted in the area of machine learning and related fields. A classical method is Chow & Liu's algorMnn for estimating a nmltidimensional .joint distribution as a dependency tree, ill a way which is both el-~cient and theoretically sound (C.how and I,iu, 1968). More recent.ly (Suzuki, 1993) extended their algorithm so that it estimates the target ,joint. distribution as a dependency Forest. or 'dendroid distrihution', allowing for the possibility of learning one group of random variables to be completely independent of another. Since nlany of the random variables (case slots) in case flame patterns are esseutially independent, this feature is crucial in our context, and we thus employ Suzuki's algorithm for learning our case frame patterns. Figure 1 shows the detail of this Mgorithm, where ki denotes the nun> her of possible values assumed by node (random variable) Xi, N the input data size, and qog' denotes the logarithm to the base 2. It is easy to see that the nulnber of parameters in a dendroid distribution is of the order O(k2ne), where k is the maxinmni of all ki, and n is the. number of random variables, and the time complexity of the algorithm is of the same order, as it is linear in the number of parameters.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A dendroid distribution can be represenled by a dependency forest (i.e. a set of dependency trees), whose nodes represent the random variaMes, and whose directed arcs represent the dependencies that exist between these random w/riahles, each labeled with a number of parameters specil}'ing the probabilistic dependency. ", "mid_sen": "(A dendroid distribution can also be considered as a re.stricted form of the Bayesian Network (Pearl, 1988) .) It is not difficult t.o see tha.t there are 7 and only 7 such representations for the joint distribution P(X1, X,2, X3) disregarding the actual nmnerical values of t.he probability parameters. ", "after_sen": "Now we turn to the problem of how to select the best dendroid distribution fi:om among all possible ones to approximate a target joint distribution based on input data generated by it. "}
{"citeStart": 229, "citeEnd": 250, "citeStartToken": 229, "citeEndToken": 250, "sectionName": "UNKNOWN SECTION NAME", "string": "On top of the compound splitting, we applied the lexical redundancy normalization (CS+Norm1). We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the finegrained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008) . Similar to CS2, We tested the delimited version of normalized words (CS+Norm2). Table 7 shows the results of compound splitting and normalization methods. As a result, normalization on top of compounding did not perform well. Besides, experiments showed that compound word decomposition is crucial and helps vastly to improve translation results 0.43 BLEU points on average over the best system described in Section 2.5. ", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On top of the compound splitting, we applied the lexical redundancy normalization (CS+Norm1). ", "mid_sen": "We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the finegrained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008) . Similar to CS2, We tested the delimited version of normalized words (CS+Norm2). ", "after_sen": "Table 7 shows the results of compound splitting and normalization methods. "}
{"citeStart": 266, "citeEnd": 290, "citeStartToken": 266, "citeEndToken": 290, "sectionName": "UNKNOWN SECTION NAME", "string": "1 -regularized log-linear models ( 1 -LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; . However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training 1 -LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008) .", "mid_sen": "1 -regularized log-linear models ( 1 -LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; . However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training 1 -LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. ", "after_sen": "In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy."}
{"citeStart": 72, "citeEnd": 90, "citeStartToken": 72, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "Eighteen baseline features and six additional features are proposed in (Jiang and Ng, 2006) for Nom-Bank argument identification. As the improvement of the F1 score due to the additional features is not statistically significant, we use the set of eighteen baseline features for simplicity. These features are reproduced in Table 1 for easy reference.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Eighteen baseline features and six additional features are proposed in (Jiang and Ng, 2006) for Nom-Bank argument identification. ", "after_sen": "As the improvement of the F1 score due to the additional features is not statistically significant, we use the set of eighteen baseline features for simplicity. "}
{"citeStart": 90, "citeEnd": 103, "citeStartToken": 90, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "Sentence planning is a set of inter-related but distinct tasks, one of which is sentence scoping, i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into sentences. 1 For example, consider the required capabilities of a sentence planner for a mixed-initiative spoken dialog system for travel planning: (D1) System1: Welcome.... What airport would you like to fly out of? User2: I need to go to Dallas. System3: Flying to Dallas. What departure airport was that? User4: from Newark on September the 1st. System5: What time would you like to travel on September the 1st to Dallas from Newark? Utterance System1 requests information about the caller's departure airport, but in User2, the caller takes the initiative to provide information about her destination. In System3, the system's goal is to implicitly confirm the destination (because of the possibility of error in the speech recognition component), and request information (for the second time) of the caller's departure airport. In User4, the caller provides this information but also provides the month and day of travel. Given the system's dialog strategy, the communicative goals for its next turn are to implicitly confirm all the information that the user has provided so far, i.e. the departure and destination cities and the month and day information, as well as to request information about the time of travel. The system's representation of its communicative goals for utterance System5 is in Figure 1 . The job of the sentence planner is to decide among the large number of potential realizations of these communicative goals. Some example alternative realizations are in Figure 2 . 2 implicit-confirm(orig-city:NEWARK) implicit-confirm(dest-city:DALLAS) implicit-confirm(month:9) implicit-confirm(day-number:1) request(depart-time) In this paper, we present SPoT, for \"Sentence Planner, Trainable\". We also present a new methodology for automatically training SPoT on the basis of feedback provided by human judges. In order to train SPoT, we reconceptualize its task as consisting of two distinct phases. In the first phase, the sentence-plan-generator (SPG) generates a potentially large sample of possible sentence plans for a given text-plan input. In the second phase, the sentence-plan-ranker (SPR) ranks the sample sentence plans, and then selects the top-ranked output to input to the surface realizer. Our primary contribution is a method for training the SPR. The SPR uses rules automatically learned from training data, using techniques similar to (Collins, 2000; Freund et al., 1998) .", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our primary contribution is a method for training the SPR. ", "mid_sen": "The SPR uses rules automatically learned from training data, using techniques similar to (Collins, 2000; Freund et al., 1998) .", "after_sen": "Our method for training a sentence planner is unique in neither depending on hand-crafted rules, nor on the existence of a text or speech corpus in the domain of the sentence planner obtained from the interaction of a human with a system or another human. "}
{"citeStart": 163, "citeEnd": 177, "citeStartToken": 163, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "Our system maintains a set of beliefs about the domain and about the user's beliefs. Associated with each belief is a strength that represents the agent's confidence in holding that belief. We model the strength of a belief using endorsements, which are explicit records of factors that affect one's certainty in a hypothesis (Cohen, 1985) , following (Galliers, 1992; Logan et al., 1994) . Our endorsements are based on the semantics of the utterance used to convey a befief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. The belief level of the dialogue model consists of mutual beliefs proposed by the agents' discourse actions. When an agent proposes a new belief and gives (optional) supporting evidence for it, this set of proposed beliefs is represented as a belief tree, where the belief represented by a child node is intended to support that represented by its parent. The root nodes of these belief trees (rap-level beliefs) contribute to problem-solving actions and thus affect the domain plan being developed. Given a set of newly proposed beliefs, the system must decide whether to accept the proposal or m initiate a negotiation dialogue to resolve conflicts. The evaluation of proposed beliefs starts at the leaf nodes of the proposed belief trees since acceptance of a piece of proposed evidence may affect acceptance of the parent belief it is intended to support. The process continues until the top-level proposed beliefs are evaluated. Conflict resolution strategies are invoked only if the top-level proposed beliefs are not accepted because if collaborative agents agree on a belief relevant to the domain plan being constructed, it is irrelevant whether they agree on the evidence for that belief (Young et al., 1994) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Associated with each belief is a strength that represents the agent's confidence in holding that belief. ", "mid_sen": "We model the strength of a belief using endorsements, which are explicit records of factors that affect one's certainty in a hypothesis (Cohen, 1985) , following (Galliers, 1992; Logan et al., 1994) . ", "after_sen": "Our endorsements are based on the semantics of the utterance used to convey a befief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. "}
{"citeStart": 16, "citeEnd": 36, "citeStartToken": 16, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "The results of each of the predictors used in the Inside/0utside method are presented in Table 3 . The results are comparable to other results reported using the Inside/Outside method (Ramshaw and Marcus, 1995 ) (see Table 7 . We have observed that most of the mistaken predictions of base NPs involve predictions with respect to conjunctions, gerunds, adverbial NPs and some punctuation marks. As reported in (Argamon et al., 1998) , most base NPs present in ~he data are less or equal than 4 words long. This implies that our predictors tend to break up long base NPs into smaller ones.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have observed that most of the mistaken predictions of base NPs involve predictions with respect to conjunctions, gerunds, adverbial NPs and some punctuation marks. ", "mid_sen": "As reported in (Argamon et al., 1998) , most base NPs present in ~he data are less or equal than 4 words long. ", "after_sen": "This implies that our predictors tend to break up long base NPs into smaller ones."}
{"citeStart": 88, "citeEnd": 89, "citeStartToken": 88, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The left-recursion elimination ~dgorithnt is adapted from a transR)rmation proposed in [4] in the context of a certain formalism (\"l,exical Grammars\") which we presented as a possible basis for bui(ding reversible grammars, a The key observation (in slightly different terms) was that, in a I)CG, ifa nontermiual g is defined (itcrMly by the two rules (the first of which is leftreeursive):", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "'t It produces a I)C(] declaratively equivalent to the. original grammar.", "mid_sen": "The left-recursion elimination ~dgorithnt is adapted from a transR)rmation proposed in [4] in the context of a certain formalism (\"l,exical Grammars\") which we presented as a possible basis for bui(ding reversible grammars, a The key observation (in slightly different terms) was that, in a I)CG, ifa nontermiual g is defined (itcrMly by the two rules (the first of which is leftreeursive):", "after_sen": ":+\\') --+ g(Y), a(v, x). ,(x) --, ~(x)."}
{"citeStart": 237, "citeEnd": 257, "citeStartToken": 237, "citeEndToken": 257, "sectionName": "UNKNOWN SECTION NAME", "string": "Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization (Snoek and Worring, 2005; ) . Although performance is often reasonable in controlled environments (such as studio news rooms), automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) (Wactlar et al., 1996) . While many researches have examined how to compensate for such noise using acoustic techniques, few have attempted to leverage information in the visual stream to improve speech recognition performance (for an exception see Murkherjee and Roy, 2003) .", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization (Snoek and Worring, 2005; ) . ", "mid_sen": "Although performance is often reasonable in controlled environments (such as studio news rooms), automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) (Wactlar et al., 1996) . ", "after_sen": "While many researches have examined how to compensate for such noise using acoustic techniques, few have attempted to leverage information in the visual stream to improve speech recognition performance (for an exception see Murkherjee and Roy, 2003) ."}
{"citeStart": 17, "citeEnd": 39, "citeStartToken": 17, "citeEndToken": 39, "sectionName": "UNKNOWN SECTION NAME", "string": "Our document content model is primarily based on the global discourse of the article as provided by the type and number of CoreSC categories. However, unlike (Teufel and Moens, 2002) , who take a fixed number of AZ categories of specific type to create rhetorical extracts, the number of categories used from each CoreSC category depends on their distribution in the original article. Any and all types of CoreSC category could potentially appear in a summary, as our summaries are meant to be representative of the entire content of the paper. Also, the ordering of the categories in the summary is learnt to reflect the ordering of categories observed in abstracts of papers from the same domain.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our document content model is primarily based on the global discourse of the article as provided by the type and number of CoreSC categories. ", "mid_sen": "However, unlike (Teufel and Moens, 2002) , who take a fixed number of AZ categories of specific type to create rhetorical extracts, the number of categories used from each CoreSC category depends on their distribution in the original article. ", "after_sen": "Any and all types of CoreSC category could potentially appear in a summary, as our summaries are meant to be representative of the entire content of the paper. "}
{"citeStart": 63, "citeEnd": 74, "citeStartToken": 63, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "For noun phrases we employ Abney's chunk grammar organization (Abney, 1996) . The noun chunk (NC) is an approximately non-recursive projection that excludes post-head complements and (adverbial) adjuncts introduced higher than pre-head modifiers and determiners but includes participial pre-modifiers with their complements. Since we perform complete context free parsing, parse forest construction, and inside-outside estimation, chunks are not motivated by deterministic parsing. Rather, they facilitate evaluation and graphical debugging, by tending to increase the span of constituents with high estimated frequency. The grammar distinguishes four subcategorisation frame classes: active (VPA), passive (VPP), infinitival (VPI) frames, and copula constructions (VPK). A frame may have maximally three arguments. Possible arguments in the frames are nominative (n), dative (d) and accusative (a) NPs, reflexive pronouns (r), PPs (p), and infinitival VPs (i). The grammar does not distinguish plain infinitival VPs from zu-infinitival VPs. The grammar is designed to partially distinguish different PP frames relative to the prepositional head of the PP. A distinct category for the specific preposition becomes visible only when a subcategorized preposition is cancelled from the subcat list. This means that specific prepositions do not figure in the evaluation discussed below. The number and the types of frames in the different frame classes are given in figure 4. German, being a language with comparatively free phrase order, allows for scrambling of arguments. Scrambling is reflected in the particu-lar sequence in which the arguments of the verb frame are saturated. Compare figure 5 for an example of a canonical subject-object order in an active transitive frame and its scrambled objectsubject order. The possibility of scrambling verb arguments yields a substantial increase in the number of rules in the grammar (e.g. 102 combinatorically possible argument rules for all in VPA frames). Adverbs and non-subcategorized PPs are introduced as adjuncts to VP categories which do not saturate positions in the subcat frame.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Agreement between the nominative NP and the tensed verb (e.g. in number) is not enforced by the grammar, in order to control the number of parameters and rules.", "mid_sen": "For noun phrases we employ Abney's chunk grammar organization (Abney, 1996) . ", "after_sen": "The noun chunk (NC) is an approximately non-recursive projection that excludes post-head complements and (adverbial) adjuncts introduced higher than pre-head modifiers and determiners but includes participial pre-modifiers with their complements. "}
{"citeStart": 114, "citeEnd": 133, "citeStartToken": 114, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997) , Charniak (1997) and Ratnaparkhi (1997) . These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993) . We used these three parsers to explore parser combination techniques.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997) , Charniak (1997) and Ratnaparkhi (1997) . ", "mid_sen": "These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993) . ", "after_sen": "We used these three parsers to explore parser combination techniques."}
{"citeStart": 193, "citeEnd": 212, "citeStartToken": 193, "citeEndToken": 212, "sectionName": "UNKNOWN SECTION NAME", "string": "An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforwardsee, for example Oepen et al. (2002) for a good discussion of the problem. Furthermore, it suggests that it may be possible to usefully tune a parser to a new domain with less annotation effort.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus.", "mid_sen": "An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforwardsee, for example Oepen et al. (2002) for a good discussion of the problem. ", "after_sen": "Furthermore, it suggests that it may be possible to usefully tune a parser to a new domain with less annotation effort."}
{"citeStart": 74, "citeEnd": 91, "citeStartToken": 74, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007) . Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006) .", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. ", "mid_sen": "Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007) . ", "after_sen": "Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. "}
{"citeStart": 56, "citeEnd": 79, "citeStartToken": 56, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent work has shown that explicit declaration of domain and expert knowledge can be highly useful for structured NLP tasks such as parsing, POS tagging and information extraction (Chang et al., 2007; Ganchev et al., 2010) . These works have encoded expert knowledge through constraints, with different frameworks differing in the type of constraints and the inference and learning algorithms used. We build on the Generalized Expectation (GE) framework (Mann and McCallum, 2007) which encodes expert knowledge through a preference (i.e. soft) constraints for parameter settings for which the predicted label distribution matches a target distribution.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These works have encoded expert knowledge through constraints, with different frameworks differing in the type of constraints and the inference and learning algorithms used. ", "mid_sen": "We build on the Generalized Expectation (GE) framework (Mann and McCallum, 2007) which encodes expert knowledge through a preference (i.e. soft) constraints for parameter settings for which the predicted label distribution matches a target distribution.", "after_sen": "In order to integrate domain knowledge with a features-based model, we develop a simple taxonomy of constraints (i.e. desired class distributions) and employ a top-down classification algorithm on top of a Maximum Entropy Model augmented with GE constraints. "}
{"citeStart": 123, "citeEnd": 147, "citeStartToken": 123, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "We have compared four complete and three partial data representation formats for the baseNP recognition task presented in (Ramshaw and Marcus, 1995) . The four complete formats all use an I tag for words that are inside a baseNP and an 0 tag for words that are outside a baseNP. They differ IOB1 O  I  I  O  I  I B O  I  O O O  I  I B  I O  IOB2 O B  I  O B  I  B O B O O O B  I  B  I  O  IOE1 O  I  I  O  I  E  I  O  I  O O O  I  E  I  I  O  IOE2 O  I E O  I  E E O E O O O  I  E  I  E O  IO I O  I  I  O  I  I  I  O  I  O O O  I  I  I  I ", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We have compared four complete and three partial data representation formats for the baseNP recognition task presented in (Ramshaw and Marcus, 1995) . ", "after_sen": "The four complete formats all use an I tag for words that are inside a baseNP and an 0 tag for words that are outside a baseNP. "}
{"citeStart": 78, "citeEnd": 95, "citeStartToken": 78, "citeEndToken": 95, "sectionName": "UNKNOWN SECTION NAME", "string": "Although a number of automatic sentence alignment methods have been proposed (Brown et al. 1991 ; Gale & Church 1991 b; Kay & Roscheisen 1993; Chen 1993 ), they are not very reliable for real noisy bilingual texts.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, most of theln assume that the input corpora m'e aligned sentence by sentence, which reduces their applicability remarkably.", "mid_sen": "Although a number of automatic sentence alignment methods have been proposed (Brown et al. 1991 ; Gale & Church 1991 b; Kay & Roscheisen 1993; Chen 1993 ), they are not very reliable for real noisy bilingual texts.", "after_sen": "Second, the statistical methods usually require a very large corpus as their input. "}
{"citeStart": 102, "citeEnd": 121, "citeStartToken": 102, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "The first two phases are approached as straightforward classification in a maximum entropy framework (Berger et al., 1996) . The maximum entropy algorithm produces a distribution p", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the final step, phase three, an embedded clause structure is inferred from these start and end predictions.", "mid_sen": "The first two phases are approached as straightforward classification in a maximum entropy framework (Berger et al., 1996) . ", "after_sen": "The maximum entropy algorithm produces a distribution p"}
{"citeStart": 164, "citeEnd": 177, "citeStartToken": 164, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting point for the implementation of a feature structure tagger was a second-0rdcr-IIMM tagger (trigrams) based on a modified version of the Viterbi algorithm (Viterbi, 1967; Church, 1988) which we had earlier implemented in C (Kempe ,1994 ). There we replaced the function which estimated the contextual probability of a tag (state transition probability) hy dividing a trigram frequency by a bigram frequency (eq. 3) with a flmction which accomplished this calculus either using PF1Ls in the above-described way (eq.s 6, 7) or by consulting a decision tree ( fig. 3) .", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Starting point for the implementation of a feature structure tagger was a second-0rdcr-IIMM tagger (trigrams) based on a modified version of the Viterbi algorithm (Viterbi, 1967; Church, 1988) which we had earlier implemented in C (Kempe ,1994 ). ", "after_sen": "There we replaced the function which estimated the contextual probability of a tag (state transition probability) hy dividing a trigram frequency by a bigram frequency (eq. 3) with a flmction which accomplished this calculus either using PF1Ls in the above-described way (eq.s 6, 7) or by consulting a decision tree ( fig. 3) ."}
{"citeStart": 86, "citeEnd": 109, "citeStartToken": 86, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "2. Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003) , using the German version trained on the Negra corpus (Rafferty and Manning, 2008) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This was necessary for the parser (see below) to perform with an adequate accuracy.", "mid_sen": "2. Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003) , using the German version trained on the Negra corpus (Rafferty and Manning, 2008) .", "after_sen": "3. Syntactic and semantic properties of sentences were annotated, among them elabo-rateness (e.g., fragments and full sentences), speech acts (e.g., greetings, instructions, corrections, feedback) and localisation strategies -for instance, whether positions were described in relation to present objects (\"to the right of the circle\"), by describing absolute locations of the board itself (\"into the bottomleft corner\"), or by using metaphors (such as points of the compass, floors of buildings for rows: \"south of the circle\")."}
{"citeStart": 87, "citeEnd": 102, "citeStartToken": 87, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "As before, the indices on the nonterminals are redundant, as the heads are always located at an edge of each constituent, so they need not be computed or stored and the CFG can be parsed in O(n 3 ) time. The steps involved in CKY parsing with this grammar correspond closely to those of the McDonald (2006) second-order PBDG parsing algorithm.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As before, the indices on the nonterminals are redundant, as the heads are always located at an edge of each constituent, so they need not be computed or stored and the CFG can be parsed in O(n 3 ) time. ", "mid_sen": "The steps involved in CKY parsing with this grammar correspond closely to those of the McDonald (2006) second-order PBDG parsing algorithm.", "after_sen": "An O(n 3 ) dependent-head grammar"}
{"citeStart": 131, "citeEnd": 147, "citeStartToken": 131, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012) . In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. Since robots need to collaborate with human partners to establish a joint perceptual basis, referring expression generation (REG) becomes an equally important problem in situated dialogue. Robots have much lower perceptual capabilities of the environment than humans. How can a robot effectively generate referential descriptions about the environment so that its human partner can understand which objects are being referred to?", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, referential communication between the human and the robot becomes difficult.", "mid_sen": "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work (Liu et al., 2012) . ", "after_sen": "In that work, the main focus is on reference resolution: given referential descriptions from human partners, how to identify referents in the environment even though the robot only has imperfect perception of the environment. "}
{"citeStart": 50, "citeEnd": 65, "citeStartToken": 50, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "In the experiments, our primary goal is to evaluate the effectiveness of the proposed model using all features available to us. Additionally, we are interested in knowing the contribution of each information source, namely of morpho-syntactic and bilingual features. Therefore, we study the performance of models including the full feature schemata as well as models that are restricted to feature subsets according to the feature types as described in Section 5.2. The models are as follows: Monolingual-Word, including LM-like and stem n-gram features only; Bilingual-Word, which also includes bilingual lexical features; 7 Monolingual-All, which has access to all the information available in the target language, including morphological and syntactic features; and finally, Bilingual-All, which includes all feature types from Table 2. For each model and language, we perform feature selection in the following manner. The features are represented as feature templates, such as \"POS=X\", which generate a set of binary features corresponding to different instantiations of the template, as in \"POS=NOUN\". In addition to individual features, conjunctions of up to three features are also considered for selection (e.g., \"POS=NOUN & Number=plural\"). Every conjunction of feature templates considered contains at least one predicate on the prediction y t , and up to two predicates on the context. The feature selection algorithm performs a greedy forward stepwise feature selection on the feature templates so as to maximize development set accuracy. The algorithm is similar to the one described in (Toutanova, 2006) . After this process, we performed some manual inspection of the selected templates, and finally obtained 11 and 36 templates for the Monolingual-All and Bilingual-All settings for Russian, respectively. These templates generated 7.9 million and 9.3 million binary feature instantiations in the final model, respectively. The corresponding numbers for Arabic were 27 feature templates (0.7 million binary instantiations) and 39 feature templates (2.3 million binary instantiations) for Monolingual-All and Bilingual-All, respectively. Table 5 shows the accuracy of predicting word forms for the baseline and proposed models. We report accuracy only on words that appear in our lexicons. Thus, punctuation, English words occurring in the target sentence, and words with unknown lemmas are excluded from the evaluation. The reported accuracy measure therefore abstracts away from the is-7 Overall, this feature set approximates the information that is available to a state-of-the-art statistical MT system.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The feature selection algorithm performs a greedy forward stepwise feature selection on the feature templates so as to maximize development set accuracy. ", "mid_sen": "The algorithm is similar to the one described in (Toutanova, 2006) . ", "after_sen": "After this process, we performed some manual inspection of the selected templates, and finally obtained 11 and 36 templates for the Monolingual-All and Bilingual-All settings for Russian, respectively. "}
{"citeStart": 268, "citeEnd": 279, "citeStartToken": 268, "citeEndToken": 279, "sectionName": "UNKNOWN SECTION NAME", "string": "Language reuse involves two components: a source text written by a human and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993) , the use of entire factual sentences extracted from corpora (e.g., \"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995) . In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. ", "mid_sen": "Some examples of language reuse include collocation analysis (Smadja, 1993) , the use of entire factual sentences extracted from corpora (e.g., \"'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995) . ", "after_sen": "In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. "}
{"citeStart": 161, "citeEnd": 185, "citeStartToken": 161, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "The straightforward approach is to generalize existing recognition algorithms. The same techniques that are used for calculating the intersection of a FSA and a CFG can be applied in the case of DCGs. In order to compute the intersection of a DCG and a FSA we assume that FSA are represented as before. DCGs are represented using the same notation we used for context-free grammars, but now of course the category symbols can be first-order terms of arbitrary complexity (note that without loss of generality we don't take into account DCGs having exter-]In fact, the standard compilation of DCG into Prolog clauses does something similar using variables instead of actual state names. This also illustrates that this method is not very useful yet; all the work has still to be done. But if we use existing techniques for parsing DCGs, then we are also confronted with an undecidability problem: the recognition problem for DCGs is undecidable (Pereira and Warren, 1983) . A fortiori the problem of deciding whether the intersection of a FSA and a DCG is empty or not is undecidable.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This also illustrates that this method is not very useful yet; all the work has still to be done. ", "mid_sen": "But if we use existing techniques for parsing DCGs, then we are also confronted with an undecidability problem: the recognition problem for DCGs is undecidable (Pereira and Warren, 1983) . ", "after_sen": "A fortiori the problem of deciding whether the intersection of a FSA and a DCG is empty or not is undecidable."}
{"citeStart": 51, "citeEnd": 70, "citeStartToken": 51, "citeEndToken": 70, "sectionName": "UNKNOWN SECTION NAME", "string": "Through experiments of text categorization, we empirically compare the HP-TOP kernel with the linear kernel and the PLSI-based Fisher kernel. We use Reuters-21578 dataset 2 with ModApte-split (Dumais et al., 1998) . In addition, we delete some texts from the result of ModAptesplit, because those texts have no text body. After the deletion, we obtain 8815 training examples and 3023 test examples. The words that occur less than five times in the whole training set are excluded from the original feature set. We do not use all the 8815 training examples. The size of the actual training data ranges from 1000 to 8000. For each dataset size, experiments are executed 10 times with different training sets.The result is evaluated with Fmeasures for the most frequent 10 categories ( Table 1) . The total number of categories is actually 116. However, for small categories, reliable statistics cannot be obtained. For this reason, we regard the remaining categories other than the 10 most frequent categories as one category. Therefore, the model for negative examples is a mixture of 10 component models (9 out of the 10 most frequent categories and the new category consisting of the remaining categories).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Through experiments of text categorization, we empirically compare the HP-TOP kernel with the linear kernel and the PLSI-based Fisher kernel. ", "mid_sen": "We use Reuters-21578 dataset 2 with ModApte-split (Dumais et al., 1998) . ", "after_sen": "In addition, we delete some texts from the result of ModAptesplit, because those texts have no text body. "}
{"citeStart": 114, "citeEnd": 126, "citeStartToken": 114, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "In the literature of norl-incremental generation, the need for defaults is hardly ever taken into account. The conunon point of view restricts the iulmt to be sulIicient for generation (see, e.g., the Te:ct Slructure by (Meteer, 1990 ) for a syntactic generator). In incremental gm,eration, most authors agree on the necessity of using defaults (see, e.g., (l)e Smedt, 1990; Kitano, 1990; Ward, 1991) ). Nevertheless, they do not in sufficient depth answer the question of how to guide the processes of default handling an(l repair wil;hin a generator. This I~roblem is the starting--point tbr the following considerations. We assume tlm.t generation is a decision-making process witll the aim o[' producing it phmsiMe ul:t(wance 1)ased on given information. As mentioned in section 1, there are cases where this I)rocess stops (caused by underspccifical.ioll of the input) before finishing its output.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the literature of norl-incremental generation, the need for defaults is hardly ever taken into account. ", "mid_sen": "The conunon point of view restricts the iulmt to be sulIicient for generation (see, e.g., the Te:ct Slructure by (Meteer, 1990 ) for a syntactic generator). ", "after_sen": "In incremental gm,eration, most authors agree on the necessity of using defaults (see, e.g., (l)e Smedt, 1990; Kitano, 1990; Ward, 1991) ). "}
{"citeStart": 110, "citeEnd": 128, "citeStartToken": 110, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set. This can be done by smoothing the observed frequencies (Church and Mercer, 1993) , or by class-based methods (Brown et al., 1991; Pereira and Tishby, 1992; Pereira et ah, 1993; Hirschman, 1986; Resnik, 1992; Brill et ah, 1990; Dagan et al., 1993) . In comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics. This allows the system to learn successfully from very sparse data.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Traditionally, the problem of sparse data is approached by estimating the probability of unobserved cooccurrences using the actual cooccurrences in the training set. ", "mid_sen": "This can be done by smoothing the observed frequencies (Church and Mercer, 1993) , or by class-based methods (Brown et al., 1991; Pereira and Tishby, 1992; Pereira et ah, 1993; Hirschman, 1986; Resnik, 1992; Brill et ah, 1990; Dagan et al., 1993) . ", "after_sen": "In comparison to these approaches, we use similarity information throughout training, and not merely for estimating cooccurrence statistics. "}
{"citeStart": 69, "citeEnd": 85, "citeStartToken": 69, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "Following the work of, for example, Marslen-Wilson (1973), .lust and Carpenter (1980) and Altma.nn al]d Steedrnan (1988) , it has heroine widely accepted that semantic i11terpretation in hnman sentence processing can occur beibre sentence boundaries and even before clausal boundaries. It is less widely accepted that there is a need for incremental interpretation in computational applications.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following the work of, for example, Marslen-Wilson (1973), .lust and Carpenter (1980) and Altma.nn al]d Steedrnan (1988) , it has heroine widely accepted that semantic i11terpretation in hnman sentence processing can occur beibre sentence boundaries and even before clausal boundaries. ", "after_sen": "It is less widely accepted that there is a need for incremental interpretation in computational applications."}
{"citeStart": 71, "citeEnd": 84, "citeStartToken": 71, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov , 1998) , either by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents. The CogNIAC algorithm (Baldwin, 1997) uses six heuristic rules to resolve coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, lexical reiteration or immediate reference). Both these algorithm rely only on part-of-speech tagging of texts and on patterns for NP identification. Their performance (dose to 90% for certain types of pronouns) indicates that full syntactic knowledge is not required by certain forms of pronominal coreference.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Nevertheless, RkPSTAT, a version of RAP obtained by using statistically measured preference patterns for the antecedents, prodticed a slight enhancement of performance over RAP.", "mid_sen": "Other pronominal resolution approaches promote knowledge-poor methods (Mitkov , 1998) , either by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents. ", "after_sen": "The CogNIAC algorithm (Baldwin, 1997) uses six heuristic rules to resolve coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, lexical reiteration or immediate reference). "}
{"citeStart": 71, "citeEnd": 91, "citeStartToken": 71, "citeEndToken": 91, "sectionName": "UNKNOWN SECTION NAME", "string": "Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000) . Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. For grammars extracted from the Penn Treebank (in our case CCGbank (Hockenmaier, 2003) ), enumerating all parses is infeasible. One approach to this problem is to sample the parse space for estimation, e.g. Osborne (2000) . In this paper we use a dynamic programming technique applied to a packed chart, similar to those proposed by Geman and Johnson (2002) and Miyao and Tsujii (2002) , which efficiently estimates the model parameters over the complete space without enumerating parses. The estimation method is similar to the inside-outside algorithm used for estimating a PCFG (Lari and Young, 1990 ). Miyao and Tsujii (2002) apply their estimation technique to an automatically extracted Tree Adjoining Grammar using Improved Iterative Scaling (IIS, Della Pietra et al. (1997) ). However, their model has significant memory requirements which limits them to using 868 sentences as training data. We use a parallelised version of Generalised Iterative Scaling (GIS, Darroch and Ratcliff (1972) ) on a Beowulf cluster which allows the complete WSJ Penn Tree-bank to be used as training data.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Following Abney, we propose a loglinear framework which incorporates long-range dependencies as features without loss of consistency.", "mid_sen": "Log-linear models have previously been applied to statistical parsing (Johnson et al., 1999; Toutanova et al., 2002; Riezler et al., 2002; Osborne, 2000) . ", "after_sen": "Typically, these approaches have enumerated all possible parses for model estimation and finding the most probable parse. "}
{"citeStart": 34, "citeEnd": 59, "citeStartToken": 34, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "The lexical-semantic structure adopted for UNITRAN is an augmented form of Jackendoff's representation in which events are distinguished from states (as before), but events are further subdivided into activities, achievements, and accomplishments. The subdivision is achieved by means of three features proposed by Bennett etal. (1990) following the framework of Moens and Steedman (1988) : -t-dynamic (i.e., events vs. states, as in the Jackendoff framework), +telic (i.e., culminative events (transitions) vs. noneulminative events (activities)), and -I-atomic (i.e., point events vs. extended events). We impose this system of features on top of the current lexical-semantic framework. For example, the lexical entry for all three verbs, ransack, obliterate, and destroy, would contain the following lexical-semantic representation:", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The subdivision is achieved by means of three features proposed by Bennett etal. ", "mid_sen": "(1990) following the framework of Moens and Steedman (1988) : -t-dynamic (i.e., events vs. states, as in the Jackendoff framework), +telic (i.e., culminative events (transitions) vs. noneulminative events (activities)), and -I-atomic (i.e., point events vs. extended events). ", "after_sen": "We impose this system of features on top of the current lexical-semantic framework. "}
{"citeStart": 55, "citeEnd": 80, "citeStartToken": 55, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Therefore the tagged Spoken English Corpus was chosen ['lh.ylor ,~ Knowles, 1988] . This featlu'es some very long seutences, and includes rich and varied punctuation. Since IJle corpus has l)cen l)unctnated IYlallually, by several different people, some idiosyncrasy occurs ill tile pnnctuatlollal style, I)ul, there is little punctuation which wonld be deemed inappropriate to the positidn it'oceurs in.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Sinlihu'ly, for s(~iltCllCes including only ()lie or two marks of pllnctilation, l.he llSO of punctliatlon is likely to bc raLller procedural, and hence not necessarily very revealing.", "mid_sen": "Therefore the tagged Spoken English Corpus was chosen ['lh.ylor ,~ Knowles, 1988] . ", "after_sen": "This featlu'es some very long seutences, and includes rich and varied punctuation. "}
{"citeStart": 179, "citeEnd": 202, "citeStartToken": 179, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "The use of corpora with various levels of annotation has been studied, but the recommendations are that much manual work is required to turn corpus examples into test cases e.g., Balkan and Fouvry, 1995 . The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. Corpora thus are used only as an inspiration.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The TSNLP project Lehmann and Oepen, 1996 and its successor DiET Netter et al., 1998 , which built large multilingual testsuites, likewise fall into this category.", "mid_sen": "The use of corpora with various levels of annotation has been studied, but the recommendations are that much manual work is required to turn corpus examples into test cases e.g., Balkan and Fouvry, 1995 . ", "after_sen": "The reason given is that corpus sentences neither contain linguistic phenomena in isolation, nor do they contain systematic variation. "}
{"citeStart": 156, "citeEnd": 180, "citeStartToken": 156, "citeEndToken": 180, "sectionName": "UNKNOWN SECTION NAME", "string": "(2) woj = loger (vj) Both the information-theoretical network and the back-propagation network compute the posterior probabilities for an association task (Gorin and Levinson, 1989; Robinson, 1992) . However, only the information-theoretical network is isomorphic to the directly interconnected verbal systems in the dualcoding theory. Besides, an information-theoretical network has the following advantages: (1) it learns fast. The network can learn in a single pass without gradient decent. (2) it is adaptive. It can incrementally adapt to new experiences simply by adding new data to the training samples and modifying the associations according to the changed statistics. These make the network more psychologically plausible.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The weight of the connection between the bias unit in one layer and unitj in the other layer is", "mid_sen": "(2) woj = loger (vj) Both the information-theoretical network and the back-propagation network compute the posterior probabilities for an association task (Gorin and Levinson, 1989; Robinson, 1992) . ", "after_sen": "However, only the information-theoretical network is isomorphic to the directly interconnected verbal systems in the dualcoding theory. "}
{"citeStart": 32, "citeEnd": 44, "citeStartToken": 32, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precision feature extraction and the use of relaxation-labeling in order to find the semantic orientation of potential opinion words. The review-mining work most relevant to our research is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004) . Both identify product features from reviews, but OPINE significantly improves on both. (Hu and Liu, 2004) doesn't assess candidate features, so its precision is lower than OPINE's. (Kobayashi et al., 2004) employs an iterative semi-automatic approach which requires human input at every iteration. Neither model explicitly addresses composite (feature of feature) or implicit features. Other systems (Morinaga et al., 2002; Kushal et al., 2003) also look at Web product reviews but they do not extract opinions about particular product features. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) . Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al., 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) . Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words. The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. So far, OPINE's focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The paper's global optimization approach and use of multiple sources of constraints on a word's semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information. ", "mid_sen": "Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al., 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative. ", "after_sen": "So far, OPINE's focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity."}
{"citeStart": 0, "citeEnd": 14, "citeStartToken": 0, "citeEndToken": 14, "sectionName": "UNKNOWN SECTION NAME", "string": "Context sensitive rewrite rules have been widely used in several areas of natural language processing. Johnson (1972) has shown that such rewrite rules are equivalent to finite state transducers in the special case that they are not allowed to rewrite their own output. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994) . Improvements and extensions to this algorithm have been provided by Karttunen (1995) , Karttunen (1997) , Karttunen (1996) and Mohri and Sproat (1996) . In this paper, the algorithm will be extended to provide a limited form of backreferencing.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Context sensitive rewrite rules have been widely used in several areas of natural language processing. ", "mid_sen": "Johnson (1972) has shown that such rewrite rules are equivalent to finite state transducers in the special case that they are not allowed to rewrite their own output. ", "after_sen": "An algorithm for compilation into transducers was provided by Kaplan and Kay (1994) . "}
{"citeStart": 25, "citeEnd": 44, "citeStartToken": 25, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "There are two machine learning tasks in our problem. The first is Dialogue Act (DA) Tagging, in which we assign DAs to every Dialogue Functional Unit (DFU). The second is Link prediction, in which we predict if two DFUs form a link pair. In this paper, we assume that the DFUs are given. We propose three systems to tackle the problem. The first system is a non-strawman Baseline Heuristics system, which uses the structural characteristics of dialogue. The second is Regular SVM. The third is Structured SVM. Structured SVM is a discriminative method that can predict complex structured output. Recently, discriminative Probabilistic Graphical Models have been widely applied in structural problems (Getoor and Taskar, 2007) such as link prediction. However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure. sequence labeling (Tsochantaridis et al., 2005) . We have adapted Structured SVM to our problem, provided a novel method for link prediction, and shown that it is superior in some aspects to Regular SVM.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recently, discriminative Probabilistic Graphical Models have been widely applied in structural problems (Getoor and Taskar, 2007) such as link prediction. ", "mid_sen": "However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure. ", "after_sen": "sequence labeling (Tsochantaridis et al., 2005) . "}
{"citeStart": 51, "citeEnd": 79, "citeStartToken": 51, "citeEndToken": 79, "sectionName": "UNKNOWN SECTION NAME", "string": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction.", "mid_sen": "In the same vein, researchers at Brown University (Caraballo and Charniak, 1999) , (Berland and Charniak, 1999) , (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy.", "after_sen": "We have a different perspective than these lines of inquiry. "}
{"citeStart": 205, "citeEnd": 237, "citeStartToken": 205, "citeEndToken": 237, "sectionName": "UNKNOWN SECTION NAME", "string": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975) , it has been applied computationally to IR with various levels of success (Preece, 1982) , with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997) . Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) , have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001) , arguably due to Wikipedia's larger conceptual coverage.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic memory (Collins and Loftus, 1975) , it has been applied computationally to IR with various levels of success (Preece, 1982) , with the biggest hurdle in this regard the cost of creating an associative network or knowledge base with adequate conceptual coverage (Crestani, 1997) . ", "mid_sen": "Recent knowledge-based methods for computing semantic similarity between texts based on Wikipedia, such as Wikipedia Link-based Measure (WLM) (Witten and Milne, 2008) and Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) , have been found to outperform earlier WordNet-based methods (Budanitsky and Hirst, 2001) , arguably due to Wikipedia's larger conceptual coverage.", "after_sen": "WLM treats the anchor text in Wikipedia articles as links to other articles (all links are treated equally), and compare concepts based on how much overlap exists in the out-links of the articles representing them. "}
{"citeStart": 149, "citeEnd": 172, "citeStartToken": 149, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "We describe the experiments and results we achieved with different linguistic preprocessing and learning algorithms and provide some interpretations. We start out from the corpus of categorized emails described in Section 2. In order to normalize the vectors representing the preprocessing results of texts of different length, and to concentrate on relevant material (cf. (Yang and Pedersen, 1997) ), we define the relevancy vector as follows. First, all documents are preprocessed, yielding a list of results for each category. From each of these lists, the 100 most frequent results -according to a TF/IDF measure -are selected. The relevancy vector consists of all selected results, where doubles are eliminated. Its length was about 2500 for the 47 categories; it slightly varied with the kind of preprocessing used.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We start out from the corpus of categorized emails described in Section 2. ", "mid_sen": "In order to normalize the vectors representing the preprocessing results of texts of different length, and to concentrate on relevant material (cf. (Yang and Pedersen, 1997) ), we define the relevancy vector as follows. ", "after_sen": "First, all documents are preprocessed, yielding a list of results for each category. "}
{"citeStart": 179, "citeEnd": 191, "citeStartToken": 179, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003) . Most of these techniques have relied on particular linguistic patterns, such as \"NP such as NP.\" The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora. The effort of searching for other clues indicating hyponymy relations is thus significant.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Hyponymy relations can play a crucial role in various NLP systems, and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora (Hearst, 1992; Caraballo, 1999; Imasumi, 2001; Fleischman et al., 2003; Morin and Jacquemin, 2003; Ando et al., 2003) . ", "after_sen": "Most of these techniques have relied on particular linguistic patterns, such as \"NP such as NP.\" The frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora. "}
{"citeStart": 46, "citeEnd": 64, "citeStartToken": 46, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "Berkeley Parser. We previously showed optimal Berkeley parser (Petrov et al. 2006) parameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets. 19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline. Others had used the Berkeley parser for French, but on an older revision of the FTB. To our knowledge, we are the first to use the Berkeley parser for MWE identification.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Berkeley Parser. ", "mid_sen": "We previously showed optimal Berkeley parser (Petrov et al. 2006) parameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets. ", "after_sen": "19 For Arabic, our pre-processing and parameter settings significantly increased the best-published Berkeley ATB baseline. "}
{"citeStart": 122, "citeEnd": 134, "citeStartToken": 122, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. For instance, Grimshaw (1990) defines the thematic hierarchy as:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). ", "mid_sen": "PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . ", "after_sen": "PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . "}
{"citeStart": 267, "citeEnd": 289, "citeStartToken": 267, "citeEndToken": 289, "sectionName": "UNKNOWN SECTION NAME", "string": "Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm (Pang and Lee, 2005) ; by taking a semisupervised graph-based learning approach (Goldberg and Zhu, 2006) ; and by using \"optimal stacks\" of SVMs (Koppel and Schler, 2006) . However, each of these methods have shortcomings (Section 2). Additionally, during the learning process, all approaches employ a set of word/punctuation features collected across all rating categories. Hence, the number of features may be very large compared to the number of training samples, which can lead to the model overfitting the data.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Consequently, these methods generally do not perform well, while methods which incorporate sample similarity information achieve improved performance (Pang and Lee, 2005) .", "mid_sen": "Sample similarity in the multi-way sentiment detection setting has previously been considered by using Support Vector Machines (SVMs) in conjunction with a metric labeling metaalgorithm (Pang and Lee, 2005) ; by taking a semisupervised graph-based learning approach (Goldberg and Zhu, 2006) ; and by using \"optimal stacks\" of SVMs (Koppel and Schler, 2006) . ", "after_sen": "However, each of these methods have shortcomings (Section 2). "}
{"citeStart": 10, "citeEnd": 29, "citeStartToken": 10, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "Following White et al. (2007) , we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2-21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. While these models are considerably smaller than the ones used in (Langkilde-Geary, 2002; Velldal and Oepen, 2005) , the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation). The models employ interpolated Kneser-Ney smoothing with the default frequency cutoffs. The best performing model interpolates a word trigram model with a trigram model that chains a POS model with a supertag model, where the POS model conditions on the previous two POS tags, and the supertag model conditions on the previous two POS tags as well as the current one.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Following White et al. (2007) , we use factored trigram models over words, part-of-speech tags and supertags to score partial and complete realizations. ", "after_sen": "The language models were created using the SRILM toolkit (Stolcke, 2002) on the standard training sections (2-21) of the CCGbank, with sentenceinitial words (other than proper names) uncapitalized. "}
{"citeStart": 16, "citeEnd": 36, "citeStartToken": 16, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "We have used the baseNP data presented in (Ramshaw and Marcus, 1995) 2. This data was divided in two parts. The first part was training data and consisted of 211727 words taken from sections 15, 16, 17 and 18 from the Wall Street Journal corpus (WSJ). The second part was test data and consisted of 47377 words taken from section 20 of the same corpus. The words were part-of-speech (POS) tagged with the Brill tagger and each word was classified as being inside or outside a baseNP with the IOB1 representation scheme. The chunking classification was made by (Ramshaw and Marcus, 1995) based on the parsing information in the WSJ corpus. The performance of the baseNP recognizer can be measured in different ways: by computing the percentage of correct classification tags (accuracy), the percentage of recognized baseNPs that are correct (precision) and the percentage of baseNPs inthe corpus that are found (recall). We will follow (Argamon et al., 1998) and use a combination of the precision and recall rates: F~=I = (2\" precision*recall) / (precision+recall).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The performance of the baseNP recognizer can be measured in different ways: by computing the percentage of correct classification tags (accuracy), the percentage of recognized baseNPs that are correct (precision) and the percentage of baseNPs inthe corpus that are found (recall). ", "mid_sen": "We will follow (Argamon et al., 1998) and use a combination of the precision and recall rates: F~=I = (2\" precision*recall) / (precision+recall).", "after_sen": "In our first experiment series we have tried to discover the best word/part-of-speech tag context for each representation format. "}
{"citeStart": 19, "citeEnd": 43, "citeStartToken": 19, "citeEndToken": 43, "sectionName": "UNKNOWN SECTION NAME", "string": "Recently we have applied our methodology to the Penn-III Treebank, a more balanced corpus resource with a number of text genres. Penn-III consists of the WSJ section from Penn-II as well as a parse-annotated subset of the Brown corpus. The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. It has been shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary across linguistic domains. Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. The f-structure annotation algorithm was extended with only minor amendments to cover the parsed Brown corpus. The most important of these was the way in which we distinguish between oblique and adjunct. We noted in Section 4 that our method of assigning an oblique annotation in Penn-II was precise, albeit conservative. Because of a change of annotation policy in Penn-III, the -CLR tag (indicating a close relationship between a PP and the local syntactic head), information which we had previously exploited, is no longer used. For Penn-III the algorithm annotates all PPs which do not carry a Penn adverbial functional tag (such as -TMP or -LOC) and occur as the sisters of the verbal head of a VP as obliques.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Brown corpus comprises 24,242 trees compiled from a variety of text genres including popular lore, general fiction, science fiction, mystery and detective fiction, and humor. ", "mid_sen": "It has been shown (Roland and Jurafsky 1998) that the subcategorization tendencies of verbs vary across linguistic domains. ", "after_sen": "Our aim, therefore, is to increase the scope of the induced lexicon not only in terms of the verb lemmas for which there are entries, but also in terms of the frames with which they co-occur. "}
{"citeStart": 85, "citeEnd": 107, "citeStartToken": 85, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "Selection of learning algorithm and its algorithmspecific parameters were done as follows. For each of the 7 classification tasks (one per relationship type), for each of the 128 pattern clustering schemes, we prepared a list of most of the compatible algorithms available in Weka, and we automatically selected the model (a parameter set and an algorithm) which gave the best 10-fold cross-validation results. The winning algorithms were LWL (Atkeson et al., 1997) , SMO (Platt, 1999) , and K* (Cleary and Trigg, 1995 ) (there were 7 tasks, and different algorithms could be selected for each task). We then used the obtained model to classify the testing set. This allowed us to avoid fixing parameters that are best for a specific dataset but not for others. Since each dataset has only 140 examples, the computation time of each learning algorithm is negligible.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each of the 7 classification tasks (one per relationship type), for each of the 128 pattern clustering schemes, we prepared a list of most of the compatible algorithms available in Weka, and we automatically selected the model (a parameter set and an algorithm) which gave the best 10-fold cross-validation results. ", "mid_sen": "The winning algorithms were LWL (Atkeson et al., 1997) , SMO (Platt, 1999) , and K* (Cleary and Trigg, 1995 ) (there were 7 tasks, and different algorithms could be selected for each task). ", "after_sen": "We then used the obtained model to classify the testing set. "}
{"citeStart": 171, "citeEnd": 190, "citeStartToken": 171, "citeEndToken": 190, "sectionName": "UNKNOWN SECTION NAME", "string": "We trained the word alignment in two directions: English to French, and French to English. The alignment results for both directions were refined with 'GROW' heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006) . We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000) . Table 1 shows the word alignment accuracy of the three methods trained with 10k, 50k, and 100k additional sentence pairs. For all settings, our proposed method outperformed other conventional methods. This result shows that synonym information is effective for improving word alignment quality as we expected.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We trained the word alignment in two directions: English to French, and French to English. ", "mid_sen": "The alignment results for both directions were refined with 'GROW' heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006) . ", "after_sen": "We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000) . "}
{"citeStart": 97, "citeEnd": 117, "citeStartToken": 97, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011) . However, this work does not handle citation context. Other approaches to citation classification include work by Wilbur et al. (2006) , who annotated a 101 sentence corpus on focus, polarity, certainty, evidence and directionality. Piao et al. (2007) proposed a system to attach sentiment information to the citation links between biomedical papers by using existing semantic lexical resources and NLP tools.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "While different schemes have been proposed for annotating citations according to their function (Spiegel-Rosing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000) , the only recent work on citation sentiment detection using a relatively large corpus is by Athar (2011) . ", "after_sen": "However, this work does not handle citation context. "}
{"citeStart": 440, "citeEnd": 452, "citeStartToken": 440, "citeEndToken": 452, "sectionName": "UNKNOWN SECTION NAME", "string": "Thougll a 50\"/,, precision and recall might be reasonable for human assisted tasks, like in lexicography, supervised translation, etc., it is not \"fair enough\" if collocational analysis must serve a fully automated system. In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In fact, corpus linguistics became a popular research field because of the claim that shallow techniques could overcome the lexical coverage bottleneck of traditional NLP techniques. ", "mid_sen": "Among the applications of collocational analysis for lexical acquisition are: the derivation of syntactic disambiguation cues (Basili et al. 1991 (Basili et al. , 1993a Hindle and Rooths 1991,1993; Sekine 1992 ) (Bogges et al. 1992 , sense preference (Yarowski 1992), acquisition of selectional restrictions (Basili et al. 1992b (Basili et al. , 1993b Utsuro et al. 1993) , lexical preference in generation (Smadjia 1991), word clustering (Pereira 1993; Hindle 1990; Basili et al. 1993c) , etc.", "after_sen": "In the majority of these papers, even though the (precedent or subsequent) statistical processing reduces the number of accidental associations, very large corpora (10,000,000 words) are necessary to obtain reliable data on a \"large enough\" number of words. "}
{"citeStart": 113, "citeEnd": 134, "citeStartToken": 113, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper presents an alternative discriminative method for word alignment. We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001 ). The inference algo-rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping features. Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting. We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003) . This models one-to-many alignments, where each target word is aligned with zero or more source words. Many-to-many alignments are recoverable using the standard techniques for superimposing predicted alignments in both translation directions.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper presents an alternative discriminative method for word alignment. ", "mid_sen": "We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001 ). ", "after_sen": "The inference algo-rithms are tractable and efficient, thereby avoiding the need for heuristics. "}
{"citeStart": 0, "citeEnd": 20, "citeStartToken": 0, "citeEndToken": 20, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the earliest approaches is performed in the context of the Genia project and corpus of medical texts from MEDLINE. In a first stage, only MEDLINE abstracts are used (Yang et al., 2004) , later other-anaphora, a very specific sub-task, are investigated using full paper content (Chen et al., 2008) . Gasperin (2009) presents a full annotation of anaphora and coreference in biomedical text, but only noun phrases referring to biomedical entities are considered. On the basis of this annotation, she implements a probabilistic anaphora resolution system. In contrast, Cohen et al. (2010) build a corpus of 97 full-text journal articles in the biomedical domain where every co-referring noun phrase is annotated (CRAFT -Colorado Richly Annotated Full Text). Their annotation guidelines follow those of the OntoNotes project (Hovy et al., 2006) , adapted to the biomedical domain. OntoNotes itself is a text corpus of approx. one million words from mainly news texts (newswire, magazines, broadcast conversations, web pages). It also contains general anaphoric coreference annotations (Pradhan et al., 2007) : events and (like in our annotation) unlimited noun phrase entity types. Kim and Webber (2006) investigate a special aspect, citation sentences where a pronoun such as \"they\" refers to a previous citation. The study is performed on astronomy journal articles and a maximum-entropy classifier is trained. Kaplan et al. (2009) investigate coreferences and citations as well, but only at a very small scale (4 articles from the Computational Linguistics journal). They focus on so-called c-sites which are the sentences following a citation that also refer to the same paper (typically by anaphora). The authors train a specific coreference model for this phenomenon. They show that exploitation of coreference chains improves the extraction of citation contexts which they then use for research paper summarization.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The study is performed on astronomy journal articles and a maximum-entropy classifier is trained. ", "mid_sen": "Kaplan et al. (2009) investigate coreferences and citations as well, but only at a very small scale (4 articles from the Computational Linguistics journal). ", "after_sen": "They focus on so-called c-sites which are the sentences following a citation that also refer to the same paper (typically by anaphora). "}
{"citeStart": 117, "citeEnd": 125, "citeStartToken": 117, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "Beside HTML markups, other criteria may also be incorporated. For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994) . We hope to implement such correspondences in our future research.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Beside HTML markups, other criteria may also be incorporated. ", "mid_sen": "For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994) . ", "after_sen": "We hope to implement such correspondences in our future research."}
{"citeStart": 14, "citeEnd": 33, "citeStartToken": 14, "citeEndToken": 33, "sectionName": "UNKNOWN SECTION NAME", "string": "The models of Kompe et al. (1994) and Mast et al. (1996) are the most similar to our model in terms of incorporating a language model. Mast et al. achieve a recall rate of 85.0% and a precision of 53.1% on identifying dialog acts in a German corpus. Their model employs richer acoustic modeling, however, it does not account for other aspects of utterance modeling, such as speech repairs. Conclusion", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Secondly, their decision trees are used to classify each data point independently of the next, whereas we find the best interpretation over the entire turn, and incorporate speech repairs.", "mid_sen": "The models of Kompe et al. (1994) and Mast et al. (1996) are the most similar to our model in terms of incorporating a language model. ", "after_sen": "Mast et al. "}
{"citeStart": 129, "citeEnd": 133, "citeStartToken": 129, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "To resolve the opposition between surface order and the PAS in a free word order language, one can let the type shifted categories of terms proliferate, or reformulate CCG in such a way that arguments of the verbs are sets, rather than lists whose arguments are made available one at a time. The former alternative makes the spurious ambiguity problem of CG parsing (Karttunen, 1989 ) even more severe. Multiset CCG (Hoffman, 1995) is an example of the setoriented approach. It is known to be computationally tractable but less efficient than the polynomial time CCG algorithm of Vijay-Shanker and Weir (1993) . I try to show in this paper that the traditional curried notation of CG with type shifting can be maintained to account for Surface Form+-~PAS mapping without leading to proliferation of argument categories or to spurious ambiguity.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Multiset CCG (Hoffman, 1995) is an example of the setoriented approach. ", "mid_sen": "It is known to be computationally tractable but less efficient than the polynomial time CCG algorithm of Vijay-Shanker and Weir (1993) . ", "after_sen": "I try to show in this paper that the traditional curried notation of CG with type shifting can be maintained to account for Surface Form+-~PAS mapping without leading to proliferation of argument categories or to spurious ambiguity."}
{"citeStart": 117, "citeEnd": 144, "citeStartToken": 117, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "written as fully specified relations between words, rather, only what is supposed to be changed is specified. Consider, for example, the lexical rule in Figure 2 , which encodes a passive lexicai rule like the one presented by Pollard and Sag (1987, 215) in terms of the setup of Pollard and Sag (1994, ch. 9) . This lexical rule could be used in a grammar of English to relate past participle forms of verbs to their passive form2 ° The rule takes the index of the least oblique complement of the input and assigns it to the subject of the output. The index that the subject bore in the input is assigned to an optional prepositional complement in the output.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "written as fully specified relations between words, rather, only what is supposed to be changed is specified. ", "mid_sen": "Consider, for example, the lexical rule in Figure 2 , which encodes a passive lexicai rule like the one presented by Pollard and Sag (1987, 215) in terms of the setup of Pollard and Sag (1994, ch. ", "after_sen": "Consider, for example, the lexical rule in Figure 2 , which encodes a passive lexicai rule like the one presented by Pollard and Sag (1987, 215) in terms of the setup of Pollard and Sag (1994, ch. 9) . "}
{"citeStart": 172, "citeEnd": 200, "citeStartToken": 172, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "Example 8 (d93-18.1 utt47) it/PRP takes/VBP one/CD Push you/PRP The result of this is that the non-null tag values are treated just as if they were lexical items. 3 Furthermore, if an editing term is completed, or the extent of a repair is known, we can also clean up the editing term or reparandum, respectively, in the same way that Stolcke and Shriberg (1996b) clean up filled pauses, and simple repair patterns. This means that we can then generalize between fluent speech and instances that have a repair. For instance, in the two examples below, the context for the word \"get\" and its POS tag will be the same for both, namely \"so/CC_D we/PRP need/VBP to/TO\".", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Example 8 (d93-18.1 utt47) it/PRP takes/VBP one/CD Push you/PRP The result of this is that the non-null tag values are treated just as if they were lexical items. ", "mid_sen": "3 Furthermore, if an editing term is completed, or the extent of a repair is known, we can also clean up the editing term or reparandum, respectively, in the same way that Stolcke and Shriberg (1996b) clean up filled pauses, and simple repair patterns. ", "after_sen": "This means that we can then generalize between fluent speech and instances that have a repair. "}
{"citeStart": 34, "citeEnd": 55, "citeStartToken": 34, "citeEndToken": 55, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996) , , and (Pedersen and . The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments. The senses and their ire- quency distribution are shown in Table 2 . Unlike line, the sense distribution is skewed; the majority sense occurs in 53% of the sentences, while the smallest minority sense occurs in less than 1%.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences.", "mid_sen": "The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. ", "after_sen": "This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996) , , and (Pedersen and . "}
{"citeStart": 143, "citeEnd": 157, "citeStartToken": 143, "citeEndToken": 157, "sectionName": "UNKNOWN SECTION NAME", "string": "We have used a corpus of technical texts manually annotated for coreference. We have decided on The corpus contains 28,272 words, with 19,305 noun phrases and 422 pronouns, out of which 362 are anaphoric. The files that were used are: \"Beowulf HOW TO\" (referred in Table  1 as BEO), \"Linux CD-Rom HOW TO\" (CDR), \"Access HOW TO\" (ACC), \"Windows Help file\" (WIN). The evaluation files were pre-processed to remove irrelevant information that might alter the quality of the evaluation (tables, sequences of code, tables of contents, tables of references). The texts were annotated for full coreferential chains using a slightly modified version of the MUC annotation scheme. All instances of identity-of-reference direct nominal anaphora were annotated. The annotation was performed by two people in order to minimize human errors in the testing data (see for further details). Table 1 describes the values obtained for the success rate and precision 6 of the three anaphora resolvers on the evaluation corpus. The overall success rate calculated for the 422 pronouns found in the texts was 56.9% for Mitkov's method, 49.72% for Cogniac and 61.6% for Kennedy and Boguraev's method. Table 2 presents statistical results on the evaluation corpus, including distribution of pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors. 7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996) , Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. By contrast, the evaluation workbench enables a uniform and balanced comparison of the algorithms in that (i) the evaluation is done on the same data and (ii) each algorithm employs the same pre-processing tools and performs the resolution in fully automatic fashion. Our experiments also confirm the finding of Orasan, Evans and that fully automatic resolution is more difficult than previously thought with the performance of all the three algorithms essentially lower than originally reported.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 2 presents statistical results on the evaluation corpus, including distribution of pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors. ", "mid_sen": "7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996) , Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. ", "after_sen": "By contrast, the evaluation workbench enables a uniform and balanced comparison of the algorithms in that (i) the evaluation is done on the same data and (ii) each algorithm employs the same pre-processing tools and performs the resolution in fully automatic fashion. "}
{"citeStart": 181, "citeEnd": 202, "citeStartToken": 181, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques (Bouillon et al., 2002) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2. highlighting of the corpus-specic structures conveying the target element.", "mid_sen": "In addition to its explanatory capacity, this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques (Bouillon et al., 2002) .", "after_sen": ""}
{"citeStart": 18, "citeEnd": 27, "citeStartToken": 18, "citeEndToken": 27, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper describes a system designed to assist humans in translating expressions that do not necessarily have a literal or compositional equivalent in the target language (TL). In the spirit of (Kay, 1997) , it is intended as a translator's amenuensis \"under the tight control of a human translator … to help increase his productivity and not to supplant him\".", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This paper describes a system designed to assist humans in translating expressions that do not necessarily have a literal or compositional equivalent in the target language (TL). ", "mid_sen": "In the spirit of (Kay, 1997) , it is intended as a translator's amenuensis \"under the tight control of a human translator … to help increase his productivity and not to supplant him\".", "after_sen": "One area where human translators particularly appreciate assistance is in the translation of expressions from the general lexicon. "}
{"citeStart": 164, "citeEnd": 182, "citeStartToken": 164, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data. We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data. Two primary factors appear to be determining the efficacy of our self-training approach. First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010) , despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009) .", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010) , despite being a single generative PCFG. ", "mid_sen": "Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009) .", "after_sen": "In future work, we plan to investigate additional methods for increasing the diversity of our selftrained models. "}
{"citeStart": 183, "citeEnd": 205, "citeStartToken": 183, "citeEndToken": 205, "sectionName": "UNKNOWN SECTION NAME", "string": "It has been noted in previous work that the felicity of certain forms of ellipsis is dependent on the type of coherence relationship extant between the antecedent and elided clauses (Levin and Prince, 1982; Kehler, 1993b) . In this section we review the relevant facts for two such forms of ellipsis, namely gapping and VP-ellipsis, and also compare these with facts concerning non-elliptical event reference.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "It has been noted in previous work that the felicity of certain forms of ellipsis is dependent on the type of coherence relationship extant between the antecedent and elided clauses (Levin and Prince, 1982; Kehler, 1993b) . ", "after_sen": "In this section we review the relevant facts for two such forms of ellipsis, namely gapping and VP-ellipsis, and also compare these with facts concerning non-elliptical event reference."}
{"citeStart": 43, "citeEnd": 71, "citeStartToken": 43, "citeEndToken": 71, "sectionName": "UNKNOWN SECTION NAME", "string": "Just because an action is obligatory with respect to a set of rules R does not mean that the agent will perform the action. So we do not adopt the model suggested by [Shoham and Tennenholtz, 1992] in which agents' behavior cannot violate the defined social laws. If an obligation is not satisfied, then this means that one of the rules must have been broken. We assume that agents generally plan their actions to violate as few rules as possible, and so obligated actions will usually occur. But when they directly conflict with the agent's personal goals, the agent may choose to violate them. Obligations are quite different from and can not be reduced to intentions and goals. In particular, an agent may be obliged to do an action that is contrary to his goals (for example, consider a child who has to apologize for hitting her younger brother).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Just because an action is obligatory with respect to a set of rules R does not mean that the agent will perform the action. ", "mid_sen": "So we do not adopt the model suggested by [Shoham and Tennenholtz, 1992] in which agents' behavior cannot violate the defined social laws. ", "after_sen": "If an obligation is not satisfied, then this means that one of the rules must have been broken. "}
{"citeStart": 167, "citeEnd": 187, "citeStartToken": 167, "citeEndToken": 187, "sectionName": "UNKNOWN SECTION NAME", "string": "Since the SemEval dataset is of a very specific nature, we have also applied our classification framework to the (Nastase and Szpakowicz, 2003) dataset, which contains 600 pairs labeled with 5 main relationship types. We have used the exact evaluation procedure described in (Turney, 2006) , achieving a class f-score average of 60.1, as opposed to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al., 2006) . This shows that our method produces superior results for rather differing datasets.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since the SemEval dataset is of a very specific nature, we have also applied our classification framework to the (Nastase and Szpakowicz, 2003) dataset, which contains 600 pairs labeled with 5 main relationship types. ", "mid_sen": "We have used the exact evaluation procedure described in (Turney, 2006) , achieving a class f-score average of 60.1, as opposed to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al., 2006) . ", "after_sen": "This shows that our method produces superior results for rather differing datasets."}
{"citeStart": 177, "citeEnd": 194, "citeStartToken": 177, "citeEndToken": 194, "sectionName": "UNKNOWN SECTION NAME", "string": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). Like Cowie, Guthrie, and Guthrie (1992) , we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c ; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A third difference concerns the granularity of WSD attempted, which one can illustrate in terms of the two levels of semantic distinctions found in many dictionaries: homograph and sense (see Section 3.1). ", "mid_sen": "Like Cowie, Guthrie, and Guthrie (1992) , we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g., Brown et al. 1991; Gale, Church, and Yarowsky 1992c ; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions.", "after_sen": "In this paper we attempt to show that the high level of results more typical of systems trained on many instances of a restricted vocabulary can also be obtained by large vocabulary systems, and that the best results are to be obtained from an optimization of a combination of types of lexical knowledge (see Section 2)."}
{"citeStart": 46, "citeEnd": 58, "citeStartToken": 46, "citeEndToken": 58, "sectionName": "UNKNOWN SECTION NAME", "string": "While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds. Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. While such patterns produce false training examples, the resulting noise often only introduces minor distortions.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Brent (1993) has proposed the use of simple word patterns for the acquisition of verb subcategorisation information. ", "mid_sen": "An analogous approach to compounds is used in Lauer (1994) and constitutes one scheme evaluated below. ", "after_sen": "While such patterns produce false training examples, the resulting noise often only introduces minor distortions."}
{"citeStart": 101, "citeEnd": 120, "citeStartToken": 101, "citeEndToken": 120, "sectionName": "UNKNOWN SECTION NAME", "string": "Natural language involves statements that do not contain complete, exact, and unbiased information. Many of these are subjective, which share the common property described in narrative theory (Banfield, 1982) as \"(subjective statements) must all be referred to the speaking subject for interpretation\". Wiebe (1990) further adapted this definition of subjectivity to be \"the linguistic expression of private states (Quirk et al., 1985) \". So far, linguistic cues have played an important role in research of subjectivity recognition (e.g. (Wilson et al., 2006) ), sentiment analysis (e.g. Pang and Lee, 2004) ), and emotion studies (e.g. (Pennebaker et al., 2001) ). While most linguistic cues are grouped under the general rubric of subjectivity, they are usually originated from different dimensions, including:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Wiebe (1990) further adapted this definition of subjectivity to be \"the linguistic expression of private states (Quirk et al., 1985) \". ", "mid_sen": "So far, linguistic cues have played an important role in research of subjectivity recognition (e.g. (Wilson et al., 2006) ), sentiment analysis (e.g. Pang and Lee, 2004) ), and emotion studies (e.g. (Pennebaker et al., 2001) ). ", "after_sen": "While most linguistic cues are grouped under the general rubric of subjectivity, they are usually originated from different dimensions, including:"}
{"citeStart": 55, "citeEnd": 76, "citeStartToken": 55, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "The parsing model is based on MALTParser, a transition-based parser, and uses part-of-speech and morphological information as input. Morphological information is annotated using RFTagger (Schmid and Laws, 2008) , a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger). While transitionbased parsers are quite fast in general, an SVM classifier (which is used in MALTParser by default) becomes slower with increasing training set. In contrast, using the MALTParser interface to LibLinear by Cassel (2009) , we were able to reach a much larger speed of 55 sentences per second (against 0.4 sentences per second for a more feature-rich SVMbased model that reaches state of the art performance).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The parsing model is based on MALTParser, a transition-based parser, and uses part-of-speech and morphological information as input. ", "mid_sen": "Morphological information is annotated using RFTagger (Schmid and Laws, 2008) , a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger). ", "after_sen": "While transitionbased parsers are quite fast in general, an SVM classifier (which is used in MALTParser by default) becomes slower with increasing training set. "}
{"citeStart": 26, "citeEnd": 38, "citeStartToken": 26, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann & Xia, 2003) . However, unlike in rule-and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, unlike in rule-and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. ", "mid_sen": "Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. ", "after_sen": "While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table."}
{"citeStart": 116, "citeEnd": 133, "citeStartToken": 116, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "We also compared our technique for automatic packing of words with the exploitation of manually developed resources. More specifically, we used a 1-to-n Chinese-English bilingual dictionary, described in Section 3.4, and used it in place of the automatically acquired dictionary. Words are thus grouped according to this dictionary, and we then apply the same word aligner as for previous experiments. In this case, since we are not bootstrapping from the output of a word aligner, this can actually be seen as a pre-processing step prior to alignment. These resources follow more or less the same format as the output of the word segmenter mentioned in Section 5.1.2 (Zhao et al., 2001) , so the experiments are carried out using this segmentation.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this case, since we are not bootstrapping from the output of a word aligner, this can actually be seen as a pre-processing step prior to alignment. ", "mid_sen": "These resources follow more or less the same format as the output of the word segmenter mentioned in Section 5.1.2 (Zhao et al., 2001) , so the experiments are carried out using this segmentation.", "after_sen": ""}
{"citeStart": 102, "citeEnd": 121, "citeStartToken": 102, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper reports on a comparison between the transformation-based error-driven learner described in Ferro et al. (1999) and the memory-based learner for GRs described in Buchholz et al. (1999) on finding GRs to verbs 1 by retraining the memory-based learner with the data used in Ferro et al. (1999) . We find that the transformation versus memory-based difference only seems to cause a small difference in the results. Most of the result differences seem to instead be caused by differences in the representations and information used by the learners. An example is that different GR length measures are used. In English, one measure seems better for recovering simple argument GRs, while another measure seems better for modifier GRs. We also find that partitioning the data sometimes helps memory-based learn-That is, GRs that have a verb as the relation target. For example, in Cats eat., there is a \"subject\" relation that has eat as the target and Cats as the source.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our starting point is the work described in Ferro et al. (1999) , which used a fairly small training set.", "mid_sen": "This paper reports on a comparison between the transformation-based error-driven learner described in Ferro et al. (1999) and the memory-based learner for GRs described in Buchholz et al. (1999) on finding GRs to verbs 1 by retraining the memory-based learner with the data used in Ferro et al. (1999) . ", "after_sen": "We find that the transformation versus memory-based difference only seems to cause a small difference in the results. "}
{"citeStart": 166, "citeEnd": 185, "citeStartToken": 166, "citeEndToken": 185, "sectionName": "UNKNOWN SECTION NAME", "string": "\"Take it,\" said Emma, smiling, and pushing the paper towards Harriet \"it is for you. Take your own.\" Our approach to extract and classify social events builds on our previous work , which in turn builds on work from the relation extraction community (Nguyen et al., 2009) . Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004) . To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005) . Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005) .", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "\"Take it,\" said Emma, smiling, and pushing the paper towards Harriet \"it is for you. ", "mid_sen": "Take your own.\" Our approach to extract and classify social events builds on our previous work , which in turn builds on work from the relation extraction community (Nguyen et al., 2009) . ", "after_sen": "Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. "}
{"citeStart": 93, "citeEnd": 113, "citeStartToken": 93, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "We employed the Max Margin Online Learning Algorithms for parameter estimation of the model (Crammer et al., 2006; McDonald et al., 2007) . In preliminary experiments, this algorithm yielded equal or better results compared to SVMs. As the feature representation, φ(x, S) , of polarity-shifting model, we used the local context of three words to the left and right of the target sentiment word. We used the polynomial kernel of degree 2 for polarity-shifting model and the linear kernel for oth- and φ (S) are normalized respectively.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We employed the Max Margin Online Learning Algorithms for parameter estimation of the model (Crammer et al., 2006; McDonald et al., 2007) . ", "after_sen": "In preliminary experiments, this algorithm yielded equal or better results compared to SVMs. "}
{"citeStart": 201, "citeEnd": 220, "citeStartToken": 201, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980) , Categorial Grammar (Ades and Steedman, 1982) , PATR-II (Shieber, 1986) , and lexicalized TAG (Schabes et al, 1988) all deal with complementation by including in one form or another a notion of \"subcategorization frame\" that specifies a sequence of complement phrases and constraints on them. Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could \"apply\" to.", "mid_sen": "Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980) , Categorial Grammar (Ades and Steedman, 1982) , PATR-II (Shieber, 1986) , and lexicalized TAG (Schabes et al, 1988) all deal with complementation by including in one form or another a notion of \"subcategorization frame\" that specifies a sequence of complement phrases and constraints on them. ", "after_sen": "Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain."}
{"citeStart": 190, "citeEnd": 204, "citeStartToken": 190, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "The fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by Prolog, but instead needs to be implemented by additional programming. While theoretically possible, it becomes quite problematic to actually implement. The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called \"abstract syntax\" in (Miller, 1991) and \"higher-order abstract syntax\" in (Pfenning and Elliot, 1988) . This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. ", "mid_sen": "This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics -see, for example, (Miller & Nadathur, 1986) , (Pareschi, 1989) , and (Pereira, 1990) .", "after_sen": ""}
{"citeStart": 78, "citeEnd": 98, "citeStartToken": 78, "citeEndToken": 98, "sectionName": "UNKNOWN SECTION NAME", "string": "The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998) . The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local context while the other represents topical context. The latter utilize a Naive Bayesian classifier. In both cases context is represented by a set of topical and local features. The topical features correspond to the open-class words that occur in a two sentence window of context. The local features occur within a window of context three words to the left and right of the ambiguous word and include co-occurrence features as well as the part-of-speech of words in this window. These features are represented as local & topical b-o-w and p-o-s in Table 6 . (Towell and Voorhees, 1998) report accuracy of 87% while (Leacock et al., 1998) report accuracy of 84%.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The two most accurate methods in this study proved to be a Naive Bayesian classifier (72%) and a perceptron (71%).", "mid_sen": "The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998) . ", "after_sen": "The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local context while the other represents topical context. "}
{"citeStart": 197, "citeEnd": 221, "citeStartToken": 197, "citeEndToken": 221, "sectionName": "UNKNOWN SECTION NAME", "string": "With the growing wdume of text available in electronic lorm, a number of methods have been proposed tor extracting word correspondences from bilingual corpora automatically. These methods can be divided into those taking a statistical approach (Gale & Church 1991a; Kupiec 1993; Dagan et al. 1993; Inoue & Nogaito 1993; Fung 1995) and those taking a linguistic approach (Yamamoto & Sakamoto 1993; Kum~mo & Hirakawa 1994; Ishimoto & Nagao 1994) . The statistical approach utilizes the occurrence frequencies and locations of words in a parallel corpus to calculate the pairwise correlations between the words in the two languages. The linguistic approach primarily extracts correspondences between compound words by consulting a bilingual dictionary of simple words.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With the growing wdume of text available in electronic lorm, a number of methods have been proposed tor extracting word correspondences from bilingual corpora automatically. ", "mid_sen": "These methods can be divided into those taking a statistical approach (Gale & Church 1991a; Kupiec 1993; Dagan et al. 1993; Inoue & Nogaito 1993; Fung 1995) and those taking a linguistic approach (Yamamoto & Sakamoto 1993; Kum~mo & Hirakawa 1994; Ishimoto & Nagao 1994) . ", "after_sen": "The statistical approach utilizes the occurrence frequencies and locations of words in a parallel corpus to calculate the pairwise correlations between the words in the two languages. "}
{"citeStart": 177, "citeEnd": 196, "citeStartToken": 177, "citeEndToken": 196, "sectionName": "UNKNOWN SECTION NAME", "string": "A total of 203,803 sentences have been annotated from 1,034 paper-reference pairs. Although this annotation been performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable inter-annotator agreement (Teufel et al., 2006 ). An example annotation is given in Figure 2 , where the first column shows the line number and the second one shows the class label for the citation to Smadja (1993) . It should be noted that since annotation is always per-formed for a specific citation only, sentences such as the one at line 32, which carry sentiment but refer to a different citation, are marked as excluded from the context.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A total of 203,803 sentences have been annotated from 1,034 paper-reference pairs. ", "mid_sen": "Although this annotation been performed by the first author only, we know from previous work that similar styles of annotation can achieve acceptable inter-annotator agreement (Teufel et al., 2006 ). ", "after_sen": "An example annotation is given in Figure 2 , where the first column shows the line number and the second one shows the class label for the citation to Smadja (1993) . "}
{"citeStart": 121, "citeEnd": 144, "citeStartToken": 121, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "The PCC176 (Stede, 2004 ) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (12-15 sentences), with 33.000 tokens in total. The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate 3 tool. In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007) ) and rhetorical structure according to RST (Mann and Thompson, 1988) . Our annotation software architecture consists of a variety of standard, external tools that can be used effectively for the different annotation types. Their XML output is then automatically converted to a generic format (PAULA, (Dipper, 2005) ), which is read into the linguistic database ANNIS (Dipper et al., 2004) , where the annotations are aligned, so that the data can be viewed and queried across annotation levels.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate 3 tool. ", "mid_sen": "In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007) ) and rhetorical structure according to RST (Mann and Thompson, 1988) . ", "after_sen": "Our annotation software architecture consists of a variety of standard, external tools that can be used effectively for the different annotation types. "}
{"citeStart": 103, "citeEnd": 121, "citeStartToken": 103, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "In all experiments the SVM_Light system outperformed other learning algorithms, which confirms Yang's (Yang and Liu, 1999) results for SVMs fed with Reuters data. The k-nearest neighbor algorithm IB performed surprisingly badly although different values ofk were used. For IB, ID3, C4.5, C5.0, Naive Bayes, RIPPER and SVM_Light, linguistic preprocessing increased the overall performance. In fact, the method performing best, SVM_Light, gained 3.5% by including the task-oriented heuristics. However, the boosted RIPPER and LVQ scored a decreased accuracy value there. For LVQ the decrease may be due to the fact that no adaptations to 5If no results were found this way, MorphAna was applied instead.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All experiments were carried out using 10-fold cross-validation on the data described in Section 2.", "mid_sen": "In all experiments the SVM_Light system outperformed other learning algorithms, which confirms Yang's (Yang and Liu, 1999) results for SVMs fed with Reuters data. ", "after_sen": "The k-nearest neighbor algorithm IB performed surprisingly badly although different values ofk were used. "}
{"citeStart": 0, "citeEnd": 59, "citeStartToken": 0, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "In (Riley, 1989 ), Riley describes a decision-tree based approach to the problem. His performance on /he Brown corpus is 99.8%, using a model learned t'rom a corpus of 25 million words. Liberman and Church suggest in (Liberlnan and Church, 1992) that. a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate. but, do not actually build such a system.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "His performance on /he Brown corpus is 99.8%, using a model learned t'rom a corpus of 25 million words. ", "mid_sen": "Liberman and Church suggest in (Liberlnan and Church, 1992) that. ", "after_sen": "a system could be quickly built to divide newswire text into sentences with a nearly negligible error rate. "}
{"citeStart": 161, "citeEnd": 175, "citeStartToken": 161, "citeEndToken": 175, "sectionName": "UNKNOWN SECTION NAME", "string": "The advantages of the AWM model is that it was shown to reproduce, in simulation, mlmy results on human memory ~md lem'ning. Because search st~n'ts from the current pointer location, items that have been stored most recently are more likely to be retrieved, predicting recency effects [BMdeley, 19861. Because items that are stored in multiple locations are more likely to be retrieved, tbe model predicts fiequency effects [Hiutzmmm and Block, 1971 I. Because items are stored in chrono-IogicN sequence, the model produces natural associativity effects [Landaner, 19751 . Because deliberation and means-end re~tsoning can only operate on salient belietls, limited attention produces a concomitlmt inl)rential limitation, i.e. if a belief is nol salient it cannot be used in deliberation or mcans-end-reltsoniug. This means that mistakes that agents make in their planning process have a plansible cognitive basis. Agents can both fail to access a belief that would idlow them to produce ,an optim~d plan, its well as make a mistake in pl~mning if a belief about bow the world has changed its a result of pllmning is not sldicnt. I)epending on the preceding discourse, and the agent's attentionld capacity, the propositions that im agent knows may or may not be salient when a proposed is made.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The advantages of the AWM model is that it was shown to reproduce, in simulation, mlmy results on human memory ~md lem'ning. ", "mid_sen": "Because search st~n'ts from the current pointer location, items that have been stored most recently are more likely to be retrieved, predicting recency effects [BMdeley, 19861. ", "after_sen": "Because items that are stored in multiple locations are more likely to be retrieved, tbe model predicts fiequency effects [Hiutzmmm and Block, 1971 I. "}
{"citeStart": 203, "citeEnd": 216, "citeStartToken": 203, "citeEndToken": 216, "sectionName": "UNKNOWN SECTION NAME", "string": "The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992) ). A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986) , RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988) . We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992) ). ", "mid_sen": "A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986) , RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988) . ", "after_sen": "We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion."}
{"citeStart": 190, "citeEnd": 213, "citeStartToken": 190, "citeEndToken": 213, "sectionName": "UNKNOWN SECTION NAME", "string": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen's Semantic Distance software package (Pedersen et al., 2004) .", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We compare the results for the 1911 and 1987 Roget's Thesauri with a variety of WordNet-based semantic relatedness measures -see Table 5 . ", "mid_sen": "We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997) , Resnik (Resnik, 1995) , Lin (Lin, 1998) , W&P (Wu and Palmer, 1994) , L&C (Leacock and Chodorow, 1998) , H&SO (Hirst and St-Onge, 1998) , Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002) , and finally Vector and Vector Pair (Patwardhan, 2003) . ", "after_sen": "The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. "}
{"citeStart": 88, "citeEnd": 116, "citeStartToken": 88, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Hal Karl Marie geki~'flt f Has Karl Marie kissed? \"Did Karl kiss Mary?\" for which we adopt the argument composition analysis presented in Hinrichs and Nakazawa (1989) : the subeat list of the auxiliary verb is partially instantiated in the lexicon and only becomes fully instantiated upon its combination with its verbal complement, the main verb. The phrase structure rule that describes this construction is 1", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Hal Karl Marie geki~'flt f Has Karl Marie kissed? ", "mid_sen": "\"Did Karl kiss Mary?\" for which we adopt the argument composition analysis presented in Hinrichs and Nakazawa (1989) : the subeat list of the auxiliary verb is partially instantiated in the lexicon and only becomes fully instantiated upon its combination with its verbal complement, the main verb. ", "after_sen": "The phrase structure rule that describes this construction is 1"}
{"citeStart": 99, "citeEnd": 117, "citeStartToken": 99, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "To evaluate SPATTER's performance on this domain, I am using the PARSEVAL measures, as defined in (Black et al., 1991 The precision and recall measures do not consider constituent labels in their evaluation of a parse, since the treebank label set will not necessarily coincide with the labels used by a given grammar. Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall. These measures are computed by considering a constituent to be correct if and only if it's label matches the label in the treebank. Figures 5, 6, and 7 illustrate the performance of SPATTER as a function of sentence length. SPAT-TER's performance degrades slowly for sentences up to around 28 words, and performs more poorly and more erratically as sentences get longer. Figure 4 indicates the frequency of each sentence length in the test corpus. ....................................................................................... ", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "seconds per sentence on an SGI R4400 with 160 megabytes of RAM.", "mid_sen": "To evaluate SPATTER's performance on this domain, I am using the PARSEVAL measures, as defined in (Black et al., 1991 The precision and recall measures do not consider constituent labels in their evaluation of a parse, since the treebank label set will not necessarily coincide with the labels used by a given grammar. ", "after_sen": "Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall. "}
{"citeStart": 113, "citeEnd": 131, "citeStartToken": 113, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "To better handle the complexity in the gazespeech pattern, we propose to use statistical translation models. Given a time window of enough length, a speech input that contains a list of spoken references (e.g., definite noun phrases) is always accompanied by a list of naturally occurred eye fixations and therefore a list of objects receiving those fixations. All those pairs of speech references and corresponding fixated objects could be viewed as parallel, i.e. they co-occur within the time window. This situation is very similar to the training process of translation models in statistical machine translation (Brown et al., 1993) , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns. The same idea can be borrowed here: by exploring the co-occurrence statistics, we hope to uncover the exact mapping between those eye fixated objects and spoken references. The intuition is that, the more often a fixation is found to exclusively co-occur with a spoken reference, the more likely a mapping should be established between them.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All those pairs of speech references and corresponding fixated objects could be viewed as parallel, i.e. they co-occur within the time window. ", "mid_sen": "This situation is very similar to the training process of translation models in statistical machine translation (Brown et al., 1993) , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns. ", "after_sen": "The same idea can be borrowed here: by exploring the co-occurrence statistics, we hope to uncover the exact mapping between those eye fixated objects and spoken references. "}
{"citeStart": 15, "citeEnd": 19, "citeStartToken": 15, "citeEndToken": 19, "sectionName": "UNKNOWN SECTION NAME", "string": "Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly ) are as useful (if not more useful) as the full probabilities P0(w), at least in those cases for which the ultimate goal is syntactic analysis. Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly ) are as useful (if not more useful) as the full probabilities P0(w), at least in those cases for which the ultimate goal is syntactic analysis. ", "mid_sen": "Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.", "after_sen": "The problem of estimating parameters for log-linear models is not new. "}
{"citeStart": 233, "citeEnd": 255, "citeStartToken": 233, "citeEndToken": 255, "sectionName": "UNKNOWN SECTION NAME", "string": "In (McCarthy et al., 2004) , a method was presented to determine the predominant sense of a word in a corpus. However, in (Chan and Ng, 2005) , we showed that in a supervised setting where one has access to some annotated training data, the EMbased method in section 5 estimates the sense priors more effectively than the method described in (Mc-Carthy et al., 2004) . Hence, we use the EM-based algorithm to estimate the sense priors in the WSJ evaluation data for each of the 21 nouns. The sense with the highest estimated sense prior is taken as the predominant sense of the noun.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (McCarthy et al., 2004) , a method was presented to determine the predominant sense of a word in a corpus. ", "mid_sen": "However, in (Chan and Ng, 2005) , we showed that in a supervised setting where one has access to some annotated training data, the EMbased method in section 5 estimates the sense priors more effectively than the method described in (Mc-Carthy et al., 2004) . ", "after_sen": "Hence, we use the EM-based algorithm to estimate the sense priors in the WSJ evaluation data for each of the 21 nouns. "}
{"citeStart": 94, "citeEnd": 115, "citeStartToken": 94, "citeEndToken": 115, "sectionName": "UNKNOWN SECTION NAME", "string": "SMT Team (2003) also used minimum error training as in Och (2003) , but used a large number of feature functions. More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output. By reranking a 1000-best list generated by the baseline MT system from Och (2003) , the BLEU (Papineni et al., 2001 ) score on the test dataset was improved from 31.6% to 32.9%.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output. ", "mid_sen": "By reranking a 1000-best list generated by the baseline MT system from Och (2003) , the BLEU (Papineni et al., 2001 ) score on the test dataset was improved from 31.6% to 32.9%.", "after_sen": ""}
{"citeStart": 128, "citeEnd": 139, "citeStartToken": 128, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "We chose the best parameters of heuristics by executing several experiments. Accuracy is measured as the percentage of constituents in the test sentences which do not cross any Penn treebank constituents (Black, 1991) . Table 1 shows the results of the robust parser on WSJ. In table 1, 5th, 6th and 7th raw mean that the percentage of sentences which have no crossing constituents, less than one crossing and less than two crossing respectively. With heuris-tics, our robust parser can enhance the processing time and reduce the number of edges. Also, the accuracy is improved from 72.8% to 77.1% even if the heuristics differentiate edges and prefer some edges. It shows that the proposed heuristics is valid in parsing the real sentences. The experiment says that our robust parser with heuristics can recover perfectly about 23 sentences out of 100 sentences which axe just failed in normal parsing, as the percentage of no-crossing sentences is about 23.28%. Table 2 is the results of the robust parser on ATIS which we did not refer to before. The accuracy of the result on ATIS is lower than WSJ because the parameters of the heuristics are a~justed not by ATIS itself but by WSJ. However, the percentage of sentences with constituents crossing less than 2 is higher than the WSJ, as sentences of ATIS are more or less simple.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We chose the best parameters of heuristics by executing several experiments. ", "mid_sen": "Accuracy is measured as the percentage of constituents in the test sentences which do not cross any Penn treebank constituents (Black, 1991) . ", "after_sen": "Table 1 shows the results of the robust parser on WSJ. "}
{"citeStart": 197, "citeEnd": 209, "citeStartToken": 197, "citeEndToken": 209, "sectionName": "UNKNOWN SECTION NAME", "string": "The PCC10 is a sub-corpus of 10 commentaries that serves as \"testbed\" for further developing the annotation levels. On the one hand, we are applying recent guidelines on annotation of information structure (Götze et al., 2007) . On the other hand, based on experiences with the RST annotation, we are replacing the rhetorical trees with a set of distinct, simpler annotation layers: thematic structure, conjunctive relations (Martin, 1992) , and argumentation structure (Freeman, 1991); these are complemented by the other levels mentioned above for the PCC176. The primary motivation for this step is the high degree of arbitrariness that annotators reported when producing the RST trees (see (Stede, 2007) ). By separating the thematic from the intentional information, and accounting for the surface-oriented conjunctive relations (which are similar to what is annotated in the PDTB, see Section 6), we hope to", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "On the one hand, we are applying recent guidelines on annotation of information structure (Götze et al., 2007) . ", "mid_sen": "On the other hand, based on experiences with the RST annotation, we are replacing the rhetorical trees with a set of distinct, simpler annotation layers: thematic structure, conjunctive relations (Martin, 1992) , and argumentation structure (Freeman, 1991); these are complemented by the other levels mentioned above for the PCC176. ", "after_sen": "The primary motivation for this step is the high degree of arbitrariness that annotators reported when producing the RST trees (see (Stede, 2007) ). "}
{"citeStart": 199, "citeEnd": 220, "citeStartToken": 199, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "We have presented an approach to word order for DG which combines traditional notions (semantically motivated dependencies, topological fields) with contemporary techniques (logical description language, model-theoretic semantics). Word order domains are sets of partially ordered words associated with words. A word is contained in an order domain of its head, or may float into an order domain of a transitive head, resulting in a discontinuous dependency tree while retaining a projective order domain structure. Restrictions on the floating are expressed in a lexicalized fashion in terms of dependency relations. An important benefit is that the proposal is lexicalized without reverting to lexical ambiguity to represent order variation, thus profiting even more from the efficiency considerations discussed by Schabes et al. (1988) .", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Restrictions on the floating are expressed in a lexicalized fashion in terms of dependency relations. ", "mid_sen": "An important benefit is that the proposal is lexicalized without reverting to lexical ambiguity to represent order variation, thus profiting even more from the efficiency considerations discussed by Schabes et al. (1988) .", "after_sen": "It is not yet clear what the generative capacity of such lexicalized discontinuous DGs is, but at least some index languages (such as anbnc n) can be characterized. "}
{"citeStart": 134, "citeEnd": 138, "citeStartToken": 134, "citeEndToken": 138, "sectionName": "UNKNOWN SECTION NAME", "string": "Probably the best known algorithm for tracking narrative progression is that developed by Kamp (1979) , Hinrichs (1981) , and Partee (1984) , which formalises the observation that an event will occur just after a preceding event, while a state will overlap with a preceding event. This algorithm gives the correct results in examples such as the following:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Probably the best known algorithm for tracking narrative progression is that developed by Kamp (1979) , Hinrichs (1981) , and Partee (1984) , which formalises the observation that an event will occur just after a preceding event, while a state will overlap with a preceding event. ", "after_sen": "This algorithm gives the correct results in examples such as the following:"}
{"citeStart": 52, "citeEnd": 64, "citeStartToken": 52, "citeEndToken": 64, "sectionName": "UNKNOWN SECTION NAME", "string": "The major topic in the development of word-Pos guessers is the strategy which is to be used for the acquisition of the guessing rules. A rule-based tagger described in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition. A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources. In (Zhang&Kim, 1990) a system for the automated learning of morphological word-formation rules is described. This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features. Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus. A statistical-based suffix learner is presented in (Schmid, 1994) . From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure. Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art, they require specially prepared training data --a pretagged training corpus, training examples, etc.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus. ", "mid_sen": "A statistical-based suffix learner is presented in (Schmid, 1994) . ", "after_sen": "From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure. "}
{"citeStart": 56, "citeEnd": 68, "citeStartToken": 56, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Psycholinguists have been proposed methods for measuring similarity. One of the pioneering works is 'semantic differential' [Osgood, 1952] which analyses meaning of words into a range of different dimensions with the opposed adjectives at both ends (see Figure 1) , and locates the words in the semantic space.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Psycholinguists have been proposed methods for measuring similarity. ", "mid_sen": "One of the pioneering works is 'semantic differential' [Osgood, 1952] which analyses meaning of words into a range of different dimensions with the opposed adjectives at both ends (see Figure 1) , and locates the words in the semantic space.", "after_sen": "Recent works on knowledge representation are somewhat related to Osgood's semantic differential. "}
{"citeStart": 121, "citeEnd": 143, "citeStartToken": 121, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "word sense disambiguation, information retrieval, natural language generation and so on. Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Also, the use of collocations in different applications has been discussed by various authors ((McRoy, 1992) , (Pnstejovsky et al., 1992) , (Smadja and McKeown, 1990) etc.). ", "mid_sen": "However, collocations are not only considered usefnl, but also a problem both in certain applications (e.g. generation, (Nirenburg et al., 1988) , machine translation, (Heid and Raab, 1989) ) and fiom a more theoretical point of view (e.g. (Abeill6 and Schabes, 1989) , (Krenn and Erbach, to appear) ).", "after_sen": "We have been concerned with investigating the lexical . ['unctions (IJTs) of Mel'0,uk (Mel'6uk and Zolkovsky, 1984) as a candidate interllngual device for tbe translation of adjectival and verbal collocates. "}
{"citeStart": 186, "citeEnd": 210, "citeStartToken": 186, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "On the other hand, we were not able to achieve accuracies on the sentiment classification problem comparable to those reported for standard topic-based categorization, despite the several different types of features we tried. Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classification work (McCallum and Nigam, 1998) .", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unigram presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. ", "mid_sen": "Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classification work (McCallum and Nigam, 1998) .", "after_sen": "What accounts for these two differences -difficulty and types of information proving usefulbetween topic and sentiment classification, and how might we improve the latter? "}
{"citeStart": 1, "citeEnd": 25, "citeStartToken": 1, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "For training the LMs, a subset of 43 million words from the Estonian Segakorpus was used (Segakorpus, 2005) , preprocessed with a morphological analyzer (Alumäe, 2006) . After selecting the item types, segmenting the training corpora and generation of a pronunciation dictionary, LMs were trained for each lexical item type. Table 1 shows the text format for LM training data after segmentation with each model. As can be seen, the wordbased approach doesn't use word boundary tokens. (Siivola and Pellom, 2005) were used with no limits as to the order of n-grams, but limiting the number of counts to 4.8 and 5 million counts. In some models this growing method resulted in the inclusion of very frequent long item sequences to the varigram, up to a 28gram. Models of both 5000 and 60000 lexical items were trained in order to test if and how the modeling approaches would scale to smaller and therefore much faster vocabularies. Distribution of counts in n-gram orders can be seen in figure 1. The performance of the statistical language models is often evaluated by perplexity or cross-entropy. However, we decided to only report the real ASR performance, because perplexity does not suit well to the comparison of models that use different lexica, have different OOV rates and have lexical units of different lengths.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As can be seen, the wordbased approach doesn't use word boundary tokens. ", "mid_sen": "(Siivola and Pellom, 2005) were used with no limits as to the order of n-grams, but limiting the number of counts to 4.8 and 5 million counts. ", "after_sen": "In some models this growing method resulted in the inclusion of very frequent long item sequences to the varigram, up to a 28gram. "}
{"citeStart": 73, "citeEnd": 90, "citeStartToken": 73, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "The COBUILD project (Sinclair 1987) adopts this view of meaning by attempting to anchor dictionary senses in current usage by creating sense divisions on the basis of clusters of citations in a corpus. Atkins (1987) and Kilgarriff (forthcoming) also implicitly adopt the view of Harris (1954) , according to which each sense distinction is reflected in a distinct context. A similar view underlies the class-based methods cited in Section 2.4.3 (Brown et al. 1992; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993) . In this volume, Schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Atkins (1987) and Kilgarriff (forthcoming) also implicitly adopt the view of Harris (1954) , according to which each sense distinction is reflected in a distinct context. ", "mid_sen": "A similar view underlies the class-based methods cited in Section 2.4.3 (Brown et al. 1992; Pereira and Tishby 1992; Pereira, Tishby, and Lee 1993) . ", "after_sen": "In this volume, Schiitze continues in this vein and proposes a technique that avoids the problem of sense distinction altogether: he creates sense clusters from a corpus rather than relying on a pre-established sense list."}
{"citeStart": 346, "citeEnd": 364, "citeStartToken": 346, "citeEndToken": 364, "sectionName": "UNKNOWN SECTION NAME", "string": "To ensure that our coding scheme leads to less biased annotation than some of the other resources available for building summarisation systems, and to ensure that other researchers besides ourselves can use it to replicate our results on different types of texts, we wanted to examine two properties of our scheme: stability and reproducibility (Krippendorff, 1980) . Stability is the extent to which an annotator will produce the same classifications at different times. Reproducibility is the extent to which different annotators will produce the same classification. We use the Kappa coefficient (Siegel and Castellan, 1988) to measure stability and reproducibility. The rationale for using Kappa is explained in (Carletta, 1996) . The studies used to evaluate stability and reproducibility we describe in more detail in (Teufel et al., To Appear) . In brief, 48 papers were annotated by three extensively trained annotators. The training period was four weeks consisting of 5 hours of annotation per week. There were written instructions (guidelines) of 17 pages. Skim-reading and annotation of an average length (3800 word) paper typically took 20-30 minutes. The studies show that the training material is reliable. In particular, the basic annotation scheme is stable (K=.82, .81, .76; N=1220; k=2 for all three annotators) and reproducible (K=.71, N=4261, k=3), where k denotes the number of annotators, N the number of sentences annotated, and K gives the Kappa value. The full annotation scheme is stable (K=.83, .79, .81; N=1248; k-2 for all three annotators) and reproducible (K=.78, N=4031, k=3). Overall, reproducibility and stability for trained annotators does not quite reach the levels found for, for instance, the best dialogue act coding schemes, which typically reach Kappa values of around K=.80 (Carletta et al., 1997; Jurafsky et al., 1997) . Our annotation requires more subjective judgements and is possibly more cognitively complex. Our reproducibility and stability results are in the range which Krippendorff (1980) describes as giving marginally significant results for reasonable size data sets when correlating two coded variables which would show a clear correlation if there were perfect agreement. As our requirements are less stringent than Krippendorff's, we find the level of agreement which we achieved acceptable. Figure 3 , which gives the overall distribution of categories, shows that OWN is by far the most frequent category. Figure 4 reports how well the four non-basic categories could be distinguished from all other categories, measured by Krippendorff's diagnostics for category distinctions (i.e. collapsing all other distinctions). When compared to the overall reproducibility of .71, we notice that the annotators were good at distinguishing AIM and TEX-TUAL, and less good at determining BASIS and CON-TRAST. This might have to do with the location of those types of sentences in the paper: AIM and TEX-TUAL are usually found at the beginning or end of the introduction section, whereas CONTRAST, and even more so BASIS, are usually interspersed within longer stretches of OWN. As a result, these categories are more exposed to lapses of attention during annotation.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "To ensure that our coding scheme leads to less biased annotation than some of the other resources available for building summarisation systems, and to ensure that other researchers besides ourselves can use it to replicate our results on different types of texts, we wanted to examine two properties of our scheme: stability and reproducibility (Krippendorff, 1980) . ", "after_sen": "Stability is the extent to which an annotator will produce the same classifications at different times. "}
{"citeStart": 151, "citeEnd": 168, "citeStartToken": 151, "citeEndToken": 168, "sectionName": "UNKNOWN SECTION NAME", "string": "Entity Attributes: Previous studies have shown that entity types and entity mention types of arg 1 and arg 2 are very useful (Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006b ). To represent a single entity attribute, we can take a subgraph that contains only the node representing the head word of the argument, labeled with the entity type or entity mention type. A particularly useful type of features are conjunctive entity features, which are conjunctions of two entity attributes, one for each argument. To represent a conjunctive feature such as \"arg 1 is a Person entity and arg 2 is a Bounded-Area entity\", we can take a subgraph that contains two nodes, one for each argument, and each labeled with an entity attribute. Note that in this case, the subgraph contains two disconnected components, which is allowed by our definition.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "See Figures 1, 2 and 3 for some examples.", "mid_sen": "Entity Attributes: Previous studies have shown that entity types and entity mention types of arg 1 and arg 2 are very useful (Zhao and Grishman, 2005; Zhou et al., 2005; Zhang et al., 2006b ). ", "after_sen": "To represent a single entity attribute, we can take a subgraph that contains only the node representing the head word of the argument, labeled with the entity type or entity mention type. "}
{"citeStart": 171, "citeEnd": 183, "citeStartToken": 171, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "The similarity metrics reported in the literature can be characterised depending on the text patterns they are applied on. So, the word-based metrics compare individual words of the two sentences in terms of their morphological paradigms, synonyms, hyperonyms, hyponyms, antonyms, pos tags... [Nirenburg 93 ] or use a semantic distance d (0~d<l) which is determinM by the Most Specific Common Abstraction (MSCA) obtained from a thesaurus abstraction hierarchy [Sumita 91 ]. Then, a similarity metric is devised, which reflects the similarity of two sentences, hy combining the individual contributions towards similarity stemming from word comparisons.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The similarity metrics reported in the literature can be characterised depending on the text patterns they are applied on. ", "mid_sen": "So, the word-based metrics compare individual words of the two sentences in terms of their morphological paradigms, synonyms, hyperonyms, hyponyms, antonyms, pos tags... [Nirenburg 93 ] or use a semantic distance d (0~d<l) which is determinM by the Most Specific Common Abstraction (MSCA) obtained from a thesaurus abstraction hierarchy [Sumita 91 ]. ", "after_sen": "Then, a similarity metric is devised, which reflects the similarity of two sentences, hy combining the individual contributions towards similarity stemming from word comparisons."}
{"citeStart": 350, "citeEnd": 361, "citeStartToken": 350, "citeEndToken": 361, "sectionName": "UNKNOWN SECTION NAME", "string": "In this dialogue, speaker B is not confident that he will be able to identify the intersection at Lowell Street, and so suggests that the intersection might be marked. Speaker A replies with an elaboration of the initial expression, and B finds that he is now confident, and so accepts the reference. This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent (Appelt, 1985a; Appelt and Kronfeld, 1987; Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1992; Searle, 1969) , are copreseut with the referent (Heeman and Hirst, 1992; Cohen, 1981) , or have the referent in their focus of attention (Reiter and Dale, 1992) . In these theories, the speaker has the intention that the hearer either know the referent or identijy it immediately.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Speaker A replies with an elaboration of the initial expression, and B finds that he is now confident, and so accepts the reference. ", "mid_sen": "This type of reference is different from the type that has been studied traditionally by researchers who have usually assumed that the agents have mutual knowledge of the referent (Appelt, 1985a; Appelt and Kronfeld, 1987; Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1992; Searle, 1969) , are copreseut with the referent (Heeman and Hirst, 1992; Cohen, 1981) , or have the referent in their focus of attention (Reiter and Dale, 1992) . ", "after_sen": "In these theories, the speaker has the intention that the hearer either know the referent or identijy it immediately."}
{"citeStart": 147, "citeEnd": 174, "citeStartToken": 147, "citeEndToken": 174, "sectionName": "UNKNOWN SECTION NAME", "string": "In this study, we explore a wider range of features for AVC, focusing particularly on various ways to mix syntactic with lexical information. Dependency relation (DR): Our way to overcome data sparsity is to break lexicalized frames into lexicalized slots (a.k.a. dependency relations). Dependency relations contain both syntactic and lexical information (4). However, augmenting PP with nouns selected by the preposition (e.g. PP(with:fork)) still gives rise to data sparsity. We therefore decide to break it into two individual dependency relations: PP(with), PP-fork. Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004) , their utility in AVC still remains untested.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We therefore decide to break it into two individual dependency relations: PP(with), PP-fork. ", "mid_sen": "Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004) , their utility in AVC still remains untested.", "after_sen": "Co-occurrence (CO): CO features mostly convey lexical information only and are generally considered not particularly sensitive to argument structures (Rohde et al., 2004) . "}
{"citeStart": 116, "citeEnd": 117, "citeStartToken": 116, "citeEndToken": 117, "sectionName": "UNKNOWN SECTION NAME", "string": "In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar 5 . This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, using STSG, we can build more specific U-trees for translation.", "mid_sen": "In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar 5 . ", "after_sen": "This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. "}
{"citeStart": 194, "citeEnd": 214, "citeStartToken": 194, "citeEndToken": 214, "sectionName": "UNKNOWN SECTION NAME", "string": "Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary disambiguation (Palmer et al. 1997) , parsing (Magerman 1995) and word segmentation (Meknavin et al. 1997) . We employ the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm for word extraction.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Decision tree induction algorithms have been successfully applied for NLP problems such as sentence boundary disambiguation (Palmer et al. 1997) , parsing (Magerman 1995) and word segmentation (Meknavin et al. 1997) . ", "after_sen": "We employ the c4.5 (Quinlan 1993 ) decision tree induction program as the learning algorithm for word extraction."}
{"citeStart": 112, "citeEnd": 128, "citeStartToken": 112, "citeEndToken": 128, "sectionName": "UNKNOWN SECTION NAME", "string": "For our experiments, we use naive Bayes as the learning algorithm. The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work (Lee and Ng, 2002) . In performing WSD with a naive Bayes classifier, the sense s assigned to an example with features f 1 , . . . , f n is chosen so as to maximize:", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The knowledge sources we use include parts-of-speech, local collocations, and surrounding words. ", "mid_sen": "These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work (Lee and Ng, 2002) . ", "after_sen": "In performing WSD with a naive Bayes classifier, the sense s assigned to an example with features f 1 , . . . , f n is chosen so as to maximize:"}
{"citeStart": 164, "citeEnd": 177, "citeStartToken": 164, "citeEndToken": 177, "sectionName": "UNKNOWN SECTION NAME", "string": "To the above, one adds language-independent issues in spell checking such as the four Damerau transformations: omission, insertion, transposition and substitution (Damerau, 1964) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We shall discuss this in more detail in section 4. 4", "mid_sen": "To the above, one adds language-independent issues in spell checking such as the four Damerau transformations: omission, insertion, transposition and substitution (Damerau, 1964) .", "after_sen": ""}
{"citeStart": 16, "citeEnd": 41, "citeStartToken": 16, "citeEndToken": 41, "sectionName": "UNKNOWN SECTION NAME", "string": "1 Introduction (Van Halteren et al., 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers. Their results have been obtained by combining the output of different taggers with system combination techniques such as majority voting. This approach cancels errors that are made by the minority of the taggers. With the best voting technique, the combined results decrease the lowest error rate of the component taggers by as much as 19% (Van Halteren et al., 1998) . The fact that combination of classifiers leads to improved performance has been reported in a large body of machine learning work.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "1 Introduction (Van Halteren et al., 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers. ", "after_sen": "Their results have been obtained by combining the output of different taggers with system combination techniques such as majority voting. "}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of \"similar\" events that have been seen. For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs. This requires a reasonable definition of verb similarity and a similarity estimation method. In Hindle's proposal, words are similar if we have strong statistical evidence that they tend to participate in the same events. His notion of similarity seems to agree with our intuitions in many cases, but it is not clear how it can be used directly to construct word classes and corresponding models of association. Our research addresses some of the same questions and uses similar raw data, but we investigate how to factor word association tendencies into associations of words to certain hidden senses classes and associations between the classes themselves. While it may be worth basing such a model on preexisting sense classes (Resnik, 1992) , in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p(clw ) for each word w. Most other class-based modeling techniques for natural language rely instead on \"hard\" Boolean classes (Brown et al., 1990) . Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as noted above. Our approach avoids both problems.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. ", "mid_sen": "Hindle (1990) proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of \"similar\" events that have been seen. ", "after_sen": "For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs. "}
{"citeStart": 131, "citeEnd": 148, "citeStartToken": 131, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989) . The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984) . For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Therefore, just as it is necessary to smooth empirical ngram models, it is also necessary to smooth empirical decision-tree models.", "mid_sen": "The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989) . ", "after_sen": "The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984) . "}
{"citeStart": 119, "citeEnd": 131, "citeStartToken": 119, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "An event greater improvement over the baseline is illustrated by the increase in the number of event clauses correctly classified, i.e. event rr£all. As shown in Table 7 , an event recall of 67.7% was achieved by the classification rule, as compared to speech tagging (Church, 1988; Alien, 1995) . I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I the 0.0% event recall achieved by the baseline, while suffering no loss in overall accuracy. This difference in recall is more dramatic than the accuracy improvement because of the dominance of stative clauses in the test set. A favorable tradeoff in recall with no loss in accuracy presents an advantage for applications that weigh the identification of nondominant instances more heavily (Cardie and Howe, 1997) . For example, it is advantageous for a medical system that identifies medical procedures to identify event clauses, since procedures are a type of event.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An event greater improvement over the baseline is illustrated by the increase in the number of event clauses correctly classified, i.e. event rr£all. ", "mid_sen": "As shown in Table 7 , an event recall of 67.7% was achieved by the classification rule, as compared to speech tagging (Church, 1988; Alien, 1995) . ", "after_sen": "I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I the 0.0% event recall achieved by the baseline, while suffering no loss in overall accuracy. "}
{"citeStart": 35, "citeEnd": 61, "citeStartToken": 35, "citeEndToken": 61, "sectionName": "UNKNOWN SECTION NAME", "string": "Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003 In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like Figure 1 which we call Semantic Tree (ST). In the semantic tree, arguments are replaced with the most important wordoften referred to as the semantic head.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given a sentence (or query), we first parse it into a syntactic tree using a syntactic parser (i.e. Charniak parser) and then we calculate the similarity between the two trees using the general tree kernel function (Section 4.1).", "mid_sen": "Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003 In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like Figure 1 which we call Semantic Tree (ST). ", "after_sen": "In the semantic tree, arguments are replaced with the most important wordoften referred to as the semantic head."}
{"citeStart": 122, "citeEnd": 132, "citeStartToken": 122, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Douglas I~il)et: has sl, udied l;exl, variat.ion along scv eral l)aranmtcrs, and found that t,cxt.s can I) ( I~,il>er's [ivl' . of dimm,siot,s, s,,ch that l.hcy can bc cxplai,,(~d in i,,t,tit.iwdy siml)le terms to l.hc ,,so,\" of a.n informal.ion rel.riewd ~Hq)liea-- groups. These l'uuetious can l.llen l)e used I.o predicl, the ca+l.egory mlmd)ershil)s of new iudividuals based on tJmir )ara.met(!r scores (Tal.sluoka, 1971 ; M ustouen, 1965 as seen in table 1. We ran discriminant analysis on the texts in the corl)us using seve.ral different features as seen in table 2. We used the SPSS system for statistical data analysis, which has as one of its fcatm.es a complete discriminant analysis (SPSS, 1990 ). The diseriminant flmction extracted t?om the data by the analysis is a linear combination of tlle parameters. To categorize a set into N categories N -1 functions need to be determined, llowever, if we are content with being able to plot all categories on a two-dimensional plane,", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ral different features as seen in table 2. ", "mid_sen": "We used the SPSS system for statistical data analysis, which has as one of its fcatm.es a complete discriminant analysis (SPSS, 1990 ). ", "after_sen": "The diseriminant flmction extracted t?om the data by the analysis is a linear combination of tlle parameters. "}
{"citeStart": 67, "citeEnd": 84, "citeStartToken": 67, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "Our corpus comprises 266 scientific papers from the ACL Anthology (Bird et al., 2008) sections P08 (ACL-2008 long papers), D07 (EMNLP-CoNLL-2007) and C02 (COLING 2002) . Texts were extracted from PDF using a commercial OCR program which gurantees uniform, though not perfect, quality of the resulting text files. We did not rely on the original ACL-ARC text files converted with PDFBox because they contained considerable extraction errors depending on the (various) PDF tools that were used to generate the PDFs. Hence quality of extraction would have dependent on the PDF generator, and OCR-based extraction is much more independent from the generation process.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Our corpus comprises 266 scientific papers from the ACL Anthology (Bird et al., 2008) sections P08 (ACL-2008 long papers), D07 (EMNLP-CoNLL-2007) and C02 (COLING 2002) . ", "after_sen": "Texts were extracted from PDF using a commercial OCR program which gurantees uniform, though not perfect, quality of the resulting text files. "}
{"citeStart": 123, "citeEnd": 132, "citeStartToken": 123, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "It shouldn't therefore come as a surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics. In this paper we present and evaluate a system that transforms texts into logical formulas -using the C&C tools and Boxer (Bos, 2008) -in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It shouldn't therefore come as a surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics. ", "mid_sen": "In this paper we present and evaluate a system that transforms texts into logical formulas -using the C&C tools and Boxer (Bos, 2008) -in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012) .", "after_sen": "We will first sketch the background and the basics of the formalism that we employ in our analysis of negation (Section 2). "}
{"citeStart": 13, "citeEnd": 36, "citeStartToken": 13, "citeEndToken": 36, "sectionName": "UNKNOWN SECTION NAME", "string": "We introduce two types of chunks. The first is simply the phrase type, such as NP, PP, of current chunk. The column CHUNK 1 illustrates this kind of chunk type definition. The second is more complicated. Inspired by (Klein and Manning, 2003) , we split one phrase type into several subsymbols, which contain category information of current constituent's parent. For example, an NP immediately dominated by a S, will be substituted by NPˆS. This strategy severely increases the number of chunk types and make it hard to train chunking models. To shrink this number, we linguistically use a cluster of CTB phrasal types, which was introduced in . The column CHUNK 2 illustrates this definition. E.g., NPˆS implicitly represents Subject while NPˆVP represents Object.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The second is more complicated. ", "mid_sen": "Inspired by (Klein and Manning, 2003) , we split one phrase type into several subsymbols, which contain category information of current constituent's parent. ", "after_sen": "For example, an NP immediately dominated by a S, will be substituted by NPˆS. "}
{"citeStart": 32, "citeEnd": 34, "citeStartToken": 32, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12] (which we will refer to as the predicate dictionary below). The Cihai dictionary includes 12,000 entries and 700,000 collocation instances, predicate dictionary includes about 3000 verbs with their semantic information, case relations and detailed collocation information. These two dictionaries both include some of the collocation information that the algorithm needs.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "According to table 2.1, a classification algorithm was designed, and we use two resources to implement our algorithm: ", "mid_sen": "The Contemporary Chinese Cihai [11] (which we will refer to as the Cihai below) dictionary and the Machine Tractable Dictionary of Contemporary Chinese Predicate Verbs [12] (which we will refer to as the predicate dictionary below). ", "after_sen": "The Cihai dictionary includes 12,000 entries and 700,000 collocation instances, predicate dictionary includes about 3000 verbs with their semantic information, case relations and detailed collocation information. "}
{"citeStart": 69, "citeEnd": 80, "citeStartToken": 69, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "This type of learners constructs a representation for document vectors belonging to a certain class during the learning phase, e.g. decision trees, decision rules or probability weightings. During the categorization phase, the representation is used to assign the appropriate class to a new document vector. Several pruning or specialization heuristics can be used to control the amount of generalization. We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. Support Vector Machines (SVMs): SVMs are described in (Vapnik, 1995) . SVMs are binary learners in that they distinguish positive and negative examples for each class. Like eager learners, they construct a representation during the learning phase, namely a hyper plane supported by vectors of positive and negative examples. For each class, a categorizer is built by computing such a hyper plane. During the categorization phase, each categorizer is applied to the new document vector, yielding the probabilities of the document belonging to a class. The probability increases with the distance of thevector from the hyper plane. A document is said to belong to the class with the highest probability. We chose SVM_Light (Joachims, 1998) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Several pruning or specialization heuristics can be used to control the amount of generalization. ", "mid_sen": "We used ID3 (Quinlan, 1986) , C4.5 (Quinlan, 1992) and C5.0, RIPPER (Cohen, 1995) , and the Naive Bayes inducer (Good, 1965) contained in the MLCq-q-library. ", "after_sen": "ID3, C4.5 and C5.0 produce decision trees, RIPPER isa rulebased learner and the Naive Bayes algorithm computes conditional probabilities of the classes from the instances. "}
{"citeStart": 144, "citeEnd": 163, "citeStartToken": 144, "citeEndToken": 163, "sectionName": "UNKNOWN SECTION NAME", "string": "A wide range of XML-based tools for NLP applications lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. In processing the HOLJ documents we have built a pipeline using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000) , (Thompson et al., 1997) and the xmlperl program (McKelvie, 1999) . The overall processing stages contained in our pipeline are shown in Figure 1 .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A wide range of XML-based tools for NLP applications lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. ", "mid_sen": "In processing the HOLJ documents we have built a pipeline using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000) , (Thompson et al., 1997) and the xmlperl program (McKelvie, 1999) . ", "after_sen": "The overall processing stages contained in our pipeline are shown in Figure 1 ."}
{"citeStart": 49, "citeEnd": 62, "citeStartToken": 49, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "c5.0 3 , a commercial version of c4.5 Quinlan, 1993, performs top-down induction of decision trees tdidt. On the basis of an instance base of examples, c5.0 constructs a decision tree which compresses the classi cation information in the instance base by exploiting differences in relative importance of di erent features. Instances are stored in the tree as paths A demo of the NP and VP chunker is available at http: www.sfb441.unituebingen.de ~dejean chunker.h tml 3 Available from http: www.rulequest.com of connected nodes ending in leaves which contain classi cation information. Nodes are connected via arcs denoting feature values. Feature information gain mutual information between features and class is used to determine the order in which features are employed as tests at all levels of the tree Quinlan, 1993 . With the full input representation words and POS tags, we were not able to run complete experiments. We therefore experimented only with the POS tags with a context of two left and right. We h a ve used the default parameter setting with decision trees combined with value grouping. We have used a nearest neighbor algorithm ib1-ig, here listed as MBL and a decision tree algorithm IGTree from the TiMBL learning package Daelemans et al., 1999b . Both algorithms store the training data and classify new items by choosing the most frequent classi cation among training items which are closest to this new item. Data items are represented as sets of feature-value pairs. Each feature receives a weight which is based on the amount of information which it provides for computing the classi cation of the items in the training data. ib1-ig uses these weights for computing the distance between a pair of data items and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree Daelemans et al., 1999b . We will use their default parameters except for the ib1-ig parameter for the numberof examined nearest neighbors k which we have set to 3 Daelemans et al., 1999a . The classi ers use a left and right context of four words and partof-speech tags. For the four IO representations we have used a second processing stage which used a smaller context but which included information about the IO tags predicted by the rst processing phase Tjong Kim Sang, 2000. When building a classi er, one must gather evidence for predicting the correct class of an item from its context. The Maximum Entropy MaxEnt framework is especially suited for integrating evidence from various information sources. Frequencies of evidence class combinations called features are extracted from a sample corpus and considered to beproperties of the classi cation process. Attention is constrained to models with these properties. The MaxEnt principle now demands that among all the probability distributions that obey these constraints, the most uniform is chosen. During training, features are assigned weights in such a way that, given the MaxEnt principle, the training data is matched as well as possible. During evaluation it is tested which features are active i.e. a feature is active when the context meets the requirements given by the feature. For every class the weights of the active features are combined and the best scoring class is chosen Berger et al., 1996 . For the classier built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence. A mixture of simple features consisting of one of the mentioned information sources and complex features combinations thereof were used. The left context never exceeded 3 words, the right context was maximally 2 words. The model was calculated using existing software Dehaspe, 1997 . MBSL Argamon et al., 1999 uses POS data in order to identify baseNPs. Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP including complete phrases. These sequences may include a few context tags, up to a prespeci ed max context. During inference, MBSL tries to 'tile' each POS string with parts of noun-phrases from the memory. If the string could befully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm. Adding a context extends the possibilities for tiling, thereby giving more opportunities to better candidates. The approach of MBSL to the problem of identifying baseNPs is sequence-based rather than word-based, that is, decisions are taken per POS sequence, or per candidate, but not for a single word. In addition, the tiling process gives no preference to any direction in the sentence. The tiles may b e of any length, up to the maximal length of a phrase in the training data, which gives MBSL a generalization power that compensates for the setup of using only POS tags. The results presented here were obtained by optimizing MBSL parameters based on 5-fold CV on the training data.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The left context never exceeded 3 words, the right context was maximally 2 words. ", "mid_sen": "The model was calculated using existing software Dehaspe, 1997 . MBSL Argamon et al., 1999 uses POS data in order to identify baseNPs. ", "after_sen": "Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP including complete phrases. "}
{"citeStart": 51, "citeEnd": 62, "citeStartToken": 51, "citeEndToken": 62, "sectionName": "UNKNOWN SECTION NAME", "string": "Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004) . The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual training data L. Then, a set of source language sentences, U , is translated based on the current model. A subset of good translations and their sources, T i , is selected in each iteration and added to the training data. These selected sentence pairs are replaced in each iteration, and only the original bilingual training data, L, is kept fixed throughout the algorithm. The process of generating sentence pairs, selecting a subset of good sentence pairs, and updating the model is continued until a stopping condition is met. Note that we run this algorithm in a transductive setting which means that the set of sentences U is drawn either from a development set or the test set that will be used eventually to evaluate the SMT system or from additional data which is relevant to the development or test set. In Algorithm 1, changing the definition of Estimate, Score and Select will give us the different semi-supervised learning algorithms we will discuss in this paper. Given the probability model p(t | s), consider the distribution over all possible valid translations t for a particular input sentence s. We can initialize this probability distribution to the uniform distribution for each sentence s in the unlabeled data U . Thus, this distribution over translations of sentences from U will have the maximum entropy. Under certain precise conditions, as described in (Abney, 2004) , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U . However, this is true only when the functions Estimate, Score and Select have very prescribed definitions. In this paper, rather than analyze the convergence of Algorithm 1 we run it for a fixed number of iterations and instead focus on finding useful definitions for Estimate, Score and Select that can be experimentally shown to improve MT performance.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, this distribution over translations of sentences from U will have the maximum entropy. ", "mid_sen": "Under certain precise conditions, as described in (Abney, 2004) , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U . ", "after_sen": "However, this is true only when the functions Estimate, Score and Select have very prescribed definitions. "}
{"citeStart": 120, "citeEnd": 141, "citeStartToken": 120, "citeEndToken": 141, "sectionName": "UNKNOWN SECTION NAME", "string": "Although a number of automatic sentence alignment methods have been proposed (Brown et al. 1991 ; Gale & Church 1991 b; Kay & Roscheisen 1993; Chen 1993 ), they are not very reliable for real noisy bilingual texts.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, most of theln assume that the input corpora m'e aligned sentence by sentence, which reduces their applicability remarkably.", "mid_sen": "Although a number of automatic sentence alignment methods have been proposed (Brown et al. 1991 ; Gale & Church 1991 b; Kay & Roscheisen 1993; Chen 1993 ), they are not very reliable for real noisy bilingual texts.", "after_sen": "Second, the statistical methods usually require a very large corpus as their input. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "The results show a difference between adulta.ml child-directed speech, in that the latter is ea.s-i~l' I,o sc~gm~ilt giv(m both distril)ution ;ul(I l)honotactics. This lends quantitative SUl)l)ort to research which suggests that mothercse dilthrs fl'o]u normal adult speech in ways possibly useful to the language-learning inl'aut (Aslin et al.) . In fact, the factors making motherese more learnable might be chmidated using this technique: coral)are the resuits of sevcra.I (lilt~rent models, each containing a. dilrercnt factor or combination of factors, looking for those in which a siibstautial i)erformanee dill~,renec exists b(itwecn child-and adult-directed Sl~eech. ()ur model uses phonotaetie constraints as al)solute requirenlent!s on the structure of individual wt~l'tls; this iml)lies th;tt phonot;teties have been h'arncd prior to ;tttempts at segmentation. We must. therefore show that phonotactics can indeed he learned without access to a lexicon--without such a tlemonsl.raliion, we are trapped in circular rc;Lsoning. Gafos :and Brent (1994) demonstrate that phonotacties can be learned with high accuracy from the same unsegmented utterances we used in our simulations. In general, two meth-89 ods exist for combining information sources in ttle MI)I, I)aradign-l: one is to have absolute require-nllHits Oil plausible hyl*otheses (like our i)honota(:tic co,lsl.rainl,s) -these requirements must I)e in-dCl)C,l¢lcntly learnable; the other method of combination is to include an information source in the internal representation of hypotheses (like our distrib,ltional information)---all components of the representation are learned simultaneously (see EIl ison, 1992, for an example of multiple components in a representation).", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "raliion, we are trapped in circular rc;Lsoning. ", "mid_sen": "Gafos :and Brent (1994) demonstrate that phonotacties can be learned with high accuracy from the same unsegmented utterances we used in our simulations. ", "after_sen": "In general, two meth-89 ods exist for combining information sources in ttle MI)I, I)aradign-l: one is to have absolute require-nllHits Oil plausible hyl*otheses (like our i)honota(:tic co,lsl.rainl,s) -these requirements must I)e in-dCl)C,l¢lcntly learnable; the other method of combination is to include an information source in the internal representation of hypotheses (like our distrib,ltional information)---all components of the representation are learned simultaneously (see EIl ison, 1992, for an example of multiple components in a representation)."}
{"citeStart": 27, "citeEnd": 44, "citeStartToken": 27, "citeEndToken": 44, "sectionName": "UNKNOWN SECTION NAME", "string": "Due to the di erent trigger situations we performed two tests: One where we use only acoustic triggers and another where the existence of a perfect word fragment detector is assumed. The input were unsegmented transliterated utterance to exclude in uences a word recognizer. We restrict the processing time on a SUN ULTRA 300MHZ to 10 seconds. The parser was simulated by a w ord trigram. Training and testing were done on two separated parts of the German part of the Verbmobil corpus 12558 turns training 1737 turns test. Test 1 49  70  70  Test 2 71  85 62  83 A direct comparison to other groups is rather di cult due to very di erent corpora, evaluation conditions and goals. Nakatani and Hirschberg, 1993 suggest a acoustic prosodic detector to identify IPs but don't discuss the problem of nding the correct segmentation in depth. Also their results are obtained on a corpus where every utterance contains at least one repair. Shriberg, 1994 also addresses the acoustic aspects of repairs. Parsing approaches like in Bear et al., 1992; Hindle, 1983; Core and Schubert, 1999 must be proved to work with lattices rather than transliterated text. An algorithm which is inherently capable of lattice processing is proposed by Heeman Heeman, 1997 . He rede nes the word recognition problem to identify the best sequence of words, corresponding POS tags and special repair tags. He reports a recall rate of 81 and a precision of 83 for detection and 78 80 for correction. The test settings are nearly the same as test 2. Unfortunately, nothing is said about the processing time of his module.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Shriberg, 1994 also addresses the acoustic aspects of repairs. ", "mid_sen": "Parsing approaches like in Bear et al., 1992; Hindle, 1983; Core and Schubert, 1999 must be proved to work with lattices rather than transliterated text. ", "after_sen": "An algorithm which is inherently capable of lattice processing is proposed by Heeman Heeman, 1997 . "}
{"citeStart": 288, "citeEnd": 305, "citeStartToken": 288, "citeEndToken": 305, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we report on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons (hence BL) corpus introduced by Lin et al. (2006) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we report on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons (hence BL) corpus introduced by Lin et al. (2006) .", "after_sen": "Corpus. "}
{"citeStart": 104, "citeEnd": 118, "citeStartToken": 104, "citeEndToken": 118, "sectionName": "UNKNOWN SECTION NAME", "string": "Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting befiefs; therefore we employ a re, cursive model for collaboration that captures extended negotiation and represents the structure of the discourse. Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. In addition, we provide a process for selecting among multiple possible pieces of evidence.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Caswey et al. ", "mid_sen": "(Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user's existing belief and to generate responses for information retrieval dialogues in a library domain. ", "after_sen": "They argued that in the library dialogues they analyzed, \"in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution:' (Logan et al., 1994, page 141) . "}
{"citeStart": 113, "citeEnd": 125, "citeStartToken": 113, "citeEndToken": 125, "sectionName": "UNKNOWN SECTION NAME", "string": "This example represents a general phenomenon: many expressions allow intervening noun phrases and/or modifying terms. For example: \"stepped on <mods> toes\" Ex: stepped on the boss' toes \"dealt <np> <mods> blow\" Ex: dealt the company a decisive blow \"brought <np> to <mods> knees\" Ex: brought the man to his knees (Riloff and Wiebe, 2003) also showed that syntactic variations of the same verb phrase can be-have very differently. For example, they found that passive-voice constructions of the verb \"ask\" had a 100% correlation with opinion sentences, but active-voice constructions had only a 63% correlation with opinions. Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features. We used three types of features in our research: unigrams, bigrams, and IE patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003). 1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004) . AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. Au-toSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996) , as well as for opinion analysis (Riloff and Wiebe, 2003) . Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to infinitive VPs, and AuxVP refers to VPs where the main verb is a form of \"to be\" or \"to have\". Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be extracted by the patterns. 2", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. ", "mid_sen": "Au-toSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996) , as well as for opinion analysis (Riloff and Wiebe, 2003) . ", "after_sen": "Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. "}
{"citeStart": 81, "citeEnd": 107, "citeStartToken": 81, "citeEndToken": 107, "sectionName": "UNKNOWN SECTION NAME", "string": "We perform various clustering experiments in order to test, on the one hand, the usefulness of our enhanced subcategorisation frames. On the other hand, we intend to discover which is the natural partition of the data that best accommodates our target classification. The target classification is a hierarchy of three levels, each of them dividing the data into 3, 15, or 31 levels. For this reason, we experiment on 3, 15, and 31 desired output clusters, and evaluate them on each of the target classification levels, respectively. Table 2 shows the evaluation results of the clustering experiment that takes as input bare subcategorisation frames. Table 3 shows the evaluation results of the experiment that includes named entity recognition in the features describing the verbs. In both tables, each line reports the results of a classification task. The average Silhouette measure is shown in the second column. We can observe that the best classification tasks in terms of the Silhouette measure are the 3-way and 15-way classifications. The baseline is calculated, for each task, as the average value of the Adjusted Rand measure for 100 random cluster assignations. Although all the tasks perform better than the baseline, the increase is so small that it is clear that some improvements have to be done on the experiments. According to the Adjusted Rand measure, the clustering algorithm seems to perform better in the tasks with a larger number of classes. On the other hand, the enhanced features are useful on the 15-way and 3-way classifications, but they are harmful in the 31-way classification. In spite of these results, a qualitative observation of the output clusters reveals that they are intuitively plausible, and that the evaluation is penalised by the fact that the target classes are of very different sizes. On the other hand, our data takes into account syntactic information, while the target classification is not only based on syntax, but also on other aspects of the properties of the verbs. These results compare poorly to the performance achieved by (Schulte im Walde, 2003) , who obtains an Adjusted Rand measure of 0.15 in a similar task, in which she classifies 168 German verbs into 43 semantic verb classes. Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003) , where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These results compare poorly to the performance achieved by (Schulte im Walde, 2003) , who obtains an Adjusted Rand measure of 0.15 in a similar task, in which she classifies 168 German verbs into 43 semantic verb classes. ", "mid_sen": "Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003) , where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.", "after_sen": ""}
{"citeStart": 75, "citeEnd": 96, "citeStartToken": 75, "citeEndToken": 96, "sectionName": "UNKNOWN SECTION NAME", "string": "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999) , a memory-based learner (MBL), for both phases. We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001) ; allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space. We mostly use the default settings of TiMBL-the IB1 learning algorithm and overlap comparison metric between instances-and experiment with different values of k.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use TiMBL (Daelemans et al., 2010; Daelemans et al., 1999) , a memory-based learner (MBL), for both phases. ", "mid_sen": "We use TiMBL because MBL has been shown to work well with small data sets (Banko and Brill, 2001) ; allows for the use of both text-based and numeric features; and does not suffer from a fragmented class space. ", "after_sen": "We mostly use the default settings of TiMBL-the IB1 learning algorithm and overlap comparison metric between instances-and experiment with different values of k."}
{"citeStart": 10, "citeEnd": 23, "citeStartToken": 10, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example 'X can accommodate up to Y'. Yet, many of these templates share a similar meaning, e.g. 'X accommodate up to Y', 'X can accommodate up to Y', 'X will accommodate up to Y', etc. Following Sekine (2005) , we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word cooccurrence statistics per predicate.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example 'X can accommodate up to Y'. Yet, many of these templates share a similar meaning, e.g. 'X accommodate up to Y', 'X can accommodate up to Y', 'X will accommodate up to Y', etc. ", "mid_sen": "Following Sekine (2005) , we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word cooccurrence statistics per predicate.", "after_sen": "Next, we applied some clean-up preprocessing to the ReVerb extractions. "}
{"citeStart": 375, "citeEnd": 397, "citeStartToken": 375, "citeEndToken": 397, "sectionName": "UNKNOWN SECTION NAME", "string": "Some inflectional features, specifically gender and number, are expressed using different morphemes in different words (even within the same POS). There are four sound gender-number suffixes in Arabic: 5 +φ (null morpheme) for masculine singular, + + for feminine singular, + +wn for masculine plural, and + +At for feminine plural. Form-based GENDER and NUMBER feature values are set only according to these four morphemes (and a few others, ignored for simplicity). There are exceptions and alternative ways to express GENDER and NUMBER, however, and functional feature values take them into account: Depending on the lexeme, plurality can be expressed using sound plural suffixes or using a pattern change together with singular suffixes. A sound plural example is the word pair / Hafiyd+a /Hafiyd+At ('granddaughter/granddaughters.) On the other hand, the plural of the inflectionally and morphemically feminine singular word madras+a ('school') is the word madAris+φ ('schools'), which is feminine and plural inflectionally, but has a masculine singular suffix. This irregular inflection, known as broken plural, is similar to the English mouse/mice, but is much more common in Arabic (over 50% of plurals in our training data). A similar inconsistency appears in feminine nominals that are not inflected using sound gender suffixes, for example, the feminine form of the masculine singular adjective Âzraq+φ ('blue') is zarqA'+φ not * *Âzraq+a . To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007) , we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features. 6 Most available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012) . The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012) . See Section 5.2 for more details.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To address this inconsistency in the correspondence between inflectional features and morphemes, and inspired by Smrž (2007) , we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features. ", "mid_sen": "6 Most available Arabic NLP tools and resources model morphology using formbased (\"surface\") inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004) , the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012) . ", "after_sen": "The Elixir-FM analyzer (Smrž 2007) readily provides the functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. "}
{"citeStart": 93, "citeEnd": 102, "citeStartToken": 93, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "Then, we carry out a partial character repetition removal. If the same character is repeated more than 3 times, the rest of repetitions are removed. In this way, the words are normalised, but we can still recognise if the original words had repeated characters. We do not remove all repetitions as they can be very useful to detect subjectivity in texts [Saif 2012] . For example, the words \"gooood\" and \"gooooood\" would be normalised to \"goood\", but the word \"good\" would remain the same. We assume the ambiguity of this example, which can refer to both \"good\" and \"god\". Figure 2 shows an example of this normalisation process.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this way, the words are normalised, but we can still recognise if the original words had repeated characters. ", "mid_sen": "We do not remove all repetitions as they can be very useful to detect subjectivity in texts [Saif 2012] . ", "after_sen": "For example, the words \"gooood\" and \"gooooood\" would be normalised to \"goood\", but the word \"good\" would remain the same. "}
{"citeStart": 49, "citeEnd": 65, "citeStartToken": 49, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "We also evaluated our method by using a constructed thesaurus in a pp-attachment disan> bigua.tion experiment. We used as training data the same 180,000 case fl'ames in Experiment 1. We also extracted as our test data 172 (verb, no~nll,prep,'noune) patterns Dora the data in the same corpus, which is not used in the training data. For the 150 words that appear in the position of ,oun.e in the test data, we constructed a thesaurus based on the co-occurrences between heads and slot. values of the fl'ames in the training data. This is because in our disambiguation test we only need a. thesaurus consisting of these 150 words. We then applied the learning method proposed in (Li and Abe, 1995) to learn case fl'ame patterns with the constructed thesaurus as input using the same training data. That is, we used it to learn the conditional distributions P ( Classlll,erb, prep), P(Classe [n, ounl, prep) , where Class1 and Classe vary over the internal nodes in a certain 'cut' in the thesaurus tree l0", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is because in our disambiguation test we only need a. thesaurus consisting of these 150 words. ", "mid_sen": "We then applied the learning method proposed in (Li and Abe, 1995) to learn case fl'ame patterns with the constructed thesaurus as input using the same training data. ", "after_sen": "That is, we used it to learn the conditional distributions P ( Classlll,erb, prep), P(Classe [n, ounl, prep) , where Class1 and Classe vary over the internal nodes in a certain 'cut' in the thesaurus tree l0"}
{"citeStart": 290, "citeEnd": 308, "citeStartToken": 290, "citeEndToken": 308, "sectionName": "UNKNOWN SECTION NAME", "string": "The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002) , selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002) , manually or randomly selected (Teufel, 2010, p.60) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As the emphasis of this approach was the identification of the argumentative zones, less attention was given to the sentence selection criteria for the extractive summaries.", "mid_sen": "The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002) , selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002) , manually or randomly selected (Teufel, 2010, p.60) .", "after_sen": "More recently Contractor et al. (2012) have used automatically annotated argumentative zones (Guo et al., 2011) to guide the creation of extractive summaries of scientific articles. "}
{"citeStart": 266, "citeEnd": 271, "citeStartToken": 266, "citeEndToken": 271, "sectionName": "UNKNOWN SECTION NAME", "string": "The main factor seems to be that even though Expt is not syntactically a question, the little red piece is the focus of a question, and as such is in focus despite the fact that the syntactic construction there is supposedly focuses a hole in the green plunger ... [Sid79] . These examples suggest that a questioned entity is left focused until the point in the dialogue at which the question is resolved. The fact that well has been noted as a marker of response to questions supports this analysis [Sch87] . Thus the relevant factor here may be the switching of control among discourse participants [WS88]. These mixed-initiati.ve features make these sequences inherently different than text.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "us this is questionable.", "mid_sen": "The main factor seems to be that even though Expt is not syntactically a question, the little red piece is the focus of a question, and as such is in focus despite the fact that the syntactic construction there is supposedly focuses a hole in the green plunger ... [Sid79] . ", "after_sen": "These examples suggest that a questioned entity is left focused until the point in the dialogue at which the question is resolved. "}
{"citeStart": 107, "citeEnd": 127, "citeStartToken": 107, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "The HPSG grammar developed with MicroCUF models a fragment of German. Since our focus is on the lexicon, the range of syntactic variation treated is currently limited to simplex sentences with canonical word order. We have incorporated some recent developments of HPSG, esp. the revisions of Pollard & Sag (1994, ch. 9) , Manning & Sag (1995) 's proposal for an independent level of argument structure and Bouma (1997) 's use of argument structure to eliminate procedural lexical rules in favour of relational constraints. Our elaborate ontology of semantic types -useful for non-trivial acquisition of selectional restrictions and nominal sorts -was derived from a systematic corpus study of a biological domain (Knodel 1980, 154-188) . The grammar also covers all valence classes encountered in the corpus. As for the lexicon format, we currently list full forms only. Clearly, a morphology component would supply more contextual information from known affixes but would still require the processing of unknown stems.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since our focus is on the lexicon, the range of syntactic variation treated is currently limited to simplex sentences with canonical word order. ", "mid_sen": "We have incorporated some recent developments of HPSG, esp. the revisions of Pollard & Sag (1994, ch. 9) , Manning & Sag (1995) 's proposal for an independent level of argument structure and Bouma (1997) 's use of argument structure to eliminate procedural lexical rules in favour of relational constraints. ", "after_sen": "Our elaborate ontology of semantic types -useful for non-trivial acquisition of selectional restrictions and nominal sorts -was derived from a systematic corpus study of a biological domain (Knodel 1980, 154-188) . "}
{"citeStart": 242, "citeEnd": 263, "citeStartToken": 242, "citeEndToken": 263, "sectionName": "UNKNOWN SECTION NAME", "string": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . However none of these methods has considered the way of dealing both phenomena in the same concrete system. We propose in this paper an algorithm that deals with both phenomena, in the same analyser. The anaphora module pertains to the recent methods, uses a set of resolution rules based on the focusing approach, see (Sidner, 1983) . These rules are applied to the conceptual representation and their output is a set of candidate antecedents. Concerning the PPs, unattached prepositions involve empty or unfilled roles in the Conceptual Structures (CSs), expressed in a frame-based language (Zarri, 1999) . The disambiguation procedure alms at filling the empty roles using attachment rules.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Several methods have been proposed to deal with anaphora resolution and prepositional phrase (PP) attachment phenomenon and separately, so that the literature is very abundant : for PPs see e.g., (Frazier and Fodor, 1979; Hobbs, 1990) , and (Wilks and Huang, 1985) , and for anaphora see e.g., (Carter, 1986; Reinhart, 1983) and (Sidner, 1983) . ", "after_sen": "However none of these methods has considered the way of dealing both phenomena in the same concrete system. "}
{"citeStart": 247, "citeEnd": 259, "citeStartToken": 247, "citeEndToken": 259, "sectionName": "UNKNOWN SECTION NAME", "string": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . An important aspect of this tagger is that it will give good accuracy with a minimal amount of manually tagged training data. 96% accuracy correct assignment of tags to word token, compared with a human annotator, is quoted, over a 500000 word corpus.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The first major use of HMMs for part of speech tagging was in CLAWS (Garside et al., 1987) in the 1970s. ", "mid_sen": "With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alter-natives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988) , Brill (Brill and Marcus, 1992; Brill, 1992) , DeRose (DeRose, 1988) and gupiec (Kupiec, 1992) . ", "after_sen": "One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992) . "}
{"citeStart": 173, "citeEnd": 183, "citeStartToken": 173, "citeEndToken": 183, "sectionName": "UNKNOWN SECTION NAME", "string": "Word vectors reflecting word meanings are expected to enable numerical approaches to semantics. Some early attempts at vector representation in I)sycholinguistics were the semantic d (O'erential approach (Osgood et al. 1957) and the associative distribution apl)roach (Deese 1962) . llowever, they were derived manually through psychological experiments. An early attempt at automation was made I)y Wilks el aL (t990) us-.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Word vectors reflecting word meanings are expected to enable numerical approaches to semantics. ", "mid_sen": "Some early attempts at vector representation in I)sycholinguistics were the semantic d (O'erential approach (Osgood et al. 1957) and the associative distribution apl)roach (Deese 1962) . llowever, they were derived manually through psychological experiments. ", "after_sen": "An early attempt at automation was made I)y Wilks el aL (t990) us-."}
{"citeStart": 115, "citeEnd": 126, "citeStartToken": 115, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "The typical solution to the redundancy problem is to group verbs according to their argument realization patterns (Levin, 1993) , possibly arranged in an inheritance hierarchy. The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class. In addition, lexical rules could be formulated to derive certain alternations from more basic forms.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The major drawback of this approach, however, is the tremendous amount of redundancy in the lexicon-for example, the class of prototypical transitive verbs where the agent appears as the subject and the theme as the direct object must all duplicate this pattern.", "mid_sen": "The typical solution to the redundancy problem is to group verbs according to their argument realization patterns (Levin, 1993) , possibly arranged in an inheritance hierarchy. ", "after_sen": "The argument structure and syntax-tosemantics mapping would then only need to be specified once for each verb class. "}
{"citeStart": 33, "citeEnd": 47, "citeStartToken": 33, "citeEndToken": 47, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we provided the first probabilistic, treebank-trained parser for French. In Experiment 1, we established an unlexicalized baseline model, which yielded a labeled precision and recall of about 66%. We experimented with a number of tree transformation that take account of the peculiarities of the annotation of the French Treeures are slightly lower than in Table 3. bank; the best performance was obtained by raising coordination and contracting compounds (which have internal structure in the FTB). In Experiment 2, we explored a range of lexicalized parsing models, and found that lexicalization improved parsing performance by up to 15%: Collins' Models 1 and 2 performed at around 80% LR and LP. No significant improvement could be achieved by switching to Dubey and Keller's (2003) sister-head model, which has been claimed to be particularly suitable for treebanks with flat annotation, such as the FTB. A small but significant improvement (to 81% LR and LP) was obtained by a bigram model that combines features of the sister-head model and Collins' model. These results have important implications for crosslinguistic parsing research, as they allow us to tease apart language-specific and annotationspecific effects. Previous work for English (e.g., Magerman, 1995; Collins, 1997) has shown that lexicalization leads to a sizable improvement in parsing performance. English is a language with nonflexible word order and with a treebank with a nonflat annotation scheme (see Table 2 ). Research on German (Dubey and Keller, 2003) showed that lexicalization leads to no sizable improvement in parsing performance for this language. German has a flexible word order and a flat treebank annotation, both of which could be responsible for this counterintuitive effect. The results for French presented in this paper provide the missing piece of evidence: they show that French behaves like English in that it shows a large effect of lexicalization. Like English, French is a language with non-flexible word order, but like the German Treebank, the French Treebank has a flat annotation. We conclude that Dubey and Keller's (2003) results for German can be attributed to a language-specific factor (viz., flexible word order) rather than to an annotation-specific factor (viz., flat annotation). We confirmed this claim in Experiment 3 by showing that the effects of lexicalization observed for English, French, and German are preserved if the size of the training set is kept constant across languages.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These results have important implications for crosslinguistic parsing research, as they allow us to tease apart language-specific and annotationspecific effects. ", "mid_sen": "Previous work for English (e.g., Magerman, 1995; Collins, 1997) has shown that lexicalization leads to a sizable improvement in parsing performance. ", "after_sen": "English is a language with nonflexible word order and with a treebank with a nonflat annotation scheme (see Table 2 ). "}
{"citeStart": 68, "citeEnd": 80, "citeStartToken": 68, "citeEndToken": 80, "sectionName": "UNKNOWN SECTION NAME", "string": "Traditionally, some of the most relevant approaches to solve anaphora have been those called poor-knowledge approaches. They use limited knowledge (lexical, morphological and syntactic information sources) for the detection of the correct antecedent. These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferrández et al., 1999) . Taking this basis, it is possible to improve the results of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They use limited knowledge (lexical, morphological and syntactic information sources) for the detection of the correct antecedent. ", "mid_sen": "These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferrández et al., 1999) . ", "after_sen": "Taking this basis, it is possible to improve the results of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information."}
{"citeStart": 106, "citeEnd": 129, "citeStartToken": 106, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "In this paper, we use mean concreteness scores for words as published in the large-scale norming study by Brysbaert et al. (2013) . The dataset has a reasonable coverage for our data; thus, 78% of tokens in Set A have a concreteness rating. The ratings are real numbers on the scale of 1 through 5; for example, essentialness has the concreteness of 1.04, while sled has the concreteness of 5. The representation used by the baseline system bins the continuous values into 17 bins, starting with 1 and incrementing by 0.25 (the topmost bin has words with concreteness value of 5). Compared to a representation using a single continuous variable, the binned representation allows the machine-learner to provide different weights to dif- ferent bins, thus modeling a non-linear relationship between concreteness and metaphoricity. Indeed, the logistic regression classifier has made precisely such use of this representation; Figure 1 shows the weights assigned by the classifier to the various bins, in a baseline model with unweighted examples trained on Set A data. Specifically, it is clear that abstract words receive a negative weight (predict the class \"non-metaphor\"), while concreteness values above 2.5 generally receive a positive weight (apart form the top bin, which happens to have only a single word in it).", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In this paper, we use mean concreteness scores for words as published in the large-scale norming study by Brysbaert et al. (2013) . ", "after_sen": "The dataset has a reasonable coverage for our data; thus, 78% of tokens in Set A have a concreteness rating. "}
{"citeStart": 106, "citeEnd": 129, "citeStartToken": 106, "citeEndToken": 129, "sectionName": "UNKNOWN SECTION NAME", "string": "The last two columns of Table 1 present the results of adjective disambiguation by a combination of syntactic and semantic indicator attributes. The disambiguating rules we used are given in Table 2 . The syntactic indicator attributes, predicative and infinitival, were applied first. Afterward, if a target adjective sense was not resolved, semantic indicator attributes were applied; no individual indicator nouns were used. The semantic attributes that were applied were animate, body part, color, concrete, human, and text type; Church and Hanks (1989) had pointed to two of these attributes, person and body part (also time, previously mentioned above) in a seemingly casual listing of just five attributes potentially useful for describing the lexico-syntactic regularities of noun-verb relations. Table 1 shows that these few, general attributes cover almost three-quarters of all instances of the target adjectives. Disambiguation by these syntactic and semantic attributes is effectively as reliable as disambiguation using significant indicator nouns: having three apparent errors in disambiguation is not significantly worse than the errorless performance of the significant indicator nouns in the 100-sentence samples. In fact, under a deeper analysis, these three cases are consistent with the pertinent attributes and should not be treated as errors at all. In one sentence,", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Afterward, if a target adjective sense was not resolved, semantic indicator attributes were applied; no individual indicator nouns were used. ", "mid_sen": "The semantic attributes that were applied were animate, body part, color, concrete, human, and text type; Church and Hanks (1989) had pointed to two of these attributes, person and body part (also time, previously mentioned above) in a seemingly casual listing of just five attributes potentially useful for describing the lexico-syntactic regularities of noun-verb relations. ", "after_sen": "Table 1 shows that these few, general attributes cover almost three-quarters of all instances of the target adjectives. "}
{"citeStart": 75, "citeEnd": 90, "citeStartToken": 75, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "To resolve the opposition between surface order and the PAS in a free word order language, one can let the type shifted categories of terms proliferate, or reformulate CCG in such a way that arguments of the verbs are sets, rather than lists whose arguments are made available one at a time. The former alternative makes the spurious ambiguity problem of CG parsing (Karttunen, 1989 ) even more severe. Multiset CCG (Hoffman, 1995) is an example of the setoriented approach. It is known to be computationally tractable but less efficient than the polynomial time CCG algorithm of Vijay-Shanker and Weir (1993) . I try to show in this paper that the traditional curried notation of CG with type shifting can be maintained to account for Surface Form+-~PAS mapping without leading to proliferation of argument categories or to spurious ambiguity.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To resolve the opposition between surface order and the PAS in a free word order language, one can let the type shifted categories of terms proliferate, or reformulate CCG in such a way that arguments of the verbs are sets, rather than lists whose arguments are made available one at a time. ", "mid_sen": "The former alternative makes the spurious ambiguity problem of CG parsing (Karttunen, 1989 ) even more severe. ", "after_sen": "Multiset CCG (Hoffman, 1995) is an example of the setoriented approach. "}
{"citeStart": 112, "citeEnd": 132, "citeStartToken": 112, "citeEndToken": 132, "sectionName": "UNKNOWN SECTION NAME", "string": "Space precludes illustrating the substitutional approach through further examples, though more are discussed in Cooper et al., 1994b) . The coverage is basically the same as DSP's: Antecedent Contained Deletion: A sloppy substitution for every person that Simon did in the sentence John greeted every person that Simon did results in re-introducing the ellipsis in its own resolution. This leads to an uninterpretable cyclic QLF in much the same way that DSP obtain a violation of the occurs check on sound unification.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Space precludes illustrating the substitutional approach through further examples, though more are discussed in Cooper et al., 1994b) . ", "after_sen": "The coverage is basically the same as DSP's: Antecedent Contained Deletion: A sloppy substitution for every person that Simon did in the sentence John greeted every person that Simon did results in re-introducing the ellipsis in its own resolution. "}
{"citeStart": 142, "citeEnd": 162, "citeStartToken": 142, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight & Srinivasan, 2003; Ruch et al., 2007; Hirohata et al., 2008; Björne et al., 2009) . The work of Hirohata et al. (2009) has been integrated with the MEDIE service (Miyao et al., 2006) , allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al., 1999; Teufel & Moens, 2002; Teufel et al., 2009; Teufel, 2010 ). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012) . Modality and negation in text have also been the focus of recent workshops (Farkas et al (2010) , Morante & Sporleder (2012) ). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al. (2008) , a cut-down version of the scheme proposed by Teufel et al. (2009) and CoreSC (1 st layer), from general to specific.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. ", "mid_sen": "AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. ", "after_sen": "Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. "}
{"citeStart": 101, "citeEnd": 121, "citeStartToken": 101, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "An approach combining a hand-crafted contextfree grammar and stochastic probabilities is pursued in (Lee and Seneff, 2006) , but it is designed for a restricted domain only. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to recognize a variety of errors. It achieves 55% precision and 23% recall overall, on evaluation data that partially overlap with those of the present paper. Unfortunately, results on verb form errors are not reported separately, and comparison with our approach is therefore impossible.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Carefully hand-crafted rules, when used alone, tend to yield high precision; they may, however, be less equipped to detect verb form errors within a perfectly grammatical sentence, such as the example given in §3.2.", "mid_sen": "An approach combining a hand-crafted contextfree grammar and stochastic probabilities is pursued in (Lee and Seneff, 2006) , but it is designed for a restricted domain only. ", "after_sen": "A maximum entropy model, using lexical and POS features, is trained in (Izumi et al., 2003) to recognize a variety of errors. "}
{"citeStart": 78, "citeEnd": 90, "citeStartToken": 78, "citeEndToken": 90, "sectionName": "UNKNOWN SECTION NAME", "string": "One reason why the deep system does not perform better seems to be the limited size of the training data available for the machine learning component. As we cannot expect the necessary amount of training data to be available in the near future, we currently investigate the data more closely in order to arrive at a more controlled model of textual entailment. In another current effort, we work on an interface to upper-level ontologies (Reiter, 2007) in order to access more \"world-knowledge\" which is a desideratum in natural language processing in general, as in many approaches to textual entailment.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As we cannot expect the necessary amount of training data to be available in the near future, we currently investigate the data more closely in order to arrive at a more controlled model of textual entailment. ", "mid_sen": "In another current effort, we work on an interface to upper-level ontologies (Reiter, 2007) in order to access more \"world-knowledge\" which is a desideratum in natural language processing in general, as in many approaches to textual entailment.", "after_sen": "The figures we present in the following are all computed with the LogitBoost classifier."}
{"citeStart": 252, "citeEnd": 271, "citeStartToken": 252, "citeEndToken": 271, "sectionName": "UNKNOWN SECTION NAME", "string": "(I) Words and phrases that can function as connectives can also serve other roles. (Eg, when can be a relative pronoun, as well as a subordinating conjunction.) It has been difficult to identify all and only those cases where a token functions as a discourse connective, and in many cases, the syntactic analysis in the Penn TreeBank (Marcus et al., 1993) provides no help. For example, is as though always a subordinating conjunction (and hence a connective) or do some tokens simply head a manner adverbial (eg, seems as though . . . versus seems more rushed as though . . . )? Is also sometimes a discourse connective relating two abstract objects and other times, an adverb that presupposes that a particular property holds of some other entity? If so, when one and when the other? In the PDTB, annotation has erred on the side of false positives.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(I) Words and phrases that can function as connectives can also serve other roles. ", "mid_sen": "(Eg, when can be a relative pronoun, as well as a subordinating conjunction.) It has been difficult to identify all and only those cases where a token functions as a discourse connective, and in many cases, the syntactic analysis in the Penn TreeBank (Marcus et al., 1993) provides no help. ", "after_sen": "For example, is as though always a subordinating conjunction (and hence a connective) or do some tokens simply head a manner adverbial (eg, seems as though . . . versus seems more rushed as though . . . )? "}
{"citeStart": 57, "citeEnd": 81, "citeStartToken": 57, "citeEndToken": 81, "sectionName": "UNKNOWN SECTION NAME", "string": "We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1) . Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Due to space limitations, we omit the details of the parsing algorithm.", "mid_sen": "We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1) . ", "after_sen": "Note that features listed in the 'Between bunsetsus' row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. "}
{"citeStart": 103, "citeEnd": 130, "citeStartToken": 103, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "proposed in (Tsarfaty, 2006) . In our third model GT ppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007) . In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an (2007) . Finally, model GT v = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima'an, 2007; Cohen and Smith, 2007) . For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "proposed in (Tsarfaty, 2006) . ", "mid_sen": "In our third model GT ppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007) . ", "after_sen": "In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an (2007) . "}
{"citeStart": 161, "citeEnd": 178, "citeStartToken": 161, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "The judges consisted of twenty volunteers, all native English speakers without explicit linguistic training. We gave them general guidelines about what constituted fluency, mentioning that they should consider grammaticality but deliberately not giving detailed instructions on the manner for doing this, as we were interested in the level of agreement of intuitive understanding of fluency. We instructed them also that they should evaluate the sentence without considering its content, using Colourless green ideas sleep furiously as an example of a nonsensical but perfectly fluent sentence. The judges were then presented with the 50 sentences in random order, and asked to score the sentences according to their own scale, as in magnitude estimation (Bard et al., 1996) ; these scores were then normalised in the range [0,1].", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We instructed them also that they should evaluate the sentence without considering its content, using Colourless green ideas sleep furiously as an example of a nonsensical but perfectly fluent sentence. ", "mid_sen": "The judges were then presented with the 50 sentences in random order, and asked to score the sentences according to their own scale, as in magnitude estimation (Bard et al., 1996) ; these scores were then normalised in the range [0,1].", "after_sen": "Some judges noted that the task was difficult because of its subjectivity. "}
{"citeStart": 98, "citeEnd": 116, "citeStartToken": 98, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "The parser was trained on the first 30,800 sentences from the Lancaster treebank. The test set included 1,473 new sentences, whose lengths range from 3 to 30 words, with a mean length of 13.7 words. These sentences are the same test sentences used in the experiments reported for IBM's parser in (Black et al., 1993) . In (Black et al., 1993 ), IBM's parser was evaluated using the 0-crossingbrackets measure, which represents the percentage of sentences for which none of the constituents in the parser's parse violates the constituent boundaries of any constituent in the correct parse. After over ten years of grammar development, the IBM parser achieved a 0-crossing-brackets score of 69%. On this same test set, SPATTER scored 76%.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The test set included 1,473 new sentences, whose lengths range from 3 to 30 words, with a mean length of 13.7 words. ", "mid_sen": "These sentences are the same test sentences used in the experiments reported for IBM's parser in (Black et al., 1993) . ", "after_sen": "In (Black et al., 1993 ), IBM's parser was evaluated using the 0-crossingbrackets measure, which represents the percentage of sentences for which none of the constituents in the parser's parse violates the constituent boundaries of any constituent in the correct parse. "}
{"citeStart": 133, "citeEnd": 143, "citeStartToken": 133, "citeEndToken": 143, "sectionName": "UNKNOWN SECTION NAME", "string": "All the work mentioned so far uses statistical models of various kinds. As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel, Carberry, and Vijay-Shanker (1998) is transformation-based learning (Brill 1993) . Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999) , from which further techniques could be borrowed.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. ", "mid_sen": "A nonprobabilistic approach for DA labeling proposed by Samuel, Carberry, and Vijay-Shanker (1998) is transformation-based learning (Brill 1993) . ", "after_sen": "Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999) , from which further techniques could be borrowed."}
{"citeStart": 24, "citeEnd": 51, "citeStartToken": 24, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "Another common approach is term translation, e.g., via a bilingual lexicon. (Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996) . While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. Other studies on the value of disambiguation for cross-lingual IR include Hiernstra and de Jong, 1999; Hull, 1997 . Sanderson, 1994 studied the issue of disarnbiguation for mono-lingual IR.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Another common approach is term translation, e.g., via a bilingual lexicon. ", "mid_sen": "(Davis and Ogden, 1997; Ballesteros and Croft, 1997; Hull and Grefenstette, 1996) . ", "after_sen": "While word sense disambiguation has been a central topic in previous studies for cross-lingual IR, our study suggests that using multiple weighted translations and compensating for the incompleteness of the lexicon may be more valuable. "}
{"citeStart": 152, "citeEnd": 173, "citeStartToken": 152, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "mid_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . ", "after_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) ."}
{"citeStart": 73, "citeEnd": 99, "citeStartToken": 73, "citeEndToken": 99, "sectionName": "UNKNOWN SECTION NAME", "string": "Text simplification has so far been approached with two different aims. One is to offer simplified versions of original text to human readers, such as foreign language learners (Petersen and Ostendorf, 2007; Medero and Ostendorf, 2011) ; aphasic people (Devlin and Unthank, 2006) ; low literacy individuals (Specia, 2010) and others. On the other hand, simplified text is seen as input for further natural language processing to enhance its proficiency, e.g. in machine translation or information retrieval tools (Klebanov et al., 2004) . The earliest simplification systems employed a rule-based approach and focused on syntactic structure of the text (Chandrasekar et al., 1996) . The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers. Together with syntactic analysis and transformations similar to those of Chandrasekar et al. (1996) , they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992) . Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The PSET project (Carroll et al., 1998) dealt with simplification of news articles in English for aphasic readers. ", "mid_sen": "Together with syntactic analysis and transformations similar to those of Chandrasekar et al. (1996) , they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992) . ", "after_sen": "Therefore, the most frequent of a set of synonyms for every content word of the input text was chosen to appear in its simplified version."}
{"citeStart": 258, "citeEnd": 270, "citeStartToken": 258, "citeEndToken": 270, "sectionName": "UNKNOWN SECTION NAME", "string": "1 Introduction Shallow parsing is studied as an alternative to full-sentence parsers. Rather than producing a complete analysis of sentences, the alternative is to perform only partial analysis of the syntactic structures in a text (Harris, 1957; Abney, 1991; Greffenstette, 1993) . Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large-scale language processing applications including information extraction and text summarization. A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. The observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information -has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A lot of the work on shallow parsing over the past years has concentrated on manual construction of rules. ", "mid_sen": "The observation that shallow syntactic information can be extracted using local information -by examining the pattern itself, its nearby context and the local part-of-speech information -has motivated the use of learning methods to recognize these patterns (Church, 1988; Ramshaw and Marcus, 1995; Argamon et al., 1998; Cardie and Pierce, 1998) .", "after_sen": "This paper presents a general learning approach for identifying syntactic patterns, based on the SNoW learning architecture (Roth, 1998; Roth, 1999) . "}
{"citeStart": 127, "citeEnd": 150, "citeStartToken": 127, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980) , Categorial Grammar (Ades and Steedman, 1982) , PATR-II (Shieber, 1986) , and lexicalized TAG (Schabes et al, 1988) all deal with complementation by including in one form or another a notion of \"subcategorization frame\" that specifies a sequence of complement phrases and constraints on them. Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "which cannot be felicitously uttered except in a context where there is something in the discourse that a restriction could \"apply\" to.", "mid_sen": "Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980) , Categorial Grammar (Ades and Steedman, 1982) , PATR-II (Shieber, 1986) , and lexicalized TAG (Schabes et al, 1988) all deal with complementation by including in one form or another a notion of \"subcategorization frame\" that specifies a sequence of complement phrases and constraints on them. ", "after_sen": "Handling all the possible variations in complement distribution in such formalisms inevitably leads to an explosion in the number of such frames, and a correspondingly more difficult task in porting to a new domain."}
{"citeStart": 96, "citeEnd": 109, "citeStartToken": 96, "citeEndToken": 109, "sectionName": "UNKNOWN SECTION NAME", "string": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. These connections are being examined elsewhere (Zadrozny forthcoming).", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since techniques developed elsewhere may prove useful, at least for comparison, it is worth mentioning at this point that the proposed metarules are distant cousins of \"unique-name assumption\" (Genesereth and Nilsson 1987) , \"domain closure assumption\" (ibid.), \"domain circumscription\" (cf. Etherington and Mercer 1987) , and their kin. ", "mid_sen": "Similarly, the notion of R + M-abduction is spiritually related to the \"abductive inference\" of Reggia (1985) , the \"diagnosis from first principles\" of Reiter (1987) , \"explainability\" of Poole (1988) , and the subset principle of Berwick (1986) . ", "after_sen": "But, obviously, trying to establish precise connections for the metarules or the provability and the R + M-abduction would go much beyond the scope of an argument for the correspondence of paragraphs and models. "}
{"citeStart": 275, "citeEnd": 294, "citeStartToken": 275, "citeEndToken": 294, "sectionName": "UNKNOWN SECTION NAME", "string": "The most relevant prior work is (Wiebe et al. 98) , who dealt with meeting scheduling dialogs (see also (Alexandersson et al. 97) , (Busemann et al. 97)) , where the goal is to schedule a time for the meeting. The temporal references in meeting scheduling are somewhat more constrained than in news, where (e.g., in a historical news piece on toxic dumping) dates and times may be relatively unconstrained. In addition, their model requires the maintenance of a focus stack. They obtained roughly .91 Precision and .80 Recall on one test set, and .87 Precision and .68 Recall on another. However, they adjust the reference time during processing, which is something that we have not yet addressed. More recently, (Setzer and Gaizauskas 2000) have independently developed an annotation scheme which represents both time values and more fine-grained interevent and event-time temporal relations. Although our work is much more limited in scope, and doesn't exploit the internal structure of events, their annotation scheme may be leveraged in evaluating aspects of our work. The MUC-7 task (MUC-7 98) did not require VALs, but did test TIMEX recognition accuracy. Our 98 F-measure on NYT can be compared for just TIMEX with MUC-7 (MUC-7 1998) results on similar news stories, where the best performance was .99 Precision and .88 Recall. (The MUC task required recognizing a wider variety of TIMEXs, including event-dependent ones. However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. ) Finally, there is a large body of work, e.g., (Moens and Steedman 1988) , (Passoneau 1988) , (Webber 1988) , (Hwang 1992) , (Song and Cohen 1991) , that has focused on a computational analysis of tense and aspect. While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "(The MUC task required recognizing a wider variety of TIMEXs, including event-dependent ones. ", "mid_sen": "However, at least 30% of the dates and times in the MUC test were fixed-format ones occurring in document headers, trailers, and copyright notices. ) Finally, there is a large body of work, e.g., (Moens and Steedman 1988) , (Passoneau 1988) , (Webber 1988) , (Hwang 1992) , (Song and Cohen 1991) , that has focused on a computational analysis of tense and aspect. ", "after_sen": "While the work on event chronologies is based on some of the notions developed in that body of work, we hope to further exploit insights from previous work."}
{"citeStart": 181, "citeEnd": 191, "citeStartToken": 181, "citeEndToken": 191, "sectionName": "UNKNOWN SECTION NAME", "string": "The extracted translations may serve as training data for statistical machine translation systems. To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003) . We then added the extracted translation pairs as additional parallel training corpus. This resulted in a 0.57 increase of BLEU score based on the test data in the 2006 NIST MT Evaluation Workshop. Nagata et al. (2001) made the first proposal to mine translations from the web. Their work was concentrated on terminologies, and assumed the English terms were given as input. Wu and Chang (2007) , Kwok et al. (2005) also employed search engines and assumed the English term given as input, but their focus was on name transliteration. It is difficult to build a truly large-scale translation lexicon this way because the English terms themselves may be hard to come by. Cao et al. (2007) , like us, used a 300GB collection of web documents as input. They used supervised learning to build models that deal with phonetic transliterations and semantic translations separately. Our work relies on unsupervised learning and does not make a distinction between translations and transliterations. Furthermore, we are able to extract two orders of magnitude more translations from than (Cao et al., 2007) .", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The extracted translations may serve as training data for statistical machine translation systems. ", "mid_sen": "To evaluate their effectiveness for this purpose, we trained a baseline phrase-based SMT system (Koehn et al, 2003; Brants et al, 2007) with the FBIS Chinese-English parallel text (NIST, 2003) . ", "after_sen": "We then added the extracted translation pairs as additional parallel training corpus. "}
{"citeStart": 10, "citeEnd": 31, "citeStartToken": 10, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Let e m 1 be the input source sentence, andf p 1 be the output target phrase sequence. Our word-to-phrase alignment a i ∈ [1, p], 1 ≤ i ≤ m, maps a source token position i to a target phrase position a i . Next, we introduce our source dependency tree T . Each source token e i is also a node in T . We define T (e i ) to be the subtree of T rooted at e i . We define a local tree to be a head node and its immediate modifiers. With this notation in place, we can define our projected spans. Following Lin and Cherry (2003) , we define a head span to be the projection of a single token e i onto the target phrase sequence:", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "With this notation in place, we can define our projected spans. ", "mid_sen": "Following Lin and Cherry (2003) , we define a head span to be the projection of a single token e i onto the target phrase sequence:", "after_sen": "spanH (e i , T, a m 1 ) = [a i , a i ]"}
{"citeStart": 26, "citeEnd": 54, "citeStartToken": 26, "citeEndToken": 54, "sectionName": "UNKNOWN SECTION NAME", "string": "Example definitions at each scale point selected by the authors were shown for the concept \"hidden markov model\". Figure 1 : Distribution of ratings across the 5 scale points; LSP: lexico-syntactic patterns, DA: deep analysis 2: The passage is not a good enough description of the concept to serve as a definition; for instance, it's too general, unfocused, or a subconcept/superconcept of the target concept is defined instead 1: The passage does not describe the concept at all The judges participating in the rating experiment were PhD students, postdoctoral researchers, or researchers of comparable expertise, active in the areas of computational linguistics/natural language processing/language technology. One of the raters was one of the authors of this paper. The raters were explicitly instructed to think along the lines of \"what they would like to see in a glossary of computational linguistics terms\". Figure 1 shows the distribution of ratings across the five scale points for the two systems. Around 57% of the LSP ratings and 60% of DA ratings fall within the top three scale-points (positive ratings) and 43% and 40%, respectively, within the bottom two scale-points (low ratings). Krippendorff's ordinal α (Hayes and Krippendorff, 2007) was 0.66 (1,000 bootstrapped samples) indicating a modest degree of agreement, at which, however, tentative conclusions can be drawn. Figure 2 : Mode values of ratings per method for the individual domain terms; see Table 4 Figure 2 shows the distribution of mode ratings of the individual domain terms used in the evaluation. Definitions of 6 terms extracted using the LSP method were rated most frequently at 4 or 5 as opposed to the majority of ratings at 3 for most terms in case of the DA method.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Around 57% of the LSP ratings and 60% of DA ratings fall within the top three scale-points (positive ratings) and 43% and 40%, respectively, within the bottom two scale-points (low ratings). ", "mid_sen": "Krippendorff's ordinal α (Hayes and Krippendorff, 2007) was 0.66 (1,000 bootstrapped samples) indicating a modest degree of agreement, at which, however, tentative conclusions can be drawn. ", "after_sen": "Figure 2 : Mode values of ratings per method for the individual domain terms; see Table 4 Figure 2 shows the distribution of mode ratings of the individual domain terms used in the evaluation. "}
{"citeStart": 64, "citeEnd": 82, "citeStartToken": 64, "citeEndToken": 82, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluate the methods described in Section 3 on two Arabic-to-English translation tasks, both comprising the NW and UG. The first evaluation set is the Gen&Topic benchmark (van der Wees et al., 2015b), which consists of manually translated web-crawled news articles and their respective manually translated user comments, both covering five different topics. Since this evaluation set has controlled topic distributions per genre, differences in translation quality between genres can be entirely attributed to actual genre differences. The second evaluation set contains NIST OpenMT Arabic-English test sets, using NIST 2006 for tuning, and NIST 2008 and NIST 2009 combined for testing. These data sets cover the genres NW and UG weblogs but are not controlled for topic distributions. Specifications for both evaluation sets are shown in Table 2 . Note that Gen&Topic contains one reference translation per sentence, while NIST has four sets of reference translations. We perform our experiments using an inhouse phrase-based SMT system similar to Moses . All runs use lexicalized reordering, distinguishing between monotone, swap, and discontinuous reordering, with respect to the previous and next phrase . Other features include linear distortion with limit 5, lexical weighting (Koehn et al., 2003) , and a 5gram target language model trained with Kneser-Ney smoothing (Chen and Goodman, 1999) . The feature weights are tuned using pairwise ranking optimization (PRO) (Hopkins and May, 2011) . For all experiments, tuning is done separately for the two genre-specific development sets. All runs use parallel corpora made available for NIST OpenMT 2012, excluding the UN data. While LDC-distributed data sets contain substantial portions of documents within the NW genre, they only contain small portions of UG documents. To alleviate this imbalance we augment our LDC-distributed training data with a variety of web-crawled manually translated documents, containing user comments that are of a similar nature as the UG documents in the Gen&Topic, set as well as a number of other genres. Table 3 lists the corpus statistics of the training data, split by manual subcorpus labels as used for the subcorpus VSM variant (see Section 3.2). While our manually grouped subcorpora approximate those used by Chen et al. (2013) , exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Table 3 lists the corpus statistics of the training data, split by manual subcorpus labels as used for the subcorpus VSM variant (see Section 3.2). ", "mid_sen": "While our manually grouped subcorpora approximate those used by Chen et al. (2013) , exact agreement was impossible to obtain, illustrating that it is not trivial to manually generate optimal subcorpus labels.", "after_sen": "We tokenize all Arabic data using MADA (Habash and Rambow, 2005) , ATB scheme. "}
{"citeStart": 198, "citeEnd": 214, "citeStartToken": 198, "citeEndToken": 214, "sectionName": "UNKNOWN SECTION NAME", "string": "One of the standard ways of arranging concepts is in a concept hierarchy or taxonomy such as the WordNet noun taxonomy (Fellbaum, 1998) . The fundamental relationship between objects in a taxonomy is called hyponymy, where y is a hyponym of x if every y is also an x. For example, every trout is also a fish, so we say that trout is a hyponym (\"below name\") of fish and conversely, fish is a hypernym (\"above name\") of trout. Other names exist for variants of the hyponymy relationship, such as an IS A relationship, a parent-node / child-node relationship, and a broader term / narrower term relationship. It is also noted that the genus of an object, in traditional lexicographic terms, is often a hypernym of that object (Guthrie et al., 1996) . Throughout this paper we will write y < x for the relationship \"y is a hyponym of x\". In this paper, we use the hyponymy relationship to describe subset relationships, so we regard y < x to be true if the set of y's can reasonably be said to be a subset of the set of x's. 1 Because hyponymy relationships are so central to knowledge engineering, there have been numerous attempts to learn them from text, beginning with those of Hearst (1992) . We review this work in Section 2, where we reproduce similar experiments as a baseline from which to expand. The rest of the paper demonstrates ways in which other mathematical models built from text corpora can be used to improve hyponymy extraction. In Section 3, we show how latent semantic analysis can be used to filter potential relationships according to their \"semantic plausibility\". In Section 4, we show how correctly extracted relationships can be used as \"seed-cases\" to extract several more relationships, thus improving recall; this work shares some similarities with that of Caraballo (1999) . In Section 5 we show that combining the techniques of Section 3 and Section 4 improves both precision and recall. Section 6 demonstrates that 1 Another possible view is that \"hyponymy\" should only refer to core relationships, not contingent ones (so pheasant < bird might be accepted but pheasant < food might not be, because it depends on context and culture). We use the broader \"subset\" definition because contingent relationships are an important part of world-knowledge (and are therefore worth learning), and because in practice we found the distinction difficult to enforce. Another definition is given by Caraballo (1999) : \". . . a word A is said to be a hypernym of a word B if native speakers of English accept the sentence 'B is a (kind of) A.' \" linguistic tools such as lemmatization can be used to reliably put the extracted relationships into a normalized or \"canonical\" form for addition to a semantic resource.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In Section 3, we show how latent semantic analysis can be used to filter potential relationships according to their \"semantic plausibility\". ", "mid_sen": "In Section 4, we show how correctly extracted relationships can be used as \"seed-cases\" to extract several more relationships, thus improving recall; this work shares some similarities with that of Caraballo (1999) . ", "after_sen": "In Section 5 we show that combining the techniques of Section 3 and Section 4 improves both precision and recall. "}
{"citeStart": 198, "citeEnd": 210, "citeStartToken": 198, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "Even moderately long documents typically address sew~ral topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of linear text segmentation is to discover the topic boundaries. ", "mid_sen": "The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "after_sen": "This paper focuses on domain independent methods for segmenting written text. "}
{"citeStart": 131, "citeEnd": 153, "citeStartToken": 131, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "Purely as a method of estimation as well, the superiority of MI)L over MLE is supported by convincing theoretical findings (c.f. ( Barton and Cover, 1991; Yamanishi, 1992) ). For instance, the speed of convergence of the models selected by MDL to the true model is known to be near optiinal. (The models selected by MDL converge to the true model approximately at the rate of 1/s where s is the nmnber of parameters in the true model, whereas for MLE the rate is l/t, where t is the size of the domain, or in our context, the total number of elements of N\" x V.) 'Consistency' is another desirable property of MDL, which is not shared by MLE. That is, the number of parame- (Rissanen, 1989) . Both of these prol>erties of MI)I, ar~ Oml>irically w'ri/ied in our present (;Ollt(?x[,, as will be show,: in t.ho t:(,xl section. In particular, we haw~ compared l,h(' p(u'forn:a.nc0 of employing an M1)L-based simula.ted annealing against that of one 1)ascd on M[,I\", ill hierarchical woM clust.c'ring.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus, in contrast to employing MDL, it will not have the effect of smoothing a.t all.", "mid_sen": "Purely as a method of estimation as well, the superiority of MI)L over MLE is supported by convincing theoretical findings (c.f. ( Barton and Cover, 1991; Yamanishi, 1992) ). ", "after_sen": "For instance, the speed of convergence of the models selected by MDL to the true model is known to be near optiinal. "}
{"citeStart": 212, "citeEnd": 225, "citeStartToken": 212, "citeEndToken": 225, "sectionName": "UNKNOWN SECTION NAME", "string": "Finding these types of cycles in the magic part of the compiled grammar is in general undecidable. It is possible though to 'trim' the magic predicates by applying an abstraction function. As a result of the explicit representation of filtering we do not need to postpone abstraction until run-time, but can trim the magic predicates off-line. One can consider this as bringing abstraction into the logic as the definite clause representation of filtering is weakened such that only a mild form of connectedness results which does not affect completeness (Shieber, 1985) . Consider the following magic rule:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a result of the explicit representation of filtering we do not need to postpone abstraction until run-time, but can trim the magic predicates off-line. ", "mid_sen": "One can consider this as bringing abstraction into the logic as the definite clause representation of filtering is weakened such that only a mild form of connectedness results which does not affect completeness (Shieber, 1985) . ", "after_sen": "Consider the following magic rule:"}
{"citeStart": 107, "citeEnd": 133, "citeStartToken": 107, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "proposed in (Tsarfaty, 2006) . In our third model GT ppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007) . In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an (2007) . Finally, model GT v = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima'an, 2007; Cohen and Smith, 2007) . For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an (2007) . ", "mid_sen": "Finally, model GT v = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima'an, 2007; Cohen and Smith, 2007) . ", "after_sen": "For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein."}
{"citeStart": 53, "citeEnd": 67, "citeStartToken": 53, "citeEndToken": 67, "sectionName": "UNKNOWN SECTION NAME", "string": "The analysis accounts for the range of data given in Kehler (1993b) , although one point of departure exists between that account and the current one with respect to clauses conjoined with but. In the previous account these cases are all classified as non-parallel, resulting in the prediction that they only require semantic source representations. In our analysis, we expect cases of pure contrast to pattern with the parallel class since these are Common Topic constructions; this is opposed to the violated expectation use of but which indicates a Coherent Situation relation. The current account makes the correct predictions; examples (20) and (21), where but has the contrast meaning, appear to be markedly less acceptable than examples (22) and 23 To summarize thus far, the data presented in the earlier account as well as examples that conflict with that analysis are all predicted by the account given here. As a final note, we consider the interaction between VP-ellipsis and gapping. The following pair of examples are adapted from those of Sag (1976, pg. 291): lZThese examples have been adapted from several in Kehler (1993b) . 24:Iohn supports Clinton, and Mary $ Bush, although she doesn't know why she does.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In these cases the anaphoric expression is resolved on purely semantic grounds; therefore VPellipsis is only constrained to having a suitable semantic antecedent.", "mid_sen": "The analysis accounts for the range of data given in Kehler (1993b) , although one point of departure exists between that account and the current one with respect to clauses conjoined with but. ", "after_sen": "In the previous account these cases are all classified as non-parallel, resulting in the prediction that they only require semantic source representations. "}
{"citeStart": 186, "citeEnd": 200, "citeStartToken": 186, "citeEndToken": 200, "sectionName": "UNKNOWN SECTION NAME", "string": "To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. ", "mid_sen": "This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996) . ", "after_sen": "Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000) , manually split the tagset into a finer-grained one (Klein and Manning, 2003a) , or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006) . "}
{"citeStart": 73, "citeEnd": 93, "citeStartToken": 73, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "We would like to improve phrase translation accuracy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments. One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006) . In this work, we present a generic discriminative phrase pair extraction framework that can integrate multiple features aiming to identify correct phrase translation candidates. A significant deviation from most other approaches is that the framework is parameterized and can be optimized jointly with the decoder to maximize translation performance on a development set. Within the general framework, the main work is on investigating useful metrics. We employ features based on word alignment models and alignment matrix. We also propose information metrics that are derived from both bilingual and monolingual perspectives. All these features are data-driven and independent of languages. The proposed phrase extraction framework is general to apply linguistic features such as semantic, POS tags and syntactic dependency.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We would like to improve phrase translation accuracy and at the same time extract as many as possible valid phrase pairs that are missed due to incorrect word alignments. ", "mid_sen": "One approach is to leverage underlying word alignment quality such as in Ayan and Dorr (2006) . ", "after_sen": "In this work, we present a generic discriminative phrase pair extraction framework that can integrate multiple features aiming to identify correct phrase translation candidates. "}
{"citeStart": 118, "citeEnd": 140, "citeStartToken": 118, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Various learning algorithms have been used for relation classification. Common choices include variations of SVM (Girju et al., 2004; Nastase et al., 2006) , decision trees and memory-based learners. Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007) . In this paper we did not focus on a single ML algorithm, letting algorithm selection be automatically based on cross-validation results on the training set, as in (Hendrickx et al., 2007) but using more algorithms and allowing a more flexible parameter choice.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Common choices include variations of SVM (Girju et al., 2004; Nastase et al., 2006) , decision trees and memory-based learners. ", "mid_sen": "Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007) . ", "after_sen": "In this paper we did not focus on a single ML algorithm, letting algorithm selection be automatically based on cross-validation results on the training set, as in (Hendrickx et al., 2007) but using more algorithms and allowing a more flexible parameter choice."}
{"citeStart": 33, "citeEnd": 46, "citeStartToken": 33, "citeEndToken": 46, "sectionName": "UNKNOWN SECTION NAME", "string": "In the experiments, the language model is a Chinese 5-gram language model trained with the Chinese part of the LDC parallel corpus and the Xinhua part of the Chinese Gigaword corpus with about 27 million words. We used an SMT system similar to Chiang (2005) , in which FBIS corpus is used as the bilingual training data. The training corpus for Mo-ME model consists of the Chinese Peen Treebank and the Chinese part of the LDC parallel corpus with about 2 million sentences. The Bi-ME model is trained with FBIS corpus, whose size is smaller than that used in Mo-ME model training.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In the experiments, the language model is a Chinese 5-gram language model trained with the Chinese part of the LDC parallel corpus and the Xinhua part of the Chinese Gigaword corpus with about 27 million words. ", "mid_sen": "We used an SMT system similar to Chiang (2005) , in which FBIS corpus is used as the bilingual training data. ", "after_sen": "The training corpus for Mo-ME model consists of the Chinese Peen Treebank and the Chinese part of the LDC parallel corpus with about 2 million sentences. "}
{"citeStart": 200, "citeEnd": 219, "citeStartToken": 200, "citeEndToken": 219, "sectionName": "UNKNOWN SECTION NAME", "string": "We can represent this symbolically as L.1.Professor where we describe the whole space of useful features of this form as: {direction = (L)eft, (C)urrent, (R)ight}.{distance = 1, 2, 3, ...}.{value = Professor, book, ...}. We can conceptualize this structure as a tree, where each slot in the symbolic name of a feature is a branch and each period between slots represents another level, going from root to leaf as read left to right. Thus a subsection of the entire feature tree for the token Caldwell could be drawn as in Figure 1 (zoomed in on the section of the tree where the L.1.Professor feature resides). Representing feature spaces with this kind of tree, besides often coinciding with the explicit language used by common natural language toolkits (Cohen, 2004) , has the added benefit of allowing a model to easily back-off, or smooth, to decreasing levels of specificity. For example, the leaf level of the feature tree for our sample Sentence 1 tells us that the word Professor is important, with respect to labeling person names, when located one slot to the left of the current word being classified. This may be useful in the context of an academic corpus, but might be less useful in a medical domain where the word Professor occurs less often. Instead, we might want to learn the related feature L.1.Dr. In fact, it might be useful to generalize across multiple domains the fact that the word immediately preceding the current word is often important with respect LeftToken.* LeftToken.IsWord.* LeftToken.IsWord.IsTitle.* LeftToken.IsWord.IsTitle.equals.* LeftToken.IsWord.IsTitle.equals.mr Table 1 : A few examples of the feature hierarchy to the named entity status of the current word. This is easily accomplished by backing up one level from a leaf in the tree structure to its parent, to represent a class of features such as L.1.*. It has been shown empirically that, while the significance of particular features might vary between domains and tasks, certain generalized classes of features retain their importance across domains (Minkov et al., 2005) . By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks. Some examples illustrating this idea are shown in table 1.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is easily accomplished by backing up one level from a leaf in the tree structure to its parent, to represent a class of features such as L.1.*. ", "mid_sen": "It has been shown empirically that, while the significance of particular features might vary between domains and tasks, certain generalized classes of features retain their importance across domains (Minkov et al., 2005) . ", "after_sen": "By backing-off in this way, we can use the feature hierarchy as a prior for transferring beliefs about the significance of entire classes of features across domains and tasks. "}
{"citeStart": 49, "citeEnd": 68, "citeStartToken": 49, "citeEndToken": 68, "sectionName": "UNKNOWN SECTION NAME", "string": "Although the type of reference that we wish to model does not rely oll these assumptions, we can nevertheless draw from these theories. Thus, we base our model on the work of Chtrk and Wilkes-Gibbs (1986) , and Heeman and Hirst (1992) who both modeled (the first psychologically, and the second computationally) how people collaborate on reference to objects for which they have mutual knowledge. We will briefly discuss these models, before we describe our own.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Although the type of reference that we wish to model does not rely oll these assumptions, we can nevertheless draw from these theories. ", "mid_sen": "Thus, we base our model on the work of Chtrk and Wilkes-Gibbs (1986) , and Heeman and Hirst (1992) who both modeled (the first psychologically, and the second computationally) how people collaborate on reference to objects for which they have mutual knowledge. ", "after_sen": "We will briefly discuss these models, before we describe our own."}
{"citeStart": 15, "citeEnd": 26, "citeStartToken": 15, "citeEndToken": 26, "sectionName": "UNKNOWN SECTION NAME", "string": "We use Lucene (Goetz, 2002) for indexing and retrieving sentences and images. Lucene is an open source indexing and information retrieval library that has been shown to scale up efficiently and handle large numbers of queries. We index using fields derived from word-lemmas, grammatical relations and named entities. At the same time, these complex representations are hidden from the user, who, as a first step, performs a simple keyword search; for example \"express Vnd\". This returns all sentences that contain the words \"express\" and \"Vnd\" (search is on lemmatised words, so morphological variants of \"express\" will be retrieved). Different colours represent different types of biological entities and processes (green for a gene), and blue shows the entered search terms in the result sentences. An example sentence retrieved for the above query follows:It is possible that like ac , sc and l'sc , vnd is expressed initially in cell clusters and then restricted to single cells .Next, the user can select specific words in the returned sentences to indirectly specify a relation. Clicking on a word will select it, indicated by underlining of the word. In the example above, the words \"vnd\" and \"expressed\" have been selected by the user. This creates a new query that returns sentences where \"vnd\" is the subject of \"express\" and the clause is in passive voice. This retrieval is based on a sophisticated grammatical analysis of the text, and can retrieve sentences where the words in the relation are far apart. An example of a sentence retrieved for the refined query is shown below:First , vnd might be spatially regulated in a manner similar to ac and sc and selectively expressed in these clusters .Camtology offers two other functionalities. The user can browse the MeSH (Medical Subject Headings) ontology and retrieve papers relevant to a MeSH term. Also, for both search and MeSH browsing, retrieved papers are plotted on a world map; this is done by converting the affiliations of the authors into geospatial coordinates. The user can then directly access papers from a particular site. ", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We use Lucene (Goetz, 2002) for indexing and retrieving sentences and images. ", "after_sen": "Lucene is an open source indexing and information retrieval library that has been shown to scale up efficiently and handle large numbers of queries. "}
{"citeStart": 136, "citeEnd": 155, "citeStartToken": 136, "citeEndToken": 155, "sectionName": "UNKNOWN SECTION NAME", "string": "To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as 'is the third bit of the POS tag encoding equal to one?' Figure 1 shows a POS classification tree. The binary encoding for a POS tag is determined by the sequence of top and bottom edges that leads from the root node to the node for the POS tag.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "After the tree is grown, a heldout dataset is used to smooth the probabilities of each node with its parent (Bahl et al., 1989) .", "mid_sen": "To allow the decision tree to ask about the words and POS tags in the context, we cluster the words and POS tags using the algorithm of Brown et al. (1992) into a binary classification tree. ", "after_sen": "This gives an implicit binary encoding for each word and POS tag, thus allowing the decision tree to ask about the words and POS tags using simple binary questions, such as 'is the third bit of the POS tag encoding equal to one?' Figure 1 shows a POS classification tree. "}
{"citeStart": 97, "citeEnd": 116, "citeStartToken": 97, "citeEndToken": 116, "sectionName": "UNKNOWN SECTION NAME", "string": "Our disambiguation method is based on the shnilarity of context vectors, which was originated by Wilks el al. (1990) . In this method, a context vector is the sum of its constituent word vectors (except the target word itself). That is, tile context vector for context,", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For example, V(!ronis and Ide (1990) used reference networks as neural networks, llearst (1991) used (shallow) syntactic similarity between contexts, Cowie el al. (1992) used simulated annealing for quick parallel disambignation, and Yarowsky (1992) used co-occurrence statistics between words and thesaurus categories.", "mid_sen": "Our disambiguation method is based on the shnilarity of context vectors, which was originated by Wilks el al. (1990) . ", "after_sen": "In this method, a context vector is the sum of its constituent word vectors (except the target word itself). "}
{"citeStart": 49, "citeEnd": 66, "citeStartToken": 49, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "Based on the pronunciation error data of learners of English as a second language as reported in (Swan and Smith, 2002) , we propose the use of what we will term pseudofeatures. The pseudo features in this study are same as in Tao et al. (2006) . Swan & Smith (2002) 's study covers 25 languages including Asian languages such as Thai, Korean, Chinese and Japanese, European languages such as German, Italian, French and Polish, and Middle East languages such as Arabic and Farsi. The substitution/insertion/deletion errors of phonemes were collected from this data. The following types of errors frequently occur in second language learners' speech production.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Based on the pronunciation error data of learners of English as a second language as reported in (Swan and Smith, 2002) , we propose the use of what we will term pseudofeatures. ", "mid_sen": "The pseudo features in this study are same as in Tao et al. (2006) . ", "after_sen": "Swan & Smith (2002) 's study covers 25 languages including Asian languages such as Thai, Korean, Chinese and Japanese, European languages such as German, Italian, French and Polish, and Middle East languages such as Arabic and Farsi. "}
{"citeStart": 550, "citeEnd": 576, "citeStartToken": 550, "citeEndToken": 576, "sectionName": "UNKNOWN SECTION NAME", "string": "The classification algorithm described above was originally developed in response to Sibun and Spitz's work. There is another approach to language identification, which has a certain Incorrect  1560  6  128  7  37  9  18  2  5  1  5  1  2  0  2  0  1  0  3  0  1  0  2  0  2  0  1  0  7  0   All  1566  135  46  20  6  6  2  2  1  3  1  2 1 7 Incorrect  173  0  22  0  31  1  45  3  34  4  65  8  73  4  83  3  84  3  89  2  84  0  128  1  131  0  175  1  253  0   All  173  22  32  48  38  73  77  86  87  91  84  129  131  176  253   Table 6 : Categories remaining at end of input (genre identification) amount in common with ours, described in a patent by Martino and Paulsen (1996) . Their approach is to build tables of the most frequent words in each language, and assign them a normalised score, based on the frequency of occurrence of the word in one language compared to the total across all the languages. Only the most frequent words for each language are used. The algorithm works by accumulating scores, until a preset number of words has been read or a minimum score has been reached. They also apply the technique to genre identification. Since there is a clear similarity, it is perhaps worth highlighting the differences. In terms of the algorithm, the most important difference is that no confidence measures are included. The complexities of splitting the data into different frequency bands for calculating probabilities are thus avoided, but no test analogous to overlapping confidence intervals can be applied. Martino and Paulsen say they obtain a high degree of confidence in the decision after about words, without saying what the actual success rate is; we can compare this with around 10 words (or tokens) for convergence here.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The classification algorithm described above was originally developed in response to Sibun and Spitz's work. ", "mid_sen": "There is another approach to language identification, which has a certain Incorrect  1560  6  128  7  37  9  18  2  5  1  5  1  2  0  2  0  1  0  3  0  1  0  2  0  2  0  1  0  7  0   All  1566  135  46  20  6  6  2  2  1  3  1  2 1 7 Incorrect  173  0  22  0  31  1  45  3  34  4  65  8  73  4  83  3  84  3  89  2  84  0  128  1  131  0  175  1  253  0   All  173  22  32  48  38  73  77  86  87  91  84  129  131  176  253   Table 6 : Categories remaining at end of input (genre identification) amount in common with ours, described in a patent by Martino and Paulsen (1996) . ", "after_sen": "Their approach is to build tables of the most frequent words in each language, and assign them a normalised score, based on the frequency of occurrence of the word in one language compared to the total across all the languages. "}
{"citeStart": 41, "citeEnd": 66, "citeStartToken": 41, "citeEndToken": 66, "sectionName": "UNKNOWN SECTION NAME", "string": "We have shown that a tagger based on Markov models yields state-of-the-art results, despite contrary claims found in the literature. For example, the Markov model tagger used in the comparison of (van Halteren et al., 1998) yielded worse results than all other taggers. In our opinion, a reason for the wrong claim is that the basic algorithms leave several decisions to the implementor. The rather large amount of freedom was not handled in detail in previous publications: handling of start-and end-of-sequence, the exact smoothing technique, how to determine the weights for context probabilities, details on handling unknown words, and how to determine the weights for unknown words. Note that the decisions we made yield good results for both the German and the English Corpus. They do so for several other corpora as well. The architecture remains applicable to a large variety of languages. According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999) , and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both.", "label": "Support", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The architecture remains applicable to a large variety of languages. ", "mid_sen": "According to current tagger comparisons (van Halteren et al., 1998; Zavrel and Daelemans, 1999) , and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996) , the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. ", "after_sen": "It is a very interesting future research topic to determine the advantages of either of these approaches, to find the reason for their high accuracies, and to find a good combination of both."}
{"citeStart": 51, "citeEnd": 75, "citeStartToken": 51, "citeEndToken": 75, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach aims at inferring important key phrases from title phrases through a semantic network. Here we take a method of synonym expansion as the baseline, called WordNet expansion here. The WordNet 2 expansion approach selects all the synonyms of the title phrases in the document as key phrases. Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only. One approach combines the title information into the Grineva's community-based method (Grineva et al., 2009) , called titlecommunity approach. The title-community approach uses the Girvan-Newman algorithm to cluster phrases into communities and selects those phrases in the communities containing the title phrases as key phrases. We do not limit the number of key phrases selected. The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005) , called title-sensitive PageRank here. The title-sensitive PageRank approach makes use of title phrases to re-weight the transitions between vertices and picks up 10% top-ranked phrases as key phrases. Table 1 summarizes the performance on the test data. The results presented in the table show that our approach exhibits the best performance among all the four approaches. It follows that the key phrases inferred from a document semantic network are not limited to the synonyms of title phrases. As the title-sensitive PageRank ap-proach totally ignores the n-ary relations, its performance is the worst. Based on binary relations, the title-community approach clusters phrases into communities and each community can be considered as an n-ary relation. However, this approach lacks of an importance propagation process. Consequently, it has the highest recall value but the lowest precision. In contrast, our approach achieves the highest precision, due to its ability to infer many correct key phrases using importance propagation among n-ary relations.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We do not limit the number of key phrases selected. ", "mid_sen": "The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005) , called title-sensitive PageRank here. ", "after_sen": "The title-sensitive PageRank approach makes use of title phrases to re-weight the transitions between vertices and picks up 10% top-ranked phrases as key phrases. "}
{"citeStart": 108, "citeEnd": 127, "citeStartToken": 108, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "Specialized glossaries serve two functions: Firstly, they are linguistic resources summarizing the terminological basis of a specialized domain. Secondly, they are knowledge resources, in that they provide definitions of concepts which the terms denote. Glossaries find obvious use as sources of reference. A survey on the use of lexicographical aids in specialized translation showed that glossaries are among the top five resources used (Durán-Muñoz, 2010) . Glossaries have also been shown to facilitate reception of texts and acquisition of knowledge during study (Weiten et al., 1999) , while selfexplanation of reasoning by referring to definitions has been shown to promote understanding (Aleven et al., 1999) . From a machine-processing point of view, glossaries may be used as input for domain ontology induction; see, e.g. (Bozzato et al., 2008) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A survey on the use of lexicographical aids in specialized translation showed that glossaries are among the top five resources used (Durán-Muñoz, 2010) . ", "mid_sen": "Glossaries have also been shown to facilitate reception of texts and acquisition of knowledge during study (Weiten et al., 1999) , while selfexplanation of reasoning by referring to definitions has been shown to promote understanding (Aleven et al., 1999) . ", "after_sen": "From a machine-processing point of view, glossaries may be used as input for domain ontology induction; see, e.g. (Bozzato et al., 2008) ."}
{"citeStart": 0, "citeEnd": 22, "citeStartToken": 0, "citeEndToken": 22, "sectionName": "UNKNOWN SECTION NAME", "string": "Given a stem such as brother, Toutanova et. al's system might generate the \"stem and inflection\" corresponding to and his brother. Viewing and and his as inflection is problematic since a mapping from the English phrase and his brother to the Arabic stem for brother is required. The situation is worse if there are English words (e.g., adjectives) separating his and brother. This required mapping is a significant problem for generalization. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a \"split in preprocessing and resynthesize in postprocessing\" approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mariño (2008) , which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the agreement phenomena that we model. Our CRF framework allows us to use more complex context features.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There has been other work on solving inflection. ", "mid_sen": "Koehn and Hoang (2007) introduced factored SMT. ", "after_sen": "We use more complex context features. "}
{"citeStart": 88, "citeEnd": 106, "citeStartToken": 88, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "For domain adaptation, either a large-scale unlabeled target domain data or a small size of labeled target domain data is required to adapt a system built on source domain data to the target domain. In this word segmentation competition, unfortunately, only a small size of unlabeled target domain data is available. Thus we focus on handling out-of-vocabulary (OOV) words. For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004) . In more detail, we adopted and extended subword-based method. Subword list is augmented with newword list recognized by accessor variety method.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Thus we focus on handling out-of-vocabulary (OOV) words. ", "mid_sen": "For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004) . ", "after_sen": "In more detail, we adopted and extended subword-based method. "}
{"citeStart": 67, "citeEnd": 84, "citeStartToken": 67, "citeEndToken": 84, "sectionName": "UNKNOWN SECTION NAME", "string": "We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework and use n-grams of length 1 to 3 as well as dependency triplets as features. The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method . This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011) . The first set of experiments focuses on simultaneous detection of sentiment and context sentences. For this purpose, we use the four-class annotated corpus described earlier. While the original annotations were performed for a window of length 4, we also experiment with asymmetrical windows of l sentences preceding the citation and r sentences succeeding it. The detailed results are given in Table 2 . Table 2 : Results for joint context and sentiment detection.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The dependency triplets are constructed by merging the relation, governor and dependent in a single string, for instance, the relation nsubj(failed, method) is represented as nsubj failed method . ", "mid_sen": "This setup has been shown to produce good results earlier as well (Pang et al., 2002; Athar, 2011) . ", "after_sen": "The first set of experiments focuses on simultaneous detection of sentiment and context sentences. "}
{"citeStart": 105, "citeEnd": 127, "citeStartToken": 105, "citeEndToken": 127, "sectionName": "UNKNOWN SECTION NAME", "string": "Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008) , sentence compression (Knight and Marcu, 2002) , and reformulation (Barzilay et al., 2001; Saggion, 2011) ; while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. News-Blaster 3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring language generation to produce a grammatical and coherent summary, and better suites the scenario of tweet summarization. Note that our method considers each tweet as the unit for summarization, which often cannot provide reliable information to compute the salience. This is one main difference between our system and the existing studies.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Abstraction and selection are two strategies employed for multi-document summarization. ", "mid_sen": "The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008) , sentence compression (Knight and Marcu, 2002) , and reformulation (Barzilay et al., 2001; Saggion, 2011) ; while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. ", "after_sen": "News-Blaster 3 and our method are examples of abstraction and selection based methods, respectively. "}
{"citeStart": 178, "citeEnd": 202, "citeStartToken": 178, "citeEndToken": 202, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary compari- son (Dagan et al., 1999) . That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools. 587,833 (80%) of the pairs served as a training set from which to calculate base probabilities. From the other 20%, we prepared test sets as follows: after discarding pairs occurring in the training data (after all, the point of similarity-based estimation is to deal with unseen pairs), we split the remaining pairs into five partitions, and replaced each nounverb pair (n, vl) with a noun-verb-verb triple (n, vl, v2) such that P(v2) ~ P(vl). The task for the language model under evaluation was to reconstruct which of (n, vl) and (n, v2) was the original cooccurrence. Note that by construction, (n, Vl) was always the correct answer, and furthermore, methods relying solely on unigram frequencies would perform no better than chance. Test-set performance was measured by the error rate, defined as T(# of incorrect choices + (# of ties)/2), where T is the number of test triple tokens in the set, and a tie results when both alternatives are deemed equally likely by the language model in question.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "We evaluated the similarity functions introduced in the previous section on a binary decision task, using the same experimental framework as in our previous preliminary compari- son (Dagan et al., 1999) . ", "after_sen": "That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools. 587,833 (80%) of the pairs served as a training set from which to calculate base probabilities. "}
{"citeStart": 41, "citeEnd": 59, "citeStartToken": 41, "citeEndToken": 59, "sectionName": "UNKNOWN SECTION NAME", "string": "To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus. Two types of training scheme are explored in this study, both unsupervised. The first employs a pattern that follows Pustejovsky (1993) in counting the occurrences of subcomponents. A training instance is any sequence of four words WlW2W3W 4 where wl, w4 ~ .h/and w2, w3 E A/'. Let county(n1, n2) be the number of times a sequence wlnln2w4 occurs in the training corpus with wl, w4 ~ At'.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Two types of training scheme are explored in this study, both unsupervised. ", "mid_sen": "The first employs a pattern that follows Pustejovsky (1993) in counting the occurrences of subcomponents. ", "after_sen": "A training instance is any sequence of four words WlW2W3W 4 where wl, w4 ~ .h/and w2, w3 E A/'. Let county(n1, n2) be the number of times a sequence wlnln2w4 occurs in the training corpus with wl, w4 ~ At'."}
{"citeStart": 16, "citeEnd": 34, "citeStartToken": 16, "citeEndToken": 34, "sectionName": "UNKNOWN SECTION NAME", "string": "So far, we always computed translations to single source words. However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm. As suggested in Rapp & Zock (2010) this can be done by looking up the ranks of each of the four given words (i.e. the words occurring in a particular word equation) within the association vector of a translation candidate, and by multiplying these ranks. So for each candidate we obtain a product of ranks. We then assume that the candidate with the smallest product will be the best translation. 3 Let us illustrate this by an example: If the given words are the variants of the word nervous in English, French, German, and Spanish, i.e. nervous, nerveux, nervös, and nervioso, and if we want to find out their translation into Italian, we would look at the association vectors of each word in our Italian target vocabulary. The association strengths in these vectors need to be inversely sorted, and in each of them we will look up the positions of our four given words. Then for each vector we compute the product of the four ranks, and finally sort the Italian vocabulary according to these products. We would then expect that the correct Italian translation, namely nervoso, ends up in the first position, i.e. has the smallest value for its product of ranks.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, if we assume, for example, that we already have word equations for four languages, and all we want is to compute the translations into a fifth language, then we can simply extend our approach to what we call the product-of-ranks algorithm. ", "mid_sen": "As suggested in Rapp & Zock (2010) this can be done by looking up the ranks of each of the four given words (i.e. the words occurring in a particular word equation) within the association vector of a translation candidate, and by multiplying these ranks. ", "after_sen": "So for each candidate we obtain a product of ranks. "}
{"citeStart": 186, "citeEnd": 211, "citeStartToken": 186, "citeEndToken": 211, "sectionName": "UNKNOWN SECTION NAME", "string": "The datasets and software to replicate our experiments are available from http://web.science.mq.edu.au/ bborschi/ tion of its effectiveness in adult speech processing (Cutler et al., 1986) . Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012) . Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007) , however, have until recently completely ignored stress. The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model , demonstrating that this leads to an improvement in segmentation performance. In this paper, we extend their work and show how to integrate stress cues into the flexible Adaptor Grammar framework (Johnson et al., 2007) . This allows us to both start from a stronger baseline model and to investigate how the role of stress cues interacts with other aspects of the model. In particular, we find that phonotactic cues to word-boundaries interact with stress cues, indicating synergistic effects for small inputs and partial redundancy for larger inputs. Overall, we find that stress cues add roughly 6% token f-score to a model that does not account for phonotactics and 4% to a model that already incorporates phonotactics. Relatedly and in line with the finding that stress cues are used by infants before phonotactic cues (Jusczyk et al., 1999a) , we observe that phonotactic cues require more input than stress cues to be used efficiently. A closer look at the knowledge acquired by our models shows that the Unique Stress Constraint of Yang (2004) can be acquired jointly with segmenting the input instead of having to be pre-specified; and that our models correctly identify the predominant stress pattern of the input but underestimate the frequency of iambic words, which have been found to be missegmented by infant learners. The outline of the paper is as follows. In Section 2 we review prior work. In Section 3 we introduce our own models. In Section 4 we explain our experimental evaluation and its results. Section 5 discusses our findings, and Section 6 concludes and provides some suggestions for future research.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The datasets and software to replicate our experiments are available from http://web.science.mq.edu.au/ bborschi/ tion of its effectiveness in adult speech processing (Cutler et al., 1986) . ", "mid_sen": "Several studies have investigated the role of stress in word segmentation using computational models, using both neural network and \"algebraic\" (as opposed to \"statistical\") approaches (Christiansen et al., 1998; Yang, 2004; Lignos and Yang, 2010; Lignos, 2011; Lignos, 2012) . Bayesian models of word segmentation (Brent, 1999; Goldwater, 2007) , however, have until recently completely ignored stress. ", "after_sen": "The sole exception in this respect is Doyle and Levy (2013) who added stress cues to the Bigram model , demonstrating that this leads to an improvement in segmentation performance. "}
{"citeStart": 104, "citeEnd": 123, "citeStartToken": 104, "citeEndToken": 123, "sectionName": "UNKNOWN SECTION NAME", "string": "Another line of research for keyword extraction has adopted graph-based methods similar to Google's PageRank algorithm (Brin and Page, 1998) . In particular, (Wan et al., 2007) attempted to use a reinforcement approach to do keyword extraction and summarization simultaneously, on the assumption that important sentences usually contain keywords and keywords are usually seen in important sentences. We also find that this assumption also holds using statistics obtained from the meeting corpus used in this study. Graph-based methods have not been used in a genre like the meeting domain; therefore, it remains to be seen whether these approaches can be applied to meetings. Not many studies have been performed on speech transcripts for keyword extraction. The most relevant work to our study is (Plas et al., 2004) , where the task is keyword extraction in the multiparty meeting corpus. They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002) . (Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. In (Rogina, 2002) , keywords were extracted from lecture slides, and then used as queries to retrieve relevant web documents, resulting in an improved language model and better speech recognition performance of lectures. There are many differences between written text and speech -meetings in particular. Thus our goal in this paper is to investi-gate whether we can successfully apply some existing techniques, as well as propose new approaches to extract keywords for the meeting domain. The aim of this study is to set up some starting points for research in this area.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "They showed that leveraging semantic resources can yield significant performance improvement compared to the approach based on the relative frequency ratio (similar to IDF). ", "mid_sen": "There is also some work using keywords for other speech processing tasks, e.g., (Munteanu et al., 2007; Bulyko et al., 2007; Wu et al., 2007; Desilets et al., 2002; Rogina, 2002) . ", "after_sen": "(Wu et al., 2007) showed that keyword extraction combined with semantic verification can be used to improve speech retrieval performance on broadcast news data. "}
{"citeStart": 54, "citeEnd": 69, "citeStartToken": 54, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "However, an alternative possibility is to directly link the partial syntax trees which can be %rmed fol: nOl>COnstituents with flmctional semantic representations. For example, a fragment missing a noun phrase such as John likes can be associated with a seman-I, ies which is a function from entities to truth values. Ilence, tam partial syntax tree given in Fig. 14 The problem of there being an arbitrary mmg)er of different partial trees for a particular fragment is refleeted in most current approaches to incrementM interpretation being either incomplete, or not flflly word by word. For example, incomplete parsers have been proposed by Stabler (11991) and Moortga.t (1988) . Stabler's system is a simple top-down parser which does not deal with left recursive grammars. Moortgat's M-Systeln is based on the Lambek (~ah:ulus: the problem of an infinite lmmber of possible tree ka.gments is replaced by a corresponding problem of initiM fl:agments having an infinite number of possible types. A colnplete incremental parser, which is not fully word by word, was proposed by Pullnan (1986) . This is ba.~ sed on arc-eager left-corner parsing (see e.g. l{esnik", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Ilence, tam partial syntax tree given in Fig. 14 The problem of there being an arbitrary mmg)er of different partial trees for a particular fragment is refleeted in most current approaches to incrementM interpretation being either incomplete, or not flflly word by word. ", "mid_sen": "For example, incomplete parsers have been proposed by Stabler (11991) and Moortga.t (1988) . ", "after_sen": "Stabler's system is a simple top-down parser which does not deal with left recursive grammars. "}
{"citeStart": 171, "citeEnd": 182, "citeStartToken": 171, "citeEndToken": 182, "sectionName": "UNKNOWN SECTION NAME", "string": "Some researchers, e.g., [Mann, 1988; KowtkoetaL, 1991] , assume a library of discourse level actions, sometimes called dialogue games, which encode common communicative interactions. To be co-operative, an agent must always be participating in one of these games. So if a question is asked, only a fixed number of activities, namely those introduced by a question, are cooperative responses. Games provide a better explanation of coherence, but still require the agent's to recognize each other's intentions to perform the dialogue game. As a result, this work can be viewed as a special case of the intentional view. An interesting model is described by [Airenti et al., 1993] , which separates out the conversational games from the task-related games in a way similar way to [Litman and Allen, 1987] . Because of this separation, they do not have to assume co-operation on the tasks each agent is performing, but still require recognition of intention and cooperation at the conversational level. It is left unexplained what goals motivate conversational co-operation.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As a result, this work can be viewed as a special case of the intentional view. ", "mid_sen": "An interesting model is described by [Airenti et al., 1993] , which separates out the conversational games from the task-related games in a way similar way to [Litman and Allen, 1987] . ", "after_sen": "Because of this separation, they do not have to assume co-operation on the tasks each agent is performing, but still require recognition of intention and cooperation at the conversational level. "}
{"citeStart": 138, "citeEnd": 149, "citeStartToken": 138, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "The system supports speech search over multiple different dimensions such as title, genre, cast, director, and year. Input can be more telegraphic with searches such as \"Legally Blonde\", \"Romantic comedy\", and \"Reese Witherspoon\", or more verbose natural language queries such as \"I'm looking for a movie called Legally Blonde\" and \"Do you have romantic comedies\". An important advantage of speech is that it makes it easy to combine multiple constraints over multiple dimensions within a single query (Cohen, 1992) . For example, queries can indicate co-stars: \"movies starring Ginger Rogers and Fred Astaire\", or constrain genre and cast or director at the same time: \"Meg Ryan Comedies\", \"show drama directed by Woody Allen\" and \"show comedy movies directed by Woody Allen and starring Mira Sorvino\". Handwriting: Handwritten pen input can also be used to make queries. When the user's pen approaches the feedback window, it expands allowing for freeform pen input. In the example in Figure 3, the user requests comedy movies with Bruce Willis using unimodal handwritten input. This is an important input modality as it is not impacted by ambient noise such as crosstalk from other viewers or currently playing content.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Input can be more telegraphic with searches such as \"Legally Blonde\", \"Romantic comedy\", and \"Reese Witherspoon\", or more verbose natural language queries such as \"I'm looking for a movie called Legally Blonde\" and \"Do you have romantic comedies\". ", "mid_sen": "An important advantage of speech is that it makes it easy to combine multiple constraints over multiple dimensions within a single query (Cohen, 1992) . ", "after_sen": "For example, queries can indicate co-stars: \"movies starring Ginger Rogers and Fred Astaire\", or constrain genre and cast or director at the same time: \"Meg Ryan Comedies\", \"show drama directed by Woody Allen\" and \"show comedy movies directed by Woody Allen and starring Mira Sorvino\". "}
{"citeStart": 74, "citeEnd": 86, "citeStartToken": 74, "citeEndToken": 86, "sectionName": "UNKNOWN SECTION NAME", "string": "The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999) . The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997) ; the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992) , and passed in that form to the dialogue manager. We refer to these as linguistic level representations.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998) ). ", "mid_sen": "Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000) . ", "after_sen": "Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993) , using a domain~independent unification grammar and a domain-specific lexicon. "}
{"citeStart": 224, "citeEnd": 241, "citeStartToken": 224, "citeEndToken": 241, "sectionName": "UNKNOWN SECTION NAME", "string": "2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The parser creates a packed parse forest represented as a graph-structured stack.", "mid_sen": "2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997) .", "after_sen": "Estimating action probabilities, consists of a) recording an action history for the correct derivation in the parse forest (for each sentence in a treebank), b) computing the frequency of each action over all action histories and c) normalizing these frequencies to determine probability distributions over conflicting (i.e. shift/reduce or reduce/reduce) actions. "}
{"citeStart": 43, "citeEnd": 52, "citeStartToken": 43, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "Jensen-Shannon is well defined for all distributions because the average of p i and q i is non-zero whenever either number is. These measures and others are surveyed in (Lee, 2001) , who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in (1999) . The skew divergence accounts for zeros in q by mixing in a small amount of p.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Jensen-Shannon is well defined for all distributions because the average of p i and q i is non-zero whenever either number is. ", "mid_sen": "These measures and others are surveyed in (Lee, 2001) , who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in (1999) . ", "after_sen": "The skew divergence accounts for zeros in q by mixing in a small amount of p."}
{"citeStart": 154, "citeEnd": 188, "citeStartToken": 154, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983 ) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980 ) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988) , spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990) , parsing (Debili 1982; Hindle and Rooth 1990) . As pointed out by Bell, Witten, and Cleary (1989) , these applications fall under two research paradigms: statistical approaches and lexical approaches. In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities. In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences. In contrast, in the lexical approach, a collocation is an element of a dictionary among a few thousand other lexical items. Collocations in the lexicographic meaning are only dealt with in the lexical approach. Aside from the work we present in this paper, most of the work carried out within the lexical approach has been done in computer-assisted lexicography by Choueka, Klein, and Neuwitz (1983) and Church and his colleagues . Both works attempted to automatically acquire true collocations from corpora. Our work builds on Choueka's, and has been developed contemporarily to Church's. Choueka, Klein, and Neuwitz (1983) proposed algorithms to automatically retrieve idiomatic and collocational expressions. A collocation, as defined by Choueka, is a sequence of adjacent words that frequently appear together. In theory the sequences can be of any length, but in actuality, they contain two to six words. In Choueka (1988) , experiments performed on an 11 million-word corpus taken from the New York Times archives are reported. Thousands of commonly used expressions such as \"fried chicken,\" \"casual sex,\" \"chop suey,\" \"home run,\" and \"Magic Johnson\" were retrieved.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Collocations in the lexicographic meaning are only dealt with in the lexical approach. ", "mid_sen": "Aside from the work we present in this paper, most of the work carried out within the lexical approach has been done in computer-assisted lexicography by Choueka, Klein, and Neuwitz (1983) and Church and his colleagues . ", "after_sen": "Both works attempted to automatically acquire true collocations from corpora. "}
{"citeStart": 14, "citeEnd": 29, "citeStartToken": 14, "citeEndToken": 29, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent theorizing in linguistics brought forth a level of representation called the Predicate-Argument Structure (PAS). PAS acts as the interface between lexical semantics and d-structure in GB (Grimshaw, 1990) , functional structure in LFG (Alsina, 1996) , and complement structure in HPSG (Wechsler, 1995) . PAS is the sole level of representation in Combinatory Categorial Grammar (CCG) (Steedman, 1996) . All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. For instance, Grimshaw (1990) defines the thematic hierarchy as:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "All formulations assume a prominence-based structured representation for PAS, although they differ in the terms used for defining prominence. ", "mid_sen": "For instance, Grimshaw (1990) defines the thematic hierarchy as:", "after_sen": "Agent > Experiencer > Goal / Location / Source > Theme \" Thanks to Mark Steedman for discussion and material, and to the anonymous reviewer of an extended version whose comments led to significant revisions. "}
{"citeStart": 66, "citeEnd": 78, "citeStartToken": 66, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "Alternatively, we can take a regression perspective by assuming that the labels come from a discretization of a continuous function feature space to a metric space. 5 If we choose R from a family of sufficiently \"gradual\" functions, then similar items necessarily receive similar labels. In particular, we consider linear, S -insensitive SVM regression (Vapnik, 1995; Smola and Schölkopf, 1998) ; the idea is to find the hyperplane that best fits the training data, but where training points whose labels are within distance S of the hyperplane incur no loss. Then, for (test) instance H , the label preference function", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "5 If we choose R from a family of sufficiently \"gradual\" functions, then similar items necessarily receive similar labels. ", "mid_sen": "In particular, we consider linear, S -insensitive SVM regression (Vapnik, 1995; Smola and Schölkopf, 1998) ; the idea is to find the hyperplane that best fits the training data, but where training points whose labels are within distance S of the hyperplane incur no loss. ", "after_sen": "Then, for (test) instance H , the label preference function"}
{"citeStart": 146, "citeEnd": 173, "citeStartToken": 146, "citeEndToken": 173, "sectionName": "UNKNOWN SECTION NAME", "string": "Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996) . For the thirdperson pronouns and reflexives, the performance was (1) 86% of 560 cases in five computer manuals and (2) 75% of 306 cases in twenty-seven Web page texts. The present FASTUS system correctly resolved 71% of 34 cases in five newspaper arti-cles. This progressive decline in performance corresponds to the progressive decline in the amount of syntactic information in the input to reference resolution. To summarize the latter decline, Lap-pin and Leass (1994) had the following components in their algorithm. 8. Decision procedure for choosing among equally preferred candidate antecedents Kennedy and Boguraev (1996) approximated the above components with a poorer syntactic input, which is an output of a part-of-speech tagger with grammatical function information, plus NPs recognized by finite-state patterns and NPs' adjunct and subordination contexts recognized by heuristics. With this input, grammatical functions and precedence relations were used to approximate 2 and 5. Finite-state patterns approximated 4. Three additional salience factors were used in 7, and a preference for intraclausal antecedents was added in 6; 3 and 8 were the same. The present algorithm works with an even poorer syntactic input, as summarized here.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system did quite well (78%) with third-person pronouns with intrasentential antecedents, the largest class of such pronouns.", "mid_sen": "Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996) . ", "after_sen": "For the thirdperson pronouns and reflexives, the performance was (1) 86% of 560 cases in five computer manuals and (2) 75% of 306 cases in twenty-seven Web page texts. "}
{"citeStart": 18, "citeEnd": 38, "citeStartToken": 18, "citeEndToken": 38, "sectionName": "UNKNOWN SECTION NAME", "string": "In previous work (Bachenko et al. 1986) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive-Liberman synthesizer (Olive and Liberman 1985) . The system generated phrase boundaries using information derived from the syntactic structure of a sentence. While we saw significant improvements in the resulting synthesized speech, we also observed problems with the system. Often these stemmed from our assumptions that both clausal structure and predicateargument relations were important in determining prosodic phrasing. This paper reconsiders those assumptions and describes an analysis of phrasing that we believe corrects many of the problems of the earlier version. Like the earlier version, it has been implemented in a text-to-speech system that uses a natural language parser and prosody rules to generate information about the location and relative strength of prosodic phrase boundaries.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "In previous work (Bachenko et al. 1986) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive-Liberman synthesizer (Olive and Liberman 1985) . ", "after_sen": "The system generated phrase boundaries using information derived from the syntactic structure of a sentence. "}
{"citeStart": 18, "citeEnd": 31, "citeStartToken": 18, "citeEndToken": 31, "sectionName": "UNKNOWN SECTION NAME", "string": "Word Grammar (WG, Hudson (1990) ) is based on general graphs instead of trees. The ordering of two linked words is specified together with their dependency relation, as in the proposition \"object of verb follows it\". Extraction of, e.g., objects is analyzed by establishing an additional dependency called visitor between the verb and the extractee, which requires the reverse order, as in \"visitor of verb precedes it\". Resulting inconsistencies, e.g. in case of an extracted object, are not resolved. This approach compromises the semantic motivation of dependencies by adding purely order-induced dependencies.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These rules have not yet been formally specified (Melc'~k & Pertsov, 1987p.187f ) (but see the proposal by Rambow & Joshi (in print)).", "mid_sen": "Word Grammar (WG, Hudson (1990) ) is based on general graphs instead of trees. ", "after_sen": "The ordering of two linked words is specified together with their dependency relation, as in the proposition \"object of verb follows it\". "}
{"citeStart": 114, "citeEnd": 131, "citeStartToken": 114, "citeEndToken": 131, "sectionName": "UNKNOWN SECTION NAME", "string": "Kernel functions can implicitly capture a large amount of features efficiently; thus, they have been widely used in various NLP tasks. Various types of features have been exploited so far for relation extraction. In (Bunescu and Mooney, 2005b) sequence of words features are utilized using a sub-sequence kernel. In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al., 2006a ) syntactic features are employed for relation extraction. Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005) , in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al., 2006a ) syntactic features are employed for relation extraction. ", "mid_sen": "Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005) , in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.", "after_sen": "In CD'01 (Collins and Duffy, 2001 ) a convolution syntactic tree kernel is proposed that generally measures the syntactic similarity between parse trees. "}
{"citeStart": 123, "citeEnd": 147, "citeStartToken": 123, "citeEndToken": 147, "sectionName": "UNKNOWN SECTION NAME", "string": "This article represents an extension of our previous work on unsupervised event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010) . In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. As data sets, we consider three different resources, including our own corpus (which is the only corpus available that encodes event coreference annotations across and within documents). In the next section, we provide additional information on how we performed the annotation of this corpus. Another major contribution of this article is an extended description of the unsupervised models for solving event coreference. In particular, we focused on providing further explanations about the implementation of the mIBP framework as well as its integration into the HDP and iHMM models. Finally, in this work, we significantly extended the experimental results section, which also includes a novel set of experiments performed over the OntoNotes English corpus (LDC-ON 2007) .", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "This article represents an extension of our previous work on unsupervised event coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010) . ", "after_sen": "In this work, we present more details on the problem of solving both within-and cross-document event coreference as well as describe a generic framework for solving this type of problem in an unsupervised way. "}
{"citeStart": 100, "citeEnd": 121, "citeStartToken": 100, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004) , where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001) , which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000) . However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. One of the reasons may be the deficiency of the dynamic programming-based systems, that the global information of sequences cannot be incorporated as features of the models. Another reason may be that the computational complexity of the models prevented the developers to invent effective features for the task. We had to wait until Tsai et al. (2006) , who combine pattern-based postprocessing with CRFs, for CRF-based systems to achieve the same level of performance as Zhou et al. As such, a key to further improvement of the performance of bio-entity recognition has been to employ global features, which are effective to capture the features of long names appearing in the bio domain.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Importance of the treatment of long names might be implicitly indicated in the performance comparison of the participants of JNLPBA shared task (Kim et al., 2004) , where the best performing system (Zhou and Su, 2004) attains their scores by extensive post-processing, which enabled the system to make use of global information of the entity labels. ", "mid_sen": "After the shared task, many researchers tackled the task by using conditional random fields (CRFs) (Lafferty et al., 2001) , which seemed to promise improvement over locally optimized models like maximum entropy Markov models (MEMMs) (McCallum et al., 2000) . ", "after_sen": "However, many of the CRF systems developed after the shared task failed to reach the best performance achieved by Zhou et al. "}
{"citeStart": 207, "citeEnd": 218, "citeStartToken": 207, "citeEndToken": 218, "sectionName": "UNKNOWN SECTION NAME", "string": "Starting from a set of texts and a lexicon, the XPOST looks up all words in the texts and assigns to them a set of one or more readings. The words are then classified into so-called ambiguity classes according to which set of readings they have been assigned. The training is performed on ambiguity classes and not on individual word tokens. Kallgren (1996) gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS (Church 1988) and VOLSUNGA (DeRose 1988) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The training is performed on ambiguity classes and not on individual word tokens. ", "mid_sen": "Kallgren (1996) gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS (Church 1988) and VOLSUNGA (DeRose 1988) .", "after_sen": "A characteristic tbature of the SUC is its high number of different tags. "}
{"citeStart": 54, "citeEnd": 74, "citeStartToken": 54, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "Recent work on the automatic extraction of LTAG (Xia, 1999; Chen and Vijay-Shanker, 2000) and disambiguation models (Chiang, 2000) has been the first on the statistical model for syntactic disambiguation based on lexicalized grammars. However, the models are based on the lexical dependencies of elementary trees, which is a simple extension of the LPCFG. That is, the models are still based on decomposition into primitive lexical dependencies. Derivation trees, the structural description in LTAG (Schabes et al., 1988) , represent the association of lexical items i.e., elementary trees. In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependencies of elementary trees, i.e., a derivation tree, describe the semantic relations of words more directly than lexicalized parse trees. For example, Figure 3 has a derivation tree corresponding to the parse tree in Figure 1 (Collins, 1997) , are elegantly represented in derivation trees. Formally, a derivation tree is represented as a set of dependencies:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "That is, the models are still based on decomposition into primitive lexical dependencies. ", "mid_sen": "Derivation trees, the structural description in LTAG (Schabes et al., 1988) , represent the association of lexical items i.e., elementary trees. ", "after_sen": "In LTAG, all syntactic constraints of words are described in an elementary tree, and the dependencies of elementary trees, i.e., a derivation tree, describe the semantic relations of words more directly than lexicalized parse trees. "}
{"citeStart": 89, "citeEnd": 103, "citeStartToken": 89, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "In this section, we describe the citation purpose classification task. Given a target paper B and its citation context (extracted using the method described above) in a given article A, we want to determine the purpose of citing B by A. The purpose is defined as intention behind selecting B and citing it by the author of A (Garfield, 1964) . We use a taxonomy that consists of six categories. We designed this taxonomy based on our study of similar taxonomies proposed in previous work. We selected the categories that we believe are more important and useful from a bibliometric point of view, and the ones that can be detected through citation text analysis. We also tried to limit the number of categories by grouping similar categories proposed in previous work under one category. The six categories, their descriptions, and an example for each category are listed in Table 2 .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Given a target paper B and its citation context (extracted using the method described above) in a given article A, we want to determine the purpose of citing B by A. ", "mid_sen": "The purpose is defined as intention behind selecting B and citing it by the author of A (Garfield, 1964) . ", "after_sen": "We use a taxonomy that consists of six categories. "}
{"citeStart": 56, "citeEnd": 69, "citeStartToken": 56, "citeEndToken": 69, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . ", "mid_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. ", "after_sen": "Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) ."}
{"citeStart": 108, "citeEnd": 130, "citeStartToken": 108, "citeEndToken": 130, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003) . This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007) . Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, Z Λ (f ), required for the log-likelihood, and the marginals, required for its derivatives.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003) . ", "mid_sen": "This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007) . ", "after_sen": "Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. "}
{"citeStart": 215, "citeEnd": 240, "citeStartToken": 215, "citeEndToken": 240, "sectionName": "UNKNOWN SECTION NAME", "string": "We then go on to compare the current approach with that of some other theories with similar aims: the \"standard\" version of quasi-logical form implemented in the Core Language Engine, as rationally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994) ; underspecified Discourse Representation Theory (Reyle 1993) ; and the \"glue language\" approach of Dalrymple et al. (1996) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim is merely to show that we can, to a first approximation, provide a reasonably fully worked out description of these phenomena in a truly bidirectional way.", "mid_sen": "We then go on to compare the current approach with that of some other theories with similar aims: the \"standard\" version of quasi-logical form implemented in the Core Language Engine, as rationally reconstructed by Alshawi and Crouch (1992) and Crouch and Pulman (1994) ; underspecified Discourse Representation Theory (Reyle 1993) ; and the \"glue language\" approach of Dalrymple et al. (1996) .", "after_sen": "Finally, we discuss some of the semantic and logical issues raised by the approach described here, in particular the extent to which the theory meets the desiderata for accounts of underspecification outlined by van Eijck and Jaspars (1996) , and the extent to which the theory supplies a methodologically satisfactory account of truth and interpretation for sentences involving contextually dependent constructs."}
{"citeStart": 128, "citeEnd": 150, "citeStartToken": 128, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "The two context vectors of a word characterize the distribution of neighboring words to its left an.d right. The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior (Finch and Chater, 1992; Sch/itze, 1993) . We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus. Here, we use the raw 250dimensional context vectors and apply the SVD to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each). We obtained 47,025 50-dimensional reduced vectors from the SVD and clustered them into 200 classes using the fast clustering algorithm Buckshot (Cutting et al., 1992) (group average agglomeration applied to a sample). This classification constitutes the baseline performance for distributional partof-speech tagging. All occurrences of a word are assigned to one class. As pointed out above, such a procedure is problematic for ambiguous words.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The two context vectors of a word characterize the distribution of neighboring words to its left an.d right. ", "mid_sen": "The concatenation of left and right context vector can therefore serve as a representation of a word's distributional behavior (Finch and Chater, 1992; Sch/itze, 1993) . ", "after_sen": "We formed such concatenated vectors for all 47,025 words (surface forms) in the Brown corpus. "}
{"citeStart": 224, "citeEnd": 238, "citeStartToken": 224, "citeEndToken": 238, "sectionName": "UNKNOWN SECTION NAME", "string": "An amdysis of proposals in a corpus of 55 problemsolving dialogues SllOWS lhat communicating agents don't always include warrants in a prol)osal, and suggest a number of hypolheses abotlt which factors alfecl lheir decision I Walker, 1993; Pollack el al., 19821. Consider a situalion in which an agent A w~mts ~u~ agent B to accept a proposal P. If B is a 'helpful' agent (llOllaHl.Ol)Ol/lOtlS), B will accepl A's proposal withoul a wammt. Altemalively, if B deliberates whether to accept 1; but B knows of no competing options, Ihen P will be the best el)lion whether or not A lells P, the WatW~Ult lor P. Since a w~u','ant makes the dialogue longer, tile Explicit-Wml'~mt strategy might be inefficient whenever either of these situalions hold.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "T tile speakef's I'ROPOSAI, in la. 1", "mid_sen": "An amdysis of proposals in a corpus of 55 problemsolving dialogues SllOWS lhat communicating agents don't always include warrants in a prol)osal, and suggest a number of hypolheses abotlt which factors alfecl lheir decision I Walker, 1993; Pollack el al., 19821. ", "after_sen": "Consider a situalion in which an agent A w~mts ~u~ agent B to accept a proposal P. "}
{"citeStart": 13, "citeEnd": 25, "citeStartToken": 13, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "(Ramshaw and Marcus, 1995) have build a chunker by applying transformation-based learning to sections of the Penn Treebank. Rather than working with bracket structures, they have represented the chunking task as a tagging problem. POS-like tags were used to account for the fact that words were inside or outside chunks. They have applied their method to two segments of the Penn Treebank and these are still being used as benchmark data sets.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "(Ramshaw and Marcus, 1995) have build a chunker by applying transformation-based learning to sections of the Penn Treebank. ", "after_sen": "Rather than working with bracket structures, they have represented the chunking task as a tagging problem. "}
{"citeStart": 34, "citeEnd": 51, "citeStartToken": 34, "citeEndToken": 51, "sectionName": "UNKNOWN SECTION NAME", "string": "The major topic in the development of word-Pos guessers is the strategy which is to be used for the acquisition of the guessing rules. A rule-based tagger described in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition. A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources. In (Zhang&Kim, 1990) a system for the automated learning of morphological word-formation rules is described. This system divides a string into three regions and from training examples infers their correspondence to underlying morphological features. Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus. A statistical-based suffix learner is presented in (Schmid, 1994) . From a pre-tagged training corpus it constructs the suffix tree where every suffix is associated with its information measure. Although the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art, they require specially prepared training data --a pretagged training corpus, training examples, etc.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The major topic in the development of word-Pos guessers is the strategy which is to be used for the acquisition of the guessing rules. ", "mid_sen": "A rule-based tagger described in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology and intuition. ", "after_sen": "A more appealing approach is an empirical automatic acquisition of such rules using available lexical resources. "}
{"citeStart": 47, "citeEnd": 65, "citeStartToken": 47, "citeEndToken": 65, "sectionName": "UNKNOWN SECTION NAME", "string": "The first experiment uses the IBM Computer Manuals domain, which consists of sentences extracted from IBM computer manuals. The training and test sentences were annotated by the University of Lancaster. The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels. This treebank is described in great detail in (Black et al., 1993) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels. ", "mid_sen": "This treebank is described in great detail in (Black et al., 1993) .", "after_sen": "The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based, unification-style probabilistic context-free grammar for parsing this domain. "}
{"citeStart": 46, "citeEnd": 74, "citeStartToken": 46, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "There are reasons to believe that the limitations of rule-based methods could be overcome with statistical or knowledge-poor approaches, which started to be used for natural language processing in the late 1980s and 1990s. Atwell (1987) was among the first to use a statistical and knowledge-poor approach to detect grammatical errors in POS-tagging. Other studies, such as those by Knight and Chandler (1994) or Han et al. (2006) , for instance, proved more successful than rule-based systems in the task of detecting article-related errors. There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection. Burstein et al. (2004) present an idea similar to the present paper, since they use n-grams for grammar checking. In their case, however, the model is much more complicated since it uses a machine learning approach trained on a corpus of correct English and using POS-tags bigrams as features apart from word bigrams. In addition, they use a series of statistical association measures instead of using plain frequency.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Other studies, such as those by Knight and Chandler (1994) or Han et al. (2006) , for instance, proved more successful than rule-based systems in the task of detecting article-related errors. ", "mid_sen": "There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection. ", "after_sen": "Burstein et al. (2004) present an idea similar to the present paper, since they use n-grams for grammar checking. "}
{"citeStart": 100, "citeEnd": 113, "citeStartToken": 100, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Tur-ney, 2002; Pang et al., 2002) where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal. Other work (Subasic and Huettner, 2001; Morinaga et al., 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of favorability. Automatic acquisition of sentiment expressions have also been studied (Hatzivassiloglou and McKeown, 1997) , but limited to adjectives, and only one sentiment could be assigned to each word. pointed out that the multiple sentiment aspects in a document should be extracted. This paper follows that approach, but exploits deeper analysis in order to avoid the analytic failures reported by , which occurred when they used a shallow parser and only addressed a limited number of syntactic phenomena. In our in-depth approach described in the next section, two types of errors out of the four reported by were easily removed .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Some prior studies on sentiment analysis focused on the document-level classification of sentiment (Tur-ney, 2002; Pang et al., 2002) where a document is assumed to have only a single sentiment, thus these studies are not applicable to our goal. ", "after_sen": "Other work (Subasic and Huettner, 2001; Morinaga et al., 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of favorability. "}
{"citeStart": 114, "citeEnd": 126, "citeStartToken": 114, "citeEndToken": 126, "sectionName": "UNKNOWN SECTION NAME", "string": "In order to train the model, we maximize (2). While the log-likelihood cannot be maximised for the parameters, Λ, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003) . Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using forward-backward inference, which yields the partition function, Z Λ (e, f ), required for the log-likelihood, and the pair-wise marginals, p Λ (a t−1 , a t |e, f ), required for its derivatives.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "While the log-likelihood cannot be maximised for the parameters, Λ, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. ", "mid_sen": "We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003) . ", "after_sen": "Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. "}
{"citeStart": 0, "citeEnd": 23, "citeStartToken": 0, "citeEndToken": 23, "sectionName": "UNKNOWN SECTION NAME", "string": "Another well recognized distinction is that of event types, such as state, process, transition, exemplified by the following examples: While verbs have an intrinsic type (e.g. wait is a process and catch is a transition), these types also apply to whole phrases, since tense, aspect, adjuncts and arguments can compose with the type of the lexical head to form a new type: Four kinds of temporal adverbials can be distinguished and are linked to the event types. Duration modifies processes, as in example (lla), but not transitions (llb); frame adverbials modify accomplishments, as in (12a), but not processes (12b); point adverbials modify achievements, as in (13); and frequency adverbials modify iterative events, as in (14) . It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases. Moens & Steedman (1988) identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas Nakhimovsky (1988) identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15. When the children crossed the road, a) they waited for the teacher to give a signal b) they stepped onto its concrete surface as if it were about to swallow them up. c) they were nearly hit by a car d) they reached the other side stricken with fear. e) they found themselves surrounded by strangers. Pustejovsky (1991) offers a much more compositional notion of event structure, where a transition is the composition of a process and a state. This analysis is more closely tied to the lexicon than Moens and Steedman's or Nakhimovsky's (and is offered in the context of a generative theory of lexical semantics). It not only accounts for the semantics of verbs, but also their compositions with adjuncts to form new types, as in 7above.", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is also clear that events are not undifferentiated masses, but rather have subparts that can be picked out by the choice of phrase type or the addition of adverbial phrases. ", "mid_sen": "Moens & Steedman (1988) identify three constituents to an event nucleus, a preparatory process, culmination, and consequent state, whereas Nakhimovsky (1988) identifies five: preparatory, initial, body, final, result, exemplified by the following: 1 15. ", "after_sen": "When the children crossed the road, a) they waited for the teacher to give a signal b) they stepped onto its concrete surface as if it were about to swallow them up. c) they were nearly hit by a car d) they reached the other side stricken with fear. e) they found themselves surrounded by strangers. "}
{"citeStart": 62, "citeEnd": 97, "citeStartToken": 62, "citeEndToken": 97, "sectionName": "UNKNOWN SECTION NAME", "string": "We have recently augmented the parser with a statistical disambiguation module. We use a framework similar to the one proposed by Briscoe and Carroll [Briscoe and Carroll, 1993] , in which the shift and reduce actions of the LR parsing tables are directly augmented with probabilities. Training of the probabilities is performed on a set of disambiguated parses. The probabilities of the parse actions induce statistical scores on alternative parse trees, which are used for disambiguation. However, additionally, we use the statistical score of the disambiguated parse as an additional evaluation feature across parses. The statistical score value is first converted into a confidence measure, such that more \"common\" parse trees receive a lower penalty score. This is done using the following formula:", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We have recently augmented the parser with a statistical disambiguation module. ", "mid_sen": "We use a framework similar to the one proposed by Briscoe and Carroll [Briscoe and Carroll, 1993] , in which the shift and reduce actions of the LR parsing tables are directly augmented with probabilities. ", "after_sen": "Training of the probabilities is performed on a set of disambiguated parses. "}
{"citeStart": 138, "citeEnd": 154, "citeStartToken": 138, "citeEndToken": 154, "sectionName": "UNKNOWN SECTION NAME", "string": "There is also a rich body of theoretical work on learning latent-variable models. Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999) , others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005) . But with the exception of Dasgupta and Schulman (2007) , there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "There is also a rich body of theoretical work on learning latent-variable models. ", "mid_sen": "Specialized algorithms can provably learn certain constrained discrete hidden-variable models, some in terms of weak generative capacity (Ron et al., 1998; Clark and Thollard, 2005; Adriaans, 1999) , others in term of strong generative capacity (Dasgupta, 1999; Feldman et al., 2005) . ", "after_sen": "But with the exception of Dasgupta and Schulman (2007) , there is little theoretical understanding of EM, let alone on complex model families such as the HMM, PCFG, and DMV."}
{"citeStart": 63, "citeEnd": 78, "citeStartToken": 63, "citeEndToken": 78, "sectionName": "UNKNOWN SECTION NAME", "string": "The inspiration for the PLTIG formalism stems from the desire to lexicalize a context-free gram-mar. There are three ways in which one might do so. First, one can modify the tree structures so that all context-free productions contain lexical items. Greibach normal form provides a well-known example of such a lexicalized context-free formalism. This method is not practical because altering the structures of the grammar damages the linguistic information stored in the original grammar (Schabes and Waters, 1994) . Second, one might propagate lexical information upward through the productions. Examples of formalisms using this approach include the work of Magerman (1995) , Charniak (1997) , Collins (1997), and Goodman (1997) . A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. The Lexicalized Tree-Adjoining Grammar (LTAG) formalism (Schabes et al., 1988) , (Schabes, 1990) , although not context-free, is the most well-known instance in this category. PLTIGs belong to this third category and generate only context-free languages.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, one might propagate lexical information upward through the productions. ", "mid_sen": "Examples of formalisms using this approach include the work of Magerman (1995) , Charniak (1997) , Collins (1997), and Goodman (1997) . ", "after_sen": "A more linguistically motivated approach is to expand the domain of productions downward to incorporate more tree structures. "}
{"citeStart": 123, "citeEnd": 139, "citeStartToken": 123, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu) ; (Ma and Way, 2009) (Ma) ; (Xi et al., 2012) (Xi) ; (Zeng et al., 2014 . The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. In contrast to these methods, the SLBD method exhibits greater generalizability.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The results demonstrate that either leveraging the same unlabeled data or providing a much larger unlabeled dataset for the monolingual semisupervised methods, the SLBD method can significantly outperform the evaluated monolingual semi-supervised methods, which indicates that the segmenting information obtained using SLBD is much more efficient at optimizing segmentation.", "mid_sen": "Finally, we evaluated SLBD in comparison with other bilingual semi-supervised methods, including (Xu et al., 2008) (Xu) ; (Ma and Way, 2009) (Ma) ; (Xi et al., 2012) (Xi) ; (Zeng et al., 2014 . ", "after_sen": "The results presented in Table 4 indicate that SLBD demonstrates much stronger performance, primarily because these other methods were developed with a focus on SMT, which causes them to preferentially decrease the perplexity of the subsequent SMT steps rather than producing a highly accurate segmentation. "}
{"citeStart": 120, "citeEnd": 140, "citeStartToken": 120, "citeEndToken": 140, "sectionName": "UNKNOWN SECTION NAME", "string": "Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in . Both systems are built around from the maximum-entropy technique (Berger et al., 1996) . We formulate the mention detection task as a sequence classification problem. While this approach is language independent, it must be modified to accomodate the particulars of the Arabic language. The Arabic words may be composed of zero or more prefixes, followed by a stem and zero or more suffixes. We begin with a segmentation of the written text before starting the classification. This segmentation process consists of separating the normal whitespace delimited words into (hypothesized) prefixes, stems, and suffixes, which become the subject of analysis (tokens). The resulting granularity of breaking words into prefixes and suffixes allows different mention type labels beyond the stem label (for instance, in the case of nominal and pronominal mentions). Additionally, because the prefixes and suffixes are quite frequent, directly processing unsegmented words results in significant data sparseness.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is these orthographic variations and complex morphological structure that make Arabic language processing challenging (Xu et al., 2001; Xu et al., 2002) .", "mid_sen": "Both tasks are performed with a statistical framework: the mention detection system is similar to the one presented in (Florian et al., 2004) and the coreference resolution system is similar to the one described in . ", "after_sen": "Both systems are built around from the maximum-entropy technique (Berger et al., 1996) . "}
{"citeStart": 171, "citeEnd": 188, "citeStartToken": 171, "citeEndToken": 188, "sectionName": "UNKNOWN SECTION NAME", "string": "We use NML to specify that Air Force together is a nominal modifier of contract. Adding this annotation better represents the true syntactic and semantic structure, which will improve the performance of downstream NLP systems. Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank. We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality. We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality. ", "mid_sen": "We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003) .", "after_sen": "We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs. "}
{"citeStart": 281, "citeEnd": 297, "citeStartToken": 281, "citeEndToken": 297, "sectionName": "UNKNOWN SECTION NAME", "string": "We can now detect and correct a repair, given a sentence annotated with POS tags and semantic classes. But how can we construct such a sequence from a word lattice? Integrating the model in a lattice algorithm requires three steps: mapping the word lattice to a tag lattice triggering IPs and extracting the possible reparandum reparans pairs introducing new paths to represent the plausible reparans The tag lattice construction is adapted from Samuelsson, 1997 . For every word edge and every denoted POS tag a corresponding tag edge is created and the resulting probability is determined. If a tag edge already exists, the probabilities of both edges are merged. The original words are stored together with their unique semantic class in a associated list. Paths through the tag graph are scored by a POS-trigram. If a trigger is active, all paths through the word before the IP need to be tested whether an acceptable repair segmentation exists. Since the scope model takes at most four words for reparandum and reparans in account it is su cient to expand only partial paths. Each of these partial paths is then processed by the scope model. To reduce the search space, paths with a low score can be pruned.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "But how can we construct such a sequence from a word lattice? ", "mid_sen": "Integrating the model in a lattice algorithm requires three steps: mapping the word lattice to a tag lattice triggering IPs and extracting the possible reparandum reparans pairs introducing new paths to represent the plausible reparans The tag lattice construction is adapted from Samuelsson, 1997 . ", "after_sen": "For every word edge and every denoted POS tag a corresponding tag edge is created and the resulting probability is determined. "}
{"citeStart": 113, "citeEnd": 133, "citeStartToken": 113, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "We also annotated 122 email threads of the Enron email corpus, consisting of email messages in the inboxes and outboxes of Enron corporation employees. Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails. We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006) . The annotator of the majority of the Loqui corpus also annotated the Enron corpus. She received additional training and guidance based on our experience with a pilot annotator who helped us develop the initial guidelines. Table 2 illustrates differences between the two corpora. The DFUs in the Loqui data are much shorter, with 5.5 words on average compared with 12.8 words in Enron. The distribution of DFU labels shows a similarly high proportion of Inform acts, comprising 50% of all Loqui DFUs and 61% of all Enron DFUs. Otherwise, the distributions are quite distinct. The Loqui interactions are all two party telephone dialogues where the callers (library patrons) tend to have limited goals (requesting books). The Enron threads consist of two or more parties, and exhibit a much broader range of communicative goals. In the Loqui data, backchannels are relatively frequent (13%) but do not occur in the email corpus for obvious reasons. There are some Commits (9%), typically reflecting cases where the librarian indicates she will send requested items to the caller by mail, or place them on reserve. There are no Commits in the Enron data. Neither corpus has many Request-Actions; the Loqui corpus has many more requests for information, which includes requests made by the librarian, e.g., for the patrons' identifying information, or by the caller.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails. ", "mid_sen": "We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006) . ", "after_sen": "The annotator of the majority of the Loqui corpus also annotated the Enron corpus. "}
{"citeStart": 133, "citeEnd": 148, "citeStartToken": 133, "citeEndToken": 148, "sectionName": "UNKNOWN SECTION NAME", "string": "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a) . This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bigrams, according to the log-likelihood ratio. This earlier approach was evaluated on the SENSEVAL-1 data and achieved an overall accuracy of 64%, whereas the bagged decision tree presented here achieves an accuracy of 68% on that data.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, each ensemble consists of 81 Naive Bayesian classifiers, making it difficult to determine which features and classifiers were contributing most significantly to disambiguation.", "mid_sen": "The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a) . ", "after_sen": "This is quite similar to the bagged decision trees of bigrams (B) presented here, except that the earlier work learns a single decision tree where training examples are represented by the top 100 ranked bigrams, according to the log-likelihood ratio. "}
{"citeStart": 15, "citeEnd": 25, "citeStartToken": 15, "citeEndToken": 25, "sectionName": "UNKNOWN SECTION NAME", "string": "This use of universal quantification to extract out c from a term containing c in this case gives the same result as a direct implementation of the rule for cooordination of unary functions (7a) would. However, this same process of recursive descent via scoped constants will work for any member of the conj rule family. For example, the following query corresponds to rule (7b). Note also that the use of the same bound variable names obj and sub causes no difficulty since the use of scoped-constants, meta-level H-reduction, and higher-order unification is used to access and manipulate the inner terms. Also, whereas (Park, 1992) requires careful consideration of handling of determiners with coordination, here such sentences are handled just like any others.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note also that the use of the same bound variable names obj and sub causes no difficulty since the use of scoped-constants, meta-level H-reduction, and higher-order unification is used to access and manipulate the inner terms. ", "mid_sen": "Also, whereas (Park, 1992) requires careful consideration of handling of determiners with coordination, here such sentences are handled just like any others.", "after_sen": "For example, the sentence Mary gave every dog a bone and some policeman a flower results in the LF"}
{"citeStart": 83, "citeEnd": 102, "citeStartToken": 83, "citeEndToken": 102, "sectionName": "UNKNOWN SECTION NAME", "string": "We are currently working toward incorporating syntactic information on the target words so as to be able to recover some of the grammatical role information lost in the classification process. In preliminary experiments, we have associated the target lexical items with supertag information (Bangalore and Joshi, 1999) . Supertags are labels that provide linear ordering constraints as well as grammatical relation information. Although associating supertags to target words increases the class set for the classifier, we have noticed that the degradation in the F-score is on the order of 3% across different corpora. The supertag information can then be exploited in the sentence construction process. The use of supertags in phrase-based SMT system has been shown to improve results (Hassan et al., 2006) .", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The supertag information can then be exploited in the sentence construction process. ", "mid_sen": "The use of supertags in phrase-based SMT system has been shown to improve results (Hassan et al., 2006) .", "after_sen": "A less obvious loss is the number of times a word or concept appears in the target sentence. "}
{"citeStart": 58, "citeEnd": 87, "citeStartToken": 58, "citeEndToken": 87, "sectionName": "UNKNOWN SECTION NAME", "string": "Most other work on clustering for language modeling (e.g. Pereira, Tishby and Lee, 1993; Ney, Essen and Kneser, 1994) has addressed the problem of data sparseness by clustering words into classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed during training. Thus conceptually at least, their processes are agglomerative: a large initial set of words is clumped into a smaller number of clusters. The approach described here is quite different. Firstly, it involves clustering whole sentences, not words. Secondly, its aim is not to tackle data sparseness by grouping a large number of objects into a smaller number of classes, but to increase the precision of the model by dividing a single object (the training corpus) into some larger number of sub-objects (the clusters of sentences). There is no reason why clustering sentences for prediction should not be combined with clustering words to reduce sparseness; the two operations are orthogonal.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Most other work on clustering for language modeling (e.g. Pereira, Tishby and Lee, 1993; Ney, Essen and Kneser, 1994) has addressed the problem of data sparseness by clustering words into classes which are then used to predict smoothed probabilities of occurrence for events which may seldom or never have been observed during training. ", "after_sen": "Thus conceptually at least, their processes are agglomerative: a large initial set of words is clumped into a smaller number of clusters. "}
{"citeStart": 227, "citeEnd": 251, "citeStartToken": 227, "citeEndToken": 251, "sectionName": "UNKNOWN SECTION NAME", "string": "'Fo date, psychologists have focused on two aspects of the speech segmentation problem. The first is the problem of parsing continuous speech into words given a developed lexicon to which incoming sounds can be matched; both psychologists (e.g., Cutler & Carter, 1987; Cutler & Butterliel (I, 1992) and designers of speech-recognition systems (e.g., (]hur (:h, 1987) have examined I~his problem. However, the problem we examined is dilferent---we want to know how infants segment speech before knowing which phonemic se-qllelW,('s form words. '1' he second aspect psychologists liaw~ focnsed (ill is the lirobleln of dcternihiilig the ill['Orluatioll SOllr(:(~s t() which ilifants are SCllSil,ive. Priluarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress. II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. Sensitivity to native-language phonotactics in 9-month-olds was re(:ently reported by Jusczyk, I,'riedcrici, Wessels, Swmkerud and Jusczyk (1993) . 'i'lles~ sl, udi(~s deruoilstratcd infants' perceptive abilil.il's wil,hout deiilonsl.ral,hig tlw usefuhicss of hli'alil,s > ll(~rcel)l,ioils. I low do childl'(,n coiubine l,li(: iiiforiii;d,ion I, hey i)crc~,iw; froln dilrerenl, SOlll'l;es'. ? Aslili el, al. Sl)(~c-Illate that infants first learn words heard in isolation, then use distribution and prosody to refine and expand their w)cabulary; however, Jusczyk (1(,)93) sliggests that sound sequences learned in isolation dill~r too greatly from those in contexi. to bc useful. He goes on to say, \"just how far inforniation in the sound structure of the input can pies, we see that Hypothesis 1 uses 48 characters and Hypothesis 2 uses 75. However, this simplistic method is inefficient; for instance, the length of lexical indices are arbitrary with respect to properties of the words themselves (e.g., in Hypothesis 2, there is no reason why/jul/was assigned tile index '10'--length two--instead of '9'--length one). Our system improves upon this simple size metri(: I)y coml)uting sizes based on ;t ('Onll)act rel)rcs(,ntat.ion motivated I)y informati(m theory. W(: inmginc hypothes(:s r(qu'(~sented ;~ a string of ones and zeros. This binary string must r(,present not only the lexical entries, their indices (called code words) and the coded sample, but also overhead information specifying the number of items coded and their arrangement in the string (information implicitly given by spacing and sl)atial placement in the introductory cxamples). Furtherrnore, the string and its components must be self-delimiting, so that a decoder could identify the endpoints of components by itself. The next section describes the binary representation and the length formulm derived from it in detail; readers satisfied with the intuitive descriptions presented so far should skip ahead to the Phonotactics subsection.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Priluarily, I.wo sotircos haw~ ll(~ell (~x--ainine~l: prosody and word stress. ", "mid_sen": "II,enults suggest I.hal, parents (~xaggcrate prosody in childdirected speech to highlight iniportant words (Fernahl & Mazzie, 1991; Aslin, Woodward, LaMendola & Bever, in press) and that infants are sensitive to prosody (e.g., Hirsh-Pasek et al., 1987) . ", "after_sen": "Word stress in English fairly accurately predicts the location of word beginnings (Cutler & Norris, 1988; Cutler & Butterfield, 1992) ; Jusczyk, Cutler and II,edanz (1993) demonstrated that 9-monthohls (but not 6-month-olds) are sensitive to the common strong/weak word stress pattern in English. "}
{"citeStart": 105, "citeEnd": 121, "citeStartToken": 105, "citeEndToken": 121, "sectionName": "UNKNOWN SECTION NAME", "string": "The idea behind this is to create a specialized grammar that retains a high coverage but allows very much faster parsing. This has turned out to be possible -speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a] .1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This has turned out to be possible -speedups compared to using the original grammar of in median 60 times were achieved at a cost in coverage of about ten percent, see [Samuelsson 1994a] .1 Another benefit from the method is a decreased error rate when the system is required to select a preferred analysis. ", "mid_sen": "In these experiments the scheme was applied to the grammar of a version of the SRI Core Language Engine [Alshawi ed. 1992] adapted to the Atis domain for a speech-translation task [Rayner el al 1993] and large corpora of real user data collected using Wizardof-Oz simulation.", "after_sen": "The resulting specialized grammar was compiled into LR parsing tables, and a special LR parser exploited their special properties, see [Samuelsson 1994b ]."}
{"citeStart": 22, "citeEnd": 45, "citeStartToken": 22, "citeEndToken": 45, "sectionName": "UNKNOWN SECTION NAME", "string": "The theoretical complexity of the generator is O (n4), where n is the size of the input. We give an informal argument for this. The complexity of the test phase is the number of evaluations that have to be made. Each node must be tested no more than twice in the worst case (due to precedence monotonicity), as one might have to try to combine its children in either direction according to the grammar rules. There are always exactly n -1 non-leaf nodes, so the complexity of the test phase is O(n). The complexity of the rewrite phase is that of locating the two TNCBs to be combined. In the worst case, we can imagine picking an arbitrary child TNCB (O(n)) and then trying to find another one with which it combines (O(n)). The complexity of this phase is therefore the product of the picking and combining complexities, i.e. O(n2). The combined complexity of the test-rewrite cycle is thus O(n3). Now, in section 3, we argued that no more than n -1 rewrites would ever be necessary, thus the overall complexity of generation (even when no solution is found) is O(n4). Average case complexity is dependent on the quality of the first guess, how rapidly the TNCB structure is actually improved, and to what extent the TNCB must be re-evaluated after rewriting. In the SLEMaT system (Poznarlski et al., 1993) , we have tried to form a good initial guess by mirroring the source structure in the target TNCB, and allowing some local structural modifications in the bilingual equivalences.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Average case complexity is dependent on the quality of the first guess, how rapidly the TNCB structure is actually improved, and to what extent the TNCB must be re-evaluated after rewriting. ", "mid_sen": "In the SLEMaT system (Poznarlski et al., 1993) , we have tried to form a good initial guess by mirroring the source structure in the target TNCB, and allowing some local structural modifications in the bilingual equivalences.", "after_sen": "Structural transfer operations only affect the efficiency and not the functionality of generation. "}
{"citeStart": 170, "citeEnd": 181, "citeStartToken": 170, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "The algorithm presented here has extended previous algorithms for rewrite rules by adding a limited version of backreferencing. This allows the output of rewriting to be dependent on the form of the strings which are rewritten. This new feature brings techniques used in Perl-like languages into the finite state calculus. Such an integration is needed in practical applications where simple text processing needs to be combined with more sophisticated computational linguistics techniques. One particularly interesting example where backreferences are essential is cascaded deterministic (longest match) finite state parsing as described for example in Abney (Abney, 1996) and various papers in (Roche and Schabes, 1997a) . Clearly, the standard rewrite rules do not apply in this domain. If NP is an NP recognizer, it would not do to.say NP ~ [NP]/A_p. Nothing would force the string matched by the NP to the left of the arrow to be the same as the string matched by the NP to the right of the arrow.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Such an integration is needed in practical applications where simple text processing needs to be combined with more sophisticated computational linguistics techniques. ", "mid_sen": "One particularly interesting example where backreferences are essential is cascaded deterministic (longest match) finite state parsing as described for example in Abney (Abney, 1996) and various papers in (Roche and Schabes, 1997a) . ", "after_sen": "Clearly, the standard rewrite rules do not apply in this domain. "}
{"citeStart": 158, "citeEnd": 170, "citeStartToken": 158, "citeEndToken": 170, "sectionName": "UNKNOWN SECTION NAME", "string": "We further optimized string recognition and plurality detection for handling citation-strings. See Table 3 for the full list of our features. While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999) . SVMs are known for being reliable and having good performance.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "See Table 3 for the full list of our features. ", "mid_sen": "While both (Soon et al., 2001) and (Ng and Cardie, 2002) induced decision trees (C5 and C4.5, respectively) we opted for using an SVM-based approach instead (Vapnik, 1998; Joachims, 1999) . SVMs are known for being reliable and having good performance.", "after_sen": ""}
{"citeStart": 113, "citeEnd": 133, "citeStartToken": 113, "citeEndToken": 133, "sectionName": "UNKNOWN SECTION NAME", "string": "Empty or displaced heads present the principal goaldirectedness problem for any head-driven generation approach (Shieber et al., 1990; K6nig, 1994 ; Gerdemann and IIinrichs, in press), where empty head refers not just to a construction in which the head has an empty phonology, but to any construction in which the head is partially unspecified. Since phonology does not guide generation, the phonological realization of the head of a construction plays no part in the generation of that construction. To better illustrate the problem that underspecified heads pose, consider the sentence:", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "", "mid_sen": "Empty or displaced heads present the principal goaldirectedness problem for any head-driven generation approach (Shieber et al., 1990; K6nig, 1994 ; Gerdemann and IIinrichs, in press), where empty head refers not just to a construction in which the head has an empty phonology, but to any construction in which the head is partially unspecified. ", "after_sen": "Since phonology does not guide generation, the phonological realization of the head of a construction plays no part in the generation of that construction. "}
{"citeStart": 173, "citeEnd": 192, "citeStartToken": 173, "citeEndToken": 192, "sectionName": "UNKNOWN SECTION NAME", "string": "3 The basic scheme of the argumentative structure we define turns out to be similar to one which was conceived of for work on legal summarisation of Chinese judgment texts (Cheung et al., 2001 ).", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These form a hierarchy of rhetorical content allowing the annotator to 'fall-back' to the basic scheme if they cannot place a sentence in a particu-2 To be more specific, the House of Lords hears civil cases from all of the United Kingdom and criminal cases from England, Wales and Northern Ireland.", "mid_sen": "3 The basic scheme of the argumentative structure we define turns out to be similar to one which was conceived of for work on legal summarisation of Chinese judgment texts (Cheung et al., 2001 ).", "after_sen": ""}
{"citeStart": 75, "citeEnd": 100, "citeStartToken": 75, "citeEndToken": 100, "sectionName": "UNKNOWN SECTION NAME", "string": "However, this approach does not fully capture the sense in which inhibitory factors play a negative and not just a neutral role. We want to distinguish between items that are unlikely to occur ever, and those that have just not happened to turn up in the training data. For example, in sentence (3) above strings 3.1, 3.2 and 3.n can never be correct. These should be distinguished from possibly correct parses that are not in the training data. In order that \"improbabilities\" can be modelled by inhibitory connections (Niles and Silverman, 1990) show how a Hidden Markov Model can be implemented by a neural network.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "These should be distinguished from possibly correct parses that are not in the training data. ", "mid_sen": "In order that \"improbabilities\" can be modelled by inhibitory connections (Niles and Silverman, 1990) show how a Hidden Markov Model can be implemented by a neural network.", "after_sen": "The theoretical ground for incorporating negative examples in a language learning process originates with Gold's work (Gold, 1967; Angluin, 1980) . "}
{"citeStart": 345, "citeEnd": 355, "citeStartToken": 345, "citeEndToken": 355, "sectionName": "UNKNOWN SECTION NAME", "string": "Even moderately long documents typically address sew~ral topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The aim of linear text segmentation is to discover the topic boundaries. ", "mid_sen": "The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999) , summarization (Reynar, 1998 ), text understanding, anaphora resolution (Kozima, 1993) , language modelling (Morris and Hirst, 1991; Beeferman et al., 199717) and improving document navigation for the visually disabled (Choi, 2000) .", "after_sen": "This paper focuses on domain independent methods for segmenting written text. "}
{"citeStart": 73, "citeEnd": 93, "citeStartToken": 73, "citeEndToken": 93, "sectionName": "UNKNOWN SECTION NAME", "string": "We consider four types of named entities: person names (PNs), location names (LNs), organization names (ONs), and transliterations of foreign names (FNs). Because any character string can in principle be a named entity of one or more types, in order to limit the number of candidates for a more effective search, we generate named entity candidates given an input string in two steps: First, for each type, we use a set of constraints (which are compiled by linguists and are represented as FSAs) to generate only those \"most likely\" candidates. Second, each of the generated candidates is assigned a class model probability. Class models are defined as generative models that are estimated on their corresponding named entity lists using MLE, together with a backoff smoothing schema, as described in Section 4.1.1. We will describe briefly the constraints and the class models here. The Chinese person-name model is a modified version of that described in Sproat et al. (1996) . Other NE models are novel, though they share some similarities with the Chinese person-name model.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We will describe briefly the constraints and the class models here. ", "mid_sen": "The Chinese person-name model is a modified version of that described in Sproat et al. (1996) . ", "after_sen": "Other NE models are novel, though they share some similarities with the Chinese person-name model."}
{"citeStart": 79, "citeEnd": 106, "citeStartToken": 79, "citeEndToken": 106, "sectionName": "UNKNOWN SECTION NAME", "string": "Step 1. Modeling contexts: The context of each word is generally modeled by a vector where each dimension corresponds to a context word and each dimension has a value indicating occurrence correlation. Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010) , in a paragraph (Fung and McKeown, 1997) , in a predefined window (Rapp, 1999; Andrade et al., 2010) ), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009) , certain dependency position (Otero and Campos, 2008) ). Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999) . Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002) , tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010) , context heterogeneity (Fung, 1995) , etc. Shao and Ng (2004) represented contexts using language models. Andrade et al. (2010) used a set of words with a positive association as a context. Andrade et al. (2011a) used dependency relations instead of context words. Ismail and Manandhar (2010) used only in-domain words in contexts. Pekar et al. (2006) constructed smoothed context vectors for rare words. Laws et al. (2010) used graphs in which vertices correspond to words and edges indicate three types of syntactic relations such as adjectival modification.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999) . ", "mid_sen": "Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002) , tf-idf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010) , context heterogeneity (Fung, 1995) , etc. Shao and Ng (2004) represented contexts using language models. ", "after_sen": "Andrade et al. (2010) used a set of words with a positive association as a context. "}
{"citeStart": 315, "citeEnd": 338, "citeStartToken": 315, "citeEndToken": 338, "sectionName": "UNKNOWN SECTION NAME", "string": "1. A tagger, a first-order HMM part-of-speech (PoS) and punctuation tag disambiguator, is used to assign and rank tags for each word and punctuation token in sequences of sentences (Elworthy, 1994 ). 2. A lemmatizer is used to replace word-tag pairs with lemma-tag pairs, where a lemma is the morphological base or dictionary headword form appropriate for the word, given the PoS assignment made by the tagger. We use an enhanced version of the GATE project stemmer (Cunningham et al., 1995) . 3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993 Carroll, , 1994 , using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994 , 1995 Carroll & Briscoe, 1996) . 4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "We use an enhanced version of the GATE project stemmer (Cunningham et al., 1995) . ", "mid_sen": "3. A probabilistic LR parser, trained on a treebank, returns ranked analyses (Briscoe &: Carroll, 1993; Carroll, 1993 Carroll, , 1994 , using a grammar written in a feature-based unification grammar formalism which assigns 'shallow' phrase structure analyses to tag networks (or 'lattices') returned by the tagger (Briscoe & Carroll, 1994 , 1995 Carroll & Briscoe, 1996) . ", "after_sen": "4. A patternset extractor which extracts subcategorization patterns, including the syntactic categories and head lemmas of constituents, from sentence subanalyses which begin/end at the boundaries of (specified) predicates."}
{"citeStart": 114, "citeEnd": 135, "citeStartToken": 114, "citeEndToken": 135, "sectionName": "UNKNOWN SECTION NAME", "string": "WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al., 2004) . A common approach to the disambiguation of nouns has been to consider a wide context around the ambiguous word and treat it as a bag of words or limited set of collocates. However, disambiguation of verbs generally benefits from more specific knowledge sources, such as the verb's relation to other items in the sentence (for example, by analysing the semantic type of its subject and object). Consequently, we believe that the disambiguation of verbs is task to which ILP is particularly wellsuited. Therefore, this paper focuses on the disambiguation of verbs, which is an interesting task since much of the previous work on WSD has concentrated on the disambiguation of nouns.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our hypothesis is that by using a very expressive representation formalism, a range of (shallow and deep) knowledge sources and ILP as learning technique, it is possible to generate models that, when compared to models produced by machine learning algorithms conventionally applied to WSD, are both more accurate for fine-grained distinctions, and \"interesting\", from a knowledge acquisition point of view (i.e., convey potentially new knowledge that can be easily interpreted by humans).", "mid_sen": "WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al., 2004) . ", "after_sen": "A common approach to the disambiguation of nouns has been to consider a wide context around the ambiguous word and treat it as a bag of words or limited set of collocates. "}
{"citeStart": 222, "citeEnd": 235, "citeStartToken": 222, "citeEndToken": 235, "sectionName": "UNKNOWN SECTION NAME", "string": "The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by Garside, Leech, and Sampson (1987) , on tagging the LOB corpus, and Church (1988) , on assigning part-of-speech labels and parsing noun phrases. Success rates ranging between 95-99% are reported, depending on how 'success' is defined. These approaches are probabilistic and based on transitional probabilities calculated from extensive pretagged corpora.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It shows the descriptive power of low-level morphology-based constraints.", "mid_sen": "The most successful achievements so far in the domain of large-scale morphological disambiguation of running text have been those for English reported by Garside, Leech, and Sampson (1987) , on tagging the LOB corpus, and Church (1988) , on assigning part-of-speech labels and parsing noun phrases. ", "after_sen": "Success rates ranging between 95-99% are reported, depending on how 'success' is defined. "}
{"citeStart": 126, "citeEnd": 150, "citeStartToken": 126, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.", "label": "Future", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. ", "mid_sen": "When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage.", "after_sen": "We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009) ). "}
{"citeStart": 0, "citeEnd": 11, "citeStartToken": 0, "citeEndToken": 11, "sectionName": "UNKNOWN SECTION NAME", "string": "In recent years, there has been increasing interest in applying natural language processing technologies to scientific literature. The overwhelmingly large number of papers published in fields like biology, genetics and chemistry each year means that researchers need tools for information access (extraction, retrieval, summarization, question answering etc). There is also increased interest in automatic citation indexing, e.g., the highly successful search tools Google Scholar and CiteSeer (Giles et al., 1998) . 1 This general interest in improving access to scientific articles fits well with research on discourse structure, as knowledge about the overall structure and goal of papers can guide better information access. Shum (1998) argues that experienced researchers are often interested in relations between articles. They need to know if a certain article criticises another and what the criticism is, or if the current work is based on that prior work. This type of information is hard to come by with current search technology. Neither the author's abstract, nor raw citation counts help users in assessing the relation between articles. And even though CiteSeer shows a text snippet around the physical location for searchers to peruse, there is no guarantee that the text snippet provides enough information for the searcher to infer the relation. In fact, studies from our annotated corpus (Teufel, 1999) , show that 69% of the 600 sentences stating contrast with other work and 21% of the 246 sentences stating research continuation with other work do not contain the corresponding citation; the citation is found in preceding Following Pereira et al, we measure word similarity by the relative entropy or Kulbach−Leibler (KL) distance, bet− ween the corresponding conditional distributions.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "1 This general interest in improving access to scientific articles fits well with research on discourse structure, as knowledge about the overall structure and goal of papers can guide better information access. ", "mid_sen": "Shum (1998) argues that experienced researchers are often interested in relations between articles. ", "after_sen": "They need to know if a certain article criticises another and what the criticism is, or if the current work is based on that prior work. "}
{"citeStart": 52, "citeEnd": 74, "citeStartToken": 52, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "Our approach to grammar lexicalization is class-based in the sense that we use classbased estimated frequencies f c (v n) of headverbs v and argument head-nouns n instead of pure frequency statistics or classbased probabilities of head word dependencies. Class-based estimated frequencies are introduced in Prescher et al. (2000) as the frequency f (v n) of a (v n)-pair in the training corpus, weighted by the best estimate of the class-membership probability p(cjv n) of an EM-based clustering model on", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our approach to grammar lexicalization is class-based in the sense that we use classbased estimated frequencies f c (v n) of headverbs v and argument head-nouns n instead of pure frequency statistics or classbased probabilities of head word dependencies. ", "mid_sen": "Class-based estimated frequencies are introduced in Prescher et al. (2000) as the frequency f (v n) of a (v n)-pair in the training corpus, weighted by the best estimate of the class-membership probability p(cjv n) of an EM-based clustering model on", "after_sen": "(v n)-pairs, i.e., f c (v n) = max c2C p(cjv n)(f (v n) + 1)."}
{"citeStart": 28, "citeEnd": 50, "citeStartToken": 28, "citeEndToken": 50, "sectionName": "UNKNOWN SECTION NAME", "string": "Note that the formulation of the adjunction generating function means that the values for ¢(X ~4 nil) for all X E V do not appear in the expectation matrix. This is a crucial difference between the test for consistency in TAGs as compared to CFGs. For CFGs, the expectation matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G). Using this it was shown in (Chaudhari et al., 1983) and (S£nchez and Bened~, 1997 ) that a single step of the inside-outside algorithm implies consistency for a probabilistic CFG. However, in the TAG case, the inclusion of values for ¢(X ~-+ nil) (which is essen-tim if we are to interpret the expectation matrix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For CFGs, the expectation matrix for a grammar G can be interpreted as the contribution of each non-terminal to the derivations for a sample set of strings drawn from L(G). ", "mid_sen": "Using this it was shown in (Chaudhari et al., 1983) and (S£nchez and Bened~, 1997 ) that a single step of the inside-outside algorithm implies consistency for a probabilistic CFG. ", "after_sen": "However, in the TAG case, the inclusion of values for ¢(X ~-+ nil) (which is essen-tim if we are to interpret the expectation matrix in terms of derivations over a sample set of strings) means that we cannot use the method used in (8) to compute the expectation matrix and furthermore the limit condition will not be convergent."}
{"citeStart": 40, "citeEnd": 57, "citeStartToken": 40, "citeEndToken": 57, "sectionName": "UNKNOWN SECTION NAME", "string": "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005 2 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features. These methods are very effective for relation extraction and show the bestreported performance on the ACE corpus. However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks.", "label": "Weakness", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model.", "mid_sen": "Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005 2 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features. ", "after_sen": "These methods are very effective for relation extraction and show the bestreported performance on the ACE corpus. "}
{"citeStart": 56, "citeEnd": 76, "citeStartToken": 56, "citeEndToken": 76, "sectionName": "UNKNOWN SECTION NAME", "string": "A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight & Srinivasan, 2003; Ruch et al., 2007; Hirohata et al., 2008; Björne et al., 2009) . The work of Hirohata et al. (2009) has been integrated with the MEDIE service (Miyao et al., 2006) , allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al., 1999; Teufel & Moens, 2002; Teufel et al., 2009; Teufel, 2010 ). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012) . Modality and negation in text have also been the focus of recent workshops (Farkas et al (2010) , Morante & Sporleder (2012) ). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al. (2008) , a cut-down version of the scheme proposed by Teufel et al. (2009) and CoreSC (1 st layer), from general to specific.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. ", "mid_sen": "AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. ", "after_sen": "Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. "}
{"citeStart": 257, "citeEnd": 269, "citeStartToken": 257, "citeEndToken": 269, "sectionName": "UNKNOWN SECTION NAME", "string": "Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997) . The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976) . They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "hnplementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997) , context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999) , entity repetition (Kan et al., 1998) , semantic similarity (Morris and Hirst, 1991; Kozima, 1993) , word distance model (Beeferman et al., 1997a ) and word frequency model (Reynar, 1999) to detect cohesion. ", "mid_sen": "Methods for finding the topic boundaries include sliding window (Hearst, 1994) , lexical chains (Morris, 1988; Kan et al., 1998) , dynamic programming (Ponte and Croft, 1997; Heinonen, 1998) , agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994) . ", "after_sen": "Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynat, 1998) ."}
{"citeStart": 180, "citeEnd": 199, "citeStartToken": 180, "citeEndToken": 199, "sectionName": "UNKNOWN SECTION NAME", "string": "In our latest implementation of this algorithm, we have recast this segmentation strategy as the composition of three distinct finite state machines. The first machine, illustrated in Figure 1 encodes the prefix and suffix expansion rules, producing a lattice of possible segmentations. The second machine is a dictionary that accepts characters and produces identifiers corresponding to dictionary entries. The final machine is a trigram language model, specifically a Kneser-Ney (Chen and Goodman, 1998) based backoff language model. Differing from (Lee et al., 2003) , we have also introduced an explicit model for un-known words based upon a character unigram model, although this model is dominated by an empirically chosen unknown word penalty. Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation. In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996) . For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine. Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data. The character based model alone achieves a 94.5% exact match segmentation accuracy, considerably less accurate then the dictionary based model.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Using 0.5M words from the combined Arabic Treebanks 1V2, 2V2 and 3V1, the dictionary based segmenter achieves a exact word match 97.8% correct segmentation. ", "mid_sen": "In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996) . ", "after_sen": "For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine. "}
{"citeStart": 0, "citeEnd": 13, "citeStartToken": 0, "citeEndToken": 13, "sectionName": "UNKNOWN SECTION NAME", "string": "Since we compute association scores independently, it is difficult to impose mutually exclusive constraints between translation candidates derived from a paired parallel sentence. Hence, our method tends to suffer from indirect association when the association score is low, as pointed out by Melamed (2001) . Although our method relies on an empirical observation that \"direct associations are usually stronger than indirect association\", it seems effective enough for multi-word translation. balanced by a As far as we know, our method is the first attempt to make an exhaustive enumeration of rigid and gapped translation candidates of both languages possible, yet avoiding combinatorial explosion. Previous approaches effectively narrow down its search space by some heuristics. Kupiec (1993) focuses on noun-phrase translations only, Smadja et al. (1996) limits to find French translation of English collocation identified by his Xtract system, and Kitamura and Matsumoto (1996) can exhaustively enumerate only rigid word sequences.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Previous approaches effectively narrow down its search space by some heuristics. ", "mid_sen": "Kupiec (1993) focuses on noun-phrase translations only, Smadja et al. (1996) limits to find French translation of English collocation identified by his Xtract system, and Kitamura and Matsumoto (1996) can exhaustively enumerate only rigid word sequences.", "after_sen": "Many of works mentioned in the last paragraph as well as ours extract non-probabilistic translation lexicons. "}
{"citeStart": 213, "citeEnd": 224, "citeStartToken": 213, "citeEndToken": 224, "sectionName": "UNKNOWN SECTION NAME", "string": "One problem with the training methods given in section 2.3 is the restriction of training data to nouns in .Af. Many nouns, especially common ones, have verbal or adiectival usages that preclude them from being in .Af. Yet when they occur as nouns, they still provide useful training information that the current system ignores. To test whether using tagged Training scheme (integers denote window widths) Figure 5 : Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Yet when they occur as nouns, they still provide useful training information that the current system ignores. ", "mid_sen": "To test whether using tagged Training scheme (integers denote window widths) Figure 5 : Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus. ", "after_sen": "Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus). "}
{"citeStart": 115, "citeEnd": 139, "citeStartToken": 115, "citeEndToken": 139, "sectionName": "UNKNOWN SECTION NAME", "string": "For each word of the sentence we compute the distance between the word's position i and the position of the words head ρ i . The arc-length score is the summed length of all those with correct head assignments (δ(ρ i , ρ i ) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e.g., main verb dependencies for tasks For the graph-based parser one can also find the higest scoring tree with correct root by setting the score of all competing arcs to −∞. Table 3 : Results for both parsers on the development set of the PTB. When training with ALS (labeled and unlabeled), we see an improvement in UAS, LAS, and ALS. Furthermore, if we use a labeled-ALS as the metric for augmented-loss training, we also see a considerable increase in LAS.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The labeled version of this score requires that the labels of the arc are also correct. ", "mid_sen": "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e.g., main verb dependencies for tasks For the graph-based parser one can also find the higest scoring tree with correct root by setting the score of all competing arcs to −∞. ", "after_sen": "Table 3 : Results for both parsers on the development set of the PTB. "}
{"citeStart": 5, "citeEnd": 17, "citeStartToken": 5, "citeEndToken": 17, "sectionName": "UNKNOWN SECTION NAME", "string": "We developed a linguistic compaction algorithm employing the ideas just described. However, we cannot present it here due to the space limitations. The preliminary results of our experiments are presented in Table 1 . Simple thresholding (removing rules that only occur once) was also to achieve the maximum compaction ratio. For labelled as well as unlabelled evaluation of the resulting parse trees we used the evalb software by Satoshi Sekine. See (Krotov, 1998) for the complete presentation of our methodology and results.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For labelled as well as unlabelled evaluation of the resulting parse trees we used the evalb software by Satoshi Sekine. ", "mid_sen": "See (Krotov, 1998) for the complete presentation of our methodology and results.", "after_sen": "As one can see, the fully compacted grammar yields poor recall and precision figures. "}
{"citeStart": 131, "citeEnd": 142, "citeStartToken": 131, "citeEndToken": 142, "sectionName": "UNKNOWN SECTION NAME", "string": "The possibility of inserting a word into a domain of some trausitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity. From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981) . In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994 ), Becker et al. (1991 ). In dependency-based accounts, the construction is represented by the dependency relation, which is typed or labelled to indicate constructional distinctions which are configurationally defined in PSG. Given this correspondence, it is natural to employ dependencies in the description of discontinuities as fol-3Note that each phrasal level in PS-based trees defines a scope for linear precedence rules, which only apply to sister nodes. lows: For each modifier of a certain head, a set of dependency types is defined which may link the direct head and the positional head of the modifier (\"gesehen\" and \"hat\", resp.). If this set is empty, both heads are identical and a contiguous attachment results. The impossibility of extraction from, e.g., a finite verb phrase may follow from the fact that the dependency embedding finite verbs, propo, may not appear on any path between a direct and a positional head. 4", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The possibility of inserting a word into a domain of some trausitive head raises the questions of how to require contiguity (as needed in most cases), and how to limit the distance between the governor and the modifier in the case of discontinuity. ", "mid_sen": "From a descriptive viewpoint, the syntactic construction is often cited to determine the possibility and scope of discontinuities (Bhatt, 1990; Matthews, 1981) . ", "after_sen": "In PSbased accounts, the construction is represented by phrasal categories, and extraction is limited by bounding nodes (e.g., Haegeman (1994 ), Becker et al. (1991 ). "}
{"citeStart": 190, "citeEnd": 204, "citeStartToken": 190, "citeEndToken": 204, "sectionName": "UNKNOWN SECTION NAME", "string": "In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993) . More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992) . We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like \"say\".", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993) . ", "mid_sen": "More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992) . ", "after_sen": "We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like \"say\"."}
{"citeStart": 211, "citeEnd": 231, "citeStartToken": 211, "citeEndToken": 231, "sectionName": "UNKNOWN SECTION NAME", "string": "It is quite small, by current corpus standards (on the order of hundreds of thousands of words, rather than millions or tens of millions); the direct annotation methodology used to create it is labor intensive (Marcus et al. (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for partof-speech annotation); and the output quality reflects the difficulty of the task (inter-annotator disagreement is on the order of 10%, as contrasted with the approximately 3% error rate reported for part-of-speech annotation by Marcus et al.) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "2Available by anonymous ftp to clarity.princeton.edu as pub/wnl. 4seracor. tar. Z; Word_Net is described by Miller et al. (1990) .", "mid_sen": "It is quite small, by current corpus standards (on the order of hundreds of thousands of words, rather than millions or tens of millions); the direct annotation methodology used to create it is labor intensive (Marcus et al. (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for partof-speech annotation); and the output quality reflects the difficulty of the task (inter-annotator disagreement is on the order of 10%, as contrasted with the approximately 3% error rate reported for part-of-speech annotation by Marcus et al.) .", "after_sen": "There have been some attempts to capture the behavior of semantic categories in a distributional setting, despite the unavailability of sense-annotated corpora. "}
{"citeStart": 117, "citeEnd": 149, "citeStartToken": 117, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. Another dialogue acquisition system has been developed by Ho (1984) . However, that system has different goals: to enable the user to consciously design a dialogue to embody a particular human-machine interaction. The acquisition system described here is aimed at dealing with ill-formed input and is completely automatic and invisible to the user. It self activates to bias recognition toward historically observed patterns but is not otherwise observable.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Average word and sentence error rate in percent, average speaking rate in words-spoken-per-minute.", "mid_sen": "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by Biermann and Krishnaswamy (1976) where program flowcharts were constructed from traces of their behaviors. ", "after_sen": "However, the \"flowcharts\" in the current project are probabilistic in nature and the problems associated with matching incoming sentences to existing nodes has not been previously addressed. "}
{"citeStart": 35, "citeEnd": 53, "citeStartToken": 35, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "Functional Generative Description (Sgall et al., 1986 ) assumes a language-independent underlying order, which is represented as a projective dependency tree. This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. Recently, Kruijff (1997) has given a categorialstyle formulation of these ordering rules. He assumes associative categorial operators, permuting the arguments to yield the surface ordering. One difference to our proposal is that we argue for a representational account of word order (based on valid structures representing word order), eschewing the non-determinism introduced by unary operators; the second difference is the avoidance of an underlying structure~ which stratifies the theory and makes incremental processing difficult.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The following overview of DG flavors shows that various mechanisms (global rules, general graphs, procedural means) are generally employed to lift the limitation of projectivity and discusses some shortcomings of these proposals.", "mid_sen": "Functional Generative Description (Sgall et al., 1986 ) assumes a language-independent underlying order, which is represented as a projective dependency tree. ", "after_sen": "This abstract representation of the sentence is mapped via ordering rules to the concrete surface realization. "}
{"citeStart": 82, "citeEnd": 105, "citeStartToken": 82, "citeEndToken": 105, "sectionName": "UNKNOWN SECTION NAME", "string": "Figure 1: Class proach, our statistical inference method for clustering is formalized clearly as an EM-algorithm. Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998) . There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. For further applications of our clustering model see Rooth et al. (1998) . We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c E C, where the classes are given no prior interpretation. The semantically smoothed probability of a pair (v, n) is defined to be: n[c ) . Note that by construction, conditioning of v and n on each other is solely made through the classes c.", "label": "Similar", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Figure 1: Class proach, our statistical inference method for clustering is formalized clearly as an EM-algorithm. ", "mid_sen": "Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998) . ", "after_sen": "There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment. "}
{"citeStart": 35, "citeEnd": 49, "citeStartToken": 35, "citeEndToken": 49, "sectionName": "UNKNOWN SECTION NAME", "string": "There has been less work on identifying general noun phrases than on recognizing baseNPs. (Osborne, 1999) extended a definite clause grammar with rules induced by a learner that was based upon the maximum description length principle. He processed other parts of the Penn Treebank than we with an F~=I rate of about 60. Our earlier effort to process the CoNLL data set was performed in the same way as described in this paper but without using the combination method for baseNPs. We obtained an F~=I rate of 82.98 (CoNLL-99, 1999) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Our earlier effort to process the CoNLL data set was performed in the same way as described in this paper but without using the combination method for baseNPs. ", "mid_sen": "We obtained an F~=I rate of 82.98 (CoNLL-99, 1999) .", "after_sen": ""}
{"citeStart": 198, "citeEnd": 220, "citeStartToken": 198, "citeEndToken": 220, "sectionName": "UNKNOWN SECTION NAME", "string": "In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar (Schabes et al., 1988, LTAG) , Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven Phrase-Structure Grammar (Pollard and Sag, 1994, HPSG) , it is possible to separate lexical category assignment -the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates -from the combinatory processes that make use of such categories -such as parsing and surface realization. One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof, as in the CCG parser in (Hockenmaier, 2003) or the chart realizer in (Carroll and Oepen, 2005) . A relatively recent technique for lexical category assignment is supertagging (Bangalore and Joshi, 1999) , a preprocessing step to parsing that assigns likely categories based on word and part-ofspeech (POS) contextual information. Supertagging was dubbed \"almost parsing\" by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006) , leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007) .", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Supertagging was dubbed \"almost parsing\" by these authors, because an oracle supertagger left relatively little work for their parser, while speeding up parse times considerably. ", "mid_sen": "Supertagging has been more recently extended to a multitagging paradigm in CCG (Clark, 2002; Curran et al., 2006) , leading to extremely efficient parsing with state-of-the-art dependency recovery (Clark and Curran, 2007) .", "after_sen": "We have adapted this multitagging approach to lexical category assignment for realization using the CCG-based natural language toolkit OpenCCG. "}
{"citeStart": 22, "citeEnd": 42, "citeStartToken": 22, "citeEndToken": 42, "sectionName": "UNKNOWN SECTION NAME", "string": "In our previous work (Zhang and Chai, 2009) , we started an initial investigation on conversation entailment. We have collected a dataset of 875 instances. Each instance consists of a conversation segment and a hypothesis (as described in Section 1). The hypotheses are statements about conversation participants and are further categorized into four types: about their profile information, their beliefs and opinions, their desires, and their communicative intentions. We developed an approach that is motivated by previous work on textual entailment. We use clauses in the logic-based approaches as the underlying representation of our system. Based on this representation, we apply a two stage entailment process similar to MacCartney et al. (2006) developed for textual entailment: an alignment stage followed by an entailment stage.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Here we address a different angle regarding conversation scripts, namely conversation entailment.", "mid_sen": "In our previous work (Zhang and Chai, 2009) , we started an initial investigation on conversation entailment. ", "after_sen": "We have collected a dataset of 875 instances. "}
{"citeStart": 131, "citeEnd": 150, "citeStartToken": 131, "citeEndToken": 150, "sectionName": "UNKNOWN SECTION NAME", "string": "Unfolding can be used to eliminate superfluous filtering steps. Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed (Minnen et al., 1996) the resulting processing behavior can be considered a generalization of the head corner generation approach (Shieber et al., 1990) : Without the need to rely on notions such as semantic head and chain rule, a head corner behavior can be mimicked in a strict bottom-up fashion.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Unfolding can be used to eliminate superfluous filtering steps. ", "mid_sen": "Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed (Minnen et al., 1996) the resulting processing behavior can be considered a generalization of the head corner generation approach (Shieber et al., 1990) : ", "after_sen": "Without the need to rely on notions such as semantic head and chain rule, a head corner behavior can be mimicked in a strict bottom-up fashion."}
{"citeStart": 85, "citeEnd": 89, "citeStartToken": 85, "citeEndToken": 89, "sectionName": "UNKNOWN SECTION NAME", "string": "The ideas discussed in Sections 2.1 and 2.2 and the results illustrated in Figures 1 and 2 suggest that studying the change in citation purpose and citation polarity allow us to predict the emergence of new techniques or the decline in impact of old techniques. For example, the analysis illustrated in Figure 2 shows that the work of Ken Church (1988) on part-of-speech tagging received significant positive feedback during the 1990s and until early 2000s before it started to receive more negative feedback. This probably can be explained by the emergence of better statistical models for part-of-speech (POS) tagging (e.g. Conditional Random Fields (Lafferty et al., 2001) ) that outperformed Church's approach. However, as indicated by the neutral citation curve, Church's work continued to be cited as a classical pioneering research on the POS tagging task, but not as the state-of-the-art approach. Similar analysis can be applied to the change in citation purpose of Shieber (1985) as illustrated in Figure 1 2", "label": "CoCoXY", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The ideas discussed in Sections 2.1 and 2.2 and the results illustrated in Figures 1 and 2 suggest that studying the change in citation purpose and citation polarity allow us to predict the emergence of new techniques or the decline in impact of old techniques. ", "mid_sen": "For example, the analysis illustrated in Figure 2 shows that the work of Ken Church (1988) on part-of-speech tagging received significant positive feedback during the 1990s and until early 2000s before it started to receive more negative feedback. ", "after_sen": "This probably can be explained by the emergence of better statistical models for part-of-speech (POS) tagging (e.g. Conditional Random Fields (Lafferty et al., 2001) ) that outperformed Church's approach. "}
{"citeStart": 369, "citeEnd": 377, "citeStartToken": 369, "citeEndToken": 377, "sectionName": "UNKNOWN SECTION NAME", "string": "There are three key issues which pertain to example-based translation : ® establishment of correspondence between units in a bi/multi-lingual text at sentence, phrase or word level • a mechanism for retrieving from the database the unit that best matches the input • exploiting the retrieved translation example to produce the actual translation of the input sentence [Brown 91 ] and [Gale 91 ] have prolx~Sed methods for establishing correspondence between sentences in bilingual corpora. [Brown 93 ], [Sadler 901 and [Kaji 92 ] have tackled the problem of establishing correspondences between words and phrases in bilingual texts.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this type of translation system, a large amount of bi/multi-lingual translation examples has been stored in a textual database and input expressions are rendered in the target language by retrieving from the database that example which is most similar to the input.", "mid_sen": "There are three key issues which pertain to example-based translation : ® establishment of correspondence between units in a bi/multi-lingual text at sentence, phrase or word level • a mechanism for retrieving from the database the unit that best matches the input • exploiting the retrieved translation example to produce the actual translation of the input sentence [Brown 91 ] and [Gale 91 ] have prolx~Sed methods for establishing correspondence between sentences in bilingual corpora. ", "after_sen": "[Brown 93 ], [Sadler 901 and [Kaji 92 ] have tackled the problem of establishing correspondences between words and phrases in bilingual texts."}
{"citeStart": 134, "citeEnd": 153, "citeStartToken": 134, "citeEndToken": 153, "sectionName": "UNKNOWN SECTION NAME", "string": "This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing. In the last decade, research in speech recognition (Jelinek 1985) , noun classification (Hindle 1988) , predicate argument relations (Church & Hanks 1989) , and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This line of research was motivated by a series of successful applications of mutual information statistics to other problems in natural language processing. ", "mid_sen": "In the last decade, research in speech recognition (Jelinek 1985) , noun classification (Hindle 1988) , predicate argument relations (Church & Hanks 1989) , and other areas have shown that mutual information statistics provide a wealth of information for solving these problems.", "after_sen": ""}
{"citeStart": 207, "citeEnd": 225, "citeStartToken": 207, "citeEndToken": 225, "sectionName": "UNKNOWN SECTION NAME", "string": "Dependency relations. There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g., Dave et al. (2003) , Yi et al. (2003)) or nouns that enjoy a dependency relation with a polarity term (e.g., Popescu and Etzioni (2005)). Wilson et al. (2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document. In comparison to previous work, we investigate the use of a larger set of dependency relations for classifying reviews.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Dependency relations. ", "mid_sen": "There have been several attempts at extracting features for polarity classification from dependency parses, but most focus on extracting specific types of information such as adjective-noun relations (e.g., Dave et al. (2003) , Yi et al. (2003)) or nouns that enjoy a dependency relation with a polarity term (e.g., Popescu and Etzioni (2005)). ", "after_sen": "Wilson et al. (2005) extract a larger variety of features from dependency parses, but unlike us, their goal is to determine the polarity of a phrase, not a document. "}
{"citeStart": 112, "citeEnd": 134, "citeStartToken": 112, "citeEndToken": 134, "sectionName": "UNKNOWN SECTION NAME", "string": "The class REPETI-TION/SUMMARY corresponds to the controller producing a redundant utterance. The utterance is either an exact repetition of previous propositional content, or a summary that realizes a proposition, P, which could have been inferred from what came before. Thus orderly control shifts occur when the controller explicitly indicates that s/he wishes to relinquish control. What unifies ABDICATIONS and REPETITION/SUMMARIES is that the controller supplies no new propositional content. The remaining class, INTERRUPTIONS, characterize shifts occurring when the noncontroller displays initiative by seizing control. This class is more general than other definitions of Interruptions. It properly contains cross-speaker interruptions that involve topic shift, similar to the true-interruptions of Grosz and Sidner[GS86] , as well as clarification subdialogues [Sid83, LA90] .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This class is more general than other definitions of Interruptions. ", "mid_sen": "It properly contains cross-speaker interruptions that involve topic shift, similar to the true-interruptions of Grosz and Sidner[GS86] , as well as clarification subdialogues [Sid83, LA90] .", "after_sen": "This classification suggests that the transfer of control is often a collaborative phenomenon. "}
{"citeStart": 159, "citeEnd": 172, "citeStartToken": 159, "citeEndToken": 172, "sectionName": "UNKNOWN SECTION NAME", "string": "In recent years, the success of statistical parsing techniques can be attributed to several factors, such as the increasing size of computing machinery to accommodate larger models, the availability of resources such as the Penn Treebank (Marcus et al., 1993) and the success of machine learning techniques for lowerlevel NLP problems, such as part-of-speech tagging (Church, 1988; Brill, 1995) , and PPattachment (Brill and Resnik, 1994; Collins and Brooks, 1995) . However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997) , but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, perhaps even more significant has been the lexicalization of the grammar formalisms being probabilistically modeled: crucially, all the recent, successful statistical parsers have in some way made use of bilexical dependencies. ", "mid_sen": "This includes both the parsers that attach probabilities to parser moves (Magerman, 1995; Ratnaparkhi, 1997) , but also those of the lexicalized PCFG variety (Collins, 1997; Charniak, 1997) .", "after_sen": "Even more crucially, the bilexical dependencies involve head-modifier relations (hereafter referred to simply as \"head relations\"). "}
{"citeStart": 65, "citeEnd": 74, "citeStartToken": 65, "citeEndToken": 74, "sectionName": "UNKNOWN SECTION NAME", "string": "The net that gave best results was a simple single layer net (Figure 3 ), derived from Wyard and Nightingale's Hodyne net (Wyard and Nightingale, 1990 ). This is conventionally a \"single layer\" net, since there is one layer of processing nodes. Multi-layer networks, which can process linearly inseparable data, were also investigated, but are not necessary for this particular processing task. The linear separability of data is related to its order, and this system uses higher order pairs and triples as input. The question of appropriate network architecture is examined in (Pao, 1989; Widrow and Lehr, 1992; Lyon, 1994 ", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The linear separability of data is related to its order, and this system uses higher order pairs and triples as input. ", "mid_sen": "The question of appropriate network architecture is examined in (Pao, 1989; Widrow and Lehr, 1992; Lyon, 1994 ", "after_sen": ""}
{"citeStart": 135, "citeEnd": 149, "citeStartToken": 135, "citeEndToken": 149, "sectionName": "UNKNOWN SECTION NAME", "string": "The interest in the 80's begun to turn considering grammar checking as an enterprise of its own right (Carbonell & Hayes, 1983) , (Ilayes & Mouradian, 1981) , (Heidorn et al., 1982) , (.lensen at al., 1983) , though many of the approaches were still in I;t1(: NLU tradition ((]harniak, 198a), (Granger, 1983) , (Kwasny & Sondheimer, 1981) , (Weischedel & Black, ] 980), (Weisehedel & Sondheimer, 1983) . A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "A 1985 Ovum report on nal;llral language applications (.lohnson, 1985) already identifies grammar and style checking as one of the seven major apt)lications of NLP. ", "mid_sen": "Currently, every project in grammar checking has as its goal the creation of a writing aid rather than a robust man-machine interface (Adriaens, 1994) , (llolioli ctal., 1992) , (Vosse, 1992) .", "after_sen": "Current systems dealillg with grammatical deviance have be(m inainly involve(t in the integi~> don of special techniques to detect and correct, when possible, these, deviances. "}
{"citeStart": 157, "citeEnd": 178, "citeStartToken": 157, "citeEndToken": 178, "sectionName": "UNKNOWN SECTION NAME", "string": "As shown in Table 1 , the grammar-based language model reduced the word error rate by 9.2% relative over the baseline system. This improvement is statistically significant on a level of < 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemar's test (Gillick and Cox, 1989) . If the parameters are optimized on all 447 sentences (i.e. on the test data), the word error rate is reduced by 10.7% relative.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "As shown in Table 1 , the grammar-based language model reduced the word error rate by 9.2% relative over the baseline system. ", "mid_sen": "This improvement is statistically significant on a level of < 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemar's test (Gillick and Cox, 1989) . ", "after_sen": "If the parameters are optimized on all 447 sentences (i.e. on the test data), the word error rate is reduced by 10.7% relative."}
{"citeStart": 100, "citeEnd": 113, "citeStartToken": 100, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995) . Our experiments on Romanian text were consistent with this figure.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "It is not clear what resources are required to adapt systems to new languages.\"", "mid_sen": "It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995) . ", "after_sen": "Our experiments on Romanian text were consistent with this figure."}
{"citeStart": 33, "citeEnd": 53, "citeStartToken": 33, "citeEndToken": 53, "sectionName": "UNKNOWN SECTION NAME", "string": "Selection of learning algorithm and its algorithmspecific parameters were done as follows. For each of the 7 classification tasks (one per relationship type), for each of the 128 pattern clustering schemes, we prepared a list of most of the compatible algorithms available in Weka, and we automatically selected the model (a parameter set and an algorithm) which gave the best 10-fold cross-validation results. The winning algorithms were LWL (Atkeson et al., 1997) , SMO (Platt, 1999) , and K* (Cleary and Trigg, 1995 ) (there were 7 tasks, and different algorithms could be selected for each task). We then used the obtained model to classify the testing set. This allowed us to avoid fixing parameters that are best for a specific dataset but not for others. Since each dataset has only 140 examples, the computation time of each learning algorithm is negligible.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "For each of the 7 classification tasks (one per relationship type), for each of the 128 pattern clustering schemes, we prepared a list of most of the compatible algorithms available in Weka, and we automatically selected the model (a parameter set and an algorithm) which gave the best 10-fold cross-validation results. ", "mid_sen": "The winning algorithms were LWL (Atkeson et al., 1997) , SMO (Platt, 1999) , and K* (Cleary and Trigg, 1995 ) (there were 7 tasks, and different algorithms could be selected for each task). ", "after_sen": "We then used the obtained model to classify the testing set. "}
{"citeStart": 19, "citeEnd": 35, "citeStartToken": 19, "citeEndToken": 35, "sectionName": "UNKNOWN SECTION NAME", "string": "To summarise, we have argued that discourse structure information will improve summarisation. Other researchers (Ono et al., 1994; Marcu, 1997) have argued similarly, although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987) . In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. Our categories are not hierarchical, and they are much less fine-grained than RST-relations. As mentioned above, we wanted them to a) provide context information for flexible summarisation, b) provide a higher degree of comparability between papers, and c) provide a fairer evaluation of superficially different sentences.", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "To summarise, we have argued that discourse structure information will improve summarisation. ", "mid_sen": "Other researchers (Ono et al., 1994; Marcu, 1997) have argued similarly, although most previous work on discourse-based summarisation follows a different discourse model, namely Rhetorical Structure Theory (Mann and Thompson, 1987) . ", "after_sen": "In contrast to RST, we stress the importance of rhetorical moves which are global to the argumentation of the paper, as opposed to more local RST-type relations. "}
{"citeStart": 0, "citeEnd": 24, "citeStartToken": 0, "citeEndToken": 24, "sectionName": "UNKNOWN SECTION NAME", "string": "In FAQs, employed a sentence retrieval approach based on a language model where the entire response to an FAQ is considered a sentence, and the questions and answers are embedded in an FAQ document. They complemented this approach with machine learning techniques that automatically learn the weights of different retrieval models. compared two retrieval approaches (TF.IDF and query expansion) and two predictive approaches (statistical translation and latent variable models). Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. Two significant differences between help-desk and FAQs are the following.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Jijkoun and de Rijke (2005) compared different variants of retrieval techniques. ", "mid_sen": "Soricut and Brill (2006) compared a predictive approach (statistical translation), a retrieval approach based on a language-model, and a hybrid approach which combines statistical chunking and traditional retrieval. ", "after_sen": "Two significant differences between help-desk and FAQs are the following."}
{"citeStart": 36, "citeEnd": 56, "citeStartToken": 36, "citeEndToken": 56, "sectionName": "UNKNOWN SECTION NAME", "string": "(III) The way in which spans are annotated as ar-guments to connectives also raises a challenge. First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003) , a span can serve as argument to >1 connective. Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005) , discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them. Instead, in annotating arguments, PDTB annotators have selected the minimal clausal text span needed to interpret the relation. This could comprise an embedded, subordinate or coordinate clause, an entire sentence, or a (possibly disjoint) sequence of sentences. As a result, there are fairly complex patterns of spans within and across sentences that serve as arguments to different connectives, and there are parts of sentences that don't appear within the span of any connective, explicit or implicit. The result is that the PDTB provides only a partial but complexly-patterned cover of the corpus. Understanding what's going on and what it implies for discourse structure (and possibly syntactic structure as well) is a challenge we're currently trying to address (Lee et al., 2006) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "First, because the PDTB annotates both structural and anaphoric connectives (Webber et al., 2003) , a span can serve as argument to >1 connective. ", "mid_sen": "Secondly, unlike in the RST corpus (Carlson et al., 2003) or the Discourse GraphBank (Wolf and Gibson, 2005) , discourse segments are not separately annotated, with annotators then identifying what discourse relations hold between them. ", "after_sen": "Instead, in annotating arguments, PDTB annotators have selected the minimal clausal text span needed to interpret the relation. "}
{"citeStart": 131, "citeEnd": 144, "citeStartToken": 131, "citeEndToken": 144, "sectionName": "UNKNOWN SECTION NAME", "string": "This necessitates a dynamic processing strategy, i.e., memoization, extended with an abstraction function like, e.g., restriction (Shieber, 1985) , to weaken filtering and a subsumption check to discard redundant results. It is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "More specifically, 'magic generation' falls prey to non-termination in the face of head recursion, i.e., the generation analog of left recursion in parsing.", "mid_sen": "This necessitates a dynamic processing strategy, i.e., memoization, extended with an abstraction function like, e.g., restriction (Shieber, 1985) , to weaken filtering and a subsumption check to discard redundant results. ", "after_sen": "It is shown that for a large class of grammars the subsumption check which often influences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic predicates derived for a particular grammar after applying an abstraction function in an off-line fashion."}
{"citeStart": 71, "citeEnd": 92, "citeStartToken": 71, "citeEndToken": 92, "sectionName": "UNKNOWN SECTION NAME", "string": "The purpose of our paper is to develop a principled technique for attaching a probabilistic interpretation to feature structures. Our techniques apply to the feature structures described by Carpenter (Carpenter, 1992) . Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994 ) their relevance to computational grammars is apparent. On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5) , it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Since these structures are the ones which are used in by Pollard and Sag (Pollard and Sag, 1994 ) their relevance to computational grammars is apparent. ", "mid_sen": "On the basis of the usefulness of probabilistic context-free grammars (Charniak, 1993, ch. 5) , it is plausible to assume that that the extension of probabilistic techniques to such structures will allow the application of known and new techniques of parse ranking and grammar induction to more interesting grammars than has hitherto been the case.", "after_sen": "The paper is structured as follows. "}
{"citeStart": 281, "citeEnd": 302, "citeStartToken": 281, "citeEndToken": 302, "sectionName": "UNKNOWN SECTION NAME", "string": "Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al., 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996) . Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "label": "Motivation", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Much of the appeal of these approaches is tied to the use of a simple formalism, which allows for the use of efficient parsing algorithms, as well as straightforward ways to train discriminative models to perform disambiguation. ", "mid_sen": "At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982) , Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) , Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000) , which represent deep syntactic structures that cannot be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.", "after_sen": "We present a novel framework that combines strengths from surface syntactic parsing and deep syntactic parsing, specifically by combining dependency and HPSG parsing. "}
{"citeStart": 30, "citeEnd": 52, "citeStartToken": 30, "citeEndToken": 52, "sectionName": "UNKNOWN SECTION NAME", "string": "The goal of a speech recognizer is to find the sequence of words l~ that is maximal given the acoustic signal A. However, for detecting and correcting speech repairs, and identifying boundary tones and discourse markers, we need to augment the model so that it incorporates shallow statistical analysis, in the form of POS tagging. The POS tagset, based on the Penn Treebank tagset (Marcus, Santorini, and Marcinkiewicz, 1993) , includes special tags for denoting when a word is being used as a discourse marker. In this section, we give an overview of our basic language model that incorporates POS tagging. Full details can be found in (Heeman and Allen, 1997; Heeman~ 1997) .", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In this section, we give an overview of our basic language model that incorporates POS tagging. ", "mid_sen": "Full details can be found in (Heeman and Allen, 1997; Heeman~ 1997) .", "after_sen": "To add in POS tagging, we change the goal of the speech recognition process to find the best word and POS tags given the acoustic signal. "}
{"citeStart": 86, "citeEnd": 103, "citeStartToken": 86, "citeEndToken": 103, "sectionName": "UNKNOWN SECTION NAME", "string": "If we restrict the model to bigrams we see a considerable drop in performance. Note that the bigram PYP-HMM outperforms the closely related BHMM (the main difference being that we smooth tag bigrams with unigrams). It is also interesting to compare the bigram PYP-1HMM to the closely related model of Lee et al. (2010) . That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Note that the bigram PYP-HMM outperforms the closely related BHMM (the main difference being that we smooth tag bigrams with unigrams). ", "mid_sen": "It is also interesting to compare the bigram PYP-1HMM to the closely related model of Lee et al. (2010) . ", "after_sen": "That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4%, well below that of our model."}
{"citeStart": 92, "citeEnd": 113, "citeStartToken": 92, "citeEndToken": 113, "sectionName": "UNKNOWN SECTION NAME", "string": "This paper describes LIIII' (Left-Ilead corner Island Parser), a parser designed for broadcoverage handling of lmrestricted text. The system interprets an extended DCG formalism to produce a robust analyser that finds parses of the input made from 'islands' of terminals (corresponding to terminals consumed by successful grammar rules). It is currently in use for processing dialogue tr,'mscripts from the tICRC Map Task Corpus (Anderson et al., 1991) , although we expect its eventual applications to he much wider. 1 Transcribed natural speech contains a number of frequent characteristic 'ungrmnmatical' phenomena: filled pauses, repetitions, restarts, etc. (as in e.g. Right I'll have ...you know, like I'll have to ...so I'm going between the picket fence and the mill, right.). ~ While a full analysis of a conversation might well take these into account, for many purposes they represent a significmlt obstacle to analysis. LIIIP provides a processing method wlfich allows selected portions of the input to be ignored or handled differently.", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "The system interprets an extended DCG formalism to produce a robust analyser that finds parses of the input made from 'islands' of terminals (corresponding to terminals consumed by successful grammar rules). ", "mid_sen": "It is currently in use for processing dialogue tr,'mscripts from the tICRC Map Task Corpus (Anderson et al., 1991) , although we expect its eventual applications to he much wider. ", "after_sen": "1 Transcribed natural speech contains a number of frequent characteristic 'ungrmnmatical' phenomena: filled pauses, repetitions, restarts, etc. (as in e.g. "}
{"citeStart": 72, "citeEnd": 85, "citeStartToken": 72, "citeEndToken": 85, "sectionName": "UNKNOWN SECTION NAME", "string": "Our grammar incorporates many ideas from existing linguistic work, e.g. Müller (2007) , Müller (1999) , Crysmann (2005 ), Crysmann (2003 . In addition, we have modeled a few constructions which occur frequently but are often neglected in formal syntactic theories. Among them are prenominal and postnominal genitives, expressions of quantity and expressions of date and time. Further, we have implemented dedicated subgrammars for analyzing written numbers, compounds and acronyms that are written as separate words. To reduce ambiguity, only noun-noun compounds are covered by the grammar. Noun-noun compounds are by far the most productive compound type.", "label": "Basis", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "This is essential for parsing long and ambiguous sentences.", "mid_sen": "Our grammar incorporates many ideas from existing linguistic work, e.g. Müller (2007) , Müller (1999) , Crysmann (2005 ), Crysmann (2003 . ", "after_sen": "In addition, we have modeled a few constructions which occur frequently but are often neglected in formal syntactic theories. "}
{"citeStart": 9, "citeEnd": 28, "citeStartToken": 9, "citeEndToken": 28, "sectionName": "UNKNOWN SECTION NAME", "string": "WordNet (Miller et al., 1990 ) is an on-line hierarchical lexical database which contains semantic information about English words (including hypernymy relations which we use in our system). We use chains of hypernyms when we need to approximate the usage of a particular word in a description using its ancestor and sibling nodes in WordNet. Particularly useful for our application are the synset offsets of the words in a description. The synset offset is a number that uniquely identifies a concept node (synset) in the WordNet hierarchy. Figure 3 shows that the synset offset for the concept \"administrator, decision maker\" is \"(07063507}', 2We haven't included relative clauses in our study. while its hypernym, \"head, chief, top dog\" has a synset offset of \"~07311393} \".", "label": "Usage", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "In our experiments, we have used two widely available tools -WordNet and Ripper.", "mid_sen": "WordNet (Miller et al., 1990 ) is an on-line hierarchical lexical database which contains semantic information about English words (including hypernymy relations which we use in our system). ", "after_sen": "We use chains of hypernyms when we need to approximate the usage of a particular word in a description using its ancestor and sibling nodes in WordNet. "}
{"citeStart": 158, "citeEnd": 181, "citeStartToken": 158, "citeEndToken": 181, "sectionName": "UNKNOWN SECTION NAME", "string": "Reversible realisers. The realiser presented here differs in mainly two ways from existing reversible realisers such as (White, 2004 )'s CCG system or the HPSG ERG based realiser (Carroll and Oepen, 2005) .", "label": "CoCoGM", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Reversible realisers. ", "mid_sen": "The realiser presented here differs in mainly two ways from existing reversible realisers such as (White, 2004 )'s CCG system or the HPSG ERG based realiser (Carroll and Oepen, 2005) .", "after_sen": "First, it permits a symbolic selection of the output paraphrase. "}
{"citeStart": 145, "citeEnd": 162, "citeStartToken": 145, "citeEndToken": 162, "sectionName": "UNKNOWN SECTION NAME", "string": "First, we summarize resources available for the experiments (Section 2). Section 3 describes the automatic procedures used for the preparation of both training and testing data, including morphological tagging, and analytical and tectogrammatical parsing of Czech input. In Section 4 we describe the process of the filtering of dictionaries used in the transfer procedure (for its characterization, see Section 5). The generation process consisting mainly of word reordering and lexical insertions is explained in Section 6, an example illustrating the generation steps is presented in Sec-tion 7. For the evaluation of the results we use the BLEU score (Papineni et al., 2001) . Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (Al-Onaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001) , trained on the same parallel corpus.", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Section 8 compares translations generated from automatically built and manually annotated tectogrammatical representations. ", "mid_sen": "We also compare the results with the output generated by the statistical translation system GIZA++/ISI ReWrite Decoder (Al-Onaizan et al., 1999; Och and Ney, 2000; Germann et al., 2001) , trained on the same parallel corpus.", "after_sen": ""}
{"citeStart": 0, "citeEnd": 16, "citeStartToken": 0, "citeEndToken": 16, "sectionName": "UNKNOWN SECTION NAME", "string": "The second class of algorithms uses cooccurrence statistics (Hindle 1990 , Lin 1998 . These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985) , words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "However, such clustering algorithms fail to name their classes. ", "mid_sen": "Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. ", "after_sen": "Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun."}
{"citeStart": 316, "citeEnd": 343, "citeStartToken": 316, "citeEndToken": 343, "sectionName": "UNKNOWN SECTION NAME", "string": "Tense interpretation has received much attention in linguistics (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986, inter alia) and natural language processing (Webber, 1988; Kameyama et al., 1993; Lascarides and Asher, 1993, inter alia) . Several researchers (Partee, 1984; Hinrichs, 1986; Nerbonne, 1986; Webber, 1988) have sought to explain the temporal relations induced by tense by treating it as anaphoric, drawing on Reichenbach's separation between event, speech, and reference times (Reichenbach, 1947) . Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. The Temporal Centering framework (Kameyama et al., 1993) integrates lWe will limit the scope of this paper by restricting the discussion to accomplishments and achievements. aspects of both approaches, but patterns with the first in treating tense as anaphoric.", "label": "Neutral", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Specifically, to account for the forward progression of time induced by successive simple past tenses in a narrative, they treat the simple past as referring to a time evoked by a previous past tense. ", "mid_sen": "For instance, in Hinrichs's (1986) proposal, accomplishments and achievements x introduce a new reference point that is temporally ordered after the time of the event itself, \"ensuring that two consecutive accomplishments or achievements in a discourse are always ordered in a temporal sequence.\" On the other hand, Lascarides and Asher (1993) take the view that temporal relations are resolved purely as a by-product of reasoning about coherence relations holding between utterances, and in doing so, argue that treating simple and complex tenses as anaphoric is unnecessary. ", "after_sen": "This approach parallels the treatment of pronoun resolution espoused by Hobbs (1979) , in which pronouns are modeled as free variables that are bound as a byproduct of coherence resolution. "}
{"citeStart": 198, "citeEnd": 210, "citeStartToken": 198, "citeEndToken": 210, "sectionName": "UNKNOWN SECTION NAME", "string": "We evaluated methods for self-training high accuracy products of latent variable grammars with large amounts of genre-matched data. We demonstrated empirically on newswire and broadcast news genres that very high accuracies can be achieved by training grammars on disjoint sets of automatically labeled data. Two primary factors appear to be determining the efficacy of our self-training approach. First, the accuracy of the model used for parsing the unlabeled data is important for the accuracy of the resulting single self-trained grammars. Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010) , despite being a single generative PCFG. Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009) .", "label": "CoCoRes", "title": "UNKNOWN TITLE", "cited_title": "UNK CITED TITLE", "prev_sen": "Second, the diversity of the individual grammars controls the gains that can be obtained by combining multiple grammars into a product model. ", "mid_sen": "Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set, rivaling discriminative reranking approaches (Charniak and Johnson, 2005) and products of latent variable grammars (Petrov, 2010) , despite being a single generative PCFG. ", "after_sen": "Our most accurate product model achieves an F score of 92.5 without the use of discriminative reranking and comes close to the best known numbers on this test set (Zhang et al., 2009) ."}
